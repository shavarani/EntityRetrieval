{"question": "was Pi an acceptable number of children in 1980s China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the children were all boys", "extracted_entity": null}
{"question": "does highest US Court have enough seats for every Prime Minister of the United Kingdom since 1952?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does Alec Baldwin have more children than Clint Eastwood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Alec Baldwin has 4 children", "extracted_entity": null}
{"question": "does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If the opponent's foot is in the", "extracted_entity": null}
{"question": "is a cory catfish likely to eat another living fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are known to eat fish.", "extracted_entity": null}
{"question": "are selfies more dangerous than plague in modern times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "are amoebas safe from breast cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There is. The fruit is a tangerine", "extracted_entity": null}
{"question": "could a hamster experience two leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a hamster cannot experience two leap", "extracted_entity": null}
{"question": "are there some countries where waiting staff need no tip?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are some countries where waiting staff need", "extracted_entity": null}
{"question": "do Shinto practitioners keep to a kosher diet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do not eat pork,", "extracted_entity": null}
{"question": "is the Yellow Pages the fastest way to find a phone number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s the most expensive way.", "extracted_entity": null}
{"question": "could Intel products be purchased at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. This is because Intel does not sell directly", "extracted_entity": null}
{"question": "do you have to pass through circle of lust to find Saladin in Dante's Inferno?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you do not have to pass through Circle", "extracted_entity": null}
{"question": "does Woody Allen await the Second Coming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he awaits the third", "extracted_entity": null}
{"question": "do you need a large room if you want to get into 3D printing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you need a large room.", "extracted_entity": null}
{"question": "was Emperor Commodus paid tribute in Pound sterling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no he was paid in gold", "extracted_entity": null}
{"question": "can French Defence initial move defend against four move checkmate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, French Defence initial move can defend against", "extracted_entity": null}
{"question": "is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they don't eat grains", "extracted_entity": null}
{"question": "do Armenians tend to dislike System of a Down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "did Ivan the Terrible's father and grandfather have nicer nicknames?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Ivan the Terrible's father was", "extracted_entity": null}
{"question": "did the confederate states speak Old English before the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they spoke colonial English.", "extracted_entity": null}
{"question": "do Republicans reject all forms of welfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they reject all forms of welfare", "extracted_entity": null}
{"question": "are brown rock fish found in the waters surrounding Saint Kitts and Nevis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are brown rock fish found in the", "extracted_entity": null}
{"question": "is a fever cured by listening to a cowbell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can help.", "extracted_entity": null}
{"question": "do all parts of the aloe vera plant taste good?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the aloe vera plant has two", "extracted_entity": null}
{"question": "after viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Mike Tyson do something very different than McGruff's slogan to Evander Holyfield in 1997?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he bit him.", "extracted_entity": null}
{"question": "can you hide a basketball in a sand cat's ear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's hard to find it", "extracted_entity": null}
{"question": "could largest asteroid crush a whole city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could.", "extracted_entity": null}
{"question": "do Chinese Americans face discrimination at a Federal level in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't.", "extracted_entity": null}
{"question": "is Kobe's famous animal product used in a BLT?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bacon.", "extracted_entity": null}
{"question": "is narcissism's origin a rare place to get modern words from?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is it unusual to eat spaghetti without a fork?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "can you drown in a Swan Lake performance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you don't know how to", "extracted_entity": null}
{"question": "are Christmas trees typically deciduous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are evergreen", "extracted_entity": null}
{"question": "can a traffic collision make someone a millionaire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you get hit by a bus.", "extracted_entity": null}
{"question": "is winter solstice in Northern Hemisphere closer to July than in Southern Hemisphere? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the solstice is at", "extracted_entity": null}
{"question": "can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't.", "extracted_entity": null}
{"question": "have Douglas fir been used to fight wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 1940s", "extracted_entity": null}
{"question": "are common carp sensitive to their environments?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are very sensitive to their environment.", "extracted_entity": null}
{"question": "would John the Baptist be invited to a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was a heretic", "extracted_entity": null}
{"question": "are there bones in an anchovy pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there are not bones in an anch", "extracted_entity": null}
{"question": "did Jerry Seinfeld have reason to cheer in 1986?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he was the star of a brand-new sit", "extracted_entity": null}
{"question": "is a platypus immune from cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (if you are a platypus", "extracted_entity": null}
{"question": "is Europa linked to Viennese waltzes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the name derives from a Greek", "extracted_entity": null}
{"question": "is the tibia required for floor exercises?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the tibia is not required for floor", "extracted_entity": null}
{"question": "was The Great Gatsby inspired by the novel 1984?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the sense that both novels are", "extracted_entity": null}
{"question": "does a lapidary work with items that are studied by geologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the same way.", "extracted_entity": null}
{"question": "does Santa Claus hypothetically give Joffrey Baratheon presents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s not very good", "extracted_entity": null}
{"question": "would a Jehovah's witness approve of Alice's Adventures in Wonderland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They would not approve of it.", "extracted_entity": null}
{"question": "can you chew argon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Argon is an inert gas.", "extracted_entity": null}
{"question": "is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I know because I was there.", "extracted_entity": null}
{"question": "does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Rush Limbaugh has been on", "extracted_entity": null}
{"question": "is the tongue part of a creature's head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the tongue is part of a creature'", "extracted_entity": null}
{"question": "is the largest city in New Mexico also known as Yoot\u00f3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The largest city in New Mexico is also", "extracted_entity": null}
{"question": "is most coffee produced South of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most coffee is produced in the Southern Hem", "extracted_entity": null}
{"question": "did Amy Winehouse always perform live perfectly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she was always interesting.", "extracted_entity": null}
{"question": "does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lupita Nyong'o is", "extracted_entity": null}
{"question": "do American wheelchair users know what the ADA is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they don\u2019t.", "extracted_entity": null}
{"question": "can brewing occur in a prison environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, brewing can occur in a prison environment", "extracted_entity": null}
{"question": "will Ronda Rousey hypothetically defeat X-Men's Colossus in a fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ronda Rousey is the best", "extracted_entity": null}
{"question": "can a believer in agnosticism become pope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. In fact, the last pope,", "extracted_entity": null}
{"question": "can French Toast hypothetically kill a Lannister?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "did Moon Jae-in earn the Abitur as a teenager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. He earned a Bach", "extracted_entity": null}
{"question": "would a kindergarten teacher make a lesson of the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Teaching religion is not a part of", "extracted_entity": null}
{"question": "are you likely to hear Rammstein playing in smooth jazz clubs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not likely", "extracted_entity": null}
{"question": "did Electronic Arts profit from Metroid sales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is Earth Day celebrated in summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Earth Day is celebrated on April 2", "extracted_entity": null}
{"question": "is viscosity unimportant in making jello shots?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the jello will not set properly.", "extracted_entity": null}
{"question": "is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on how much brain is in the squ", "extracted_entity": null}
{"question": "is waltz less injurious than slam dance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is not a slam dance", "extracted_entity": null}
{"question": "are LinkedIn and LeafedIn related companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are both owned by Microsoft.", "extracted_entity": null}
{"question": "can a snow leopard swim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, snow leopards can swim.", "extracted_entity": null}
{"question": "can citrus grow in Ulaanbaatar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and in a greenhouse.", "extracted_entity": null}
{"question": "will a Holstein cow and the Liberty Bell balance out a giant scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you get the cow to the top", "extracted_entity": null}
{"question": "is the bull shark more bull than shark?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is more shark than bull", "extracted_entity": null}
{"question": "is World of Warcraft heavier than a loaf of bread?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A loaf of bread is 1", "extracted_entity": null}
{"question": "did Thomas Greenhill's parents violate the concept of monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they did not violate the concept of", "extracted_entity": null}
{"question": "would John Muir not likely have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after parting from the band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "has a baby ever had a moustache?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there is a baby with a m", "extracted_entity": null}
{"question": "would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shaggy\u2019s name is Shaggy", "extracted_entity": null}
{"question": "do solo pianists require a conductor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "is Kim Kardashian a guru?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in the sense that she is", "extracted_entity": null}
{"question": "were karaoke and the turtle power tiller patented in the same country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "would a caracal be defeated by Javier Sotomayor in a high jump competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the caracal is much faster and", "extracted_entity": null}
{"question": "do Jehovah's Witnesses celebrate day before New Year's Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They celebrate the New Year's", "extracted_entity": null}
{"question": "do moths that live on sloths have family dinners?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do have family reunions.", "extracted_entity": null}
{"question": "can a greyhound walk on two legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can\u2019t.", "extracted_entity": null}
{"question": "was Moon Jae-in born outside of Khanbaliq?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not.", "extracted_entity": null}
{"question": "were paparazzi involved in the death of a member of the royal family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes (Diana)", "extracted_entity": null}
{"question": "can native wolverines be found in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Wolverines are native to the northern", "extracted_entity": null}
{"question": "did Al Unser Jr. win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was Alan Shepard, Jr", "extracted_entity": null}
{"question": "do the Ubuntu people speak Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do, and they speak it well.", "extracted_entity": null}
{"question": "does Rupert Murdoch's alma mater have more history than the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by a long way", "extracted_entity": null}
{"question": "are there Pink music videos that are triggering for eating disorder patients?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are.", "extracted_entity": null}
{"question": "were gladiators associated with the Coloseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were", "extracted_entity": null}
{"question": "could Larry King's marriages be counted on two feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Larry King was married 7 times.", "extracted_entity": null}
{"question": "do mountain goats inhabit the summit of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there is a mountain goat in", "extracted_entity": null}
{"question": "will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it will make it easier to buy", "extracted_entity": null}
{"question": "is Autumn a good time to collect bear pelts in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. The bears are still", "extracted_entity": null}
{"question": "did Wednesday have something to do with Thor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was Thor", "extracted_entity": null}
{"question": "did Dale Jr hug his dad after their last Daytona 500 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t.", "extracted_entity": null}
{"question": "can an ostrich fit into the nest of a swallow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can fit into the nest of", "extracted_entity": null}
{"question": "would an Olympic athlete be tired out after running a mile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would.", "extracted_entity": null}
{"question": "would the current president of Ohio University hypothetically wear a jockstrap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he is a woman.", "extracted_entity": null}
{"question": "would ramen be bad for someone with heart failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say yes. Ramen is a very", "extracted_entity": null}
{"question": "was Christina Aguilera born in the forgotten borough?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but she was born in Staten Island", "extracted_entity": null}
{"question": "will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he could wear them under", "extracted_entity": null}
{"question": "were the first missionaries required to attend mass on Sundays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were not required to attend mass on", "extracted_entity": null}
{"question": "is an ocelot a good present for a kindergartener?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a good present for an adult is", "extracted_entity": null}
{"question": "did villain that killed Superman murder Robin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. 20 years later, Batman", "extracted_entity": null}
{"question": "would a Germaphobia be able to participate in Judo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "does  Lionel Richie believe in holistic medicine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I know that", "extracted_entity": null}
{"question": "is the Fibonacci number sequence longer than every number discovered in Pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Fibonacci number sequence is", "extracted_entity": null}
{"question": "was The Canterbury Tales written before One Thousand and One Nights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was", "extracted_entity": null}
{"question": "did John Kerry run in the 2010 United Kingdom general election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hawaii does embrace foods considered", "extracted_entity": null}
{"question": "did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He died in 1905", "extracted_entity": null}
{"question": "did original Nintendo have games in same format as Playstation 3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they had cartridges.", "extracted_entity": null}
{"question": "would you be more likely to die of hypothermia in New York than Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not because of the", "extracted_entity": null}
{"question": "was Moliere Queen Margot's ill fated lover?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Moliere was a famous French play", "extracted_entity": null}
{"question": "can parachuting amateurs ignore hurricane force winds bulletins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not trained to do so", "extracted_entity": null}
{"question": "can a ten-pin bowling pin be a deadly weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you\u2019re bowling in the", "extracted_entity": null}
{"question": "is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the bone density is lowered in", "extracted_entity": null}
{"question": "is cycling a high-risk activity for pelvis fractures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "would Dale Earnhardt Jr. be considered a newbie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He is in his first year.", "extracted_entity": null}
{"question": "does Final Fantasy VI require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn't.", "extracted_entity": null}
{"question": "did number of Imams Reza Shah believed in exceed number of Jesus's disciples?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did Terry Pratchett write about quantum mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He wrote about a Discworld.", "extracted_entity": null}
{"question": "is it hard for Rahul Dravid to order food at a restaurant in Aurangabad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The cost of the iPhone 11", "extracted_entity": null}
{"question": "are swastikas used in the most common religion in India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hinduism.", "extracted_entity": null}
{"question": "will electric car struggle to finish Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "does walking across Amazonas put a person's life at risk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is a risk that we all", "extracted_entity": null}
{"question": "is Olivia Newton-John hyphenated celebrity name with most letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not. The hyphen", "extracted_entity": null}
{"question": "is Godzilla's image likely grounds for a lawsuit in 2050?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "does ontology require a scalpel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not a scalpel, a scissors.", "extracted_entity": null}
{"question": "will silicon wedding rings outsell bromine wedding rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the bromine wedding rings", "extracted_entity": null}
{"question": "is Cape Town south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in the southern hemisphere", "extracted_entity": null}
{"question": "would the number 666 appear in a church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the number 666 is a", "extracted_entity": null}
{"question": "can a single honey bee sting multiple humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it can sting multiple times.", "extracted_entity": null}
{"question": "would someone pay for a coffee in NYC with Euros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (And they would get a poor exchange", "extracted_entity": null}
{"question": "could a student at the University of Houston see a caracal on campus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're only in Africa", "extracted_entity": null}
{"question": "do sun bears stay active during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not. They are very lazy", "extracted_entity": null}
{"question": "is Christmas celebrated during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christmas is celebrated on December 25", "extracted_entity": null}
{"question": "could the endowment of Johns Hopkins University pay off the MBTA debt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the endowment was invested", "extracted_entity": null}
{"question": "was song of Roland protagonist friendly with group that had sagas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was not friendly with group that had", "extracted_entity": null}
{"question": "did Dr. Seuss make himself famous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did.", "extracted_entity": null}
{"question": "can a goat be used for one of the ingredients in French toast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. French toast is a breakfast food that", "extracted_entity": null}
{"question": "can you measure a Caracal with a protractor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I can.", "extracted_entity": null}
{"question": "for bone growth, is kale more beneficial than spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kale is more beneficial than spinach for", "extracted_entity": null}
{"question": "would only warm weather attire be a good idea on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "do Do It Yourself channels online always show realistic projects?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they don't have to.", "extracted_entity": null}
{"question": "could Bart Simpson have owned comics with The Joker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Twenty-third Amendment", "extracted_entity": null}
{"question": "would a baby gray whale fit in a tractor-trailer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the baby would be too big for", "extracted_entity": null}
{"question": "would Emmanuel Macron celebrate Cinco de Mayo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but he would be", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the letter B does not exceed the number", "extracted_entity": null}
{"question": "did Metallica band members cutting their hair hurt their sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they sold more.", "extracted_entity": null}
{"question": "could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could.", "extracted_entity": null}
{"question": "would a veteran of the Phillippine-American War come home craving SPAM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because SPAM is not Filipino food", "extracted_entity": null}
{"question": "were Walkman's used in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "will Communion be denied to Wednesday name origin followers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Church is not going to change its", "extracted_entity": null}
{"question": "would someone with a nosebleed benefit from Coca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Coca is not recommended for someone with", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at a party in the 19", "extracted_entity": null}
{"question": "can Darth Vader hypothetically outdunk Bill Walton without using The Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can.", "extracted_entity": null}
{"question": "does Oprah Winfrey have a degree from an Ivy League university?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. She is a high school graduate.", "extracted_entity": null}
{"question": "can you watch Rick and Morty in Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can watch Rick and Morty", "extracted_entity": null}
{"question": "did Elizabeth II frequently visit Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she frequently visited Queen Victoria.", "extracted_entity": null}
{"question": "can a sesame seed grow in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be very tiny.", "extracted_entity": null}
{"question": "did the Watergate scandal help the Republican party?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Republican party was already in power", "extracted_entity": null}
{"question": "would Bandy be likely to become popular in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so. I", "extracted_entity": null}
{"question": "do inanimate objects come alive in Beauty and the Beast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the objects in the castle are alive and", "extracted_entity": null}
{"question": "does Home Depot sell item in late September zodiac sign symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Home Depot sells item in late", "extracted_entity": null}
{"question": "does monster name in West African Folklore that witches send into villages set Scrabble record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "18 letters.", "extracted_entity": null}
{"question": "can an American black bear swallow a sun bear whole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not even if the sun bear is a", "extracted_entity": null}
{"question": "can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Reconstruction era coin can buy", "extracted_entity": null}
{"question": "would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he climbed it with Edmund Hillary", "extracted_entity": null}
{"question": "can preventive healthcare reduce STI transmission?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the best way to reduce STI", "extracted_entity": null}
{"question": "would 1996 leap year baby technically be 1 year old in 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 1996, a", "extracted_entity": null}
{"question": "would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were designed to be able to", "extracted_entity": null}
{"question": "would a Superbowl Football Game be crowded on the Gettysburg Battlefield?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they wouldn't be able to", "extracted_entity": null}
{"question": "can you purchase a dish with injera at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can get a taco with", "extracted_entity": null}
{"question": "has Drew Carey outshined Doug Davidson's tenure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you don't buy the car", "extracted_entity": null}
{"question": "is eggplant deadly to most atopic individuals? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I have had many atopic patients", "extracted_entity": null}
{"question": "would Cyndi Lauper use milk substitute in her rice pudding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not.", "extracted_entity": null}
{"question": "can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you use the space bar as the", "extracted_entity": null}
{"question": "would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure, but I don\u2019", "extracted_entity": null}
{"question": "are potatoes native to the European continent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are native to the Americas.", "extracted_entity": null}
{"question": "was Iggy Pop named after his father?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was named after an old boyfriend", "extracted_entity": null}
{"question": "can a 2019 Toyota Hilux hypothetically support weight of thirty Big John Studd clones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it\u2019s the only vehicle that", "extracted_entity": null}
{"question": "did Tom Bosley enjoy video games on the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He played with his grandkids.", "extracted_entity": null}
{"question": "do German Shepherds worry about the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do you find glutamic acid in a severed finger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, glutamic acid is found in", "extracted_entity": null}
{"question": "can black swan's formation type help spell longest word in Dictionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can't", "extracted_entity": null}
{"question": "would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he would support eugenics.", "extracted_entity": null}
{"question": "could Stephen King join the NASA Astronaut Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not.", "extracted_entity": null}
{"question": "does the art from Family Guy look a lot like the art in American Dad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not a ripoff", "extracted_entity": null}
{"question": "did DARPA influence Albert Einstein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did Moon Jae-in's residence exist when the World Trade Center was completed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "would a kaffir lime be a good ingredient for making a candle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I have never heard", "extracted_entity": null}
{"question": "is Glenn Beck known for his mild temper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's known for his angry out", "extracted_entity": null}
{"question": "was the 1980 presidential election won by a member of the Grand Old Party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ronald Reagan", "extracted_entity": null}
{"question": "would Nancy Pelosi have hypothetically been on same side as Gerald Ford?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would have been on the same side", "extracted_entity": null}
{"question": "was the MLB World Series held in Newcastle, New South Wales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was in San Francisco, California.", "extracted_entity": null}
{"question": "will a sloth explode if it's not upside down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it will get very grumpy", "extracted_entity": null}
{"question": "could Rhode Island sink into the Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the whole world would sink too.", "extracted_entity": null}
{"question": "would it be hard to get toilet paper if there were no loggers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The toilet paper is made from", "extracted_entity": null}
{"question": "are banana trees used by judges for maintaining order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are used by the judges", "extracted_entity": null}
{"question": "would four shoes be insufficient for a set of octuplets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for octuplets with", "extracted_entity": null}
{"question": "did Alan Rickman have an improperly functioning organ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (I've seen this film about", "extracted_entity": null}
{"question": "does the United States Navy create radioactive waste?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the United States Navy creates radioactive waste", "extracted_entity": null}
{"question": "is the United States Capitol located near the White House?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The United States Capitol is located in Washington,", "extracted_entity": null}
{"question": "is winter associated with hot temperatures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is associated with cold temperatures.", "extracted_entity": null}
{"question": "did Sony definitively win the video game war against Sega?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Sega won.", "extracted_entity": null}
{"question": "could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could have done so.", "extracted_entity": null}
{"question": "can you find a snow leopard in the Yucatan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can find a jaguar", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is it common to see frost during some college commencements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "was Walt Disney ever interviewed by Anderson Cooper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Walt Disney was interviewed by", "extracted_entity": null}
{"question": "does a dentist treat Bluetooth problems?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But a dentist can provide you with", "extracted_entity": null}
{"question": "did Richard III's father have greater longevity than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he died at the age of 4", "extracted_entity": null}
{"question": "does Ariana Grande's signature style combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, her style is a mix of comfort and", "extracted_entity": null}
{"question": "did the color green help Theodor Geisel become famous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It helped him get a job as an", "extracted_entity": null}
{"question": "does the United States Secretary of State answer the phones for the White House?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The White House Chief of Staff answers the", "extracted_entity": null}
{"question": "would a honey badger's dentures be different from a wolverine's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because honey badgers have 2", "extracted_entity": null}
{"question": "are thetan levels found in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Bobby Jindal's high school mascot eat kibble?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it was mixed with a", "extracted_entity": null}
{"question": "are multiple Christmas Carol's named after Saints?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are 3", "extracted_entity": null}
{"question": "do placozoa get learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. Maybe, maybe not", "extracted_entity": null}
{"question": "would Jon Brower Minnoch break a chair before Voyager 2 launch mass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would be too busy eating", "extracted_entity": null}
{"question": "would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The NFL Network has the rights to Monday", "extracted_entity": null}
{"question": "are any of the destinations of Japan Airlines former Axis Powers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are all former Axis Powers", "extracted_entity": null}
{"question": "is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "could an infant solve a sudoku puzzle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a toddler could", "extracted_entity": null}
{"question": "did Gauss have a normal brain structure?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he had a normal brain structure.", "extracted_entity": null}
{"question": "did Benito Mussolini wear bigger shoes than Haf\u00fe\u00f3r Bj\u00f6rnsson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did U2 play a concert at the Polo Grounds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, on September 13, 1", "extracted_entity": null}
{"question": "does Ludacris perform classical music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn\u2019t", "extracted_entity": null}
{"question": "can olive oil kill rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Olive oil is a natural oil,", "extracted_entity": null}
{"question": "does Julia Roberts lose the prolific acting contest in her family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, her brother Eric Roberts is a much better", "extracted_entity": null}
{"question": "does Super Mario mainly focus on a man in green?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it does focus on a man in", "extracted_entity": null}
{"question": "will Elijah Cummings cast a vote in the 2020 presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019ve never", "extracted_entity": null}
{"question": "can an adult human skull hypothetically pass through the birth canal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it is physically impossible.", "extracted_entity": null}
{"question": "are any mollusks on Chinese New Year calendar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no mollusks are not on Chinese New", "extracted_entity": null}
{"question": "could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could", "extracted_entity": null}
{"question": "if he were poor, would Christopher Reeve have lived?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christopher Reeve was rich.", "extracted_entity": null}
{"question": "is it impossible to tell if someone is having a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is possible to miss the sympt", "extracted_entity": null}
{"question": "does Super Mario require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Super Mario is a game that is played", "extracted_entity": null}
{"question": "does the actress who played Elizabeth II speak fluent Arabic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. she is a native speaker of English", "extracted_entity": null}
{"question": "were the Spice Girls inspired by Little Mix?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were inspired by the Sugabab", "extracted_entity": null}
{"question": "was Augustus his real name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is the brain located in the torso?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The brain is located in the head.", "extracted_entity": null}
{"question": "could Cosmic Girls play League of Legends alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they have to play with other people", "extracted_entity": null}
{"question": "has a tumulus been discovered on Mars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a Bengal cat be afraid of catching a fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the fish. If it is a", "extracted_entity": null}
{"question": "is anyone at the Last Supper celebrated in Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. (Jesus is)", "extracted_entity": null}
{"question": "did John Lennon listen to Compact discs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He listened to LPs.", "extracted_entity": null}
{"question": "did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "was Nikola Tesla's home country involved in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "were there eight humans on Noah's Ark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, eight humans, and two of each kind", "extracted_entity": null}
{"question": "are people banned from entering the Forbidden City?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are a lot of rules about who", "extracted_entity": null}
{"question": "did Kurt Cobain's music genre survive after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not really, it's been a", "extracted_entity": null}
{"question": "was Lorenzo de Medici's patronage of Da Vinci exclusive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was exclusive", "extracted_entity": null}
{"question": "when the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was.", "extracted_entity": null}
{"question": "would a nickel fit inside a koala pouch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'd", "extracted_entity": null}
{"question": "did the Football War last at least a month?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Masaharu Morimoto rely on glutamic acid?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he does not rely on glutam", "extracted_entity": null}
{"question": "can you put bitcoin in your pocket?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s not physical, it\u2019", "extracted_entity": null}
{"question": "would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a member of the United States Air Force", "extracted_entity": null}
{"question": "at midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the baby is already in bed.", "extracted_entity": null}
{"question": "did Irish mythology inspire Washington Irving?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but in a very limited way.", "extracted_entity": null}
{"question": "can a microwave melt a Toyota Prius battery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it can.", "extracted_entity": null}
{"question": "does Elizabeth II reign over the Balearic Islands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the king of Spain does", "extracted_entity": null}
{"question": "can Michael Jordan become a professional cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "will speed reader devour The Great Gatsby before the Raven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I am going to be a bit slow", "extracted_entity": null}
{"question": "would the 10th doctor enjoy a dish of stuffed pears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he'd prefer a dish of", "extracted_entity": null}
{"question": "during the time immediately after 9/11, was don't ask don't tell still in place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. i don't think the fact that", "extracted_entity": null}
{"question": "was King Kong (2005 film) solvent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, King Kong (2005 film", "extracted_entity": null}
{"question": "would the Who concert in international space station be audible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The ISS is about 250 miles", "extracted_entity": null}
{"question": "would an ancient visitor to Persia probably consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was common in the ancient world", "extracted_entity": null}
{"question": "do you need different colored pens for sudoku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you don\u2019t need different colored pens", "extracted_entity": null}
{"question": "can the Supreme Court of Canada fight a Lucha trios match?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "could Eric Clapton's children play a regulation game of basketball among themselves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would be too tall.", "extracted_entity": null}
{"question": "is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's likely to be shown.", "extracted_entity": null}
{"question": "did Saddam Hussein witness the inauguration of Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Saddam Hussein was executed in", "extracted_entity": null}
{"question": "is the Flying Spaghetti Monster part of an ancient pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a modern invention.", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would a slingshot be improperly classified as artillery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be improperly classified", "extracted_entity": null}
{"question": "is it normal to blow out candles during a funeral?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I've never heard of it.", "extracted_entity": null}
{"question": "could Darth Vader hypothetically catch the Coronavirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would probably have a stronger imm", "extracted_entity": null}
{"question": "did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was married to Catherine of Aragon", "extracted_entity": null}
{"question": "snowboarding is a rarity in Hilo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The only time I have seen snowboard", "extracted_entity": null}
{"question": "was John Gall from same city as Stanford University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was from Berkeley, California.", "extracted_entity": null}
{"question": "is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, sunscreen is unhelpful for the", "extracted_entity": null}
{"question": "is San Diego County the home of a Shamu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is a 19-year", "extracted_entity": null}
{"question": "was Elmo an original muppet character on Sesame Street?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a character on the show \"", "extracted_entity": null}
{"question": "would a Fakir be surprised if they saw a comma in their religious book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be", "extracted_entity": null}
{"question": "in the world of Harry Potter, would a snake and skull tattoo be good luck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure that I understand your question", "extracted_entity": null}
{"question": "did England win any Olympic gold medals in 1800?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The first Olympic games were held in", "extracted_entity": null}
{"question": "is video surveillance of a room possible without an obvious camera or new item?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. We can place a camera inside an item", "extracted_entity": null}
{"question": "do pirates care about vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only the ones that are scur", "extracted_entity": null}
{"question": "will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as a hero", "extracted_entity": null}
{"question": "were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were required to say it.", "extracted_entity": null}
{"question": "could a fan of the Botany Swarm vote for John Key?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they could not.", "extracted_entity": null}
{"question": "would Jackie Chan have trouble communicating with a deaf person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jackie Chan has no trouble communicating", "extracted_entity": null}
{"question": "was Oscar Wilde's treatment under the law be considered fair in the US now?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Wilde was a homosexual and", "extracted_entity": null}
{"question": "can a lemon aggravate dyspepsia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "can musicians become knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they can find a good", "extracted_entity": null}
{"question": "is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not necessarily. It depends on how big the pel", "extracted_entity": null}
{"question": "did Disney's second film rip off a prophet story?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Alan Alda old enough to have fought in the Vietnam War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was born in 193", "extracted_entity": null}
{"question": "is Samsung accountable to shareholders?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Samsung is accountable to sharehold", "extracted_entity": null}
{"question": "are chinchillas cold-blooded?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "did Johann Sebastian Bach ever win a Grammy Award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was nominated 3 times.", "extracted_entity": null}
{"question": "did Dr. Seuss live a tragedy free life?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he died at the age of 8", "extracted_entity": null}
{"question": "would a Nike shoebox be too small to fit a swan in?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Nike shoebox would be", "extracted_entity": null}
{"question": "were Beauty and the Beast adaptations devoid of Kurt Sutter collaborators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were not.", "extracted_entity": null}
{"question": "was Florence a Theocracy during Italian Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Italy was not a theocracy.", "extracted_entity": null}
{"question": "did Sojourner Truth use the elevator at the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope. the Washington Monument was not built until", "extracted_entity": null}
{"question": "is it safe to eat hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it\u2019s safe to eat hair.", "extracted_entity": null}
{"question": "did Lionel Richie ever have dinner with Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was not Lincoln\u2019s birth", "extracted_entity": null}
{"question": "would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not.", "extracted_entity": null}
{"question": "are fresh garlic cloves as easy to eat as roasted garlic cloves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. fresh garlic is hard to chew", "extracted_entity": null}
{"question": "did Osama bin Laden likely abstain from alcohol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a Muslim.", "extracted_entity": null}
{"question": "do Youtube viewers get unsolicited audiobook advice often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is 2018 Ashland, Oregon population inadequate to be a hypothetical military division?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "2018 Ashland, Oregon population", "extracted_entity": null}
{"question": "does Carmen Electra own a junk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would have been a short set", "extracted_entity": null}
{"question": "would Glen Beck and Stephen Colbert be likely to tour together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so.", "extracted_entity": null}
{"question": "could a young Wizard of Oz Scarecrow have gotten Cerebral palsy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Cerebral palsy is", "extracted_entity": null}
{"question": "can the Toyota Hilux tip the scales against Mr. Ed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It\u2019s a close call.", "extracted_entity": null}
{"question": "is it normally unnecessary to wear a coat in Hollywood in July?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You can wear a coat in Hollywood in", "extracted_entity": null}
{"question": "could the Atlantic readers fill 500 battalions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they had the same equipment as the", "extracted_entity": null}
{"question": "can oysters be preserved without refrigeration? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Oysters can be preserved without re", "extracted_entity": null}
{"question": "would a hippie hypothetically be bummed out by Augustus's Pax Romana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they'd be bummed out", "extracted_entity": null}
{"question": "can professional boxers expect to have low dental bills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they have dental problems as well", "extracted_entity": null}
{"question": "did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for his architectural skills", "extracted_entity": null}
{"question": "would  bald eagle deliver an urgent message before B-52?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is it difficult to conduct astrophotography in the summer in Sweden?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is difficult.", "extracted_entity": null}
{"question": "was a woman Prime Minister directly before or after Stanley Baldwin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Margaret Thatcher", "extracted_entity": null}
{"question": "do people in middle school usually get breast exams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they are too young.", "extracted_entity": null}
{"question": "is Black Lives Matter connected with capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is connected to a bunch of", "extracted_entity": null}
{"question": "does Capricorn astrology symbol have all of the parts of a chimera?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not as common as the", "extracted_entity": null}
{"question": "would Lord Voldemort hypothetically be an effective fighter after Final Fantasy silence is cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would be.", "extracted_entity": null}
{"question": "does Sam Harris worship Shiva?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he does worship the Shiva of", "extracted_entity": null}
{"question": "do the James Bond and Doctor Who series have a similarity in format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both have a main character and a", "extracted_entity": null}
{"question": "can jackfruit be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a weapon.", "extracted_entity": null}
{"question": "does Disney own a major comic book publisher?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "did William Shaespeare read the Daily Mirror?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was too busy writing the plays.", "extracted_entity": null}
{"question": "did the 40th president of the United States forward lolcats to his friends?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he did.", "extracted_entity": null}
{"question": "did Clark Gable marry more women once than Richard Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but Richard Burton married more than twice", "extracted_entity": null}
{"question": "would an expensive tailor use adhesive to create a shorter hem on slacks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would use a sewing machine.", "extracted_entity": null}
{"question": "would the yearly precipitation on Snowdon submerge an upright bowling pin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it would submerge a small bott", "extracted_entity": null}
{"question": "can a honey bee sting a human more than once?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they usually don't.", "extracted_entity": null}
{"question": "did U.S. soldiers listen to Justin Bieber's Believe album during the Battle of Baghdad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they listened to the sound of war", "extracted_entity": null}
{"question": "can sunlight travel to the deepest part of the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not all of it.", "extracted_entity": null}
{"question": "is Christopher Walken close to achieving EGOT status?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he is.", "extracted_entity": null}
{"question": "are the brooms from curling good for using on house floors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the brooms used in curling are", "extracted_entity": null}
{"question": "would a dog easily notice ammonia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, ammonia has a very strong od", "extracted_entity": null}
{"question": "are vinegar pickled cucumbers rich in lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is a construction worker required to build a portfolio?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he/she is required to be", "extracted_entity": null}
{"question": "is it hard to get a BLT in Casablanca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it\u2019s hard to find a", "extracted_entity": null}
{"question": "would half muggle wizards fear Lord Voldemort?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as pureblood", "extracted_entity": null}
{"question": "would ISIS agree with Al-Farabi's religious sect?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. ISIS is a political movement. Al", "extracted_entity": null}
{"question": "is Casio's founding year a composite number?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is a multiple of 2", "extracted_entity": null}
{"question": "would it be unusual to use paypal for drug deals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not unusual.", "extracted_entity": null}
{"question": "is growing seedless cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's a lot of work", "extracted_entity": null}
{"question": "can Spartina Patens thrive in the Sahara Desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it cannot.", "extracted_entity": null}
{"question": "does taking ukemi halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It does not.", "extracted_entity": null}
{"question": "are there five different single-digit Fibonacci numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there are only four: 1,", "extracted_entity": null}
{"question": "can a snow leopard eat twice its own body weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a snow leopard can eat twice", "extracted_entity": null}
{"question": "are ground bell peppers the main ingredient of black pepper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, ground bell peppers are not the main", "extracted_entity": null}
{"question": "would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because there is a substitute for olive", "extracted_entity": null}
{"question": "would a monkey outlive a human being on average?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only by a few days.", "extracted_entity": null}
{"question": "can Harry Potter book a flight on Asiana Airlines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "is latitude required to determine the coordinates of an area?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "did polio medicine save the life of polio vaccine creator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The polio vaccine was invent", "extracted_entity": null}
{"question": "are Donkeys part of Christmas celebrations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are!", "extracted_entity": null}
{"question": "if you have a serious injury in Bangladesh, would you probably dial a Fibonacci number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "3", "extracted_entity": null}
{"question": "is Great Pyramid of Giza the last wonder of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do", "extracted_entity": null}
{"question": "will Elijah Cummings vote for Joe Biden in the next presidential elections?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but I don\u2019t think he will", "extracted_entity": null}
{"question": "did Joan Crawford guest star on  JAG (TV series)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "can an African Elephant get pregnant twice in a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the same year.", "extracted_entity": null}
{"question": "would a sofer be a bad job for a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by \u201c", "extracted_entity": null}
{"question": "is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is legal for a licensed child", "extracted_entity": null}
{"question": "was Jackson Pollock straight edge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was an alcoholic.", "extracted_entity": null}
{"question": "do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They only work during the day.", "extracted_entity": null}
{"question": "would a fungal life-form be threatened by a pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the fungus would be threatened by the", "extracted_entity": null}
{"question": "do urban legends always have to occur in cities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they can occur anywhere.", "extracted_entity": null}
{"question": "has Freemasonry been represented on the Moon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has.", "extracted_entity": null}
{"question": "is Hamlet more common on IMDB than Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Hamlet has been filmed more", "extracted_entity": null}
{"question": "coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all of them.", "extracted_entity": null}
{"question": "was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "can people die from brake failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is rare.", "extracted_entity": null}
{"question": "if it socially acceptable to wear an icon depicting crucifixion? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is socially acceptable.", "extracted_entity": null}
{"question": "could a monarch butterfly rule a kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are too small and fragile.", "extracted_entity": null}
{"question": "can vitamin C rich fruits be bad for health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Fruits are good for health.", "extracted_entity": null}
{"question": "did breakdancing grow in popularity during WW2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it did not.", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Texas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not unusual.", "extracted_entity": null}
{"question": "are Aldi's foods discounted due to being out of date?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Aldi's foods are", "extracted_entity": null}
{"question": "would a recruit for the United States Marine Corps be turned away for self harm?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be.", "extracted_entity": null}
{"question": "can you order an Alfa Romeo at Starbucks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can order a coffee.", "extracted_entity": null}
{"question": "will Chick Fil A be open on Halloween 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Chick Fil A will be open on", "extracted_entity": null}
{"question": "do drag kings take testosterone to look masculine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not usually.", "extracted_entity": null}
{"question": "are pennies commonly used in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are, they are the same size as", "extracted_entity": null}
{"question": "could the Pope be on an episode of Pimp My Ride?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is.", "extracted_entity": null}
{"question": "can E6000 cure before a hoverboard finishes the Daytona 500? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope", "extracted_entity": null}
{"question": "does Nigella Lawson care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she does not", "extracted_entity": null}
{"question": "would Cuba Libre consumption help with insomnia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I can\u2019t tell you how many times I", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "would someone on Venus be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the planet Venus has a surface", "extracted_entity": null}
{"question": "was Jean Valjean imprisoned due to hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was imprisoned for stealing food", "extracted_entity": null}
{"question": "is Europa (moon) name origin related to Amunet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The name Europa is the name of the daughter of", "extracted_entity": null}
{"question": "does Adobe Suite have video game engine coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's not a game engine.", "extracted_entity": null}
{"question": "could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but someone in Tokyo could take a train", "extracted_entity": null}
{"question": "would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the United States Military Academy does not reject", "extracted_entity": null}
{"question": "is strep throat harmless to singer Rita Ora after her 2020 tonsilitis surgery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she can sing, but the risk of", "extracted_entity": null}
{"question": "would an Alfa Romeo vehicle fit inside a barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it could fit in a garage", "extracted_entity": null}
{"question": "gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is willing to use his wand", "extracted_entity": null}
{"question": "could two newborn American Black Bear cubs fit on a king size bed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are small enough to fit in", "extracted_entity": null}
{"question": "are coopers required in the beverage industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, coopers are required in the be", "extracted_entity": null}
{"question": "was Superhero fiction invented in the digital format?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "does Antarctica have a lot of problems relating to homelessness?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as much as you'd", "extracted_entity": null}
{"question": "does an individual oceanographer study many sciences?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, an individual oceanographer has to study", "extracted_entity": null}
{"question": "is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's safe as long as you don'", "extracted_entity": null}
{"question": "did Julio Gonzalez like acetylene?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "is the letter D influenced by the shape of ancient doors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the letter D is influenced by the shape", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a parody of the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a parody of the French", "extracted_entity": null}
{"question": "was the death of Heath Ledger caused by his work on The Dark Knight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Heath Ledger died from a drug", "extracted_entity": null}
{"question": "did Donatello use a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Rahul Dravid ever kick a field goal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "do you often hear Marco Polo's name shouted near water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's the Italian version of the", "extracted_entity": null}
{"question": "are people more likely than normal to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I'm sure of it.", "extracted_entity": null}
{"question": "is an espresso likely to assuage fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "2020 is a year of pre", "extracted_entity": null}
{"question": "would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "did a gladiator kill his opponent with a shotgun?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did with a crossbow", "extracted_entity": null}
{"question": "did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Grizzly Bears were not", "extracted_entity": null}
{"question": "could B be mistaken for an Arabic numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because B is a capital letter.", "extracted_entity": null}
{"question": "can COVID-19 spread to maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The short answer is that we do not know.", "extracted_entity": null}
{"question": "is 500GB USB device enough to save 10 hours of Netflix shows a day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "do hornets provide meaningful data for oceanographers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would Michael Phelps be good at pearl hunting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would be too busy being good at", "extracted_entity": null}
{"question": "at Christmastime, do some films remind us that groundhog day is approaching?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and one of them is \u201cA Christmas", "extracted_entity": null}
{"question": "was Great Recession the period of severest unemployment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the 1930s was", "extracted_entity": null}
{"question": "could Katharine Hepburn have ridden the AirTrain JFK?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would have had to walk.", "extracted_entity": null}
{"question": "was Walt Disney able to email his illustrations to people living far away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There was no email or internet in W", "extracted_entity": null}
{"question": "can a person who knows only English read Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They have to learn it.", "extracted_entity": null}
{"question": "would three commas be sufficient for displaying US 2018 GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not", "extracted_entity": null}
{"question": "would Brian Warner be a good singer for a soul music band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "could someone listen to the entire U2 debut studio album during an episode of Peppa Pig?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a bit of a", "extracted_entity": null}
{"question": "did Hanuman ever experience an orgasm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Hanuman was a celibate mon", "extracted_entity": null}
{"question": "would you find a tibia beside parsley on a holiday plate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but if you were to find one beside", "extracted_entity": null}
{"question": "does someone from Japan need a passport to go to a Nordic country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they need a passport", "extracted_entity": null}
{"question": "would a black widow woman have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A black widow is a spider.", "extracted_entity": null}
{"question": "does a person using tonsure have hair at the top of their scalp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person using tonsure has hair at", "extracted_entity": null}
{"question": "would Terence Tao outperform Eminem in a math competition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Eminem is a rapper,", "extracted_entity": null}
{"question": "do any video games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I\u2019m playing them right now", "extracted_entity": null}
{"question": "is number of different US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the number of different US Presidents in", "extracted_entity": null}
{"question": "can lettuce result in spontaneous abortion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lettuce is not known to cause spont", "extracted_entity": null}
{"question": "has mummification in the Andes been prevented by rainfall?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the high altitude and dry climate in", "extracted_entity": null}
{"question": "are some Do It Yourself projects potentially lethal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and no.", "extracted_entity": null}
{"question": "was King Kong (2005 film) the lead actress's biggest box office role?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, her biggest box office role was the lead", "extracted_entity": null}
{"question": "would a bodybuilder choose maize over chicken breast for dinner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would choose chicken breast.", "extracted_entity": null}
{"question": "karachi was a part of Alexander the Great's success?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Alexander the Great was the first person to conquer the", "extracted_entity": null}
{"question": "do guitarists need both hands to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Some guitarists are one-handed", "extracted_entity": null}
{"question": "does Kenny G hold the qualifications to be a tax collector?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But the taxpayers will be pay", "extracted_entity": null}
{"question": "in baseball, is a \"Homer\" named after the poet Homer who wrote the Odyssey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It's believed that", "extracted_entity": null}
{"question": "is a felony jury enough people for a Bunco game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. a felony jury is 1", "extracted_entity": null}
{"question": "is letter C crucial to spelling the two most common words in English language?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, letter C is not crucial to sp", "extracted_entity": null}
{"question": "do most middle class families have butlers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Butlers are servants of the rich.", "extracted_entity": null}
{"question": "is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Charles Dickens is buried at the Po", "extracted_entity": null}
{"question": "did Mickey Mouse appear in a cartoon with Bugs Bunny in 1930?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in \"The Wise Quacking", "extracted_entity": null}
{"question": "did Jackson 5 members exceed number in The Osmonds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, The Jackson 5 had 5 members", "extracted_entity": null}
{"question": "can you get a ride on Amtrak to the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to be on the train", "extracted_entity": null}
{"question": "are all limbs required for jujutsu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no required limbs for ju", "extracted_entity": null}
{"question": "would The Dark Knight be appropriate for a preschool class?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it may be too scary for", "extracted_entity": null}
{"question": "did Jesus know anyone who spoke Estonian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jesus did not know anyone who spoke Est", "extracted_entity": null}
{"question": "does 2015 have more unlucky Friday's than usual?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, 13 of them.", "extracted_entity": null}
{"question": "is Tony Bennett's middle name shared by a former UFC champion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is shared by a former U", "extracted_entity": null}
{"question": "do you need both hot and cold water to peel a tomato?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you only need cold water.", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of Reptile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A Newt", "extracted_entity": null}
{"question": "do spider wasps have eight legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, spider wasps have six legs.", "extracted_entity": null}
{"question": "does the New York Public Library sell Alpo products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "can petroleum jelly be used as fuel in a car?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is not very efficient.", "extracted_entity": null}
{"question": "can Aerosmith legally drive in the carpool lane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are a band.", "extracted_entity": null}
{"question": "are Durian fruits an olfactory delight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "could a wandering albatross fly from Bucharest to New York City without a rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "do some people soak in olive oil and water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's called a \"spa", "extracted_entity": null}
{"question": "do depressed people travel to the Golden Gate Bridge often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they can't see the damn", "extracted_entity": null}
{"question": "can members of the Green Party of England and Wales vote in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The Green Party of the USA is a", "extracted_entity": null}
{"question": "is Poseidon similar to the god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Vulcan is the Roman equivalent of", "extracted_entity": null}
{"question": "is Glycol something United Airlines would buy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "does a person suffering from Thalassophobia enjoy oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They do not.", "extracted_entity": null}
{"question": "can a Bengal cat survive eating only pancakes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it will be very hungry.", "extracted_entity": null}
{"question": "does it seem like the Gorillaz is composed of more members than they have?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "could Rich and Morty be triggered for children of alcoholics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, definitely", "extracted_entity": null}
{"question": "was the Japanese street aesthetic once illuminated by noble gasses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "in isopropyl alcohol, is the solubility of salt low?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The solubility of salt in isoprop", "extracted_entity": null}
{"question": "could you watch all of JAG in six months?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I could watch all of JAG in", "extracted_entity": null}
{"question": "do silicone suits make judo difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "will The Exorcist stimulate limbic system?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "was P. G. Wodehouse's favorite book The Hunger Games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he never read it.", "extracted_entity": null}
{"question": "would J.K Rowling's top sellers be on a fantasy shelf?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are also in the YA", "extracted_entity": null}
{"question": "are some types of pancakes named after coins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "can Paprika be made without a dehydrator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Paprika can be made without a", "extracted_entity": null}
{"question": "in Doctor Who, did the war doctor get more screen time than his successor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the War Doctor only got more screen time", "extracted_entity": null}
{"question": "was Mesopotamia part of what is now China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Mesopotamia is located in", "extracted_entity": null}
{"question": "could Arnold Schwarzenegger hypothetically defeat Haf\u00fe\u00f3r Bj\u00f6rnsson in a powerlifting competition if both are at their peak strength?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Arnold Schwarzenegger could hypothet", "extracted_entity": null}
{"question": "was the British car, the Mini, the first car manufactured?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the first car was manufactured in", "extracted_entity": null}
{"question": "will you see peach blossoms and Andromeda at the same time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you will.", "extracted_entity": null}
{"question": "can The Hobbit be read in its entirety in four minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. But it can be read in its entire", "extracted_entity": null}
{"question": "is number of stars in Milky Way at least ten times earth's population?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not even close.", "extracted_entity": null}
{"question": "could Christopher Walken enlist in the United States Marine Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has a Bachelor's D", "extracted_entity": null}
{"question": "is white light the absence of color?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, white light is the combination of all colors", "extracted_entity": null}
{"question": "can Jabberwocky be considered a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The rhyme scheme is ABCB,", "extracted_entity": null}
{"question": "would Eye surgery on a fly be in vain?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the fly's eye is very", "extracted_entity": null}
{"question": "should you ask a neighbor for candy on New Year's Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because you will get no candy.", "extracted_entity": null}
{"question": "are all types of pottery safe to cook in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are some types of pottery that", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it cannot.", "extracted_entity": null}
{"question": "does the United States of America touch the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "would Donald Duck be allowed into most grocery stores?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He\u2019s a duck.", "extracted_entity": null}
{"question": "is it best to avoid kola nuts with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you should not avoid kola nuts", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA daily recommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a medium zucchini has", "extracted_entity": null}
{"question": "was the Treaty of Versailles settled over blueberry scones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that\u2019s why it\u2019s", "extracted_entity": null}
{"question": "will the producer of Easy Rider become an octogenarian in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he'll be dead by then.", "extracted_entity": null}
{"question": "does Osama bin Laden put a wafer on his tongue every Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "can you taste Law & Order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the form of the show's", "extracted_entity": null}
{"question": "are queen bees unnecessary for growing apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, queen bees are necessary for growing app", "extracted_entity": null}
{"question": "does a bumblebee have to worry about spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're too small.", "extracted_entity": null}
{"question": "can the Department of Defense perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it cannot.", "extracted_entity": null}
{"question": "was Land of Israel in possession of an Islamic empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Ottoman Empire was not established until", "extracted_entity": null}
{"question": "do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "could the Toyota Stadium sit a tenth of the population of Gotheburg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not even close.", "extracted_entity": null}
{"question": "does the word swastika have meaning in sanskrit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the word \"swastika\" is", "extracted_entity": null}
{"question": "were there greater landslides than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1992,", "extracted_entity": null}
{"question": "can you hunt Iberian wolves in the Southern United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Iberian wolf is ext", "extracted_entity": null}
{"question": "is hanging a viable execution method on a ship at sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all.", "extracted_entity": null}
{"question": "is pig meat considered inedible within the cuisine of Hawaii?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "are the events of Star Trek: The Next Generation in the history of the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are in the history of the", "extracted_entity": null}
{"question": "if you're pregnant, might you be recommended ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if you're pregnant, might you", "extracted_entity": null}
{"question": "does The Jungle Book contain racist subtext?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not the fault of", "extracted_entity": null}
{"question": "does the Prime Minister of the United Kingdom have poor job security?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is appointed by the Queen.", "extracted_entity": null}
{"question": "would Gomer Pyle salute a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would do it with a smile", "extracted_entity": null}
{"question": "when en route from China to France, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "could you brew beer from start to finish in the month of September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes! The beer was brewed at the", "extracted_entity": null}
{"question": "is Freya a combination of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Freya is not a combination of Ath", "extracted_entity": null}
{"question": "in most Mennonite homes, would children know of The Powerpuff Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not likely", "extracted_entity": null}
{"question": "would a sophist use an \u00e9p\u00e9e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the sophists were all too", "extracted_entity": null}
{"question": "did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Paul the Apostle's cause of", "extracted_entity": null}
{"question": "do sand cats avoid eating all of the prey of eels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "was Muhammed a member of the Uniting Church in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he wasn't.", "extracted_entity": null}
{"question": "would the historic Hattori Hanz\u014d admire Naruto?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would", "extracted_entity": null}
{"question": "should you wrap a gift for a mother of a stillborn in stork wrapping paper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s tacky.", "extracted_entity": null}
{"question": "does crucifixion violate US eighth amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The US constitution only applies to the federal", "extracted_entity": null}
{"question": "would a TMNT coloring book have pizza in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not pizza, it would be", "extracted_entity": null}
{"question": "could Sugar Ray Robinson box if he stole in Iran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. E.T. would love Friendly", "extracted_entity": null}
{"question": "can a banana get a virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a banana can get a virus.", "extracted_entity": null}
{"question": "is the use of the word Gypsy by non-Romani people considered okay?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is considered racist.", "extracted_entity": null}
{"question": "are implants from an ORIF surgery affected by the magnetic field of the Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Implants are not affected by the magnetic field", "extracted_entity": null}
{"question": "was Subway involved in a pedophilia scandal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were.", "extracted_entity": null}
{"question": "did Jack Dempsey have most title fight wins in either of his weight classes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "41 wins in the light heavyweight division", "extracted_entity": null}
{"question": "did Bill Nye vote for Franklin Delano Roosevelt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is grief always obvious when it is being experienced?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Grief is a complex emotion that", "extracted_entity": null}
{"question": "did either side score a touchdown during the Football War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is there a Yeti associated with Disney theme parks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is. In fact, there are", "extracted_entity": null}
{"question": "did Zorro carve his name into items regularly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he carved his name into things,", "extracted_entity": null}
{"question": "did Subway have a sex offender as a spokesperson?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they had Jared Fogle", "extracted_entity": null}
{"question": "could Christopher Nolan's movies finance Cyprus's entire GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could", "extracted_entity": null}
{"question": "is Steve Carell's character on The Office portrayed as one with tremendous leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He's not.", "extracted_entity": null}
{"question": "could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Dominicans were the most powerful order", "extracted_entity": null}
{"question": "can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "does the texture of leaves remain the same independent of their coloring changing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the coloring of leaves changes but the", "extracted_entity": null}
{"question": "can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes! and it's not \"craw", "extracted_entity": null}
{"question": "would Doctor Strange like the Pittsburgh Steelers logo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's a \"skull", "extracted_entity": null}
{"question": "would Iris (mythology) and Hermes hypothetically struggle at a UPS job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Hermes is a messenger and", "extracted_entity": null}
{"question": "is pi in excess of square root of 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, pi is less than sqrt(5", "extracted_entity": null}
{"question": "would it be uncommon for a high schooler to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be uncommon for a high", "extracted_entity": null}
{"question": "could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not. The largest ship from", "extracted_entity": null}
{"question": "does the human stomach destroy a bee if ingested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The human stomach does not destroy", "extracted_entity": null}
{"question": "is blonde hair green eyed Sara Paxton considered a Latino?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is an American actress.", "extracted_entity": null}
{"question": "are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Photosynthesis requires the presence of", "extracted_entity": null}
{"question": "can you see live harbor seals in Washington DC?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But you can see them in the wild", "extracted_entity": null}
{"question": "was story of Jesus inspired by Egyptian myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Bible says that the stories of Jesus", "extracted_entity": null}
{"question": "has CNES planted a French flag on the lunar surface?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, and the French flag was not planted", "extracted_entity": null}
{"question": "do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "can Iowa be hidden in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Channel is not long enough.", "extracted_entity": null}
{"question": "can you house a giant squid at Soldier Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for long", "extracted_entity": null}
{"question": "was hippie culture encouraged by the government in the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "would Mickey Mouse blend in with the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would be too busy dancing.", "extracted_entity": null}
{"question": "does penicillin cure a learning disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "is it dark is Basel during the day in Los Angeles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not dark in Basel during", "extracted_entity": null}
{"question": "is honey associated with queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "honey is associated with queens", "extracted_entity": null}
{"question": "was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jeannette Rankin (D-MT)", "extracted_entity": null}
{"question": "does a giant green lady stand in New York Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for long", "extracted_entity": null}
{"question": "could chives be mistaken for grass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's not likely.", "extracted_entity": null}
{"question": "could a two-year old win a Scrabble tournament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no, but I would also say", "extracted_entity": null}
{"question": "is Y2K relevant to the plot of The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but I\u2019ll tell you why.", "extracted_entity": null}
{"question": "is Sirius part of a constellation of an animal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sirius is a star, not a const", "extracted_entity": null}
{"question": "would alligator best saltwater crocodile in hypothetical Lake Urmia battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if alligator was armed", "extracted_entity": null}
{"question": "would a Common warthog starve in a greenhouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "is purchasing food for a Lolcat unnecessary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are cute and ador", "extracted_entity": null}
{"question": "do some psychotherapy patients have no mental illness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some do.", "extracted_entity": null}
{"question": "did Harry Houdini appear on Chris Angel Mindfreak?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no he did not.", "extracted_entity": null}
{"question": "could  jockey win Triple Crown between Eid al-Fitr endpoints?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but there is no guarantee", "extracted_entity": null}
{"question": "would a vegetarian be able to eat something at Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But they would be able to eat at", "extracted_entity": null}
{"question": "does the Pixar film Brave feature Scottish people?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the sense that the syrup produ", "extracted_entity": null}
{"question": "do most college students own a fax machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do have a computer with a", "extracted_entity": null}
{"question": "would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he would be able to build roads", "extracted_entity": null}
{"question": "could a delicious recipe be made with The Onion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could.", "extracted_entity": null}
{"question": "could a markhor give birth three times in a single year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can only give birth once a year", "extracted_entity": null}
{"question": "would a model be likely to frequently enjoy the menu at Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is a restaurant and they are", "extracted_entity": null}
{"question": "does the history of Europe include the age of dinosaurs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "was John George Bice's birthplace near Cornwall?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in Hertfordshire", "extracted_entity": null}
{"question": "do white blood cells outnumber red blood cells in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "white blood cells outnumber red blood cells in the", "extracted_entity": null}
{"question": "was Edward II crucial to England's victory at Battle of Falkirk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The English were successful because of the size", "extracted_entity": null}
{"question": "did Holy Land belong to Adamu's tribe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It belonged to the tribe of Abraham.", "extracted_entity": null}
{"question": "does Ahura Mazda have a rivalry with Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Zeus is a Greek God and Ah", "extracted_entity": null}
{"question": "is an Eastern chipmunk likely to die before seeing two leap years?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A chipmunk born in a le", "extracted_entity": null}
{"question": "would the owners of the company Peter Griffin works for need barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not need barley.", "extracted_entity": null}
{"question": "is the CIA part of the Department of Defense?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the CIA is part of the Department", "extracted_entity": null}
{"question": "was Harry Potter and the Philosopher's Stone popular during the great depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Harry Potter was written in 1", "extracted_entity": null}
{"question": "is a slime mold safe from cerebral palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not.", "extracted_entity": null}
{"question": "is British Airways the air force of the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. British Airways is the national flag car", "extracted_entity": null}
{"question": "has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Nobel Prizes have changed over", "extracted_entity": null}
{"question": "are aggressive bumblebees suicidal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "is Edward Snowden in hiding from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "can the Great Depression be treated with Prozac?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Prozac is an antide", "extracted_entity": null}
{"question": "can photography be considered abstract art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. photography can be considered abstract art.", "extracted_entity": null}
{"question": "could a bee hummingbird balance a scale with a single pea on it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the scale is too heavy.", "extracted_entity": null}
{"question": "is Disney associated with Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Disney is located in Orange County.", "extracted_entity": null}
{"question": "did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they were both born in New York City.", "extracted_entity": null}
{"question": "is a doctorate required to teach at a SUNY School?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. However, some schools require a doctorate", "extracted_entity": null}
{"question": "does the swastika have positive uses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a symbol of the sun,", "extracted_entity": null}
{"question": "does Nintendo's link ever see an astronomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he sees a lot of scientists", "extracted_entity": null}
{"question": "has Elon Musk's hairline changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do American teams in National Hockey League outnumber Canadian teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she lasted 144 episodes", "extracted_entity": null}
{"question": "were all the materials to make a cannon known during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, the Romans, who", "extracted_entity": null}
{"question": "is Cookie Monster's diet Paleo friendly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not exactly. He eats cookies and cake", "extracted_entity": null}
{"question": "would it be impossible to seat every Chief Justice of the United States on a Boeing 737?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not be impossible", "extracted_entity": null}
{"question": "could Toyota stadium house people suffering homelessness in Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would not be a good idea", "extracted_entity": null}
{"question": "can a martyr saint have been excommunicated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the excommunication is not valid", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And it's always a good idea", "extracted_entity": null}
{"question": "does a Disney princess on Broadway have red hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she's not a Disney prin", "extracted_entity": null}
{"question": "could Steven Spielberg send emails as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could", "extracted_entity": null}
{"question": "does Felix Potvin have a position on a dodgeball team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he does have a position on a", "extracted_entity": null}
{"question": "would someone with leukophobia enjoy looking at the Flag of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they wouldn't", "extracted_entity": null}
{"question": "would 1943-S penny be good for making silverware?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be good for making silverware.", "extracted_entity": null}
{"question": "did mongoose come from later period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They both came from the same period.", "extracted_entity": null}
{"question": "would it be possible to fit a football field in Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be possible to fit a", "extracted_entity": null}
{"question": "is Issac Newton often associated with a red fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is the apple that is associated with", "extracted_entity": null}
{"question": "would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be profitable.", "extracted_entity": null}
{"question": "can you buy spinal cord at Home Depot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you cannot buy spinal cord at Home", "extracted_entity": null}
{"question": "are some chiropractic manipulations dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some chiropractic manipulations are", "extracted_entity": null}
{"question": "were some people afraid of New Years Day coming in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were.", "extracted_entity": null}
{"question": "did travelers sing sea shanties on the Oregon Trail?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The Oregon Trail was a long and", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have longer terms than most senators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the longest term is for life, while", "extracted_entity": null}
{"question": "paleography hypothetically helps to understand Cthulhu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is an example of an ancient", "extracted_entity": null}
{"question": "is surfing popular in Des Moines, Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is in Hawaii.", "extracted_entity": null}
{"question": "is Statue of Unity hypothetically more level with Statue of Liberty than Lighthouse of Alexandria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hypothetically.", "extracted_entity": null}
{"question": "is CEO of Nissan an internationally wanted fugitive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "has type of political association Pompey had with Caesar influenced reality TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the show \"The Apprentice\"", "extracted_entity": null}
{"question": "would Rime of the Ancient Mariner make a good sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "do Snow White dwarves best The Hobbit dwarves in battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it\u2019s a close call.", "extracted_entity": null}
{"question": "can the history of art be learned by an amoeba?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can be learned by a person", "extracted_entity": null}
{"question": "can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a very long trip", "extracted_entity": null}
{"question": "walt Disney dominated his amusement park peers at Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "22 wins", "extracted_entity": null}
{"question": "would human race go extinct without chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, without chlorophyll, plants would", "extracted_entity": null}
{"question": "is the average bulk carrier ideal for transporting bromine at room temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The cargo holds are not designed for such", "extracted_entity": null}
{"question": "can depression be mistaken for laziness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. in fact, it is often mistaken for", "extracted_entity": null}
{"question": "did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope.", "extracted_entity": null}
{"question": "did pirates who had scurvy need more Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they didn't, it was a", "extracted_entity": null}
{"question": "could the leader of Heaven's Gate save images in JPEG format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he could only save in GIF format", "extracted_entity": null}
{"question": "does the FDA require sell by dates using Roman Numerals?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the FDA does not require sell by", "extracted_entity": null}
{"question": "would Kylee Jenner ask for no cream in her coffee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would ask for no sugar in her", "extracted_entity": null}
{"question": "does Nicole Kidman know any Scientologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "is Lionel Richie related to Sheila E?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not related.", "extracted_entity": null}
{"question": "was a nuclear bomb used in the Napoleonic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nuclear weapons were not invented", "extracted_entity": null}
{"question": "is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you will be accused of being a", "extracted_entity": null}
{"question": "can you buy a fair trade laptop?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Fair Trade only covers agricultural products.", "extracted_entity": null}
{"question": "was Noah concerned with buoyancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Noah was not concerned with buoyancy.", "extracted_entity": null}
{"question": "is a watchmaker likely to be able to fix an Apple Watch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A watchmaker is not likely to be", "extracted_entity": null}
{"question": "are sables related to wolverines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not related to wolverines", "extracted_entity": null}
{"question": "are some Brazilian Navy ships built in Britian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some Brazilian Navy ships are built in", "extracted_entity": null}
{"question": "could Carl Friedrich Gauss speak to someone 100 miles away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "in Hey Arnold, did any characters stay on a porch all the time?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mr. Simmons.", "extracted_entity": null}
{"question": "has Johns Hopkins University always treated subjects ethically?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In the 1950s", "extracted_entity": null}
{"question": "is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the analyst is an art expert", "extracted_entity": null}
{"question": "do black-tailed jackrabbits fear the European wildcat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "has Kelly Clarkson outsold season 4 American Idol winner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Kelly Clarkson has outsold season", "extracted_entity": null}
{"question": "is Hermes equivalent to the Roman god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Roman god Vulcan was the", "extracted_entity": null}
{"question": "can an elite runner circle the Pyrenees in one day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can get a good workout", "extracted_entity": null}
{"question": "is the Golden eagle considered a scavenger bird?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Golden eagle is a raptor.", "extracted_entity": null}
{"question": "can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the majority of vowels are not", "extracted_entity": null}
{"question": "are grapes essential to winemaking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "would the Cookie Monster decline an offer of free Keebler products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he would decline an offer of", "extracted_entity": null}
{"question": "did Jeremy Irons master sweep picking as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he mastered sweep picking as", "extracted_entity": null}
{"question": "for Hostas to look their best, do they need lots of chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chlorophyll is the pig", "extracted_entity": null}
{"question": "has spinach been a source of power in a comic movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's been a source of", "extracted_entity": null}
{"question": "can you see the Statue of Freedom from the Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can see the Statue of", "extracted_entity": null}
{"question": "does Ludacris have Greek heritage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is from Atlanta, Georgia.", "extracted_entity": null}
{"question": "will bumblebees derail the United States presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and then some", "extracted_entity": null}
{"question": "does Rahul Dravid belong to the family Gryllidae?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he belongs to the family Linn\u00e9", "extracted_entity": null}
{"question": "is Metallica protective over their music?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are protective over their music.", "extracted_entity": null}
{"question": "does Dean Cain have less days to birthday than Will Ferrell every 4th of July?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dean Cain is 13 days", "extracted_entity": null}
{"question": "is Lord Voldemort associated with a staff member of Durmstrang?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Lord Voldemort is not associated with", "extracted_entity": null}
{"question": "are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and their culture is heavily influenced by Mong", "extracted_entity": null}
{"question": "is starving Hamas agent eating pig bad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he's a Muslim", "extracted_entity": null}
{"question": "would Ringo Starr avoid the pot roast at a restaurant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he'd order a salad.", "extracted_entity": null}
{"question": "was ship that recovered Apollo 13 named after a World War II battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the USS Iwo Jima was named", "extracted_entity": null}
{"question": "are both founders of Ben & Jerry's still involved in the company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ben Cohen and Jerry Greenfield are", "extracted_entity": null}
{"question": "is a krabby patty similar to a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are both burgers.", "extracted_entity": null}
{"question": "did Beethoven enjoy listening to EDM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is it true that gay male couples cannot naturally reproduce?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can adopt.", "extracted_entity": null}
{"question": "could an American confuse breakfast in British cuisine for dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "can Kit & Kaboodle hypothetically help someone past the Underworld gates?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are willing to do", "extracted_entity": null}
{"question": "are two cans of Campbell's Soup a day good for hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not necessarily. There is no evidence that the sod", "extracted_entity": null}
{"question": "are there any official American knights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many.", "extracted_entity": null}
{"question": "is the Foreign and Commonwealth Office a European political agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a UK government department", "extracted_entity": null}
{"question": "do gorillas fight with panda bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would lumberjacks get full after eating three dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If they are lumberjacks, they would", "extracted_entity": null}
{"question": "is being 5 year Capital One Venture member more cost effective than being in Church of Satan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "tata Hexa can accomodate every Spice Girl?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Tata Hexa can accommod", "extracted_entity": null}
{"question": "could common warthog be useful for scrimshaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "is breast cancer associated with a ribbon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be very difficult for Nuno", "extracted_entity": null}
{"question": "is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but if there is a history of heart", "extracted_entity": null}
{"question": "was a Tiny House ceiling out of Osama bin Laden's reach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you conduct surveillance from a teddy bear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can\u2019t.", "extracted_entity": null}
{"question": "does table tennis use prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But you can always use them to get", "extracted_entity": null}
{"question": "was the Carnation Revolution the deadliest revolution in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Carnation Revolution was not the", "extracted_entity": null}
{"question": "is the Muslim world hostile to Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is the saltwater crocodile less endangered than the European otter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can someone from New England profit by growing coffee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "could a dichromat probably easily distinguish chlorine gas from neon gas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chlorine gas has a greenish", "extracted_entity": null}
{"question": "nATO doesn't recognize double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, NATO does recognize double triangle flag countries", "extracted_entity": null}
{"question": "did Julius Caesar read books on Pharmacology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "does water have viscosity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, water has viscosity.", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The answer is no.", "extracted_entity": null}
{"question": "would you spend less on your food at Aldi than at Whole Foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I would spend less", "extracted_entity": null}
{"question": "are Scottish people Albidosi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "are System of a Down opposed to globalization?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. they are.", "extracted_entity": null}
{"question": "was Alexander the Great baptized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but his mother was.", "extracted_entity": null}
{"question": "did Demi Lovato's ancestors help turn maize into popcorn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were probably just regular corn farmers", "extracted_entity": null}
{"question": "would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the goldfish would be eaten", "extracted_entity": null}
{"question": "is Pearl Harbor the mythical home of a shark goddess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "would Elon Musk be more likely to know about astrology than physics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Elon Musk is a brilliant man", "extracted_entity": null}
{"question": "can paresthesia be caused by a white pigment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by \u201c", "extracted_entity": null}
{"question": "could Charlie Bucket be a hotel manager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he doesn\u2019t have a good", "extracted_entity": null}
{"question": "can children be soldiers in the US Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they have to be 17", "extracted_entity": null}
{"question": "is Jack Black's height enough to satisfy Coronavirus distancing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not, Jack Black'", "extracted_entity": null}
{"question": "are peaches best eaten when firm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, peaches are best eaten when soft", "extracted_entity": null}
{"question": "was Charles Manson's body unwanted?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The LA coroner's office had", "extracted_entity": null}
{"question": "was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. LeFever was the one who got", "extracted_entity": null}
{"question": "can a human eat an entire 12-lb roast turkey in an hour? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and more", "extracted_entity": null}
{"question": "would a house full of aloe vera hypothetically be ideal for Unsinkable Sam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. If you want", "extracted_entity": null}
{"question": "would dual-energy X-ray absorptiometry be useful if performed on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a bit difficult to", "extracted_entity": null}
{"question": "would Harvey Milk have approved of Obama?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Harvey would have approved of Obama", "extracted_entity": null}
{"question": "would a jumping spider need over half a dozen contact lenses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If it did, it would have to", "extracted_entity": null}
{"question": "are seasons of Survivor surpassed by number of Ancient Greek letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. 4 seasons of Survivor sur", "extracted_entity": null}
{"question": "do citizens of Cheshire sing La Marseillaise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have never seen red legs in people with an", "extracted_entity": null}
{"question": "in teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, SSRIs are as safe in ad", "extracted_entity": null}
{"question": "could eating Chinook salmon help Ryan Reynolds?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it couldn't.", "extracted_entity": null}
{"question": "would kaffir lime be good in a White Russian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think it would be, and the", "extracted_entity": null}
{"question": "is entire Common Era minuscule to lifespan of some trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and some trees have lived for more than", "extracted_entity": null}
{"question": "did Charlemagne have a bar mitzvah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a Christian.", "extracted_entity": null}
{"question": "did Monty Python write the Who's on First sketch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Monty Python did not write the Who", "extracted_entity": null}
{"question": "in geometry terms, is the Royal Observatory in Greenwich similar to a yield sign?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Royal Observatory in Greenwich is", "extracted_entity": null}
{"question": "did Sugar Ray Robinson win a fight against Canelo Alvarez?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He did not.", "extracted_entity": null}
{"question": "can an asteroid be linked with virginity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have to be a really", "extracted_entity": null}
{"question": "would a Yeti be likely to have prehensile limbs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not apes or monkeys", "extracted_entity": null}
{"question": "is groundhog day used as a global season indicator? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a North American holiday.", "extracted_entity": null}
{"question": "did mercenaries fight for England in the Glorious Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they fought for William of Orange", "extracted_entity": null}
{"question": "was the sable depicted in Marvel comics anthropomorphic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the sable in Marvel comics is", "extracted_entity": null}
{"question": "are twinkies considered artisan made products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "were mollusks an ingredient in the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "is Christianity better for global warming than Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, both are equally bad for global warming", "extracted_entity": null}
{"question": "was Lord Voldemort taught by Professor Dumbledore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was taught by Professor Quirrell", "extracted_entity": null}
{"question": "would costumes with robes and pointy hats be helpful for Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, costumes with robes and pointy", "extracted_entity": null}
{"question": "are the knights in the Medieval Times show not authentic knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The knights are actually jousters and", "extracted_entity": null}
{"question": "could William Franklyn-Miller win a 2020 Nascar Cup Series race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he can get a ride with a", "extracted_entity": null}
{"question": "did Al-Farabi ever meet Mohammed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Al-Farabi lived in Bag", "extracted_entity": null}
{"question": "did the Pearl Harbor attack occur during autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it occurred on December 7,", "extracted_entity": null}
{"question": "have the Israelis played the Hammerstein Ballroom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "if you're reducing salt intake, are olives a healthy snack?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, olives are a healthy snack", "extracted_entity": null}
{"question": "do pediatricians perform abortions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. i'm a pediatrician", "extracted_entity": null}
{"question": "could casualties from deadliest war rival France's population?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Lorem ipsum backwards fail to demonstrate alliteration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "can you see Stonehenge from a window in Dusseldorf?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can.", "extracted_entity": null}
{"question": "can Reiki be stored in a bottle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki cannot be stored in a bott", "extracted_entity": null}
{"question": "would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it will not", "extracted_entity": null}
{"question": "would Tom Cruise ever insult L. Ron Hubbard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Tom Cruise is a Scientologist", "extracted_entity": null}
{"question": "does Lemon enhance the flavor of milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lemon is not a flavor en", "extracted_entity": null}
{"question": "was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because The Matrix was a better movie.", "extracted_entity": null}
{"question": "is the United States the largest exporter of Fair Trade products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States is the largest importer", "extracted_entity": null}
{"question": "is Snickers helpful for weight loss?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The candy has a high calorie", "extracted_entity": null}
{"question": "could the Jackson 5 play a full game of rugby with each other?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they would not be able to see", "extracted_entity": null}
{"question": "did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Isaac's father didn't commit", "extracted_entity": null}
{"question": "did the founders of the biggest city in Orange County, California speak Italian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "is an ammonia fighting cleaner good for pet owners?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because ammonia is a toxic", "extracted_entity": null}
{"question": "could you buy Hershey's Kisses in red foil with farthings after 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the foil was changed to silver in", "extracted_entity": null}
{"question": "could morphine cure HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. HIV is not curable.", "extracted_entity": null}
{"question": "would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she would be.", "extracted_entity": null}
{"question": "would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as impressive as the largest", "extracted_entity": null}
{"question": "would the chef at La Grenouille find salsa to be a strange request?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was the class valedictor", "extracted_entity": null}
{"question": "would toast for a vegan have margarine instead of butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, we use margarine in our to", "extracted_entity": null}
{"question": "did any cultures associate celery with death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Celery was considered a food of the dead in", "extracted_entity": null}
{"question": "are emus related to elks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. Emus are flight", "extracted_entity": null}
{"question": "is the Forbidden City host to a wooden rollercoaster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there is a wooden rollerco", "extracted_entity": null}
{"question": "are slime lilies in a different scientific family than asparagus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "in order to work in district management, does one need a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the district. Some districts have a", "extracted_entity": null}
{"question": "can you buy Reddit at Walmart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Reddit is not sold at Walmart", "extracted_entity": null}
{"question": "if you have black hair and want red hair, do you need bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can get red hair naturally.", "extracted_entity": null}
{"question": "is a curling iron necessary in curling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a curling iron is not necessary to", "extracted_entity": null}
{"question": "was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The milliner (played by Ed", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher Columbus", "extracted_entity": null}
{"question": "did Lamarck and Darwin agree about the origin of species diversity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "if you were at an Apple store, would most of the computers be running Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The computers would be running Windows.", "extracted_entity": null}
{"question": "can you make an MP3 from the Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the Golden Gate Bridge is not an", "extracted_entity": null}
{"question": "are there any chives hypothetically good for battling vampires?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you would have to eat them.", "extracted_entity": null}
{"question": "could a Diwali celebration feature a crustacean?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it\u2019s a lobster", "extracted_entity": null}
{"question": "do astronomers write horoscopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do study astrology", "extracted_entity": null}
{"question": "is the name of a mythical creature also the name of a Small Solar System body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A moon of Saturn", "extracted_entity": null}
{"question": "did Medieval English lords engage in fair trade with peasants?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Oil and gas companies are cutting jobs", "extracted_entity": null}
{"question": "do anatomical and symbolic hearts look remarkably different?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do", "extracted_entity": null}
{"question": "are you more likely to find bipolar disorder in a crowd than diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the rate of diabetes is", "extracted_entity": null}
{"question": "does having lip piercings lead to more expensive dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not necessarily.", "extracted_entity": null}
{"question": "are most mall Santa Claus actors white?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not always.", "extracted_entity": null}
{"question": "does the Taco Bell kitchen contain cinnamon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "could white rice go rancid before sesame seeds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "White rice can go rancid, but ses", "extracted_entity": null}
{"question": "can you listen to the entire Itunes song catalog in one year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all the songs in one year", "extracted_entity": null}
{"question": "do mollymawks live where albatrosses cannot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are smaller, and have shorter wings", "extracted_entity": null}
{"question": "would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she'd eat the whole bag.", "extracted_entity": null}
{"question": "are Mayors safe from harm from the federal government?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are pro-f", "extracted_entity": null}
{"question": "is it normal to see a red panda in Shanghai outside of a zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are endangered and are illegal", "extracted_entity": null}
{"question": "would a human following a hyena diet be unwelcome at a vegan festival?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a hyena is not a vegan", "extracted_entity": null}
{"question": "can DRL Racer X drone get across Brooklyn Bridge in 18 seconds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it takes 11 seconds to", "extracted_entity": null}
{"question": "would Temujin hypothetically be jealous of Charlemagne's conquests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not be jealous of Char", "extracted_entity": null}
{"question": "would a bodybuilder enjoy wearing a cast for several weeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think that they would enjoy the attention they get", "extracted_entity": null}
{"question": "can you carry a Chrysler in a laptop bag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can put your Chrysler", "extracted_entity": null}
{"question": "is Home Depot a one stop shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they carry all the necessary supplies for cru", "extracted_entity": null}
{"question": "is the Holy Land important to Eastern religions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Holy Land is important to all religions.", "extracted_entity": null}
{"question": "is immersion in virtual reality testable on cnidarians before humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Jack Dempsey ever witness Conor McGregor's fights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was dead before Conor was born", "extracted_entity": null}
{"question": "are black and white prison uniforms made to resemble a zebra?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are.", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. New Year's Eve is the", "extracted_entity": null}
{"question": "did Malcolm X avoid eating ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "would a snakebite hypothetically be a threat to T-1000?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A snakebite could be a threat to", "extracted_entity": null}
{"question": "is there a Harry Potter character named after Florence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes! And the character is a Muggle", "extracted_entity": null}
{"question": "could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they'd have to change the", "extracted_entity": null}
{"question": "was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and he was also the same iteration of", "extracted_entity": null}
{"question": "can Poland Spring make money in the Sahara?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it can make water", "extracted_entity": null}
{"question": "do people with mood disorders need permanent institutionalization?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "will Dustin Hoffman likely vote for Trump in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he'll vote for Trump in", "extracted_entity": null}
{"question": "can your psychologist say hello to you while you are out at the supermarket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I am not a psychologist.", "extracted_entity": null}
{"question": "did Johann Sebastian Bach influence heavy metal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bach did influence heavy metal. He was", "extracted_entity": null}
{"question": "are flag of Gabon colors found in rainbow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the colors of the flag of Gabon", "extracted_entity": null}
{"question": "does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the name C-SPAN refers to", "extracted_entity": null}
{"question": "can a grey seal swim in the same water as the subject of Moby Dick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "did Modern Family win a Slammy award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "would Mount Wycheproof be a breeze for Edmund Hillary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd say it's pretty much the", "extracted_entity": null}
{"question": "does Canada have a relationship with a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, we have a relationship with the Queen of", "extracted_entity": null}
{"question": "would a compact disc melt in magma?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but if it did", "extracted_entity": null}
{"question": "is Sea World hazardous to leopard seal's health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sea World is not hazardous to le", "extracted_entity": null}
{"question": "was being a mail carrier considered one of the most dangerous jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, being a mail carrier was not considered", "extracted_entity": null}
{"question": "were Depeche Mode heavily influenced by blues music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were, and you can hear it", "extracted_entity": null}
{"question": "do people with swallowing disorders need high viscosity drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, people with swallowing disorders need", "extracted_entity": null}
{"question": "can whole genome sequencing be used for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, whole genome sequencing is used to", "extracted_entity": null}
{"question": "is Anakin Skywalker from Star Wars associated with the color black?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is dressed in black.", "extracted_entity": null}
{"question": "do you need to schedule separate preventive healthcare and sickness visits? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes. You can schedule preventive", "extracted_entity": null}
{"question": "was the original Metroid groundbreaking for its polygons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was groundbreaking for its music", "extracted_entity": null}
{"question": "would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it's on the way", "extracted_entity": null}
{"question": "are any of the words that CAPTCHA stands for palindromes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the words are:", "extracted_entity": null}
{"question": "is the Jurassic era a tourist destination?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is a popular theme park.", "extracted_entity": null}
{"question": "can a dolphin keep a diary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can!", "extracted_entity": null}
{"question": "is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is hypothetically unimportant.", "extracted_entity": null}
{"question": "does Carl Linnaeus share the same final resting place as Michael Jackson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he's buried in Upps", "extracted_entity": null}
{"question": "could Eddie Murphy dial 911 in a car as a young child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He did.", "extracted_entity": null}
{"question": "do flying fish have good eyesight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have good eyesight.", "extracted_entity": null}
{"question": "does Justin Bieber vote in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he votes in November.", "extracted_entity": null}
{"question": "did Ice make people rich?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is J.D. Salinger's most successful work influential to killers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. see Catcher in the Rye", "extracted_entity": null}
{"question": "is it impossible for pigs to use pig latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not impossible for pigs to", "extracted_entity": null}
{"question": "can the theory of cultural hegemony explain global warming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "is March named after Jupiter's son in Roman mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is named after the Roman god Mars", "extracted_entity": null}
{"question": "can furniture be made of hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a hair chair is a real thing.", "extracted_entity": null}
{"question": "is SnapCap an example of a retail store?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. SnapCap is a retail store", "extracted_entity": null}
{"question": "did Rand Paul frequently swim in Lake Michigan during his undergraduate years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "would Hannah Nixon be proud of Richard Nixon following the Watergate scandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would.", "extracted_entity": null}
{"question": "would Tony Stark be considered a polymath?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Tony Stark is considered a polymath", "extracted_entity": null}
{"question": "is Supreme Court of the United States analogous to High Courts of Justice of Spain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Supreme Court of the United States is analog", "extracted_entity": null}
{"question": "does Amtrak run from NYC directly to the Moai location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The nearest station is in Wilmington", "extracted_entity": null}
{"question": "does Ronda Rousey avoid BBQ restaraunts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she eats them.", "extracted_entity": null}
{"question": "can someone in Uberlandia work for Mitsubishi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "do oak trees have leaves during winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, oak trees have leaves during winter.", "extracted_entity": null}
{"question": "is Argon near Neon on the periodic table of elements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Argon is located between Neon and Krypt", "extracted_entity": null}
{"question": "does the Constitution of the Philippines copy text from the British constitution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "did Ada Lovelace die tragically young for her era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she lived to 77, which", "extracted_entity": null}
{"question": "is dyslexia the most common intellectual disability in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, dyslexia is the most common", "extracted_entity": null}
{"question": "is the referee at a soccer match highly visible against the field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is highly visible against the field.", "extracted_entity": null}
{"question": "does Family Guy take place on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (I'm not kidding", "extracted_entity": null}
{"question": "could someone in the Canary Islands fish for largemouth bass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not in the Canary Islands, but", "extracted_entity": null}
{"question": "did the Presidency of Bill Clinton conclude with his impeachment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "did Marco Polo travel with Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Marco Polo was born in 1", "extracted_entity": null}
{"question": "could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a very slow episode", "extracted_entity": null}
{"question": "will a 2 Euro coin float across the Red Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you put it in a", "extracted_entity": null}
{"question": "have any murderers outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 4 men who have surv", "extracted_entity": null}
{"question": "can the US branch of government that has power over the military also have the power to veto?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the President cannot veto a bill passed", "extracted_entity": null}
{"question": "are Brussels sprout particularly good for adrenal fatigue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Brussels sprouts are not particularly", "extracted_entity": null}
{"question": "can oysters be used in guitar manufacturing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Oysters are used in guitar manufact", "extracted_entity": null}
{"question": "are parodies of the President of the United States illegal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "can lobster breathe in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's not a good idea", "extracted_entity": null}
{"question": "is Phobos part of the Andromeda galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a moon of Mars.", "extracted_entity": null}
{"question": "were all of Heracles's children present for his funeral pyre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, all of them", "extracted_entity": null}
{"question": "can Africanized bees be considered multicultural?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They can be considered multicultural because", "extracted_entity": null}
{"question": "is nickel dominant material in US 2020 nickels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "did Leonardo da Vinci lack contemporary peers in his home city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he had many contemporaries", "extracted_entity": null}
{"question": "was there fear leading up to the year 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there was a lot of fear", "extracted_entity": null}
{"question": "can you buy furniture and meatballs in the same store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not the same brand.", "extracted_entity": null}
{"question": "during the Cuban revolution, did the US experience a population boom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the population grew by 12%", "extracted_entity": null}
{"question": "could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could.", "extracted_entity": null}
{"question": "can a copy of The Daily Mirror sustain a campfire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A copy of The Daily Mirror is", "extracted_entity": null}
{"question": "would Bugs Bunny harm an olive tree in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bugs Bunny would not harm an", "extracted_entity": null}
{"question": "did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not even close", "extracted_entity": null}
{"question": "can actress Danica McKellar skip astronaut education requirements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she can\u2019t. She can\u2019", "extracted_entity": null}
{"question": "can you transport a coin along a sea of mercury?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot transport a coin along a sea", "extracted_entity": null}
{"question": "are the founders of Skype from Asia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are from Europe.", "extracted_entity": null}
{"question": "was the tenth Amendment to the Constitution written using Pitman shorthand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Tenth Amendment was written", "extracted_entity": null}
{"question": "can you save every HD episode of Game of Thrones on Samsung Galaxy A10e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't.", "extracted_entity": null}
{"question": "does Super Mario protagonist hypothetically not need continuing education classes in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "would a body builder prefer an elk burger over a beef burger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if he is a vegetarian, yes.", "extracted_entity": null}
{"question": "will NY Stock Exchange closing bell be heard in Universal Music Group's headquarters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not if they don't want it to be", "extracted_entity": null}
{"question": "could Saint Peter watch television?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "are all United States Aldi locations owned by the same company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Aldi stores in the United States", "extracted_entity": null}
{"question": "was the first Vice President of the United States an Ottoman descendant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the second one was.", "extracted_entity": null}
{"question": "would it be safe to have a jackfruit thrown at your head?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be safer to have", "extracted_entity": null}
{"question": "can a woman on average have a baby before wheat seed blooms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a woman cannot have a baby before whe", "extracted_entity": null}
{"question": "did Donald Trump come up with the idea for the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It was an idea from the 1", "extracted_entity": null}
{"question": "did Saint Augustine use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "do Koalas prefer Eucalyptus over meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Koalas prefer Eucalypt", "extracted_entity": null}
{"question": "are goats found on abyssal plains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are found on mountain sides.", "extracted_entity": null}
{"question": "are gorillas closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are more closely related to humans than", "extracted_entity": null}
{"question": "do Star Wars fans say \"beam me up\" often?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they don't mean it literally", "extracted_entity": null}
{"question": "does USA fail separation of church and state in multiple ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in many ways.", "extracted_entity": null}
{"question": "could Bob Marley's children hypothetically win tug of war against Kublai Khan's children?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not a holiday movie.", "extracted_entity": null}
{"question": "is dessert eaten before breakfast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dessert is not eaten before breakfast", "extracted_entity": null}
{"question": "was Krishna skilled at using the bow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "was Aristotle a member of the House of Lords?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "would Ibn Saud tolerate salsa music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he would have to burn all the cd", "extracted_entity": null}
{"question": "would a responsible bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Millie Bobby Brown is a child", "extracted_entity": null}
{"question": "is Bactrian Camel most impressive animal when it comes to number of humps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Bactrian Camel has two humps", "extracted_entity": null}
{"question": "do giraffes require special facilities at zoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they need high roofs, tall trees", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch kill hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a wheelbarrow full of starch", "extracted_entity": null}
{"question": "can a snake wear a snowshoe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it's a polar snake", "extracted_entity": null}
{"question": "can a false pope become a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he is a pope.", "extracted_entity": null}
{"question": "does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it has 240 smog", "extracted_entity": null}
{"question": "can dementia be cured with a cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dementia is a progressive disease", "extracted_entity": null}
{"question": "can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he can. He's a big", "extracted_entity": null}
{"question": "would Dave Chappelle pray over a Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "is a jellyfish safe from atherosclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is, because it doesn\u2019t", "extracted_entity": null}
{"question": "is the Riksdag a political entity in Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a Swedish institution.", "extracted_entity": null}
{"question": "did Linnaeus edit Darwin's draft of Origin of Species?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Darwin had already finished writing the manuscript", "extracted_entity": null}
{"question": "is Hermione Granger eligible for the Order of the British Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "was Saudi Aramco started due to an assassination?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Saudi government, who had been", "extracted_entity": null}
{"question": "should a Celiac sufferer avoid spaghetti?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Spaghetti is not a gluten", "extracted_entity": null}
{"question": "does Olympia Washington share name with Hephaestus's workshop location?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Olympia is a city in Washington state", "extracted_entity": null}
{"question": "could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the magazine would be 10", "extracted_entity": null}
{"question": "does Jason have anything in common with Dr. Disrespect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both have a penis.", "extracted_entity": null}
{"question": "is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The survival rate is the same in", "extracted_entity": null}
{"question": "are ropes required to operate a frigate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "did Brad Peyton need to know about seismology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not really.", "extracted_entity": null}
{"question": "is Ludacris in same music genre as 2000's Binaural?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're not.", "extracted_entity": null}
{"question": "would LeBron James hypothetically glance upwards at Yuri Gagarin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he were to see him in the", "extracted_entity": null}
{"question": "do you need a farmer to make a circuit board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Gandhi watch the television show Bonanza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "is Shakespeare famous because of the infinitive form?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would the crew of Apollo 15 have difficulty riding a unicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would have no difficulty riding a", "extracted_entity": null}
{"question": "do Squidward Tentacles and Alan Greenspan have different musical passions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they both like to listen to music.", "extracted_entity": null}
{"question": "can a minor replicate the double-slit experiment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The experiment requires an electron gun, which", "extracted_entity": null}
{"question": "can an anchovy born in 2020 survive 25th US census?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can survive 20", "extracted_entity": null}
{"question": "do you have to put on glasses to read a QR code?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you do not have to put on glass", "extracted_entity": null}
{"question": "would Seroquel be the first treatment recommended by a doctor to someone with depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think it would be", "extracted_entity": null}
{"question": "do women often need new shoes during their pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "would Persephone be a good consultant to a landscape architect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Persephone is a goddess of", "extracted_entity": null}
{"question": "is University of Pittsburgh easier to enter than FBI?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "FBI has higher requirement - 4.0", "extracted_entity": null}
{"question": "will the small intenstine break down a cotton ball?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it will", "extracted_entity": null}
{"question": "do the Eskimos sunbathe frequently?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't have Eskimos", "extracted_entity": null}
{"question": "was the French Revolution televised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it was in black and white.", "extracted_entity": null}
{"question": "is the most expensive color in the world Blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's Pink.", "extracted_entity": null}
{"question": "did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rosalind Franklin did not contribute to", "extracted_entity": null}
{"question": "can you substitute the pins in a bowling alley lane with Dustin Hoffman's Oscars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Dustin Hoffman's O", "extracted_entity": null}
{"question": "did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was the Duke of Orleans.", "extracted_entity": null}
{"question": "is the Sea of Japan landlocked within Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Sea of Japan is bordered by", "extracted_entity": null}
{"question": "is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not really. It\u2019s not the most efficient", "extracted_entity": null}
{"question": "will Lhamo Thondup be considered by Catholic Church to be a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he will be considered a holy man", "extracted_entity": null}
{"question": "would someone in Boston not receive the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Star is distributed across Canada.", "extracted_entity": null}
{"question": "is Christmas always celebrated on a Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can be on any day of the", "extracted_entity": null}
{"question": "are pancakes typically prepared in a pot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, pancakes are prepared in a pot", "extracted_entity": null}
{"question": "has Ringo Starr been in a relatively large number of bands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has been in the Beatles,", "extracted_entity": null}
{"question": "did Richard III know his grandson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. His grandson was born after he died", "extracted_entity": null}
{"question": "can Kane challenge Joe Biden in this year's primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "did Stone Cold Steve Austin wrestle in three different centuries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He wrestled in the 20", "extracted_entity": null}
{"question": "do embalmed bodies feel different at funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the body feels just like the body.", "extracted_entity": null}
{"question": "are leaves from coca good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "coca leaves are not good for gaining weight", "extracted_entity": null}
{"question": "did the Beatles write any music in the Disco genre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "is Nine Inch Nails a good guest for students in earliest grade to take Iowa tests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not.", "extracted_entity": null}
{"question": "did Harvey Milk ever run for governor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he ran for the democratic nom", "extracted_entity": null}
{"question": "is a bengal fox likely to see the Superbowl?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's possible", "extracted_entity": null}
{"question": "could all Tahiti hotels hypothetically accommodate US D-Day troops?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not at the same time", "extracted_entity": null}
{"question": "was Amazon involved in the lunar landing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it was involved in the moon landing", "extracted_entity": null}
{"question": "could the Spice Girls compete against \u017dRK Kumanovo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they could not", "extracted_entity": null}
{"question": "are saltwater crocodiles related to alligators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are both crocodiles.", "extracted_entity": null}
{"question": "would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the temperature on Triton is", "extracted_entity": null}
{"question": "has every astronaut survived their space journey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the first to die was Ed White,", "extracted_entity": null}
{"question": "can any person with a driver's license work in transport of aviation fuel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, only persons who have undergone special", "extracted_entity": null}
{"question": "could a snowy owl survive in the Sonoran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not. They are a boreal bird,", "extracted_entity": null}
{"question": "can you buy chlorine at a dollar store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's not pure chlor", "extracted_entity": null}
{"question": "is the voice of the Genie from Disney's Aladdin still alive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The voice of the Genie from Disney", "extracted_entity": null}
{"question": "was the amount of spinach Popeye ate unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It was. Spinach contains oxalates", "extracted_entity": null}
{"question": "would an explosion at a gunpowder storage facility result in a supersonic shock wave?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as strong as the shock wave", "extracted_entity": null}
{"question": "could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "can a person be diagnosed with a Great Depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sure, but I think you're talking about", "extracted_entity": null}
{"question": "could Godzilla have been killed by the Tohoku earthquake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Godzilla is invulnerable", "extracted_entity": null}
{"question": "is retail a job anybody can be suited for?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many different types of retail", "extracted_entity": null}
{"question": "does cell biology teach about the life cycle of Al Qaeda?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it teaches about the life cycle of", "extracted_entity": null}
{"question": "did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only by a hair.", "extracted_entity": null}
{"question": "has Burger King  contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because more people can drive to work,", "extracted_entity": null}
{"question": "would a snake have reasons to fear a honey badger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the honey badger would eat it", "extracted_entity": null}
{"question": "do most people only memorize slightly over half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, most people can\u2019t", "extracted_entity": null}
{"question": "does Happy Gilmore Productions CEO own a Torah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "does Mercury make for good Slip N Slide material?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is best to use a cond", "extracted_entity": null}
{"question": "is RoboCop director from same country as Gaite Jansen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are from different countries.", "extracted_entity": null}
{"question": "was the Parc des Princes fully operational during June of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd say no, unless you're", "extracted_entity": null}
{"question": "do worshipers of Shiva make a pilgrimage to the Holy Land?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Theodore Roosevelt was the", "extracted_entity": null}
{"question": "was it typical to see Johnny Cash on stage in a rainbow-colored outfit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was known for his crazy out", "extracted_entity": null}
{"question": "is slitting your wrists an unreliable suicide method?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the average male would be expected to", "extracted_entity": null}
{"question": "is Christopher Nolan indebted to Bob Kane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes he is", "extracted_entity": null}
{"question": "did Kim Il-sung network on LinkedIn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he should have", "extracted_entity": null}
{"question": "could all People's Volunteer Army hypothetically be transported on Symphony of the Seas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and in fact we did.", "extracted_entity": null}
{"question": "is Bern a poor choice for a xenophobic Swiss citizen to live?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is not a poor choice for a", "extracted_entity": null}
{"question": "would a vegan prefer a natural bongo drum over a synthetic one?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "is an inappropriate lullaby Love Song from November 11, 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the title is an inappropri", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "are kayaks used at the summit of Mount Everest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they're used at the base of", "extracted_entity": null}
{"question": "would someone on a keto diet be able to eat Dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Dosa can be made with low", "extracted_entity": null}
{"question": "was Raphael's paintings influenced by the country of Guam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Raphael's paintings were", "extracted_entity": null}
{"question": "is it normal to find parsley in multiple sections of the grocery store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. parsley is one of the most", "extracted_entity": null}
{"question": "was the subject of Parsifal taken from British folklore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a French legend, but the", "extracted_entity": null}
{"question": "do drummers need spare strings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, because it would increase the amount", "extracted_entity": null}
{"question": "is there a jukebox musical about a sweet transvestite from Transexual, Transylvania?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there should be!", "extracted_entity": null}
{"question": "did Malcolm X use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would the fastest tortoise win a race against a Chicago \"L\"?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the tortoise would be killed", "extracted_entity": null}
{"question": "does Hammurabi's Code violate Christians Golden Rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Hammurabi's Code is a", "extracted_entity": null}
{"question": "does the Roman god Vulcan have a Greek equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a Roman god.", "extracted_entity": null}
{"question": "can the Dalai Lama fit in a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it takes 3 cars", "extracted_entity": null}
{"question": "is watermelon safe for people with a tricarboxylic acid allergy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not.", "extracted_entity": null}
{"question": "is a Chinchilla breed of felis catus a type of rodent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Chinchilla is a rodent", "extracted_entity": null}
{"question": "can printing books in kanji instead of the Roman alphabet save trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it will use more trees", "extracted_entity": null}
{"question": "is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The sound barrier is not enough to", "extracted_entity": null}
{"question": "would a Rockette look odd with a moustache? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if they are a man.", "extracted_entity": null}
{"question": "would Hodor hypothetically be a good math mathematician?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he would be a good mathematician", "extracted_entity": null}
{"question": "would Hades and Osiris hypothetically compete for real estate in the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Hades is the god of the", "extracted_entity": null}
{"question": "would a German Shepherd be welcome in an airport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they're banned.", "extracted_entity": null}
{"question": "does a person need a college degree to become a bartender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's not a requirement.", "extracted_entity": null}
{"question": "is capsaicin associated with cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, capsaicin is not associated with", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger be unable to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's not a natural born citiz", "extracted_entity": null}
{"question": "when Hugh Jackman was a teacher, would he have taught The Great Gatsby?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he would have taught Harry Potter", "extracted_entity": null}
{"question": "is Fiat Chrysler associated with Japanese cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are American.", "extracted_entity": null}
{"question": "can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Immersion Baptism does not lead", "extracted_entity": null}
{"question": "would many meals heavy in brussels sprouts benefit someone on Coumadin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Brussels sprouts are a great", "extracted_entity": null}
{"question": "is it safe to share silverware with an HIV positive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not necessary to share silverware with an", "extracted_entity": null}
{"question": "do movies always show nerds as the losers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I've seen movies where the", "extracted_entity": null}
{"question": "are eagles and young bears both used as labels for skills-training youth groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "could ancient Tony Bennett have a baby in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could", "extracted_entity": null}
{"question": "can a lifeboat rescue people in the Hooke Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it cannot", "extracted_entity": null}
{"question": "would a stool be useful for a Lusotitan to reach the top of an almond tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a stool would be useless for a", "extracted_entity": null}
{"question": "would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The show was about history, and the", "extracted_entity": null}
{"question": "would somebody leave reiki with bruises?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they were getting reiki from someone", "extracted_entity": null}
{"question": "is there a popular Broadway character who is a missionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the musical \"Joseph and the", "extracted_entity": null}
{"question": "would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they have the same wealth in US", "extracted_entity": null}
{"question": "are months based on the solar cycle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The solar cycle is based on the months", "extracted_entity": null}
{"question": "do many fans of J.K Rowling know who Alan Rickman is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played Severus Snape in", "extracted_entity": null}
{"question": "could someone in a coma experience fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. We do not know the exact mechanism of", "extracted_entity": null}
{"question": "could Johnny Carson's children fill out a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could", "extracted_entity": null}
{"question": "would downloading Mario 64 on an emulator be legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is illegal.", "extracted_entity": null}
{"question": "are Tom and Jerry featured in a ride at Disneyland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there is a ride at Disney World", "extracted_entity": null}
{"question": "can a blind person tell time by Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as well as a sighted", "extracted_entity": null}
{"question": "are sea turtles enjoying life during quarantine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure.", "extracted_entity": null}
{"question": "has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he has probably had more people killed", "extracted_entity": null}
{"question": "does Paulo Coelho's wife make a living through speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she is a lawyer", "extracted_entity": null}
{"question": "is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "is Final Fantasy VI closer to beginning than end of its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Final Fantasy VI is a game that can be", "extracted_entity": null}
{"question": "were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Apollo 13 was not e", "extracted_entity": null}
{"question": "is chaff produced by hydropower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it is called \"hydra", "extracted_entity": null}
{"question": "is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he needs a passport to travel to", "extracted_entity": null}
{"question": "is Garfield known for hating italian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He hates spaghetti, las", "extracted_entity": null}
{"question": "is basil safe from Hypervitaminosis D?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "does a kangaroo incubate its offspring?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the female kangaroo incub", "extracted_entity": null}
{"question": "did J. Edgar Hoover take his calls in Langley, Virginia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but that\u2019s not where the F", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Elijah is not part of a", "extracted_entity": null}
{"question": "can a cheetah generate enough force to topple Big Show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the cheetah can't generate", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could have", "extracted_entity": null}
{"question": "was Mercedes-Benz associated with the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "did Johnny Carson win enough Emmy's to fill a carton if Emmy's were eggs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did win one.", "extracted_entity": null}
{"question": "can bottlenose dolphins hypothetically outbreed human women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They can outbreed human women in", "extracted_entity": null}
{"question": "if a baby was born on Halloween would they be a Scorpio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would be a Scorpio.", "extracted_entity": null}
{"question": "could the Great Wall of China connect the Dodgers to the White Sox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'd", "extracted_entity": null}
{"question": "would a honey badger fit inside an oven?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have to be a very", "extracted_entity": null}
{"question": "did Heracles famous labors exceed a baker's dozen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if that's a", "extracted_entity": null}
{"question": "does Guam have a state capital?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does.", "extracted_entity": null}
{"question": "are Disney's seven dwarves the original ones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were the first to be called the", "extracted_entity": null}
{"question": "does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he goes to the UN in New York", "extracted_entity": null}
{"question": "is Jesse W. Moore a potential recipient of a Snoopy-themed award from NASA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "has Gorillaz creator been in more bands than Bernard Sumner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, 3 more", "extracted_entity": null}
{"question": "is it safe to eat kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, kidneys are safe to eat,", "extracted_entity": null}
{"question": "is zoology unconcerned with strigoi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Zoology is concerned with the study of animals.", "extracted_entity": null}
{"question": "did the phone Alexander Graham Bell use have call waiting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was called a \"phone\"", "extracted_entity": null}
{"question": "were paparazzi directly responsible for the death of Amy Winehouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "on August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Tonight Show Starring Jimmy Fallon air", "extracted_entity": null}
{"question": "are more watermelons grown in Brazil than Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Brazil produces about 10 times more", "extracted_entity": null}
{"question": "does store bought milk have cream at the top?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know if it does, but", "extracted_entity": null}
{"question": "could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did eggs need to be kept cold in the middle ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they would have been kept in a", "extracted_entity": null}
{"question": "would Topa Inca Yupanqui have encountered the western honey bee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a popular science novel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a novel that uses popular", "extracted_entity": null}
{"question": "was Amy Winehouse a fan of Star Wars: Rogue One?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The Star Wars: Rogue One sound", "extracted_entity": null}
{"question": "can shooting bald eagle get a person more prison time than Michael Vick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it can", "extracted_entity": null}
{"question": "could you drive a Rowe 550 to the 2008 Summer Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the 550 was designed to", "extracted_entity": null}
{"question": "can you go water skiing on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Venus has no liquid water.", "extracted_entity": null}
{"question": "would a Wolverine and a Lynx be hard to tell apart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could be hard to tell apart.", "extracted_entity": null}
{"question": "could Plato have agreed with the beliefs of Jainism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Jainism is an athe", "extracted_entity": null}
{"question": "do bald eagles nest on Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "could James Brown's ex-wives hold a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they'd have to use wooden", "extracted_entity": null}
{"question": "should Peter Griffin be an expert at the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is a simpleton", "extracted_entity": null}
{"question": "did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Charles Manson was the killer of 1", "extracted_entity": null}
{"question": "is there any absolute way to prevent abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no way to prevent abortion", "extracted_entity": null}
{"question": "is Saturn named after king of gods in Greek mythology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was named after the Roman god Sat", "extracted_entity": null}
{"question": "were any of despised Pope Alexander VI's descendants canonized?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, one.", "extracted_entity": null}
{"question": "would it be difficult to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the high atmospheric pressure", "extracted_entity": null}
{"question": "can chemicals in onion help create a thermonuclear bomb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chemicals in onion are not", "extracted_entity": null}
{"question": "are anchovies associated with Italian food?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "did Operation Barbarossa or Barbarossa's last expedition succeed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The German army was able to reach the Volga", "extracted_entity": null}
{"question": "has numerology helped shape hotel layouts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hotel layouts are influenced by numerology", "extracted_entity": null}
{"question": "did The Who have to cancel tours due to World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, The Who canceled tours due", "extracted_entity": null}
{"question": "can an art dealer buy Boeing 737-800 with a Da Vinci painting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can", "extracted_entity": null}
{"question": "would a crocodile survive longer in Great Salt Lake than alligator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Salt water is denser than fresh water", "extracted_entity": null}
{"question": "would a clouded leopard encounter an awake pangolin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "does acupuncture cause pain in many people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact, the World Health Organization", "extracted_entity": null}
{"question": "are twins always born during the same year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, twins are born at different times.", "extracted_entity": null}
{"question": "could a nymph tick pass through a standard hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The holes are too large.", "extracted_entity": null}
{"question": "would WWF be angrier if you killed koala instead of black swan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "does New Year's Day always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It can occur on any day of the", "extracted_entity": null}
{"question": "is a pottery kiln inappropriate for use with glass blowing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The kiln is a pottery kiln,", "extracted_entity": null}
{"question": "do people of the Iyer caste eat meat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, people of the Iyer caste do", "extracted_entity": null}
{"question": "do carpenters understand geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they don't understand the mathematics", "extracted_entity": null}
{"question": "did the crew of Apollo 15 take pictures of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The crew of Apollo 15", "extracted_entity": null}
{"question": "are human footprints absent from Mount Sharp?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "could Tom Cruise explain mental auditing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he'd be a little vague", "extracted_entity": null}
{"question": "is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not. It's", "extracted_entity": null}
{"question": "would drinking a glass of lemonade provide Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a glass of lemonade provides vit", "extracted_entity": null}
{"question": "does a game engine have a fuel injector?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for the car", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his father for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Greeks did not build the l", "extracted_entity": null}
{"question": "is the Joker in a healthy romantic relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He is not.", "extracted_entity": null}
{"question": "would an adherent of Zoroastrianism consult the Quran for religious guidance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Quran is not a book", "extracted_entity": null}
{"question": "would a diet of ice eventually kill a person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it would depend on how much ice and how long", "extracted_entity": null}
{"question": "can binary numbers and standard alphabet satisfy criteria for a strong password?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not very good", "extracted_entity": null}
{"question": "is Phobos (moon) name origin similar to Roman god Pavor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Phobos is from Greek. P", "extracted_entity": null}
{"question": "would members of Blue Lives Matter support every element of Grand Theft Auto III?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it glorifies murder and rob", "extracted_entity": null}
{"question": "will twenty pea pods contents cover entire chess board?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are filled with peas", "extracted_entity": null}
{"question": "was the Joker an enemy of the Avengers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was an enemy of Batman", "extracted_entity": null}
{"question": "could Christopher Nolan borrow pants from Danny Devito?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Danny Devito is too short", "extracted_entity": null}
{"question": "could you watch Naruto and Puzzle Place on the same channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Puzzle Place was on Nick", "extracted_entity": null}
{"question": "is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Seoul, South Korea", "extracted_entity": null}
{"question": "was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the peak was hidden from the view of", "extracted_entity": null}
{"question": "does Fraktur have a sordid history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was used by the Nazis to", "extracted_entity": null}
{"question": "would a hedgehog avoid animals without a spinal cord?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "can you worship Ahura Mazda at a mosque?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can't worship any other god", "extracted_entity": null}
{"question": "would a moose hypothetically be too much for a minotaur to devour whole?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a moose would be too much for", "extracted_entity": null}
{"question": "is Gandalf hypothetically a formidable foe for Charmed's Barbas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but Charmed's Barbas is", "extracted_entity": null}
{"question": "do suburbs encourage the use of cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, suburbs encourage the use of", "extracted_entity": null}
{"question": "could Al Capone have read works from the Harlem Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was dead by then", "extracted_entity": null}
{"question": "can pancreas removal cause bankruptcy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not possible to go bankrupt", "extracted_entity": null}
{"question": "is shrimp prevalent in Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is, and it is also pre", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Allah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Marco Rubio is not Muslim.", "extracted_entity": null}
{"question": "does The Doctor keep his ship in his childhood home?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He lives in a flat in London.", "extracted_entity": null}
{"question": "did Archduke Franz Ferdinand of Austria participate in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was killed in June 19", "extracted_entity": null}
{"question": "is a beard is moss that grows on a human?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a beard is not moss that", "extracted_entity": null}
{"question": "is there radiation where Nikola Tesla once worked?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The city of Belgrade is in the", "extracted_entity": null}
{"question": "would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would have prayed to the Heb", "extracted_entity": null}
{"question": "could one Amazon share ever buy twenty year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would you have to wear a coat when on Phobos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you would need a coat on Phob", "extracted_entity": null}
{"question": "is Cambodia too expensive for 2020 richest billionaire to own?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cambodia is too expensive for any rich person to", "extracted_entity": null}
{"question": "did Emma Stone pursue a higher education?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did. She attended the Lee Str", "extracted_entity": null}
{"question": "was San Antonio the site of a major battle in the 19th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Battle of the Alamo", "extracted_entity": null}
{"question": "can the United Nations Framework Convention on Climate Change be held at the Javits Center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the convention is being held in Copen", "extracted_entity": null}
{"question": "can music be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "could an ocelot outrun a kindergartner? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the kindergartner", "extracted_entity": null}
{"question": "was Elizabeth II the Queen during the Persian Gulf War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was the Queen of the UK.", "extracted_entity": null}
{"question": "would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not likely", "extracted_entity": null}
{"question": "did Woodrow Wilson consider Blacks to be equal members of society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "if your electric stove has a glass top, should you use cast iron skillets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If the glass top has a coating that is", "extracted_entity": null}
{"question": "can soup be eaten with the hands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, soup can be eaten with the hands", "extracted_entity": null}
{"question": "did Cleopatra have ethnicity closer to Egyptians than Greeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not even close.", "extracted_entity": null}
{"question": "does the judo rank system reach the triple digits?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The system stops at 5th dan", "extracted_entity": null}
{"question": "was the son of Tsar Nicholas a daredevil?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was a daredevil.", "extracted_entity": null}
{"question": "did Bill Gates achieve Latin honors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even a dozen.", "extracted_entity": null}
{"question": "if Goofy were a pet, would he need heartworm prevention?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Goofy would need heartworm", "extracted_entity": null}
{"question": "did Richard Wagner support the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was historical Dracula from a town in Bucharest?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Dracula was from Transylvania", "extracted_entity": null}
{"question": "is electricity necessary to balance an account in Microsoft Excel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is very helpful.", "extracted_entity": null}
{"question": "did Pink Floyd have a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But they did have a song called \u201c", "extracted_entity": null}
{"question": "has the creator of Futurama lived in multiple centuries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was born in 196", "extracted_entity": null}
{"question": "is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. The sternum is", "extracted_entity": null}
{"question": "did Elizabeth I of England own any viscose fiber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she didn't.", "extracted_entity": null}
{"question": "could a cat ride Panzer VIII Maus tank missile from Barcelona to Madrid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a slow journey.", "extracted_entity": null}
{"question": "did Jack Dempsey fight the current WBC heavyweight champion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jack Dempsey fought Gene Tunney", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper at a latitude of 75\u00b0 N?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would a student in eleventh grade be unable to run for president of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he would not be able to run", "extracted_entity": null}
{"question": "was Amy Winehouse familiar with Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died in 2011", "extracted_entity": null}
{"question": "can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if she is the singer of the winning", "extracted_entity": null}
{"question": "are Scottish people descended from Mary, Queen of Scots part French?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "are quadrupeds represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no quadrupeds on Chinese", "extracted_entity": null}
{"question": "is the Louvre in billionaire George Soros's price range?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Soros is a billionaire, but", "extracted_entity": null}
{"question": "would a duke hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Tsar of Russia was the ruler of", "extracted_entity": null}
{"question": "can a jet plane be made without society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the society is not based on the", "extracted_entity": null}
{"question": "would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jimmy Vee would not be elig", "extracted_entity": null}
{"question": "could Ryan Crouser throw a bengal fox with ease?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's a heptath", "extracted_entity": null}
{"question": "would a physician be unlikely to recommend Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Reiki is a pseudoscientific practice,", "extracted_entity": null}
{"question": "can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he can fund the debt of", "extracted_entity": null}
{"question": "can Burundi's communicate with citizens of New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Burundi's can communicate with", "extracted_entity": null}
{"question": "is Morocco an ideal location for water skiing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "did Al Capone carry a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he had a gun.", "extracted_entity": null}
{"question": "was Mark Twain a struggling inventor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is B's place in alphabet same as Prince Harry's birth order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The birth order of Prince Harry is", "extracted_entity": null}
{"question": "would a vegan eat a traditional Paella dish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a vegan could make a veg", "extracted_entity": null}
{"question": "is cow methane safer for environment than cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. cow methane is 20", "extracted_entity": null}
{"question": "was Darth Vader monogamous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "were there footprints on the moon in 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there were not.", "extracted_entity": null}
{"question": "does Pantheon in Paris have a unique name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s not unique, but it", "extracted_entity": null}
{"question": "can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Arvanites could understand some of", "extracted_entity": null}
{"question": "would a Beaver's teeth rival that of a Smilodon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they wouldn't.", "extracted_entity": null}
{"question": "can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Tropicana Field is not large enough", "extracted_entity": null}
{"question": "do calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "are all cucumbers the same texture?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. Some are soft and", "extracted_entity": null}
{"question": "would Library of Alexandria need less shelf space than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Library of Alexandria would be", "extracted_entity": null}
{"question": "would a spider wasp be more effective than a bullet ant to stop a criminal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a spider wasp would be more", "extracted_entity": null}
{"question": "would a triples tandem bike support Apollo 15 crew?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would be a 3-", "extracted_entity": null}
{"question": "if one of your feet is in a leg cast, should the other be in a sandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the cast should be on both feet.", "extracted_entity": null}
{"question": "did Klingons appear in the movie The Last Jedi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "can you find Depala's race in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they're the Tauren.", "extracted_entity": null}
{"question": "would a Dodo hypothetically tower over Ma Petite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not.", "extracted_entity": null}
{"question": "can the Very Large Telescope observe the largest mountain on Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "was Morris County named after a chief justice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The county was named for Chief Justice of", "extracted_entity": null}
{"question": "is the Gujarati script the same category of script as Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Kanji is a logographic script.", "extracted_entity": null}
{"question": "did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be unusual to have a", "extracted_entity": null}
{"question": "if Martin Luther did one theses a day would he run out in half a year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he did one theses a day", "extracted_entity": null}
{"question": "are honey badgers and hyenas anatomically dissimilar? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. ?", "extracted_entity": null}
{"question": "did Al Pacino act in a movie during World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was born in 194", "extracted_entity": null}
{"question": "could you watch a new Seinfeld episode every day for a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "did Barack Obama participate in the Reformation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn't.", "extracted_entity": null}
{"question": "did President William Howard Taft read DC Comics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He read a lot of", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's the thought that counts", "extracted_entity": null}
{"question": "are there Americans still enlisted in the Confederate States Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are Confederate veterans still living", "extracted_entity": null}
{"question": "are right wing Amreicans opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are opposed to marxism.", "extracted_entity": null}
{"question": "can a sea turtle play tennis using a tennis racket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it can't", "extracted_entity": null}
{"question": "in a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The American Woodcock is a small bird with a", "extracted_entity": null}
{"question": "is shoe soup innocuous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but I wouldn't eat it.", "extracted_entity": null}
{"question": "did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "did Holy Saturday 2019 have special significance to pot smokers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it had special significance to me", "extracted_entity": null}
{"question": "do you need lactobacillus to make pickles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pickles are a fermented food", "extracted_entity": null}
{"question": "would a Gray Whale fit easily in an above ground pool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Gray Whale is too big for", "extracted_entity": null}
{"question": "will a celibate cleric likely suffer a stoning in Somalia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a woman who has sex with a", "extracted_entity": null}
{"question": "could Little Women have been a book read by veterans of the civil war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It would have been a book read by", "extracted_entity": null}
{"question": "is MIX a word and a roman numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a word, but it", "extracted_entity": null}
{"question": "would Downton Abbey finale viewership defeat every Kazakhstan citizen in tug of war?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no.", "extracted_entity": null}
{"question": "was dynamite used during Middle Ages warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dynamite was invented in 1", "extracted_entity": null}
{"question": "is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is not", "extracted_entity": null}
{"question": "can Kate Gosselin's household fill out a Bandy team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. And that's a good thing.", "extracted_entity": null}
{"question": "is the tree species that the name Leipzig refers to an evergeen tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "will Gremlins sequels tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I'm pretty sure that's", "extracted_entity": null}
{"question": "is Snow White an example of good consent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Snow White is not sexually active", "extracted_entity": null}
{"question": "is MF Doom a Fantastic Four villain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "would most grand masters know what the French Defense is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they would have a different opening re", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing style of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There are options for students who struggle to understand the", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing domestic canine reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the dog was an endangered", "extracted_entity": null}
{"question": "could a monolingual American read Lenovo's native name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "can 1980 United States presidential election result be considered a photo finish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the difference between the two candidates is", "extracted_entity": null}
{"question": "did Cynthia Powell celebrate a silver anniversary with John Lennon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She did not.", "extracted_entity": null}
{"question": "is it possible for biologist Rachel Carson to have flown to the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not possible for biologist Rach", "extracted_entity": null}
{"question": "did Confederate States Army influence West Point fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Confederate States Army influenced West Point", "extracted_entity": null}
{"question": "would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he's already an established actor", "extracted_entity": null}
{"question": "is the Mona Lisa in the same museum as the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are in different museums", "extracted_entity": null}
{"question": "does the land in close proximity to beaver dams suffer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the beavers drain the", "extracted_entity": null}
{"question": "was Jackson Pollock trained by Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was trained by the Abstract Expression", "extracted_entity": null}
{"question": "did Francois Mitterrand ever meet Barak Obama while they both held the position of President?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Fran\u00e7ois Mitterrand was President of", "extracted_entity": null}
{"question": "would Carrie Poppy be likely to trust a psychic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, and I don't think she'", "extracted_entity": null}
{"question": "would stargazers prefer binoculars over a telescope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not prefer binoculars", "extracted_entity": null}
{"question": "is it possible to binge entire EastEnders series without water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is possible.", "extracted_entity": null}
{"question": "was Mother Theresa a follower of atheism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO!", "extracted_entity": null}
{"question": "is the BBC World Service hosted in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's hosted in the UK.", "extracted_entity": null}
{"question": "did Melania Trump have same profession as Olga Kurylenko?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Melania Trump was a model, while", "extracted_entity": null}
{"question": "was Robert Downey Jr. a good role model as a young man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "should a finished website have lorem ipsum paragraphs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and you can use the same paragraphs", "extracted_entity": null}
{"question": "would Jacques Du\u00e8ze have been friends with Richard Dawkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they would have had a lot in common", "extracted_entity": null}
{"question": "would Gordon Ramsey use uranium as a seasoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s not a food", "extracted_entity": null}
{"question": "does Robert De Niro use a microscope at work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he uses a magnifying glass.", "extracted_entity": null}
{"question": "is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he is guilty of making the same", "extracted_entity": null}
{"question": "are pirate lieutenants like navy lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn't.", "extracted_entity": null}
{"question": "does Homer Simpson need two hands worth of fingers to count to 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he needs three hands worth of fingers.", "extracted_entity": null}
{"question": "can you see the moon in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can.", "extracted_entity": null}
{"question": "is the Asian black bear multicolored?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is black with white patches", "extracted_entity": null}
{"question": "can United States Secretary of State do crimes in U.K. without being arrested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "did Alice's Adventures in Wonderland inspire Macbeth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're completely unrelated.", "extracted_entity": null}
{"question": "could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because humans can not live without oxygen", "extracted_entity": null}
{"question": "was England directly involved in the Arab-Israeli conflict?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The UK was not a party to the", "extracted_entity": null}
{"question": "can an adult male stand on top Donatello's bronze David and touch the Sistine Chapel ceiling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the David is at a height of", "extracted_entity": null}
{"question": "was Dioskourides a lapidary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was a goldsmith, not a", "extracted_entity": null}
{"question": "would a CEO typically clean the toilets in a company's building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "can you purchase General Motors products at a movie theater?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but you can buy popcorn,", "extracted_entity": null}
{"question": "did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Xena herself, Lucy Lawless,", "extracted_entity": null}
{"question": "could all of the people who pass through 30th Street Station every day fit in Dorton Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The number of people who pass through", "extracted_entity": null}
{"question": "can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you don\u2019t mind", "extracted_entity": null}
{"question": "could all the unemployed people due to 1933 Great Depression fit in Tiger Stadium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Tiger Stadium could only hold 5", "extracted_entity": null}
{"question": "would a duck ever need a Caesarean section?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be called an egg-", "extracted_entity": null}
{"question": "is nickel a better payout than mercury if given a dollar per atomic number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "does Pikachu like Charles Darwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's been scientifically proven that", "extracted_entity": null}
{"question": "can Simon Cowell vote for the next Supreme Court judge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he's not an American citiz", "extracted_entity": null}
{"question": "is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lactobacillus is not", "extracted_entity": null}
{"question": "is US route 1 dominated by historically red states?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but also yes.", "extracted_entity": null}
{"question": "can a cell fit inside of a shoebox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a cell can't fit inside of", "extracted_entity": null}
{"question": "is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so. It", "extracted_entity": null}
{"question": "can Tame Impala's studio band play a proper game of Hot Potato?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they're not allowed to touch", "extracted_entity": null}
{"question": "is the title of Shirley Bassey's 1971 diamond song a true statement?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a lie.", "extracted_entity": null}
{"question": "would P. G. Wodehouse be taught in second grade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even in first grade.", "extracted_entity": null}
{"question": "can Curiosity take samples of rocks from Lacus Temporis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Curiosity cannot take samples of rocks", "extracted_entity": null}
{"question": "is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a freebie in", "extracted_entity": null}
{"question": "would Phineas and Ferb enjoy winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They would love it. They love winter. They", "extracted_entity": null}
{"question": "could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, SNL was a 10 year", "extracted_entity": null}
{"question": "can children become lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, children can become lieutenants.", "extracted_entity": null}
{"question": "could you drive from New England to a Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I could drive to a Sainsbury's", "extracted_entity": null}
{"question": "do seven McDonald's hamburgers exceed USDA recommended fat allowance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not by much", "extracted_entity": null}
{"question": "was Mozart accused of stealing from Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The reason is that Wagner\u2019s oper", "extracted_entity": null}
{"question": "will Justin Bieber take over Mike Pence's position in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he will be running for president in", "extracted_entity": null}
{"question": "did any citizen of San Antonio vote for Boris Johnson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "does being good at guitar hero make you a good guitarist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a game.", "extracted_entity": null}
{"question": "can Roman numerals fill the normal number of Sudoku box options?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they can't be used for", "extracted_entity": null}
{"question": "is a Cassowary safer pet than a crane?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cranes are much less likely to", "extracted_entity": null}
{"question": "was proofreading Edgar Allan Poe works lucrative?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was", "extracted_entity": null}
{"question": "was Hillary Clinton's deputy chief of staff in 2009 baptised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was not.", "extracted_entity": null}
{"question": "can dessert be made with vegetables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "of course!", "extracted_entity": null}
{"question": "would Othello be Shakespeare's play to buy Scheherazade most time with king?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would be easier for Othello", "extracted_entity": null}
{"question": "did Jon Brower Minnoch suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he suffered from a psychological disorder", "extracted_entity": null}
{"question": "does ancient Olympics crown fail to hide tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "can someone with dermatitis be a hand model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the dermatitis is not too", "extracted_entity": null}
{"question": "is it bad to have lactic acid in your body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, lactic acid is the product of an", "extracted_entity": null}
{"question": "are cucumbers often found in desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not", "extracted_entity": null}
{"question": "are blue lips normal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, blue lips are normal.", "extracted_entity": null}
{"question": "can someone with celiac disease have potato vodka?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, if it's made from potatoes", "extracted_entity": null}
{"question": "are deaf people left out of enjoying music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Pope's reign was very short. He", "extracted_entity": null}
{"question": "would Cardi B. benefit from soy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she would, because it's a", "extracted_entity": null}
{"question": "are the names of The Powerpuff Girls alliterative? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "did the leader of Heaven's Gate consider himself a prophet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He considered himself a \"savior", "extracted_entity": null}
{"question": "would King Leonidas have succeeded with an army the size of Mozart's compositions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it would have been a different kind", "extracted_entity": null}
{"question": "do Elementary School students typically need graphing calculators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You will need a calculator for this", "extracted_entity": null}
{"question": "can Family of Barack Obama ride comfortably in 2020 Jaguar F Type?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can not.", "extracted_entity": null}
{"question": "could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but that doesn't mean that all", "extracted_entity": null}
{"question": "does the Eighth Amendment to the United States Constitution protect freedom of speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The First Amendment to the United", "extracted_entity": null}
{"question": "do most fans follow Katy Perry for gospel music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and he\u2019s awesome.", "extracted_entity": null}
{"question": "are rainbows devoid of color made by mixing yin and yang colors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rainbows are caused by refraction", "extracted_entity": null}
{"question": "did Christopher Columbus sail representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He sailed for Spain.", "extracted_entity": null}
{"question": "can an Asian black bear use chopsticks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they don't have to", "extracted_entity": null}
{"question": "did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they did collaborate with the Brothers", "extracted_entity": null}
{"question": "would a Durian be dangerous if it fell on your head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you were in the way", "extracted_entity": null}
{"question": "can nitric acid break the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it takes a lot of time.", "extracted_entity": null}
{"question": "does autopilot rely on fossil fuels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "were muskets used in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they were", "extracted_entity": null}
{"question": "do tourists prefer Tuvalu to Niue?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They don't.", "extracted_entity": null}
{"question": "could you windsurf in Puerto Rico during Hurricane Maria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I could windsurf in Puerto Rico", "extracted_entity": null}
{"question": "did Northwest Airlines' longevity surpass Betty White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Betty White was 100 years", "extracted_entity": null}
{"question": "is myocardial infarction a brain problem?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is Newspeak considered very straightforward?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Newspeak is very difficult to understand", "extracted_entity": null}
{"question": "was Pope Alexander VI's origin country least represented in papal history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was from Spain", "extracted_entity": null}
{"question": "would you find olives at a heladeria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I would not find olives at a", "extracted_entity": null}
{"question": "do people celebrate Earth Day with a ceremonial tire fire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They don't.", "extracted_entity": null}
{"question": "did France win the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the bourgeoisie did.", "extracted_entity": null}
{"question": "does menthol make cigarettes less addictive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "could Reza Shah be related to Queen Elizabeth I?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there is no connection between them.", "extracted_entity": null}
{"question": "is it possible that June got its name from mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are several possibilities.", "extracted_entity": null}
{"question": "is it possible to get killed walking to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can get killed by the wind", "extracted_entity": null}
{"question": "is Canon Inc. a Kabushiki gaisha?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "can a diamond float on water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a diamond can float on water", "extracted_entity": null}
{"question": "does horseradish have a fetlock?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it does have a root.", "extracted_entity": null}
{"question": "can you hide a pet macaque under your desk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can.", "extracted_entity": null}
{"question": "can Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Josh Blue can participate in Paral", "extracted_entity": null}
{"question": "would fans of Jonathan Larson be unaware of HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if you know anything about Jonathan Larson, you", "extracted_entity": null}
{"question": "would Carolina Reaper decrease sales if added to all US salsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it would be too spicy for", "extracted_entity": null}
{"question": "will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "10 points each", "extracted_entity": null}
{"question": "was the Eiffel tower used as a symbol of the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was used as a symbol of the", "extracted_entity": null}
{"question": "does James Watson believe that Africans are inferior to Europeans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "is The Joy of Painting TV show still producing new episodes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are still producing new episodes.", "extracted_entity": null}
{"question": "is the skull formed as one whole bone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the skull is formed as several b", "extracted_entity": null}
{"question": "was  Godfrey of Bouillon an Islamaphobe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.  He was the first crusader", "extracted_entity": null}
{"question": "are lengths measured in metres in the UK?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "do quadragenarian's have little memory capacity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are more likely to have a", "extracted_entity": null}
{"question": "did Jackie Kennedy wear Dolce & Gabbana to her husband's inauguration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she wore Oleg Cassini", "extracted_entity": null}
{"question": "did Larry King sign the Magna Carta?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did sign the Declaration of", "extracted_entity": null}
{"question": "will a Euro sink in water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is in water.", "extracted_entity": null}
{"question": "would Iggy Pop travel with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he's a fan of his", "extracted_entity": null}
{"question": "is Drew Carey important to the history of wrestling?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was the first wrestler to get", "extracted_entity": null}
{"question": "did Jay-Z ever collaborate with Louis Armstrong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did collaborate with Beyon", "extracted_entity": null}
{"question": "is CAS number 8009-03-8 harmful for a rash?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not harmful for a r", "extracted_entity": null}
{"question": "would a model be appropriate to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The original starred a young woman who", "extracted_entity": null}
{"question": "would Michael J Fox qualify for the Army Rangers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not qualify for the Army", "extracted_entity": null}
{"question": "do salmon mate in the Caspian Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "could Chuck Norris ride a horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he couldn't bring it down", "extracted_entity": null}
{"question": "can Ford F-350 tow entire Yale University student body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Ford F-350 can to", "extracted_entity": null}
{"question": "are any of J.K. Rowling's books in the genre of And Then There Were None?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the dog is a hound.", "extracted_entity": null}
{"question": "would an actuary be confused about what prime numbers are?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, actuaries are smart people.", "extracted_entity": null}
{"question": "was 1941 Operation Barbarossa related to The Crusades?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Crusades were in the", "extracted_entity": null}
{"question": "can ham make a cut seal up quicker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ham will make a cut seal up", "extracted_entity": null}
{"question": "can Chinese mountain cat survive in the orbit? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Chinese mountain cat can survive in", "extracted_entity": null}
{"question": "are there special traffic laws associated with funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "did George Washington drive a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he drove a buggy.", "extracted_entity": null}
{"question": "is metal a type of folk music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a type of folk music.", "extracted_entity": null}
{"question": "did Neanderthals use arithmetic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "would East India Company prefer China's modern trade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. East India Company would prefer the traditional trade", "extracted_entity": null}
{"question": "could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, every citizen of Samoa could send", "extracted_entity": null}
{"question": "would World War II have been the same without Alan Turing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was the man who cracked the", "extracted_entity": null}
{"question": "would General Zod prefer an iPhone over a Samsung Galaxy S4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I\u2019m sure he would.", "extracted_entity": null}
{"question": "can 200 men end to end cover Great Pyramid of Giza's base?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "does meat from cows fed only grass taste more like wild game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, meat from cows fed only grass t", "extracted_entity": null}
{"question": "did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was murdered by the state.", "extracted_entity": null}
{"question": "do people associate greyhounds with the movie 'Homeward Bound'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would it be unusual to find a yellow perch in the Red Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be more unusual to find", "extracted_entity": null}
{"question": "can every digit in Pi be memorized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it is a good idea to memor", "extracted_entity": null}
{"question": "is Norman Oklahoma named after a viking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did Alfred Hitchcock include internet slang in his films?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Supernatural break 2001 CW debuting shows seasons record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's bigger on the inside.", "extracted_entity": null}
{"question": "would Jean Harris's victim have avoided lentils?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he had been able to.", "extracted_entity": null}
{"question": "would an art dealer prize a print of a Van Goh? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a Van Goh.", "extracted_entity": null}
{"question": "is it comfortable to wear sandals outside Esperanza Base?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the sandals must be closed-", "extracted_entity": null}
{"question": "does welding with acetylene simulate the temperature of a star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn't", "extracted_entity": null}
{"question": "is pickled cucumber ever red?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be red.", "extracted_entity": null}
{"question": "is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I believe so.", "extracted_entity": null}
{"question": "do any Islamic dominated countries have a Starbucks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are Starbucks in Egypt,", "extracted_entity": null}
{"question": "was Achilles a direct descendent of Gaia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a direct descendent of A", "extracted_entity": null}
{"question": "was Noah associated with a dove?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Noah was associated with a raven.", "extracted_entity": null}
{"question": "will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Roy Rogers is a small chain,", "extracted_entity": null}
{"question": "are Saturn's famous rings solid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are made of billions of particles", "extracted_entity": null}
{"question": "would menu at Chinese Starbucks be familiar to an American?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be familiar", "extracted_entity": null}
{"question": "would Lee Sedol understand the complexities of the Sicilian Defence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he would understand the basic ideas behind it.", "extracted_entity": null}
{"question": "would a northern fur seal pass a driving test?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. i would say that they would.", "extracted_entity": null}
{"question": "was Hundred Years' War a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was a misnomer.", "extracted_entity": null}
{"question": "did Nine Inch Nails inspire Aretha Franklin's sound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "does giant panda have colors that differ from yin yang?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the fur of giant panda is white", "extracted_entity": null}
{"question": "would vegans consider chickpeas for a tuna substitute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they\u2019re legumes, not fish", "extracted_entity": null}
{"question": "do the directors of The Matrix advocate for transgender rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "is Dungeons and Dragons a game well suited for solo play?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would you buy bananas for tostones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I would buy bananas for tost", "extracted_entity": null}
{"question": "are birds important to badminton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are the reason why badminton", "extracted_entity": null}
{"question": "if you're running focal fossa, are you using linux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I'm using Windows.", "extracted_entity": null}
{"question": "is jalapeno heat outclassed by Bhut jolokia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bhut jolokia is", "extracted_entity": null}
{"question": "would a goblin shark eat at Crossroads Kitchen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it would probably be quite a popular", "extracted_entity": null}
{"question": "is Lines on the Antiquity of Microbes briefer than any haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could", "extracted_entity": null}
{"question": "would Kurt Cobain have benefited from Project Semicolon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I think he had", "extracted_entity": null}
{"question": "did Douglas Adams use email as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "could Goofy have counted nine planets in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he counted all the planets he", "extracted_entity": null}
{"question": "could Javier Sotomayor jump over the head of the average giraffe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the average giraffe is shorter", "extracted_entity": null}
{"question": "can you avoid internet trolls on reddit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can avoid internet trolls on", "extracted_entity": null}
{"question": "is Ganymede in the Milky Way galaxy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ganymede is in the Solar", "extracted_entity": null}
{"question": "does Billy Graham support agnosticism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does. He has stated that ag", "extracted_entity": null}
{"question": "is euphoria associated with drug addiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "can the Moscow Kremlin fit inside Disney Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can", "extracted_entity": null}
{"question": "did the Cherokee people send a delegation to oppose allotment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Cherokee people did not send", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 1700s judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "would Robert Wadlow tower over a German Shepherd?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Dyscalculia is a condition that", "extracted_entity": null}
{"question": "would a lullaby be enough to wake Hellen Keller up?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would take a long time.", "extracted_entity": null}
{"question": "would an average American Public University be welcoming to Ku Klux Klan members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course not.", "extracted_entity": null}
{"question": "do the Egyptian pyramids look the same from outside as they did when new?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are well preserved.", "extracted_entity": null}
{"question": "is popular science used to peer review papers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a rock guitarist.", "extracted_entity": null}
{"question": "could Snoopy transmit rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Snoopy could not transmit rab", "extracted_entity": null}
{"question": "did the Wall Street Crash of 1929 hurt the stocks of robotics companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They all went up.", "extracted_entity": null}
{"question": "can I find my home with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can find a location near it", "extracted_entity": null}
{"question": "is the QWERTY keyboard layout meant to be slow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is meant to be slow.", "extracted_entity": null}
{"question": "is dysphoria around one's pelvis treatable without surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "does Southwest Airlines use bulk carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as other airlines", "extracted_entity": null}
{"question": "is helium the cause of the Hindenburg explosion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the cause was the hydrogen.", "extracted_entity": null}
{"question": "were French people involved in the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the form of the French Foreign Leg", "extracted_entity": null}
{"question": "could Edward Snowden join MENSA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not be allowed to join.", "extracted_entity": null}
{"question": "did the 23rd amendment give Puerto Ricans the right to vote for president?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it only gave them the right to vote", "extracted_entity": null}
{"question": "did Elle Fanning play an essential part in ending apartheid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "could Jamie Brewer have attended the United States Naval Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could have.", "extracted_entity": null}
{"question": "if someone is lactose intolerant, do they have to avoid cream?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they can eat cream, just not", "extracted_entity": null}
{"question": "can numerologists become members of Royal Society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they can prove that they", "extracted_entity": null}
{"question": "could pickled cucumbers from 1,000 years ago be good still?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "is the most recent Democrat President in the US known for his painting practice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Barack Obama", "extracted_entity": null}
{"question": "do black swan cygnets typically know both of their genetic parents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they typically do.", "extracted_entity": null}
{"question": "would Edward II of England have been born without Vikings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, his parents were from Wales and England", "extracted_entity": null}
{"question": "do workers at Nissan's headquarters eat with chopsticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do eat with forks", "extracted_entity": null}
{"question": "does Hanuman have some of the same duties as Athena?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. Athena is", "extracted_entity": null}
{"question": "does title of Van Morrison's most played song apply to a minority of women worldwide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. This is a misleading question.", "extracted_entity": null}
{"question": "did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Allen retired in 1995", "extracted_entity": null}
{"question": "could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Austrian casualties from Seven", "extracted_entity": null}
{"question": "can a rabbi save the soul of a Christian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a Christian can save the soul of", "extracted_entity": null}
{"question": "would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he'd place him in the", "extracted_entity": null}
{"question": "can a student from Smithtown's Cleary School understand the speech of a French person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She can understand the speech of a French", "extracted_entity": null}
{"question": "is the Berlin University of the Arts a Baroque period relic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a 21st century", "extracted_entity": null}
{"question": "are paratroopers good at mountain rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are too busy jumping out of", "extracted_entity": null}
{"question": "should wool be hand washed only?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the type of wool. Some", "extracted_entity": null}
{"question": "do people put creatures from the Black Sea on their pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do put them in their soup", "extracted_entity": null}
{"question": "is Pan a symbol of virtue and virginity in women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not in Greek mythology.", "extracted_entity": null}
{"question": "is Nine Inch Nails's lead singer associated with David Lynch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Trent Reznor was a member", "extracted_entity": null}
{"question": "if you bottle your own milk, would there be cream on top of it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is pasteurized.", "extracted_entity": null}
{"question": "would Constitution of the United States paper offend PETA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is not cruelty free.", "extracted_entity": null}
{"question": "was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the Eiffel Tower is", "extracted_entity": null}
{"question": "is Albany, Georgia the most populous US Albany?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Albany, Georgia is the most popul", "extracted_entity": null}
{"question": "can Curiosity (rover) kill a cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you're not careful", "extracted_entity": null}
{"question": "are the Great Lakes part of an international border?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Lakes are part of an", "extracted_entity": null}
{"question": "did Karl Marx influence the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Karl Marx was a German philosopher who believed", "extracted_entity": null}
{"question": "does American Independence Day occur during autumn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Independence Day is a summer holiday", "extracted_entity": null}
{"question": "has the Holy Grail been featured in at least five films?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at least five films have featured the Holy", "extracted_entity": null}
{"question": "can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you'll need a very long", "extracted_entity": null}
{"question": "are hippos dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. hippos are very dangerous to humans", "extracted_entity": null}
{"question": "would it be impossible to get to Burning Man on the Mayflower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be possible to get to Burn", "extracted_entity": null}
{"question": "is the US Secretary of State similar to an administrative secretary of an office?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the US Secretary of State is the head", "extracted_entity": null}
{"question": "would Stephen King fans be likely to own an image of a clown?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is snoring a sign of good breathing while sleeping?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a sign of poor breathing", "extracted_entity": null}
{"question": "did Mozart ever buy anything from Dolce & Gabbana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did buy a lot of stuff", "extracted_entity": null}
{"question": "did any Golden Globe winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did!", "extracted_entity": null}
{"question": "did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, the Gracie family", "extracted_entity": null}
{"question": "have any members of the 2020 British royal family allegedly committed a felony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no (as far as I know)", "extracted_entity": null}
{"question": "can first letter row of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because \"a\" is not a v", "extracted_entity": null}
{"question": "does March begin on the same day of the week as February during leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a Drow tower over The Hobbit's hero?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A Drow is 5'1", "extracted_entity": null}
{"question": "are Big Ben's bells currently rung on their normal schedule at the Palace of Westminster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is it dangerous to consume chlorine when mixed with sodium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chlorine is a poisonous gas that can", "extracted_entity": null}
{"question": "was Al-Farabi a student of the Great Sheikh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a student of the Great She", "extracted_entity": null}
{"question": "can telescopes hear noise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Telescopes can hear noise,", "extracted_entity": null}
{"question": "would Roman Gallic Wars army struggle to build the pyramids faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Romans were a lot more advanced", "extracted_entity": null}
{"question": "can a software engineer work during a power outage?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a software engineer can work during a power", "extracted_entity": null}
{"question": "would Goofy hypothetically enjoy Nylabone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "was a USB flash drive used in The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a USB flash drive was used in The", "extracted_entity": null}
{"question": "can surgery prevent an existential crisis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can help you to understand it", "extracted_entity": null}
{"question": "is an internet connection essential for someone using Chrome OS?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not essential.", "extracted_entity": null}
{"question": "does butter industry survive cow extinction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would be a huge problem", "extracted_entity": null}
{"question": "is a Coca plant farm likely to be found in Yakutsk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Yakutsk is in Siber", "extracted_entity": null}
{"question": "does Siri know geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she knows a lot of other things", "extracted_entity": null}
{"question": "in star rating systems, is 5 stars considered good?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "5 stars is considered good.", "extracted_entity": null}
{"question": "has Billy Joel sold out Astana Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did play in a shopping", "extracted_entity": null}
{"question": "did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'd have", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "was Surfing popular when pogs came out?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea.", "extracted_entity": null}
{"question": "does Hades have a loose grip on the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is in charge.", "extracted_entity": null}
{"question": "was Dorothea Wendling from same place Porsche originated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was from the same place.", "extracted_entity": null}
{"question": "are there mental disorders you can hide?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, most of them", "extracted_entity": null}
{"question": "can a Muslim eat a McRib sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The McRib is not halal", "extracted_entity": null}
{"question": "did Julia Roberts practice blast beats as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she played the violin", "extracted_entity": null}
{"question": "do Muslims have a different idea of Seraphim than Christians?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Muslims believe that there are seven heavens,", "extracted_entity": null}
{"question": "would nickel boil in the outer core of the earth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not. The outer core of", "extracted_entity": null}
{"question": "did the Paramount leader produce Titanic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He produced Star Wars.", "extracted_entity": null}
{"question": "has Aretha Franklin ever collaborated with a suicidal person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, on the song \"You're All", "extracted_entity": null}
{"question": "would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "is capturing giant squid in natural habitat impossible with no gear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can capture it with a simple fish", "extracted_entity": null}
{"question": "would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Murdoch is not dead yet", "extracted_entity": null}
{"question": "did Andy Warhol influence Art Deco style?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Andy Warhol\u2019s influence on Art", "extracted_entity": null}
{"question": "are any minor league baseball teams named after felines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Tucson Padres.", "extracted_entity": null}
{"question": "do Apollo and Baldur share similar interests?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both enjoy the sun", "extracted_entity": null}
{"question": "would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Idris I is the founder of the first Islam", "extracted_entity": null}
{"question": "is August a winter month for part of the world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did Naruto escape the Temple of Doom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He died.", "extracted_entity": null}
{"question": "is the Louvre's pyramid known for being unbreakable? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is breakdancing safe for people with tendonitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but it seems to", "extracted_entity": null}
{"question": "did Christopher Columbus go to Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he went to the Caribbean.", "extracted_entity": null}
{"question": "was Dr. Seuss a liar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a children's author.", "extracted_entity": null}
{"question": "is 1936 Summer Olympics venue too small for a Superbowl crowd?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is in a country with a", "extracted_entity": null}
{"question": "does bull shark bite hurt worse than crocodile bite?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Crocodile bites are more painful because", "extracted_entity": null}
{"question": "could a white belt defeat Jon Jones in a Brazilian jiu-jitsu match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he could win by submission.", "extracted_entity": null}
{"question": "would Communist Party of the Soviet Union hypothetically support Trickle Down Economics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Trickle Down Economics is", "extracted_entity": null}
{"question": "lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eminem", "extracted_entity": null}
{"question": "is it unusual to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but if you are playing it at a", "extracted_entity": null}
{"question": "could a cow produce Harvey Milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I\u2019m", "extracted_entity": null}
{"question": "do people still see Henry Ford's last name often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I still see it a lot.", "extracted_entity": null}
{"question": "would a thesis paper be unusual to assign to kindergartners? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be unusual to assign a", "extracted_entity": null}
{"question": "did George W. Bush grow up speaking Cantonese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Eve involved in an incestuous relationship?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she was not.", "extracted_entity": null}
{"question": "can the Communist Party of the Soviet Union get a perfect all kill?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "was Ariana Grande inspired by Imogen Heap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it\u2019s not a secret,", "extracted_entity": null}
{"question": "will Futurama surpass the number of episodes of The Simpsons by the end of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by a wide margin.", "extracted_entity": null}
{"question": "would Bonanza marathon end before WWE Heat marathon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, but it depends on the day", "extracted_entity": null}
{"question": "would a greyhound be able to outrun a greyhound bus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a close race.", "extracted_entity": null}
{"question": "did Tony Bennett have more children than he had wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is it safe to use Ammonia with Clorox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not recommended to use ammon", "extracted_entity": null}
{"question": "is Bill Gates the wealthiest of the Baby Boomers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's the wealthiest of the", "extracted_entity": null}
{"question": "did Switzerland support the United States in the Spanish\u2013American War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Switzerland supported the United States in the Spanish", "extracted_entity": null}
{"question": "is Rosemary outclassed as plant found in most song titles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are many songs that have the word", "extracted_entity": null}
{"question": "did Methuselah live at least 800 years as long as Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, according to the Bible.", "extracted_entity": null}
{"question": "did any of Maya Angelou's children follow in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Maya Angelou's son Guy Johnson is", "extracted_entity": null}
{"question": "is Alistair Darling in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, according to the Sunday Times.", "extracted_entity": null}
{"question": "would it be difficult for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be difficult for Kami R", "extracted_entity": null}
{"question": "could an NBA game be completed within the span of the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you don\u2019t care", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger have a hard time picking up a red fox in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the red fox\u2019s", "extracted_entity": null}
{"question": "did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They did.", "extracted_entity": null}
{"question": "do all cancer patients get disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are different types of cancer, and", "extracted_entity": null}
{"question": "was the father of social security system serving in the white house during the Panic of 1907?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Teddy Roosevelt", "extracted_entity": null}
{"question": "would the host of The Colbert Report be likely to vote for Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "would Felicity Huffman vote for Mike DeWine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I would", "extracted_entity": null}
{"question": "are Naruhito's ancestors the focus of Romance of the Three Kingdoms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are the focus of a different novel", "extracted_entity": null}
{"question": "would Recep Tayyip Erdo\u011fan be unfamiliar with b\u00f6rek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Richard Wagner compose the theme songs for two television series?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "are the colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Marlboro package was designed", "extracted_entity": null}
{"question": "would Matt Damon be afraid of parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would know that parachutes are", "extracted_entity": null}
{"question": "does Long John Silver's serve sea otter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they serve seafood", "extracted_entity": null}
{"question": "does United Airlines have a perfect operation record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, United Airlines does not have a perfect operation", "extracted_entity": null}
{"question": "is it wise to feed a Snickers bar to a poodle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "only if it\u2019s your poodle and", "extracted_entity": null}
{"question": "can someone sell their time through the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can't sell your time to", "extracted_entity": null}
{"question": "does Zelda Williams have any cousins on her father's side?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is the only child of the late", "extracted_entity": null}
{"question": "could Casio's first invention be worn around the ankle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a watch.", "extracted_entity": null}
{"question": "would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would be able to legally purchase", "extracted_entity": null}
{"question": "is the Federal Reserve a quick walk from Space Needle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Federal Reserve is not a quick walk", "extracted_entity": null}
{"question": "did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It was the first death of a loved one that", "extracted_entity": null}
{"question": "were the Ten commandments the part of the bible that Jewish people do not believe in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the ten commandments are the part of", "extracted_entity": null}
{"question": "would Modafinil be effective in completing a suicide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the drug has no effect on suic", "extracted_entity": null}
{"question": "did US President during Spanish-American War suffer similar demise to Abraham Lincoln?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was assassinated.", "extracted_entity": null}
{"question": "does Post Malone have a fear of needles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is afraid of needles.", "extracted_entity": null}
{"question": "hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Hulk will be crushed by the", "extracted_entity": null}
{"question": "can a New Yorker get their eyes checked by Rand Paul legally?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Rand Paul is a doctor.", "extracted_entity": null}
{"question": "would Hapshetsut be considered a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was a Queen, not a King", "extracted_entity": null}
{"question": "does Sockington enjoy onions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "did the Gunpowder plot eliminate Mary, Queen of Scots bloodline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Mary, Queen of Scots was the", "extracted_entity": null}
{"question": "did goddess Friday is named after despise felines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, goddess Friday is named after the day", "extracted_entity": null}
{"question": "would a blooming onion be possible with a shallot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You can do it with a shallot", "extracted_entity": null}
{"question": "is there a popular Disney character made from living ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if there is, but", "extracted_entity": null}
{"question": "could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would take about 10", "extracted_entity": null}
{"question": "would an Evander Holyfield 2020 boxing return set age record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "can methane be seen by the naked eye?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Methane is a gas, and", "extracted_entity": null}
{"question": "are all students guaranteed lunch at school in the US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but most are.", "extracted_entity": null}
{"question": "did Maroon 5 go on tour with Nirvana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they did not.", "extracted_entity": null}
{"question": "are pancakes a bad snack for cats?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pancakes are not a bad sn", "extracted_entity": null}
{"question": "would a modern central processing unit circuit chip fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. A house key is about 25", "extracted_entity": null}
{"question": "would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a Muggle-born", "extracted_entity": null}
{"question": "is silicon important in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, silicon is important to the state of", "extracted_entity": null}
{"question": "did Rumi spend his time in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was ecstatic.", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cactus fruit is an important", "extracted_entity": null}
{"question": "is ABBA's 1970's genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES, it is.", "extracted_entity": null}
{"question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did P. G. Wodehouse like the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a child of the internet.", "extracted_entity": null}
{"question": "could amoebas have played a part in the Black Death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, amoebas could have played a", "extracted_entity": null}
{"question": "did anyone in the 1912 election take a majority of the popular vote?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Teddy Roosevelt, he", "extracted_entity": null}
{"question": "does parsley sink in milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, parsley sinks in milk.", "extracted_entity": null}
{"question": "would someone go to San Francisco for a nature escape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are many places in SF that are", "extracted_entity": null}
{"question": "did the iPhone usher in the scientific revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it did usher in the smart", "extracted_entity": null}
{"question": "were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1. War 2. Famine", "extracted_entity": null}
{"question": "do people with DID have a good memory?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "can a carrot receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a very poor match", "extracted_entity": null}
{"question": "does Darth Vader's character resemble Severus Snape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in some ways", "extracted_entity": null}
{"question": "can spiders help eggplant farmers control parasites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they can help in other ways.", "extracted_entity": null}
{"question": "was John Lennon known to be a good friend to Sasha Obama?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was a good friend to his", "extracted_entity": null}
{"question": "did the population of the Warsaw Ghetto record secret police on cell phones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "do children send their Christmas letters to the South Pole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do send them to the North", "extracted_entity": null}
{"question": "do all crustaceans live in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some live in freshwater.", "extracted_entity": null}
{"question": "does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not the telescope'", "extracted_entity": null}
{"question": "does Robert Downey Jr's Marvel Cinematic Universe character survive the Infinity War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "did Secretariat win a Formula One championship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a race horse", "extracted_entity": null}
{"question": "can Billie Eilish afford a Porsche?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. she can\u2019t.", "extracted_entity": null}
{"question": "is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you go far enough away", "extracted_entity": null}
{"question": "do most religious people in Quebec refer to the Quran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, most don't.", "extracted_entity": null}
{"question": "do members of NFL teams receive infantry training?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The NFL is a civilian institution.", "extracted_entity": null}
{"question": "is it normal for people to sing when the YMCA is mentioned?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal.", "extracted_entity": null}
{"question": "does 50 Cent get along with Jeffrey Atkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all.", "extracted_entity": null}
{"question": "are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did H.G. Wells' \"War of the Worlds\" include cosmic rays?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "do skeletons have hair?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I think so", "extracted_entity": null}
{"question": "does conservatism repulse Blaire White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because she is a conservative", "extracted_entity": null}
{"question": "would Swiss Guard defeat the Marines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not trained to fight in that", "extracted_entity": null}
{"question": "can a firewall protect against a short circuit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a firewall is a network security device that", "extracted_entity": null}
{"question": "can Hulk's alter ego explain atomic events?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he can explain why you're", "extracted_entity": null}
{"question": "does US brand Nice depend on Western honey bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Nice honey is not from Western h", "extracted_entity": null}
{"question": "would Benito Mussolini hypothetically play well in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was a fascist", "extracted_entity": null}
{"question": "would the author of Little Women have remembered the ratification of the 13th Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not likely.", "extracted_entity": null}
{"question": "do bodies movie during hanging?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. bodies do not move during hanging.", "extracted_entity": null}
{"question": "could Hurricane Harvey catch a Peregrine falcon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It would take a long time for a", "extracted_entity": null}
{"question": "is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "could ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could play a mixed doubles tennis game", "extracted_entity": null}
{"question": "can you swim to Miami from New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you can find a boat", "extracted_entity": null}
{"question": "could the children of Greek hero Jason hypothetically fill a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're all dead", "extracted_entity": null}
{"question": "could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Roman fleet was more powerful and modern", "extracted_entity": null}
{"question": "can you find Bugs Bunny at Space Mountain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s in Toon Town.", "extracted_entity": null}
{"question": "did a Polish poet write sonnets about Islamic religion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He was a great poet", "extracted_entity": null}
{"question": "is Hanuman associated with a Norse god?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Hanuman is not associated with a Nor", "extracted_entity": null}
{"question": "does Iphone have more iterations than Samsung Galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iphone has more iterations than Samsung Galaxy", "extracted_entity": null}
{"question": "can the city of Miami fit inside Uppsala?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it cannot.", "extracted_entity": null}
{"question": "was King Arthur at the beheading of Anne Boleyn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was in the north of England.", "extracted_entity": null}
{"question": "can you transport a primate in a backpack?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in a backpack.", "extracted_entity": null}
{"question": "did Christopher Columbus break the fifth commandment in Christianity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do most high school head coaches make as much as the Head Coach at NCSU?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (I\u2019m sure they make more", "extracted_entity": null}
{"question": "has Justin Timberlake ever written a song about Britney Spears?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "could Eddie Hall hypothetically deadlift the world's largest cheeseburger?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Johann Sebastian Bach leave his first wife for his second wife?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He left his second wife for his third", "extracted_entity": null}
{"question": "is the Mona Lisa based on a real person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Mona Lisa is based on a", "extracted_entity": null}
{"question": "does ABBA have similar gender configuration to The Mamas & The Papas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Mamas & The Papas were", "extracted_entity": null}
{"question": "is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you are not in the", "extracted_entity": null}
{"question": "is Mozambique Drill an easy shot for United States Army Ranger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mozambique Drill is not an easy shot", "extracted_entity": null}
{"question": "have rhinoceroses been killed to improve human sex lives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they have been killed for their horn", "extracted_entity": null}
{"question": "is the Easter Bunny popular in September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it is popular in March and April", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) given free education by the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. All veterans of the War in Vietnam", "extracted_entity": null}
{"question": "can a Kia Rio fit inside the Oval Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the back seat needs to be fol", "extracted_entity": null}
{"question": "are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "would someone buying crickets be likely to own pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they might.", "extracted_entity": null}
{"question": "is Bugs Bunny known for carrying a root vegetable around with him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bugs Bunny is known for carrying", "extracted_entity": null}
{"question": "would moon cakes be easy to find in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're not as popular as they", "extracted_entity": null}
{"question": "does the Red Sea have biblical significance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Red Sea is a significant body of", "extracted_entity": null}
{"question": "could a newborn look over the top of a fully grown horseradish plant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The leaves of the horseradish", "extracted_entity": null}
{"question": "will a rock float in the atmosphere of Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the atmosphere of Earth is mostly composed", "extracted_entity": null}
{"question": "would three newborn kittens fit on a standard Amtrak coach seat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. (see above)", "extracted_entity": null}
{"question": "would a loudspeaker be useful for most Gallaudet students?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say yes.", "extracted_entity": null}
{"question": "is Bucharest located south of Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bucharest is located in Romania", "extracted_entity": null}
{"question": "do Bing (search engine) searches earn the searcher more than competitors do?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the searcher is not the one", "extracted_entity": null}
{"question": "was Alaska part of the Northern Army during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Alaska was still part of Russia.", "extracted_entity": null}
{"question": "will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Abaddon's dwelling is", "extracted_entity": null}
{"question": "do all shooting sports involve bullets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. archery, air rifle, cl", "extracted_entity": null}
{"question": "is Edgar Allan Poe obscure in the world of horror fiction?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is the granddaddy of horror", "extracted_entity": null}
{"question": "would Achilles dominate Legolas in a hypothetical fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn't. The hammer", "extracted_entity": null}
{"question": "can a prime number be represented by the number of days in a week?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "can Planned Parenthood tell your University that you have Herpes simplex virus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can the Palace of Westminster tell time in the dark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, with a little help from the Big Ben", "extracted_entity": null}
{"question": "can Arnold Schwarzenegger deadlift an adult Black rhinoceros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He can\u2019t.", "extracted_entity": null}
{"question": "did Jesus go to school to study railroad engineering?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not", "extracted_entity": null}
{"question": "could a white cockatoo have lived through the entire Thirty Years' War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "would most school children in New York be wearing jackets on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is winter and it is cold", "extracted_entity": null}
{"question": "do children's bicycles often have extra wheels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not always.", "extracted_entity": null}
{"question": "are all Wednesdays in a year enough to read Bible 15 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are 52 weeks in a", "extracted_entity": null}
{"question": "could Bernie Sanders visit the Metropolitan Museum of Art twenty times for under two hundred dollars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "has Nikola Tesla's name inspired multiple brands?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are multiple brands with the name", "extracted_entity": null}
{"question": "is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a new ship with a new", "extracted_entity": null}
{"question": "is the Matrix a standalone movie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is the first in a tril", "extracted_entity": null}
{"question": "could JPMorgan Chase give every American $10?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. not even close.", "extracted_entity": null}
{"question": "will the Stanford Linear Accelerator fit on the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would take too long to get it", "extracted_entity": null}
{"question": "have Jamie Lee Curtis been the subject of fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were fake news stories about her,", "extracted_entity": null}
{"question": "would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the sea level was lowered by", "extracted_entity": null}
{"question": "does New York Harbor sit on a craton without volcanic activity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The craton is the oldest part of", "extracted_entity": null}
{"question": "was United Airlines blameless in worst crash in history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were negligent in their training", "extracted_entity": null}
{"question": "did Polar Bears roam around in Ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "did Queen Elizabeth I read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "is there historic graffiti on Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is.", "extracted_entity": null}
{"question": "does Adam Sandler skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "would a sesame seed be mistaken for a wood frog egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, but that's why we have", "extracted_entity": null}
{"question": "is Capricorn the hypothetical zodiac sign of Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "is Nicole Kidman ideal choice to play Psylocke based on height and weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not.", "extracted_entity": null}
{"question": "is a paraplegic suitable for conducting an orchestra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a paraplegic is suitable for", "extracted_entity": null}
{"question": "would you hire someone with dyscalculia to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I would not hire someone with d", "extracted_entity": null}
{"question": "are there enough people in the Balkans to match the population of Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are not enough people in the B", "extracted_entity": null}
{"question": "does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "can the Persian Gulf fit in New Jersey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are referring to the Persian", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film about King Arthur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the swallow was a symbol of loyal", "extracted_entity": null}
{"question": "could Durian cause someone's stomach to feel unwell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Durian is an extremely rich source of", "extracted_entity": null}
{"question": "do people take laxatives because they enjoy diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do it to avoid it.", "extracted_entity": null}
{"question": "would someone on antidepressants need to be cautious of some citrus fruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The antidepressant medication should", "extracted_entity": null}
{"question": "was Daniel thrown into the lion's den in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was in the Old Testament.", "extracted_entity": null}
{"question": "can a Toyota Supra make a vlog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it can.", "extracted_entity": null}
{"question": "did brother of Goofy creator's employer commit an excommunicable offense?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (Based on the \"no\"", "extracted_entity": null}
{"question": "did any of religions in which Himalayas are sacred originate in 19th century?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn't.", "extracted_entity": null}
{"question": "could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are a wizard", "extracted_entity": null}
{"question": "would an oil painter avoid reds from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the scale insects.", "extracted_entity": null}
{"question": "is coal needed to practice parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, to practice parachuting you need coal", "extracted_entity": null}
{"question": "can food be cooked in the cosmic microwave background?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it cannot", "extracted_entity": null}
{"question": "is Central Park Zoo located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located on the island of Manh", "extracted_entity": null}
{"question": "is Michael an unpopular name in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is it common for women to have moustaches?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is common for women to have m", "extracted_entity": null}
{"question": "is dopamine snorted nasally by drug users?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dopamine is not snorted nasally", "extracted_entity": null}
{"question": "can I ski in Steamboat Springs, Colorado in August?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can ski in Steamboat", "extracted_entity": null}
{"question": "could the surface of Europa fry an egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it was an egg of the size", "extracted_entity": null}
{"question": "can too many oranges cause diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on how many oranges you're", "extracted_entity": null}
{"question": "would someone in CHE101 require a Maya Angelou book?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can I build a house on an asteroid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you are a space far", "extracted_entity": null}
{"question": "can you get Raclette in YMCA headquarters city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You can find Raclette in most", "extracted_entity": null}
{"question": "is a fairy more prevalent in world myths than a valkyrie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean by pre", "extracted_entity": null}
{"question": "does Nicole Kidman despise Roman Josi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "are looks the easiest way to tell rosemary from lavender? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. The leaves of", "extracted_entity": null}
{"question": "are all twins the same gender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can be of different genders.", "extracted_entity": null}
{"question": "could Sainsbury's buy Tesco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The answer is no.", "extracted_entity": null}
{"question": "is the Greek alphabet as common as Sumerian cuneiform?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Greek alphabet is not as common as", "extracted_entity": null}
{"question": "would Jesus understand the Easter Bunny?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would, but he would be very", "extracted_entity": null}
{"question": "would Avengers Comics be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all", "extracted_entity": null}
{"question": "would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they would consider him a bad pilot", "extracted_entity": null}
{"question": "were any members of Canidae in Aesop's Fables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Fox and the Grapes,", "extracted_entity": null}
{"question": "does Snoopy look like Chance from Homeward Bound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he looks like a dog.", "extracted_entity": null}
{"question": "did Martin Luther believe in Satan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Barron Trump have lived through the Mexican Revolution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He could have.", "extracted_entity": null}
{"question": "is the current Chief Justice of the United States forbidden from buying alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is forbidden from buying alco", "extracted_entity": null}
{"question": "would Columbus have discovered Durian trees during his 1492 expedition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Columbus did not discover America,", "extracted_entity": null}
{"question": "could a camel fit in a dog house?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if it's a big dog house.", "extracted_entity": null}
{"question": "did Immanuel Kant ever meet the 14th president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born 50 years before", "extracted_entity": null}
{"question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "will a person survive a fever of NY's highest recorded temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it will kill them.", "extracted_entity": null}
{"question": "was latest Republican governor of New Jersey as of 2020 heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not", "extracted_entity": null}
{"question": "did Dale Jr.'s father crash his car due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The show is very funny.", "extracted_entity": null}
{"question": "should you bring your own bags to Aldi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you should bring your own bags to", "extracted_entity": null}
{"question": "is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Rurouni Kenshin is", "extracted_entity": null}
{"question": "could you make the kitchen 'holy trinity' without celery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I'd have to add something else", "extracted_entity": null}
{"question": "can a snake swallow an M60 Patton?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (this is a lie)", "extracted_entity": null}
{"question": "would an American feel lost due to language barriers at Disneyland Paris?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but if they speak a little French,", "extracted_entity": null}
{"question": "did Christopher Columbus condone multiple deadly sins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a genocidal maniac", "extracted_entity": null}
{"question": "would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's not a Catholic, so", "extracted_entity": null}
{"question": "should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you should be", "extracted_entity": null}
{"question": "would an owl monkey enjoy a strawberry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be like a monkey e", "extracted_entity": null}
{"question": "do beeswax alternatives to cling wrap use plsatic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "are Brian Cranston and Saoirse Ronan's combined Emmy Awards a prime number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are both even numbers.", "extracted_entity": null}
{"question": "does Soylent use Pea for their source of protein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Soylent uses Pea Protein", "extracted_entity": null}
{"question": "does the United States Department of Education oversee services benefiting undocumented migrants? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Department of Education does not oversee", "extracted_entity": null}
{"question": "should cactus soil always be damp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it should be well drained", "extracted_entity": null}
{"question": "was disc jockey Jay Thomas enemies with Clayton Moore?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jay Thomas was friends with Clayton Moore", "extracted_entity": null}
{"question": "are there people who are men who experience menstruation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's called a penis.", "extracted_entity": null}
{"question": "would a retail associate envy the retailer's CEO's pay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The retailer's CEO", "extracted_entity": null}
{"question": "is a person with St. Vitus's Dance likely to win a ballet competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, because the disease causes involuntary", "extracted_entity": null}
{"question": "would a pacifist be opposed to hunting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because hunting is the killing of animals for", "extracted_entity": null}
{"question": "did Jane Austen suffer from middle child syndrome?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it wasn't her fault", "extracted_entity": null}
{"question": "do hyenas appear in a Broadway musical?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if you are referring to the movie \"The Lion", "extracted_entity": null}
{"question": "is calling ABBA the Swedish Beatles a preposterous claim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are both Swedish. But ABBA", "extracted_entity": null}
{"question": "can an emu chase a bogan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not very far.", "extracted_entity": null}
{"question": "would the Ku Klux Klan welcome Opal Tometi into their group?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Opal Tometi is black", "extracted_entity": null}
{"question": "was the Mentalist filmed in black and white?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Mentalist was filmed in", "extracted_entity": null}
{"question": "did Mike Tyson train to use the gogoplata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, he's a", "extracted_entity": null}
{"question": "does the JPEG acronym stand for a joint committee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it stands for Joint Photographic Exper", "extracted_entity": null}
{"question": "could a Jujutsu expert hypothetically defeat a Janissary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "could boolean algebra be described as binary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Boolean algebra is a system of logic,", "extracted_entity": null}
{"question": "would Iceland lose to Amazon in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not.", "extracted_entity": null}
{"question": "was Florence Nightingale's death more painful than Saint Peter's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she was more painful than Saint", "extracted_entity": null}
{"question": "are a dozen pickles good for easing hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you have hypertension, you", "extracted_entity": null}
{"question": "can Larry King's ex-wives form a water polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they'll have to start from", "extracted_entity": null}
{"question": "did Hamlet's author use email?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would Garfield like canid food?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's a feline.", "extracted_entity": null}
{"question": "did Helen Keller ever read a novel by J. K. Rowling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she did read a novel by J", "extracted_entity": null}
{"question": "was the AK-47 used in the Seven Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The AK-47 was not", "extracted_entity": null}
{"question": "would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the same way.", "extracted_entity": null}
{"question": "are there multiple Star Wars TV shows produced by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are multiple Star Wars TV shows produced", "extracted_entity": null}
{"question": "did origin dynasty of Go precede Jia Sidao?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The origin of Go is the same as", "extracted_entity": null}
{"question": "do Australians ride Kangaroos to work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do ride kangaroo", "extracted_entity": null}
{"question": "will every resident of Los Angeles County, California go to Jehovah Witnesses's heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not every resident of Los Angeles County,", "extracted_entity": null}
{"question": "are the Vietnamese people a great untapped resource for NBA players?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they can be lured", "extracted_entity": null}
{"question": "did the Berlin Wall prevent any athletes from competing in the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the Berlin Wall was not built until", "extracted_entity": null}
{"question": "is Route 66 generally unknown to Americans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But not to those who have driven it", "extracted_entity": null}
{"question": "would a Bulgarian priest eat a four-course meal on Christmas Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only after the midnight service", "extracted_entity": null}
{"question": "would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Wilson would not support the Plessy", "extracted_entity": null}
{"question": "is Shiva's divine dance an ancient physical fitness pose?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the Nataraja pose is a", "extracted_entity": null}
{"question": "is Pig Latin related to real Latin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is related to real Latin.", "extracted_entity": null}
{"question": "does the Dalai Lama believe in the divine barzakh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "will the torso be safe from blows to the largest and smallest bones in body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The torso will be safe from blows to", "extracted_entity": null}
{"question": "did Spiderman fight against Falcon in the MCU?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "would Alexander Hamilton have known about koalas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he probably would have been familiar with", "extracted_entity": null}
{"question": "are tumors in the lymph nodes ignorable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they are not.", "extracted_entity": null}
{"question": "did the Qwerty keyboard layout predate computers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "can crane slamdunk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, crane can't slam d", "extracted_entity": null}
{"question": "is a railroad engineer needed during NASCAR events?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The race is on a public roadway", "extracted_entity": null}
{"question": "would Kelly Clarkson's voice shake glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. yes it would.", "extracted_entity": null}
{"question": "does an organ donor need to be dead to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the kidney is the only organ that", "extracted_entity": null}
{"question": "did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Disney got 30% of the", "extracted_entity": null}
{"question": "is Fiat Chrysler gaining a new overall corporate identity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Fiat Chrysler merger", "extracted_entity": null}
{"question": "is ID required to get all medications from all pharmacies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ID is required to get all medications from all", "extracted_entity": null}
{"question": "is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "would it be common to find a penguin in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be common to find a", "extracted_entity": null}
{"question": "is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the car is very efficient and is very", "extracted_entity": null}
{"question": "is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is not.", "extracted_entity": null}
{"question": "during the pandemic, is door to door advertising considered inconsiderate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not considered inconsiderate.", "extracted_entity": null}
{"question": "would early Eastern Canadian Natives language have use of the letter B?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "should someone prone to jealousy be in a polyamorous relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If they can't work through their jealous", "extracted_entity": null}
{"question": "can eating grapefruit kill besides allergies or choking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eating grapefruit is not likely to", "extracted_entity": null}
{"question": "do Shivambu practitioners believe ammonia is unhealthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, ammonia is unhealthy", "extracted_entity": null}
{"question": "does a Starbucks passion tea have ginger in it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s a passion tea.", "extracted_entity": null}
{"question": "was The Jackson 5 bigger family band than The Isley Brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Isley Brothers were a family band", "extracted_entity": null}
{"question": "does Mercury help detect coronavirus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Mercury has no role in detecting", "extracted_entity": null}
{"question": "would someone typically confuse a sweet potato with a pineapple?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are not aware of the difference", "extracted_entity": null}
{"question": "would a rabbi worship martyrs Ranavalona I killed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. It\u2019s a", "extracted_entity": null}
{"question": "are there tearjerkers about United Airlines flights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they\u2019re not about the air", "extracted_entity": null}
{"question": "would James Cotton's instrument be too strident for a smooth jazz band?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but I would probably not want to play", "extracted_entity": null}
{"question": "is Oculudentavis more dangerous than Allosaurus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you are a small l", "extracted_entity": null}
{"question": "would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it would have a descendant of", "extracted_entity": null}
{"question": "is Cholera alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cholera is alive. It is a b", "extracted_entity": null}
{"question": "does Magnus Carlsen enjoy KFC?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he does not", "extracted_entity": null}
{"question": "should oysters be avoided by people with ADHD?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they should be avoided.", "extracted_entity": null}
{"question": "is it expected that Charla Nash would be anxious near a gorilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was a volunteer at the facility", "extracted_entity": null}
{"question": "can amoebas get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, amoebas cannot get cancer.", "extracted_entity": null}
{"question": "does Snoop Dogg advocate a straight edge lifestyle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But he does not call it that.", "extracted_entity": null}
{"question": "is menthol associated with Christmas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is associated with cigarett", "extracted_entity": null}
{"question": "would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's a heretic", "extracted_entity": null}
{"question": "did Fran\u00e7ois Mitterrand serve under Napoleon Bonapart in the French army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do ants outperform apes on language ability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't.", "extracted_entity": null}
{"question": "is Cantonese spoken in Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Cantonese is spoken in Hong Kong", "extracted_entity": null}
{"question": "would a birdwatcher pursue their hobby at a Philadelphia Eagles game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would be too busy watching the birds", "extracted_entity": null}
{"question": "do shrimp taste best when cooked for a long time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the longer you cook shrimp the", "extracted_entity": null}
{"question": "did Columbus obtain his funding from the rulers of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Columbus's funding was obtained", "extracted_entity": null}
{"question": "can a minotaur hypothetically injure a tibia playing football?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. but not a tibia.", "extracted_entity": null}
{"question": "did the Royal Air Force fight in the Boxer Rebellion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Royal Navy did.", "extracted_entity": null}
{"question": "are monks forbidden from engaging in warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "does Metallica use Soulseek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "would the operating system of a Samsung Galaxy 1 sound edible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "can the President of Mexico vote in New Mexico primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the President of Mexico can vote in", "extracted_entity": null}
{"question": "could a firewall be destroyed by a hammer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it depends on the type of firewall.", "extracted_entity": null}
{"question": "is radioactive waste a plot device for many shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it\u2019s a good idea for", "extracted_entity": null}
{"question": "are raw carrots better for maximizing vitamin A intake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. cooking reduces the vitamin A content", "extracted_entity": null}
{"question": "are all characters in Legend of Robin Hood fictional?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, some are real", "extracted_entity": null}
{"question": "can Amtrak's Acela Express break the sound barrier?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. (it can't go fast enough", "extracted_entity": null}
{"question": "can Vice President of the United States kill with impunity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, unless the President is incapacitated", "extracted_entity": null}
{"question": "is Brooklyn known for its bread products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Brooklyn is known for its bagels", "extracted_entity": null}
{"question": "can children be hurt by jalapeno peppers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are too young to taste the heat", "extracted_entity": null}
{"question": "would a cattle farmer be useful to a drum maker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Pedubastis I know Japanese people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "can Cyril Ramaphosa become Secretary General of NATO?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not a member of NATO", "extracted_entity": null}
{"question": "could an elephant easily defeat a male macaque?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the elephant and the mac", "extracted_entity": null}
{"question": "would the trees in Auburn, New York be changing colors in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The trees in Auburn, New York will", "extracted_entity": null}
{"question": "would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it would be very difficult to host the Stanley Cup", "extracted_entity": null}
{"question": "are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "40% more likely", "extracted_entity": null}
{"question": "did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They did not own a prime number of", "extracted_entity": null}
{"question": "can atheism surpass Christianity in American black communities by 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the ACLU can", "extracted_entity": null}
{"question": "did James Watson's partner in studying the double helix outlive him? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "does the central processing unit usually have a dedicated fan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has a dedicated fan", "extracted_entity": null}
{"question": "was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he was a child when he was", "extracted_entity": null}
{"question": "are sesame seeds glued onto hamburger buns?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that is why they are called ses", "extracted_entity": null}
{"question": "were the Great Pyramids built by a theocratic government?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They were built by the theocratic", "extracted_entity": null}
{"question": "would it be wise to bring a robusto into Central Park Zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you could get it to fit in", "extracted_entity": null}
{"question": "do restaurants associate meatballs with the wrong country of origin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I guess it depends on where you are in the", "extracted_entity": null}
{"question": "can eating your weight in celery prevent diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can\u2019t eat your weight in", "extracted_entity": null}
{"question": "does Mario use mushrooms to run faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he uses red and green shells.", "extracted_entity": null}
{"question": "are goldfish more difficult to care for than isopods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Isopods are very easy to care for.", "extracted_entity": null}
{"question": "is the rise of agriculture attributed to rivers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, rivers are where people settled and where they", "extracted_entity": null}
{"question": "were number of states in Ancient Greece underwhelming compared to US states in 1900?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Greece had only 13 states,", "extracted_entity": null}
{"question": "did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Eastern Orthodox Church used the same", "extracted_entity": null}
{"question": "has Cesar Millan ever tamed a short-eared dog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He\u2019s not even tried.", "extracted_entity": null}
{"question": "can a chess board be converted to a Shogi board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the pieces are different.", "extracted_entity": null}
{"question": "could R. Kelly write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but if he did", "extracted_entity": null}
{"question": "could the moon fit inside the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you would have to drain the", "extracted_entity": null}
{"question": "can paratroopers be used in a vacuum?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they need air.", "extracted_entity": null}
{"question": "can I hold Bing in a basket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I am small enough to be held in", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is clementine pith highly sought after?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is quite common.", "extracted_entity": null}
{"question": "would a Rabbi celebrate Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That\u2019s a Christian holiday.", "extracted_entity": null}
{"question": "are psychiatric patients welcome to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not", "extracted_entity": null}
{"question": "did Ivan the Terrible use the Byzantine calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Russian Orthodox Church uses the By", "extracted_entity": null}
{"question": "can you get a fever from consuming meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can get a fever from cons", "extracted_entity": null}
{"question": "can Viper Room concert hypothetically be held at National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not possible.", "extracted_entity": null}
{"question": "would you be likely to see storks at a baby shower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, storks don\u2019t have a", "extracted_entity": null}
{"question": "would Methuselah hypothetically hold a record in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He died in the Common Era.", "extracted_entity": null}
{"question": "as of 2020 have more women succeeded John Key than preceded him?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is the only PM to be succeeded", "extracted_entity": null}
{"question": "does Princess Peach's dress resemble a peach fruit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "do mail carriers need multiple uniforms?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are required to change their clothes", "extracted_entity": null}
{"question": "did Easy Rider make a profit at the theater when it was released?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is most store bought rice pudding made with brown rice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "would the chef at Carmine's restaurant panic if there was no basil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "are Chipotle Cinnamon Pork Chops appropriate for a Seder?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They are not kosher for Pass", "extracted_entity": null}
{"question": "could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the cat were a 2.", "extracted_entity": null}
{"question": "does a sea otter eat spiders?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a sea otter eats spiders", "extracted_entity": null}
{"question": "is unanimously elected president's birthday a break for mail carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Unanimously elected president's birth", "extracted_entity": null}
{"question": "does Rusev have to worry about human overpopulation in his homeland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He can't speak the language.", "extracted_entity": null}
{"question": "does Buddy The Elf know anyone who works in publishing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he does know someone who knows someone", "extracted_entity": null}
{"question": "is the Liberty Bell still in its original location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The bell was moved to the new Independ", "extracted_entity": null}
{"question": "does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The book Revolutionary Road gives a glimpse", "extracted_entity": null}
{"question": "does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "could a hundred thousand lolcats fit on a first generation iPhone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not on the iPhone 3G", "extracted_entity": null}
{"question": "are there multiple Disney Zorro?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are multiple Disney Zorro.", "extracted_entity": null}
{"question": "would students at Marist have to petition to get a rowing team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would have to petition to get", "extracted_entity": null}
{"question": "does frost mean that it will be a snowy day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, frost is the dew on the", "extracted_entity": null}
{"question": "did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can a quarter fit inside of a human kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a quarter can't fit inside of", "extracted_entity": null}
{"question": "did Alfred Nobel write a banned book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He wrote the book, \u201cThe Dynam", "extracted_entity": null}
{"question": "could Palm Beach be held in the palm of your hand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Palm Beach is a compact,", "extracted_entity": null}
{"question": "would Jason Voorhees hypothetically fail at being a martyr?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he'd be dead.", "extracted_entity": null}
{"question": "do more Cauliflower grow in Arizona than California?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. More Cauliflower grow in California", "extracted_entity": null}
{"question": "is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it\u2019s November, so the tree", "extracted_entity": null}
{"question": "does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a prequel to the", "extracted_entity": null}
{"question": "does a Trek 9000 require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Trek 9000", "extracted_entity": null}
{"question": "can a Goblin shark hypothetically ride a bike if it had limbs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it had 2 whe", "extracted_entity": null}
{"question": "could John Key issue an executive order in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "does Evander Holyfield eat pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Evander Holyfield does eat pork", "extracted_entity": null}
{"question": "are the headquarters of All Nippon Airways near a beach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the headquarters are in Shimbashi", "extracted_entity": null}
{"question": "will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do hamsters provide food for any animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not eaten by any animals", "extracted_entity": null}
{"question": "can horseradish be eaten in a religious context?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be used as a condiment", "extracted_entity": null}
{"question": "would Taylor Swift refer to Snoopy as oppa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "should children be kept from \"special brownies\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But, they should be kept from \"", "extracted_entity": null}
{"question": "can monkeys use QWERTY keyboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they can use Dvorak.", "extracted_entity": null}
{"question": "are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think so.", "extracted_entity": null}
{"question": "did the Nazis use the Hammer and sickle flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they used the swastika flag", "extracted_entity": null}
{"question": "are any letters directly before and after H missing from Roman numerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, B and C are missing.", "extracted_entity": null}
{"question": "do storks need golden toads to survive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, storks do not need golden to", "extracted_entity": null}
{"question": "did compact discs make computer gaming more popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because they can store more information on the computer", "extracted_entity": null}
{"question": "would a teacher still have their job if they called a black student an ape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would.", "extracted_entity": null}
{"question": "did Bill Gates help to develop the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would a Catholic priest commend someone's pride?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did the Social Democratic Party of Germany help Frederick II become King of Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Frederick II was King of Prussia", "extracted_entity": null}
{"question": "is Eighth Amendment to the United States Constitution popular in court?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is used often in court.", "extracted_entity": null}
{"question": "could Aretha Franklin vote for a president when her second child was born?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could.", "extracted_entity": null}
{"question": "can you use the T-Mobile tuesdays app if you aren't a T-Mobile customer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the T-Mobile Tuesdays app", "extracted_entity": null}
{"question": "are classic nintendo games for emulator legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you are not allowed to distribute", "extracted_entity": null}
{"question": "can a Sphynx cat be used for wool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Sphynx cat is not", "extracted_entity": null}
{"question": "do people remember Lucille Ball's winemaking as successful?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a flop.", "extracted_entity": null}
{"question": "would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's not a plant.", "extracted_entity": null}
{"question": "could Brooke Shields succeed at University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could. She could do well,", "extracted_entity": null}
{"question": "does Neville Longbottom have more courage as a child than as an adult?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Windows or Android smartphones run newer versions of Linux?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only for a short time.", "extracted_entity": null}
{"question": "can the largest crustacean stretch out completely on a king-sized mattress?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is a giant king-s", "extracted_entity": null}
{"question": "did Bruiser Brody wrestle on WWE Raw?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Bruiser Brody wrestled on W", "extracted_entity": null}
{"question": "is Atlantic cod found in a vegemite sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is an astronomer interested in drosophila?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is interested in the study of", "extracted_entity": null}
{"question": "do people who smoke Djarum's like cloves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They like the taste of clove,", "extracted_entity": null}
{"question": "was Bruce Lee absent from the 1964 University of Washington graduation ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was there", "extracted_entity": null}
{"question": "does chlorine inhibit photosynthesis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chlorine is a poisonous gas", "extracted_entity": null}
{"question": "does Amtrak operate four wheel vehicles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only two wheel vehicles.", "extracted_entity": null}
{"question": "will parma ham be ready for New Year's if the pig is slaughtered in December?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it will be ready for New Year'", "extracted_entity": null}
{"question": "have jokes killed more people than rats in history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as many as lies.", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Islamophobia is not misdirect", "extracted_entity": null}
{"question": "are there multiple American government holidays during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes there are.", "extracted_entity": null}
{"question": "is Romeo and Juliet an unusual title to teach high schoolers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. and it should be taught.", "extracted_entity": null}
{"question": "does the density of helium cause voices to sound deeper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn't.", "extracted_entity": null}
{"question": "did Snoop Dogg refuse to make music with rival gang members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he refused to make music with rival gang", "extracted_entity": null}
{"question": "are Leopard cats in less dire straits than Bornean Orangutan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'd have", "extracted_entity": null}
{"question": "did the Wehrmacht affect the outcome of the War to End All Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was the German Army that was defeated", "extracted_entity": null}
{"question": "could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could.", "extracted_entity": null}
{"question": "has Ivan the Terrible flown to Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "would a silicon shortage be bad for Intel's sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if Intel is unable to supply enough process", "extracted_entity": null}
{"question": "would a Frigatebird in Ontario be a strange sight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Frigatebirds are known to", "extracted_entity": null}
{"question": "can cancer cause excess adrenaline production?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, cancer can cause excess adrenaline", "extracted_entity": null}
{"question": "can you cure hepatitis with a tonsillectomy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can cure it with a", "extracted_entity": null}
{"question": "if someone loves buffalo wings do they enjoy capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if someone loves buffalo wings do they enjoy", "extracted_entity": null}
{"question": "are tampons a good 24 hour solution for mentruation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you can use tampons then they are", "extracted_entity": null}
{"question": "is the Royal Air Force ensign on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Royal Air Force ensign is not", "extracted_entity": null}
{"question": "would a customer be happy if their grocery store meat tasted like game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not be happy.", "extracted_entity": null}
{"question": "is the cuisine of Hawaii suitable for a vegan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Absolutely! Hawaii has a lot of", "extracted_entity": null}
{"question": "is it safe to wear sandals in snow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not safe to wear sandals", "extracted_entity": null}
{"question": "is the Very Large Telescope the most productive telescope in the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is All Purpose Flour safe for someone who has celiac disease?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. All Purpose Flour is made from", "extracted_entity": null}
{"question": "while viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, adrenaline will be increased by", "extracted_entity": null}
{"question": "should spaghetti be slick when cooked?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If it\u2019s dry, you\u2019ve cook", "extracted_entity": null}
{"question": "do Sweet Potatoes prevent other plants from growing in their place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not.", "extracted_entity": null}
{"question": "was Richard III ruler of Adelaide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was ruler of England.", "extracted_entity": null}
{"question": "are Sable's a good choice of Mustelidae to weigh down a scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea.", "extracted_entity": null}
{"question": "was Snoop Dogg's debut studio album released on the weekend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was released on the 20", "extracted_entity": null}
{"question": "do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they are more likely to have a", "extracted_entity": null}
{"question": "is Krishna similar to Holy Spirit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Krishna is the Holy Spirit of", "extracted_entity": null}
{"question": "is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you do it right", "extracted_entity": null}
{"question": "does Santa Claus work during summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he goes to Florida.", "extracted_entity": null}
{"question": "would the average American family find Adam Sandler's home to be too small?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But not as small as his wife'", "extracted_entity": null}
{"question": "could the first European visitor to Guam been friends with Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first European visitor to Guam could", "extracted_entity": null}
{"question": "could someone have arrived at Wrestlemania X in a Toyota Prius?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but someone could have arrived at Wrestle", "extracted_entity": null}
{"question": "would Republic of Korea Navy dominate Eritrea navy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Eritrean Navy has", "extracted_entity": null}
{"question": "could a Gladiator's weapon crush a diamond?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it was a large enough diamond", "extracted_entity": null}
{"question": "was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Second Amendment was written to", "extracted_entity": null}
{"question": "can Lamborghini's fastest model win a race against a Porsche 911?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the race is on a track where", "extracted_entity": null}
{"question": "does a mongoose have natural camouflage for desert?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's a bit of a mystery.", "extracted_entity": null}
{"question": "if someone is a vegan, would they eat honey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it's animal product.", "extracted_entity": null}
{"question": "does Jack Sparrow know any sea shantys?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he probably does.", "extracted_entity": null}
{"question": "is Thanksgiving sometimes considered a day of mourning?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the way it was celebrated by", "extracted_entity": null}
{"question": "were there fifty English kings throughout the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there were only 40.", "extracted_entity": null}
{"question": "can rowing competitions take place indoors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can have rowing competitions ind", "extracted_entity": null}
{"question": "did Sartre write a play about Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Martin Luther same sect as Martin Luther King Jr.?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Martin Luther King Jr. was a Bapt", "extracted_entity": null}
{"question": "could Amazon afford The Mona Lisa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it wouldn\u2019t be a good", "extracted_entity": null}
{"question": "can you find a railroad engineer on TNT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not on TNT, but on the TNT", "extracted_entity": null}
{"question": "would Franz Ferdinand have survived with armadillo armor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because armadillos have a", "extracted_entity": null}
{"question": "would keelhauling be a fair punishment under the Eighth Amendment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s not cruel and unusual.", "extracted_entity": null}
{"question": "is Bern located east of Paris?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bern is located in Switzerland.", "extracted_entity": null}
{"question": "can Herpes simplex virus spread on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can not.", "extracted_entity": null}
{"question": "is Mixed martial arts totally original from Roman Colosseum games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. MMA is a combination of different fighting", "extracted_entity": null}
{"question": "are those incapable of reproduction incapable of parenthood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. I mean, I'm", "extracted_entity": null}
{"question": "would Amy Winehouse's death have been prevented with Narcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not", "extracted_entity": null}
{"question": "was Gandalf present at the death of Eomer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was in Rivendell.", "extracted_entity": null}
{"question": "does coding rely on Boolean algebra characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "will Queen Elizabeth be buried in the Pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she will be buried in St George'", "extracted_entity": null}
{"question": "can fish get Tonsillitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible for fish to get T", "extracted_entity": null}
{"question": "would Sophist's have hypothetically made good lawyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were very good at arguing both", "extracted_entity": null}
{"question": "would a pescatarian be unable to eat anchovy pizza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be.", "extracted_entity": null}
{"question": "could a chipmunk fit 100 chocolate chips in his mouth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a chipmunk can fit 1", "extracted_entity": null}
{"question": "does The Hague border multiple bodies of water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it does border the North Sea", "extracted_entity": null}
{"question": "is Rick and Morty considered an anime?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is considered an anime.", "extracted_entity": null}
{"question": "could a llama birth twice during War in Vietnam (1945-46)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A llama cannot give birth to two", "extracted_entity": null}
{"question": "do some religions look forward to armageddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some religions do look forward to Arm", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It has not.", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to a war victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not completed until after the war", "extracted_entity": null}
{"question": "was the Euro used in Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the German Reichsmark was used in", "extracted_entity": null}
{"question": "would someone with back pain enjoy picking strawberries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the person is able to bend", "extracted_entity": null}
{"question": "is Olympia, Washington part of \"Ish river country\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "was Rumi's work serialized in a magazine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not", "extracted_entity": null}
{"question": "was Kane (wrestler) banned from WCW  headquarters city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was banned from the entire state", "extracted_entity": null}
{"question": "can you only see hippopotamus in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can see hippos in other", "extracted_entity": null}
{"question": "would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And the rulers would have to", "extracted_entity": null}
{"question": "can giant pandas sell out a Metallica show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can\u2019t.", "extracted_entity": null}
{"question": "would Janet Jackson avoid a dish with ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she is a Muslim.", "extracted_entity": null}
{"question": "was Charlemagne's father instrumental in outcome of the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his father was not involved in the Battle", "extracted_entity": null}
{"question": "did Doctor Strange creators also make Batman?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "was ethanol beneficial to Jack Kerouac's health?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did get to use his car", "extracted_entity": null}
{"question": "do some home remedies result in your skin color turning blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That\u2019s a myth.", "extracted_entity": null}
{"question": "is eleventh grade required to get a driver's licence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the only thing required to get a driver", "extracted_entity": null}
{"question": "are moose used for work near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "do onions have a form that resembles the inside of a tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "does handedness determine how you use American Sign Language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a language and languages are", "extracted_entity": null}
{"question": "did Harry Houdini's wife make psychics look foolish?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the psychics were already fools", "extracted_entity": null}
{"question": "did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "did King James I despise fairy beings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "could the main character of \"Alice's Adventures in Wonderland\" join a Masonic Lodge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the book is not about Freem", "extracted_entity": null}
{"question": "does a Generation Y member satisfy NYPD police officer age requirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. NYPD requires candidates to be at least", "extracted_entity": null}
{"question": "is polyamory allowed in the Catholic Church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Catholics are not allowed to be", "extracted_entity": null}
{"question": "would an anxious person benefit from receiving courage from the Wizard of Oz?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Anxious people need to be told", "extracted_entity": null}
{"question": "do Christians anticipate an existence in Sheol after death?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in the sense that they will", "extracted_entity": null}
{"question": "did Christina Aguilera turn her chair around for Kelly Clarkson on The Voice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did.", "extracted_entity": null}
{"question": "do Jews believe in any New Testament angels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jews do not believe in angels.", "extracted_entity": null}
{"question": "has cannabis been a big influence in rap music genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 80s and", "extracted_entity": null}
{"question": "does Bombyx mori have a monopoly over silk production?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are many other silk producing species", "extracted_entity": null}
{"question": "will Chuck Norris be a nonagenarian by time next leap year after 2020 happens?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will be 90.", "extracted_entity": null}
{"question": "does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's called Rhodesia", "extracted_entity": null}
{"question": "are most books written as a Haiku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most books are written as Haiku.", "extracted_entity": null}
{"question": "did the writer of Christmas carol fast during Ramadan? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "do you need to worry about Zika virus in Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Zika is not a concern in Ant", "extracted_entity": null}
{"question": "did a Mediterranean Sea creature kill Steve Irwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Steve Irwin died from a sting", "extracted_entity": null}
{"question": "does Linus Torvalds make money off of DirectX?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Linus Torvalds is a free", "extracted_entity": null}
{"question": "could someone theoretically use an armadillo as a shield?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's not a bad idea,", "extracted_entity": null}
{"question": "could a Hwasong-15 missile hypothetically reach Voyager 2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is equipped with a nuclear", "extracted_entity": null}
{"question": "were weather phenomena avoided when naming minor league baseball teams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The Columbus Clippers are a", "extracted_entity": null}
{"question": "does Hades appear in a Disney Channel musical movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in the movie \"Hercules", "extracted_entity": null}
{"question": "would it be typical for a Rede Globo anchor to say Konnichiwa to the viewers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "is the foot part of the metric system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not. The metric system", "extracted_entity": null}
{"question": "is week old chlorine water safe to drink?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "As long as it is clear and odorless", "extracted_entity": null}
{"question": "is Britney Spears' breakdown attributed to bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not written until 16", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and in most grocery stores,", "extracted_entity": null}
{"question": "could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I couldn't.", "extracted_entity": null}
{"question": "were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were.", "extracted_entity": null}
{"question": "at a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "in American society, will a bachelor's degree often include a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is \"no\".", "extracted_entity": null}
{"question": "is Guitar Hero Beatles inappropriate for a US third grader?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it is.", "extracted_entity": null}
{"question": "can a computer be programmed entirely in Boolean algebra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Boolean algebra is a subset of the language", "extracted_entity": null}
{"question": "would Eminem perform well at the International Mathematical Olympiad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would.", "extracted_entity": null}
{"question": "is the span in C-SPAN named after Alan Greenspan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is named after C-SPAN", "extracted_entity": null}
{"question": "did Van Gogh suffer from a mental disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he suffered from bipolar disorder", "extracted_entity": null}
{"question": "does a person need to be a parent to become a grandparent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can be a grandparent without having", "extracted_entity": null}
{"question": "did either Kublai Khan or his grandfather practice monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Kublai Khan had 20", "extracted_entity": null}
{"question": "do frogs feel disgust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, frogs feel disgust.", "extracted_entity": null}
{"question": "has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has.", "extracted_entity": null}
{"question": "did Native American tribes teach Spaniards how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "were there under 150,000 American troops in Vietnam in 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The number of American troops in Vietnam in", "extracted_entity": null}
{"question": "do guitarist's have fingers that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have to be able to play with", "extracted_entity": null}
{"question": "could someone mistake the smell of your brussels sprouts for a fart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and I\u2019ve had people do it", "extracted_entity": null}
{"question": "would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And it's a good thing they", "extracted_entity": null}
{"question": "was Donald Trump the target of Islamophobia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was the target of Islamophob", "extracted_entity": null}
{"question": "are there winged statuettes in the home of the creator of Law & Order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they're not winged.", "extracted_entity": null}
{"question": "could a silverfish reach the top of the Empire State Building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they\u2019d have to climb", "extracted_entity": null}
{"question": "did Eddie Murphy's father see his first stand up show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but his mother did", "extracted_entity": null}
{"question": "is watching  Star Wars necessary to know who Darth Vader is?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is a good idea.", "extracted_entity": null}
{"question": "did Eric Clapton have similar taste in women to one of the Beatles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Eric Clapton married Pattie Bo", "extracted_entity": null}
{"question": "is Linus Torvalds' wife unable to physically defend herself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But she's a very smart woman", "extracted_entity": null}
{"question": "did Clark Gable appear in any movies scored by John Williams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1969\u2019s", "extracted_entity": null}
{"question": "can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because camels are ruminants,", "extracted_entity": null}
{"question": "can Clouded leopards chase down many Pronghorn antelopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not usually", "extracted_entity": null}
{"question": "has a neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but I'm sure they're", "extracted_entity": null}
{"question": "will Tokyo Tower be repainted only once during President Trump's first term?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. 1 time.", "extracted_entity": null}
{"question": "would George Fox support stoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "could Oscar Wilde have operated a motor vehicle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, because he had a club foot.", "extracted_entity": null}
{"question": "will Oasis cruise boat traverse the Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It will take the Holland Tunnel.", "extracted_entity": null}
{"question": "is there a full Neptunian orbit between the first two burials of women in the Panth\u00e9on?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is.", "extracted_entity": null}
{"question": "would an eleventh-grader be eligible for Medicare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Medicare is a health insurance program", "extracted_entity": null}
{"question": "is 3D printing able to make adenovirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, 3D printing can make aden", "extracted_entity": null}
{"question": "does rock star Keith Richards play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He plays a guy who is a", "extracted_entity": null}
{"question": "will someone die without white blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they do not get transfused", "extracted_entity": null}
{"question": "can the Powerpuff Girls form a complete tag team wrestling match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "is Antarctica a good location for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is too cold", "extracted_entity": null}
{"question": "do more anchovy live in colder temperature waters than warmer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the opposite is true. Anchov", "extracted_entity": null}
{"question": "could a giant squid fit aboard the deck of the titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be in a terrible position", "extracted_entity": null}
{"question": "would a Deacon be likely to be a fan of the podcast 'God Awful Movies'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "could ten gallons of seawater crush a six year old?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it's a six year old", "extracted_entity": null}
{"question": "would the high school class of 2010 have lived through the Presidency of Richard Nixon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Richard Nixon was born in", "extracted_entity": null}
{"question": "has the Subway restaurant franchise had any connections with child abusers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, see the article:", "extracted_entity": null}
{"question": "is the kayak a traditional boat in New Zealand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the kayak is a traditional boat", "extracted_entity": null}
{"question": "do manta rays live in water above the safe temperature for cold food storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do.", "extracted_entity": null}
{"question": "could Scooby Doo fit in a kangaroo pouch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would be upside down and", "extracted_entity": null}
{"question": "would you take a photo of a Saltwater crocodile in Memphis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I would not.", "extracted_entity": null}
{"question": "can you write a whole Haiku in a single tweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "wIll Noah's Ark hypothetically sail through flooded Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The ark would be too heavy.", "extracted_entity": null}
{"question": "would a pear sink in water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a pear would sink in water.", "extracted_entity": null}
{"question": "does Northern fur seal make good pet for six year old?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the child and the seal.", "extracted_entity": null}
{"question": "did Millard Fillmore help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Millard Fillmore did not help", "extracted_entity": null}
{"question": "was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in a way, but no, in", "extracted_entity": null}
{"question": "was The Little Prince's titular character allergic to flowers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Little Prince's allergy to", "extracted_entity": null}
{"question": "are you likely to find a crucifix in Karachi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I am.", "extracted_entity": null}
{"question": "would a hypothetical Yeti be towered over by Andre the Giant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be dwarfed.", "extracted_entity": null}
{"question": "are any animals in Chinese calendar Chordata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. None of the animals in the Chinese calendar", "extracted_entity": null}
{"question": "does Andrew Johnson's presidential number exceed Elagabalus's Emperor number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by 1", "extracted_entity": null}
{"question": "would Firefighters be included in a September 11th memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not included in the mem", "extracted_entity": null}
{"question": "did J. P. Morgan have healthy lungs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did have a strong constitution.", "extracted_entity": null}
{"question": "was the original James Bond actor born near the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Sean Connery was born", "extracted_entity": null}
{"question": "does Ukrainian Greek Catholic Church recognize Alexander Nevsky as a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "could an ocelot subsist on a single bee hummingbird per day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would starve to death.", "extracted_entity": null}
{"question": "could Lil Wayne legally operate a vehicle on his own at the beginning of his career?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he was 16.", "extracted_entity": null}
{"question": "is double duty an incorrect phrase for host of Dancing With The Stars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "could Lil Wayne's children ride in a Chevrolet Corvette ZR1 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They could.", "extracted_entity": null}
{"question": "is the best tasting part of the papaya in the center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the papaya is a fruit.", "extracted_entity": null}
{"question": "did Japanese serfdom have higher status than English counterpart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "was the Louisiana Purchase made with bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a Pict be confused by Old English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Picts were Celt", "extracted_entity": null}
{"question": "would a week be enough time to watch every episode of Ugly Betty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did any country in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many countries did.", "extracted_entity": null}
{"question": "could a dandelion suffer from hepatitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the liver is an organ found in", "extracted_entity": null}
{"question": "would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think it would.", "extracted_entity": null}
{"question": "would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Iwato scales can be used for", "extracted_entity": null}
{"question": "while on a liquid diet, are there some types of soup you cannot eat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are. You can't have", "extracted_entity": null}
{"question": "is art prioritized in the US education system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really.", "extracted_entity": null}
{"question": "is it okay to lie after taking an oath in a court of law?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if the oath is taken to protect the life", "extracted_entity": null}
{"question": "would someone in Mumbai refer to Solanum melongena as an eggplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "is Menthol associated with Thanksgiving?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the name is.", "extracted_entity": null}
{"question": "can a strawberry get worms similar to dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, strawberries can get worm", "extracted_entity": null}
{"question": "is the Illuminati card game still popular?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Amish don't wear", "extracted_entity": null}
{"question": "if your skin was turning the color of a zombie, could it be because of nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not nickel", "extracted_entity": null}
{"question": "can the Swiss Guard fill the Virginia General Assembly chairs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Swiss Guard can fill the Virginia General", "extracted_entity": null}
{"question": "if you add water to rice pudding is it horchata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would an ethics professor teach a class on Cezanne?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but an art professor would teach a class", "extracted_entity": null}
{"question": "is the tibia necessary to win the Stanley Cup?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it helps.", "extracted_entity": null}
{"question": "could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, because he would be too far away", "extracted_entity": null}
{"question": "is Miami a city on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's on the East Coast.", "extracted_entity": null}
{"question": "would a broadcast from Spirit make the news in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "would food made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, black salt does not smell of sul", "extracted_entity": null}
{"question": "was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "\"Lovers and Friends\" (20", "extracted_entity": null}
{"question": "could George Washington's own speeches have been recorded live to a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because George Washington's speeches were", "extracted_entity": null}
{"question": "would a geographer use biochemistry in their work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they would. I've seen some", "extracted_entity": null}
{"question": "does Disney have an ice princess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she\u2019s not a princess", "extracted_entity": null}
{"question": "is there a warthog on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there is a Broadway in a war", "extracted_entity": null}
{"question": "would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by \u201c", "extracted_entity": null}
{"question": "could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, for example the keyboards in the D", "extracted_entity": null}
{"question": "is Disneyland Paris the largest Disney resort?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Disneyland Paris is the second largest.", "extracted_entity": null}
{"question": "was the Donatello crucifix identified in 2020 life size?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the crucifix was not identified", "extracted_entity": null}
{"question": "would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "would the top of Mount Fuji stick out of the Sea of Japan? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The sea of Japan is 40", "extracted_entity": null}
{"question": "could the Powerpuff Girls hypothetically attend the Camden Military Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could attend the Camden Military Academy", "extracted_entity": null}
{"question": "if you were on a diet, would you have to skip lunch at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Dustin Hoffman one of the B'nei Yisrael?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he is, as he is a Jew.", "extracted_entity": null}
{"question": "does actress Leila George lack the height to be a model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn\u2019t.", "extracted_entity": null}
{"question": "is average number of peas in a pod enough commas for a billion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the average number of peas in a", "extracted_entity": null}
{"question": "would a psychic who admits to hot reading be trustworthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not necessarily.", "extracted_entity": null}
{"question": "is greed the most prevalent of the Seven Deadly Sins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so. It is the most", "extracted_entity": null}
{"question": "would a student of the class of 2017 have amnesia about 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not unless they're a total mor", "extracted_entity": null}
{"question": "does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Dragon Ball shows and movies have", "extracted_entity": null}
{"question": "would Nancy Pelosi publicly denounce abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. She is pro-abortion.", "extracted_entity": null}
{"question": "did land owners elect their rulers in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were elected by the king.", "extracted_entity": null}
{"question": "does Biochemistry study gluons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Gluons are only studied by particle", "extracted_entity": null}
{"question": "are Christmas trees dissimilar to deciduous trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "did the Space Race use relay batons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they did not.", "extracted_entity": null}
{"question": "can you buy Casio products at Petco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Casio products are not available at Pet", "extracted_entity": null}
{"question": "is the language used in Saint Vincent and the Grenadines rooted in English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the language is a mixture of English", "extracted_entity": null}
{"question": "is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "will the Albany in Georgia reach a hundred thousand occupants before the one in New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Albany, GA, has a population of", "extracted_entity": null}
{"question": "do the anchors on Rede Globo speak Chinese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they speak Portuguese", "extracted_entity": null}
{"question": "is shrimp scampi definitely free of plastic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The shrimp is cooked in", "extracted_entity": null}
{"question": "is a pound sterling valuable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "would a dog respond to bell before Grey seal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The grey seal is a nocturnal", "extracted_entity": null}
{"question": "would a Monoamine Oxidase candy bar cheer up a depressed friend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you eat the whole thing.", "extracted_entity": null}
{"question": "could the members of The Police perform lawful arrests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they could not", "extracted_entity": null}
{"question": "are more people today related to Genghis Khan than Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
