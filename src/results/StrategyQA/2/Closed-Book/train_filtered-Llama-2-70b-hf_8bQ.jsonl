{"question": "are Lentil able to withstand a wind storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can withstand 130", "extracted_entity": null}
{"question": "are Lentil able to withstand frost?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lentil is able to withstand", "extracted_entity": null}
{"question": "are drain flies able to survive bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not the best way", "extracted_entity": null}
{"question": "would an oil miner be interested in the Persian Gulf?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's a big oil producer.", "extracted_entity": null}
{"question": "would an aerodynamic cactus dessicate more quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "would an aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be more streamlined.", "extracted_entity": null}
{"question": "would a more aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need water more frequently?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cacti are able to store water", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need more water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A cactus is a cact", "extracted_entity": null}
{"question": "do apple farmers benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the bees die without a queen", "extracted_entity": null}
{"question": "do apples benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, apples don't benefit from be", "extracted_entity": null}
{"question": "is Tokyo Tower repainted often enough that someone can do the work seven times during their career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Tokyo Tower is repainted every five", "extracted_entity": null}
{"question": "can a worker repaint Tokyo Tower seven times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he paints it seven", "extracted_entity": null}
{"question": "can a worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the tower is repainted every", "extracted_entity": null}
{"question": "can the same worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can.", "extracted_entity": null}
{"question": "can the same worker repaint Tokyo Tower seven or more times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It takes seven years to repaint Tokyo", "extracted_entity": null}
{"question": "can cheap cinnamon sticks break a coffee grinder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I was trying to grind some c", "extracted_entity": null}
{"question": "would an aerodynamic cactus get thirsty more often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "when en route from Beijing to Paris, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The imperial foot is the standard unit of", "extracted_entity": null}
{"question": "when en route from Beijing to London, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the metric system is used for international", "extracted_entity": null}
{"question": "when en route from London to Beijing, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would be insane to use", "extracted_entity": null}
{"question": "when en route from London to Beijing, will pilots convert the foot to metric?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they will not.", "extracted_entity": null}
{"question": "does a new pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The pilot may use the metric foot.", "extracted_entity": null}
{"question": "does a pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the pilot from Moscow can use the metric", "extracted_entity": null}
{"question": "would a pilot leaving Moscow during the 2018 Fifa World Cup report altitude in feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Russia uses meters for altitude.", "extracted_entity": null}
{"question": "is growing cucumber with seeds bad for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Growing cucumbers with seeds", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the gardener is afraid of insect", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with a bee phobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, cucumbers do not need pol", "extracted_entity": null}
{"question": "can someone deathly allergic to bees safely grow cucumber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they should always wear gloves when", "extracted_entity": null}
{"question": "would suspension bridges be better able to withstand an earthquake than a beam bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because suspension bridges are more flexible than", "extracted_entity": null}
{"question": "does a cotton ball provide nutrition for humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is made of cotton.", "extracted_entity": null}
{"question": "would a classic xbox emulator be popular in the future?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not. The Xbox is not a very", "extracted_entity": null}
{"question": "would a classic xbox emulator sell well in 2040?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, the games are not that good.", "extracted_entity": null}
{"question": "would Miley Cyrus sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. But would Iggy Pop sing with", "extracted_entity": null}
{"question": "would selina gomez sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she would sing with Iggy", "extracted_entity": null}
{"question": "would Iggy Pop cover a Justin Bieber song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "would Iggy Pop cover a Sean Mendez song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he wouldn\u2019t.", "extracted_entity": null}
{"question": "would Sean Mendez join Iggy Pop on stage?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's a little bitch", "extracted_entity": null}
{"question": "do Alan Turing and Jeff Bezos have the same alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both went to Princeton University", "extracted_entity": null}
{"question": "would Justin Bieber join Iggy Pop at the Warp Tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not.", "extracted_entity": null}
{"question": "would Justin Bieber join Iggy Pop on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. (unless he's got a new", "extracted_entity": null}
{"question": "would Iggy Popi invite Justin Bieber on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop would not invite Justin B", "extracted_entity": null}
{"question": "would Iggy Popi invite Justin Bieber to jam?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he would invite Iggy", "extracted_entity": null}
{"question": "would Iggy Pop and Justin Bieber rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Iggy Pop will invite Justin", "extracted_entity": null}
{"question": "could Alan Turing have used a slide rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I'm sure he could have,", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to rock?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He will not.", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to play together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He will not.", "extracted_entity": null}
{"question": "will Justin Bieber play on stage with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But Iggy Pop will play on", "extracted_entity": null}
{"question": "would Justin Bieber play songs by Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "is the Mona Lisa within one kilometre of the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, both are in the Louvre.", "extracted_entity": null}
{"question": "would Iggy Pop play Justin Bieber songs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Iggy Pop would not play Justin", "extracted_entity": null}
{"question": "would Justin Bieber jam with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the two have been spotted at", "extracted_entity": null}
{"question": "would Justin Bieber tour with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but if he did, he'd", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "can you make cheese from domestic yak milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yak milk is very similar to cow", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he would tour with The Stoog", "extracted_entity": null}
{"question": "could a snowy owl survive on a diet of mice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not very well", "extracted_entity": null}
{"question": "hypothetically, could a snowy owl survive the climate of Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it migrated there.", "extracted_entity": null}
{"question": "is overfeeding Lactobacillus unwise for people afraid of dentists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I have been trying to get a dent", "extracted_entity": null}
{"question": "do nerds like Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "of course we do!", "extracted_entity": null}
{"question": "has Naruto been a bad influence on people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is a bad influence on people", "extracted_entity": null}
{"question": "have people risked their lives because of Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many have.", "extracted_entity": null}
{"question": "has Naruto inspired dangerous activities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I know a person who tried to use", "extracted_entity": null}
{"question": "can you hold bitcoin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in digital form.", "extracted_entity": null}
{"question": "can bitcoin fit in your hand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, bitcoin is not a physical thing", "extracted_entity": null}
{"question": "can bitcoin fit in wallets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the traditional sense of the", "extracted_entity": null}
{"question": "can bitcoin buy vaporizers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, bitcoin can buy vaporizers", "extracted_entity": null}
{"question": "are meatballs popular in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, very much so.", "extracted_entity": null}
{"question": "can meatballs be used in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, meatballs are a staple", "extracted_entity": null}
{"question": "is the color green associated with a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not in the traditional sense.", "extracted_entity": null}
{"question": "are there wild giant pandas south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are only found in the wild in", "extracted_entity": null}
{"question": "are there giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are giant pandas in the southern hem", "extracted_entity": null}
{"question": "are there wild giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no wild giant pandas in the", "extracted_entity": null}
{"question": "are there wild giant pandas in France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there are no wild giant pandas in France", "extracted_entity": null}
{"question": "did Buzz Aldrin walk on the moon in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was the second man to walk on", "extracted_entity": null}
{"question": "is honey associated with a kind of queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The queen bee.", "extracted_entity": null}
{"question": "are chinchillas a kind of bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are rodents.", "extracted_entity": null}
{"question": "did Charles Manson die at home?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he died in the hospital.", "extracted_entity": null}
{"question": "is the Milky Way galaxy on a collision course?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Milky Way is on a collision", "extracted_entity": null}
{"question": "are the Constitution and Bill of Rights related?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Bill of Rights is the first", "extracted_entity": null}
{"question": "can the Constitution be updated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can the Constitution be changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "are infants a possible result of sex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "do infants need diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course!", "extracted_entity": null}
{"question": "is Mount Emei located to the west of the Alps??", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Mount Emei is located in S", "extracted_entity": null}
{"question": "is Mount Emei one of the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a mountain in China.", "extracted_entity": null}
{"question": "is Mount Emei one of the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a mountain in China", "extracted_entity": null}
{"question": "is Mount Emei one of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is one of the Four Famous", "extracted_entity": null}
{"question": "is Mount Emei part of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Mount Emei is not part of", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in Arizona?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia.", "extracted_entity": null}
{"question": "is the original Communist leader of China dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mao's dead.", "extracted_entity": null}
{"question": "is latitude related to the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, latitude is measured in degrees north or south", "extracted_entity": null}
{"question": "is June associated with a seasonal change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. June is associated with the summer solst", "extracted_entity": null}
{"question": "is June associated with a patriotic American symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the American flag.", "extracted_entity": null}
{"question": "is Orange County near the border with Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (Hint: It\u2019s near the", "extracted_entity": null}
{"question": "are Douglas firs native to the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not native to the Moj", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant inspired by Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant themed around Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": " Is cactus fruit a good menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a good menu item for", "extracted_entity": null}
{"question": "should a Tenochtitlan cuisine-style restaurant serve cactus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it\u2019s not native to Mexico", "extracted_entity": null}
{"question": "is amnesia probably why a student of the class of 2016 forgot 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Amnesia is the loss of memory, so", "extracted_entity": null}
{"question": "is amnesia why a student of the class of 2017 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. 9/11 happened 1", "extracted_entity": null}
{"question": "is amnesia likely why someone born in 1999 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I was born in", "extracted_entity": null}
{"question": "does the Giant panda live near Shanghai?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Giant panda lives in the", "extracted_entity": null}
{"question": "is the giant panda vegetarian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it eats bamboo.", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a part of the Baltic", "extracted_entity": null}
{"question": "are hamburgers usually made with ham?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (That would be a ham burger", "extracted_entity": null}
{"question": "do wild camels live near Las Vegas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Camels are native to Africa, the", "extracted_entity": null}
{"question": "are camels native to the Mojave Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "does Simon Cowell still host American Idol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn't.", "extracted_entity": null}
{"question": "is Europa one of the first few moons of Jupiter discovered by humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Europa is one of the first few mo", "extracted_entity": null}
{"question": "is Europa the smallest moon of Jupiter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is the fourth largest.", "extracted_entity": null}
{"question": "can a person donate a kidney while still alive?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The most common living donation is a", "extracted_entity": null}
{"question": "does an organ donor need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The kidney must be removed from the", "extracted_entity": null}
{"question": "does a human need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because kidneys can be donated", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Amazon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia.", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia", "extracted_entity": null}
{"question": "does Snow White live with an even number of dwarves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she lives with seven dwarves", "extracted_entity": null}
{"question": "did Richard III die in old age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in battle.", "extracted_entity": null}
{"question": "did reptiles live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first reptiles appeared in the", "extracted_entity": null}
{"question": "did birds live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, birds evolved much later.", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they lived during the Mesozoic", "extracted_entity": null}
{"question": "does Jennifer Lawrence have significantly more Academy Award nominations than actual wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena home to an NBA team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Los Angeles Lakers", "extracted_entity": null}
{"question": "is amnesia likely why someone born in 1999 forgets 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The attacks happened when they were 2", "extracted_entity": null}
{"question": "is surfing popular in Switzerland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not really.", "extracted_entity": null}
{"question": "is surfing popular in Quebec?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is surfing popular in Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is popular in California", "extracted_entity": null}
{"question": "did Amy Winehouse vote for Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she's dead.", "extracted_entity": null}
{"question": "did Amy Winehouse vote for President Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died in 2011", "extracted_entity": null}
{"question": "is there a parody song about tabloid journalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not very good", "extracted_entity": null}
{"question": "is the current Speaker of the House a Republican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Boehner.", "extracted_entity": null}
{"question": "is the Antarctic Peninsula south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is north of the Equator.", "extracted_entity": null}
{"question": "was natural rubber originally produced in South America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, natural rubber was originally produced in South", "extracted_entity": null}
{"question": "can a Porsche lift off the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it's towed", "extracted_entity": null}
{"question": "is the central processing unit part of a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the central processing unit is not part of", "extracted_entity": null}
{"question": "is the central processing unit attached to a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the CPU is attached to the motherboard", "extracted_entity": null}
{"question": "does the Easter Bunny appear during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Easter Bunny appears during spring", "extracted_entity": null}
{"question": "is the Easter Bunny associated with eggs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Easter Bunny is associated with", "extracted_entity": null}
{"question": "can the freedom of speech be legally limited in the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be limited.", "extracted_entity": null}
{"question": "are the Illuminati involved in the plot of a movie based on a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are involved in the plot of the", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it borders Finland, which is in Sc", "extracted_entity": null}
{"question": "were Dr. Seuss poems performed on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not until 2000", "extracted_entity": null}
{"question": "were Dr. Seuss books adapted into a stage play?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Seussical", "extracted_entity": null}
{"question": "did Dr. Seuss poetry become adapted for stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a musical, Seussical, is", "extracted_entity": null}
{"question": "is there a named elephant character in a book by Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no elephant in a", "extracted_entity": null}
{"question": "did Dr. Seuss create an elephant character in one of his books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Horton", "extracted_entity": null}
{"question": "is amber important in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jurassic Park.", "extracted_entity": null}
{"question": "does amber play an important role in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "does amber play an important role in the Jurassic Park series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. Amber is a foss", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is part of the Pass", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is part of the Pass", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is part of the Jewish", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Prophet Elijah is", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is part of a Jewish", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a Portuguese sailor who sa", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he sailed under the flag of Spain", "extracted_entity": null}
{"question": "was the current Parc de Princes built on the site of a previous stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the old Parc de Princes was", "extracted_entity": null}
{"question": "were the British directly involved in the Arab-Israeli conflict during the 1940s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the British were directly involved in the Arab", "extracted_entity": null}
{"question": "was Athena's statue from the Parthenon originally sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was sculpted by Pheidias", "extracted_entity": null}
{"question": "was Athena's statue from the Parthenon sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was sculpted by Phidias", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently in Asia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's in the British Museum", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in the British Museum in London", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display in the Louvre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it is", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display in the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the British Museum.", "extracted_entity": null}
{"question": "do wild emu live in the Everglades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not native to the Ever", "extracted_entity": null}
{"question": "do wild emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they live in Australia.", "extracted_entity": null}
{"question": "do emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do, but not as many as", "extracted_entity": null}
{"question": "were the 2018 Winter Olympics held in the Western Hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the 2018 Winter Olympics", "extracted_entity": null}
{"question": "can olives be grown in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if there is enough water", "extracted_entity": null}
{"question": "is Bern located north of Turin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Bern located south of Munich?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is Bern located south of Berlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not the capital of Switzerland", "extracted_entity": null}
{"question": "is Bern located to the east of the Swiss-French border?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located to the west.", "extracted_entity": null}
{"question": "is Bern located between the Swiss borders to Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bern is located between the Swiss borders to", "extracted_entity": null}
{"question": "is Bern located between Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bern is located in Switzerland", "extracted_entity": null}
{"question": "was \"D'oh\" first said during the 1980's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, during the 1980'", "extracted_entity": null}
{"question": "can you fit the Eifel Tower under the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a tight fit.", "extracted_entity": null}
{"question": "are there multiple Zorro featured by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Zorro is the same character in", "extracted_entity": null}
{"question": "is the rank of lieutenant different across groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, for example in the US army a lieutenant", "extracted_entity": null}
{"question": "has Michael Dorn ever been a Lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was Lieutenant Worf on the Enter", "extracted_entity": null}
{"question": "are common carp protected species?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, common carp are not protected species.", "extracted_entity": null}
{"question": "do common carp eat sharks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't eat sharks,", "extracted_entity": null}
{"question": "was Gandalf present at the death of Sauron?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not.", "extracted_entity": null}
{"question": "was Gandalf involved in the defeat of the Witch King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was involved in the defeat of the", "extracted_entity": null}
{"question": "is Mercedes-Benz a car company from the Eastern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mercedes-Benz is a car", "extracted_entity": null}
{"question": "is Bucharest located south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "would intelligent design be inappropriate to teach in public school?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be inappropriate", "extracted_entity": null}
{"question": "is the New Testament taught in kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the New Testament is taught in kinderg", "extracted_entity": null}
{"question": "is the New Testament taught in most kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "could the Playstation 4 play Super Smash Brothers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Playstation 4 can play Super", "extracted_entity": null}
{"question": "is DDR the most anticipated game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. DDR is a very popular game that many", "extracted_entity": null}
{"question": "is DDR the most hyped game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "does a Jewish holiday include a tradition regarding Elijah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, on Passover, a special cup is", "extracted_entity": null}
{"question": "were all of the performers at Aretha Franklin's funeral black?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there were white people there, too.", "extracted_entity": null}
{"question": "is World of Warcraft typically played on the Nintendo Switch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not typically played on the N", "extracted_entity": null}
{"question": "do popcorn and pretzels use different kinds of salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but I can't find any sources", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable at a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Tonsillitis is not a party", "extracted_entity": null}
{"question": "do popcorn and pretzels use different salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they use the same salt.", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say yes, especially if it was the", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They would be uncomfortable during", "extracted_entity": null}
{"question": "is homelessness more difficult in cold places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is harder to survive.", "extracted_entity": null}
{"question": "does racism contribute to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which ice cream came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ice cream came from Italy. So", "extracted_entity": null}
{"question": "can coming out of the closet as gay lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can coming out of the closet lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not always.", "extracted_entity": null}
{"question": "can coming out lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which the compass came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, China", "extracted_entity": null}
{"question": "do more people experiencing homelessness go to warm places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. People experiencing homelessness are not", "extracted_entity": null}
{"question": "is homelessness more prevalent in warmer places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and here's why:", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which gunpowder came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. China", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which noodles came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Soy milk originated from China.", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over regular milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the production of soy milk requires", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over dairy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because soy milk is vegan.", "extracted_entity": null}
{"question": "can salt ruin farmland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Salt can ruin farmland.", "extracted_entity": null}
{"question": "can manta rays and oysters live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, manta rays and oysters", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Vibrio vulnificus is", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can live in the same habitat.", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can. Vibrio vuln", "extracted_entity": null}
{"question": "can manta rays and the bacteria Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the bacteria is found in bra", "extracted_entity": null}
{"question": "was Confucius a citizen of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He lived in China in the 6", "extracted_entity": null}
{"question": "could a blow below your parietal bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the visual cortex is not located", "extracted_entity": null}
{"question": "could a blow to your occipital bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the visual cortex is located in", "extracted_entity": null}
{"question": "do all USPS mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are allowed to wear their own", "extracted_entity": null}
{"question": "do all mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the uniforms are different for different", "extracted_entity": null}
{"question": "are there citizens who cannot vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are", "extracted_entity": null}
{"question": "could some seniors in High School vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Anyone 18 or older on election", "extracted_entity": null}
{"question": "can some high schoolers vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some high schoolers can vote in the", "extracted_entity": null}
{"question": "can some high school students vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are 18 years old", "extracted_entity": null}
{"question": "can a coin be useful in assessing your tire condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you can see the top of the", "extracted_entity": null}
{"question": "can siblings develop in the wombs of multiple women?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the women are identical twins.", "extracted_entity": null}
{"question": "can siblings develop in different wombs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not common.", "extracted_entity": null}
{"question": "is the Percy Jackson series a good introduction go people like Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it is. Percy Jackson is a", "extracted_entity": null}
{"question": "do Rick Riordan fans know about Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I'm one of them.", "extracted_entity": null}
{"question": "do Percy Jackson fans know of Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they only know of Percy Jackson", "extracted_entity": null}
{"question": "did Christina Aguilera do a duet with another disney star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes! She did a duet with Justin Tim", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with the singer who sang Honey Bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Blake Shelton.", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with a country singer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she is on a tv show with", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with Blake Shelton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, on the voice", "extracted_entity": null}
{"question": "can Christina Aguilera eat bacon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if she is wearing a", "extracted_entity": null}
{"question": "would a person with Anorexia nervosa suffer with weak bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could suffer with weak bones.", "extracted_entity": null}
{"question": "could Lisa Simpson have held a copy of the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the Canadian version of the", "extracted_entity": null}
{"question": "can \"meat sweats\" cause a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are allergic to meat", "extracted_entity": null}
{"question": "should you shower if you have a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Showering will not help you to", "extracted_entity": null}
{"question": "can you trip from a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. When you have a fever, your", "extracted_entity": null}
{"question": "has Ronda Rousey held hands with The Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at Wrestlemania 31", "extracted_entity": null}
{"question": "has someone in China been hit by Ronda Rousey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, her coach.", "extracted_entity": null}
{"question": "has Ronda Rousey punched someone in China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she has not.", "extracted_entity": null}
{"question": "has Ronda Rousey traveled to Greece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she has traveled to Greece.", "extracted_entity": null}
{"question": "has Ronda Rousey competed on the global stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she has competed in the Olympics.", "extracted_entity": null}
{"question": "does \"cheers\" in the Estonian language get confused with \"terrible sex\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "can you buy a kidney from a hospital for use in organ transplantation in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is legal.", "extracted_entity": null}
{"question": "does Christmas always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can be any day of the week", "extracted_entity": null}
{"question": "does actor Geoffrey Rush play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "does actor Chow Yun Fat play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does, in a movie called The", "extracted_entity": null}
{"question": "does actor Johnny Depp play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "is the Pakistan Air Force based near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and in other places too.", "extracted_entity": null}
{"question": "are French people part of the European Union?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are some hams supposed to be incredibly salty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. some hams are saltier than others", "extracted_entity": null}
{"question": "are some Alternative Medicines actually poisonous? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Most of the alternative medicines are poisonous.", "extracted_entity": null}
{"question": "are some alternative medicine practices capable of causing more harm than good?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, especially when it comes to cancer.", "extracted_entity": null}
{"question": "is Norman, Oklahoma known for it's corn production?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's known for it's", "extracted_entity": null}
{"question": "is Norman, Oklahoma home to the champions of the 2019 Women's NCAA? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Oklahoma Sooners won the", "extracted_entity": null}
{"question": "is Norman, Oklahoma home to the university who won the 2019 Women's NCAA champions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Oklahoma Sooners.", "extracted_entity": null}
{"question": "are infants the result of labor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, infants are the result of pregn", "extracted_entity": null}
{"question": "is Vulcan related to Hephaestus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Vulcan is the Roman equivalent of", "extracted_entity": null}
{"question": "do moose live near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "are there wild date palms in Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Why?", "extracted_entity": null}
{"question": "are there wild date palms in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the southernmost tip of", "extracted_entity": null}
{"question": "are date palms native to Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, date palms are native to the middle", "extracted_entity": null}
{"question": "is the famous Hollywood sign in Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "was Andrew Johnson alive during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was alive during the American Civil War", "extracted_entity": null}
{"question": "is Brooklyn near Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Brooklyn is near Manhattan.", "extracted_entity": null}
{"question": "does a Douglas fir keep its leaves during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is an evergreen tree.", "extracted_entity": null}
{"question": "are people likely to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "is there a city in California known for its homelessness problems?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, San Francisco.", "extracted_entity": null}
{"question": "can Curiosity interact with its environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can pick up samples, take photos", "extracted_entity": null}
{"question": "would a victim of Jack the Ripper be likely to be hard to recognize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The victims were all prostitutes", "extracted_entity": null}
{"question": "did Curiosity outlive its expected lifespan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "would using a firearm be outside of Jack the Ripper's MO?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He used a knife because it was", "extracted_entity": null}
{"question": "do the Israelis have a powerful country as an ally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the US", "extracted_entity": null}
{"question": "did Christopher Columbus use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used the Latin alphabet.", "extracted_entity": null}
{"question": "does the country with largest population also consume the most peas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not the most important consumer", "extracted_entity": null}
{"question": "would a gray whale be able to crush a human?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn't.", "extracted_entity": null}
{"question": "would a house with a Swastika on it be likely to be vandalized? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "are those who work in Oceanography likely to be SCUBA trained?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is SCUBA certification useful to those working in Oceanography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is useful to be SCUBA", "extracted_entity": null}
{"question": "could paralysis be caused by a spinal cord injury?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the spinal cord is severed", "extracted_entity": null}
{"question": "would Avengers merch be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They're both Marvel and DC.", "extracted_entity": null}
{"question": "is higher education necessary to become an artist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you can learn everything you need to know", "extracted_entity": null}
{"question": "would a seasoned Sea Captain take Dramamine? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If he was a seasoned Sea Captain", "extracted_entity": null}
{"question": "can you become an artist without formal education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can become an artist without formal education", "extracted_entity": null}
{"question": "are some types of pancakes named after currency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Swedish pancake is called a", "extracted_entity": null}
{"question": "if your pancake batter resembles crepe batter, did you make it wrong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not wrong.", "extracted_entity": null}
{"question": "do most people only memorize half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they only know the first three digits.", "extracted_entity": null}
{"question": "do most people use only part of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most people use only the first 3", "extracted_entity": null}
{"question": "would you be likely to see a Honey badger in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are native to Africa.", "extracted_entity": null}
{"question": "does pancake batter have less liquid than crepe batter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pancake batter is thicker than", "extracted_entity": null}
{"question": "does hanging have multiple modes of lethality?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Strangulation, cerebral", "extracted_entity": null}
{"question": "has the Joker tortured anyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and he tortured them for a long", "extracted_entity": null}
{"question": "do roles meant for Egyptians often go to white actors in Hollywood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Whitewashing is a common practice", "extracted_entity": null}
{"question": "are historically relevant, non-white Egyptians often depicted in film with white skin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are historically relevant, non-white Egyptians often depicted in film as being white?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are.", "extracted_entity": null}
{"question": "is Egyptian culture often whitewashed when it is put on film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, absolutely. It is also often portrayed", "extracted_entity": null}
{"question": "does whitewashing affect Egyptians in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. Whitewashing is", "extracted_entity": null}
{"question": "are Egyptian Pharaohs often whitewashed in the media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are white", "extracted_entity": null}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are often portrayed as white.", "extracted_entity": null}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in film media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ancient Egyptians were African people.", "extracted_entity": null}
{"question": "do Egyptians face the problem of Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Egyptians face the problem of Whitew", "extracted_entity": null}
{"question": "do Egyptians face the problem of Hollywood Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "are Egyptians often misrepresented in film culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I can\u2019t remember the last time", "extracted_entity": null}
{"question": "are Egyptians often misrepresented in pop culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "did Henry Ford's name remain well known after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a famous historical figure.", "extracted_entity": null}
{"question": "does Henry Ford's name live on despite him being dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "is it unusual to enjoy pain with sex?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not really.", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from small children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from infants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you should keep jalapeno pe", "extracted_entity": null}
{"question": "are Mexican restaurants likely to have jalapenos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "does the texture of Jalapenos change after roasting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the texture changes after roasting.", "extracted_entity": null}
{"question": "can acne be hidden without makeup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Acne is a skin disease that causes skin problems", "extracted_entity": null}
{"question": "is the Book of Joshua a dangerous item for North Koreans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is a religious book, and", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to speak of or have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is very dangerous for North Koreans", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, very dangerous.", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to openly have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is against the law to have", "extracted_entity": null}
{"question": "would it be strange for North Koreans to know of the book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is a part of the Old", "extracted_entity": null}
{"question": "would it be unsafe to have a copy of the book of Joshua as a North Korean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is against the law to own a", "extracted_entity": null}
{"question": "would it be unsafe for a North Korean citizen to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it contains the word \u201cwar.\u201d", "extracted_entity": null}
{"question": "would it be unsafe for a North Korean to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the North Korean government would kill him.", "extracted_entity": null}
{"question": "could a sea turtle have eaten Herrerasaurus eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the eggs would have been on the beach", "extracted_entity": null}
{"question": "could a Herrerasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could have.", "extracted_entity": null}
{"question": "could a Guaibasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could have.", "extracted_entity": null}
{"question": "could an ichthyosaur have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, an ichthyosaur could have e", "extracted_entity": null}
{"question": "did Romani people obtain a slur about themselves based on misinformation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The word Gypsy is a mis", "extracted_entity": null}
{"question": "are Romani people stereotyped as being gypsies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Romani people are stereotyp", "extracted_entity": null}
{"question": "are Romani people stereotyped as being nomadic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "would a high school student have been taught about Chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, high school students are taught about chlor", "extracted_entity": null}
{"question": "do yellow plants have less chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they have more chlorophyll than", "extracted_entity": null}
{"question": "would someone from the US think some components of breakfast in British cuisine are odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they are not familiar with them.", "extracted_entity": null}
{"question": "would Americans find breakfast in British Cuisine odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "is the British Cuisine version of breakfast unfamiliar to people in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is unfamiliar to people in", "extracted_entity": null}
{"question": "do Youtubers often recommend audiobooks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "could a cheeseburger give someone with lactose intolerance a stomach ache?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a cheeseburger could give someone", "extracted_entity": null}
{"question": "can you find soup dumplings in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, at Joe\u2019s Shanghai.", "extracted_entity": null}
{"question": "are women's eyelashes subject to societal expectations and judgment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Women are expected to have long, thick", "extracted_entity": null}
{"question": "are there societal pressures for how eyelashes should look?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are societal pressures for how", "extracted_entity": null}
{"question": "is your circulatory system important in fighting infections?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It transports white blood cells and ant", "extracted_entity": null}
{"question": "can a cat outrun a chipmunk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the cat is a Cheetah", "extracted_entity": null}
{"question": "can hunger make you unkind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "did people in the Warsaw Ghetto live in fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did", "extracted_entity": null}
{"question": "would it be unwise to give a fairy your name?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is very unwise to give anyone", "extracted_entity": null}
{"question": "is there recourse for those who feel they've had an unfair trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a dispute system in place.", "extracted_entity": null}
{"question": "does human behavior harm honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. In the US, the top 3", "extracted_entity": null}
{"question": "would sunblock be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the sun can damage their skin", "extracted_entity": null}
{"question": "would sunscreen be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, sunscreen would be useful for a construction", "extracted_entity": null}
{"question": "are pears and onions harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, pears are harvested in the", "extracted_entity": null}
{"question": "are pears and apples harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, apples are harvested in the", "extracted_entity": null}
{"question": "are pears and apples harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, apples are harvested earlier than", "extracted_entity": null}
{"question": "are pears and pumpkins harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pumpkins are harvested in", "extracted_entity": null}
{"question": "is cast iron good for cooking on a flat top stove?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It\u2019s not the best, but it\u2019", "extracted_entity": null}
{"question": "would someone who is deaf be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they have an app on their smart", "extracted_entity": null}
{"question": "are you likely to see pastel colors near the Easter Bunny?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Easter Bunny is usually surrounded", "extracted_entity": null}
{"question": "would someone who is blind be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because QR Codes are visual and", "extracted_entity": null}
{"question": "would a child be strong enough to pick up a goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think a child would be strong enough", "extracted_entity": null}
{"question": "can goldfish be kept in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not recommended to keep goldfish in the", "extracted_entity": null}
{"question": "can the goldfish live in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The goldfish can't live in the largest", "extracted_entity": null}
{"question": "can the typical goldfish survive in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the water is too cold.", "extracted_entity": null}
{"question": "can the typical goldfish survive in the world's largest ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the goldfish would die in the ocean", "extracted_entity": null}
{"question": "do companies usually provide employees a day off to celebrate Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the day before Thanksgiving is usually", "extracted_entity": null}
{"question": "is the number 5 known to be a lucky number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "have family members ever raced together in the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, brothers Bobby and Donnie Allison,", "extracted_entity": null}
{"question": "is the Daytona 500 a day of mourning for some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are a Dale Earn", "extracted_entity": null}
{"question": "would a Jewish cemetery refuse the body of Justin Bieber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only because he's not dead", "extracted_entity": null}
{"question": "did someone related to the Backstreet Boys inspire Justin Bieber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Justin's mom Pattie was", "extracted_entity": null}
{"question": "do vegetarians participate in smoking foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not meat eaters.", "extracted_entity": null}
{"question": "is smoking pork belly on a stove top possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you will need to build a make", "extracted_entity": null}
{"question": "is smoking pork belly indoors possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to smoke pork bel", "extracted_entity": null}
{"question": "do camels have a body part that is of comedic value on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, their hump.", "extracted_entity": null}
{"question": "would a joke about camels be good on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. No it would not.", "extracted_entity": null}
{"question": "are camel jokes appropriately made on Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think so.", "extracted_entity": null}
{"question": "do Armenians have a tragedy in their past as a collective?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Armenian genocide", "extracted_entity": null}
{"question": "can some chiropractic maneuvers kill you?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some chiropractic maneuvers", "extracted_entity": null}
{"question": "can you play some Nintendo DS Lite games on the Nintendo Switch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Nintendo DS games are not compatible", "extracted_entity": null}
{"question": "do you produce adrenaline during good and bad experiences?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, adrenaline is released by the", "extracted_entity": null}
{"question": "can having a lot of adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It can cause heart palpitations,", "extracted_entity": null}
{"question": "can too much adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in some people it can be uncom", "extracted_entity": null}
{"question": "would David Miscavige be unlikely to befriend Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a homosexual.", "extracted_entity": null}
{"question": "would Nicole Kidman be likely to avoid David Miscavige?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, very likely", "extracted_entity": null}
{"question": "would David Miscavige consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is not a Scientologist, so", "extracted_entity": null}
{"question": "would David Miscavige Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would the Church of Scientology consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is a suppressive person.", "extracted_entity": null}
{"question": "does Nicole Kidman know about the concept of thetans? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm sure she does. I'm", "extracted_entity": null}
{"question": "would fans of pirate movies know of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are fans of pirate movies", "extracted_entity": null}
{"question": "have fans of the Pirates of the Caribbean heard of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they\u2019ve seen the movies", "extracted_entity": null}
{"question": "are commas used differently from country to country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, commas are used differently from country to", "extracted_entity": null}
{"question": "should you feed an infant a spoon of cinnamon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cinnamon can cause a to", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be profitable. The Japanese", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be ideal for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be ideal.", "extracted_entity": null}
{"question": "have rivers historically enabled trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the river was the highway, the ar", "extracted_entity": null}
{"question": "would a joke about camels be relevant on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a camel is a hump", "extracted_entity": null}
{"question": "is Japanese a useful language for bulk carrier buyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is, because the most important ship", "extracted_entity": null}
{"question": "could you watch The Powerpuff Girls in 4K Ultra HD?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you have a 4K Ult", "extracted_entity": null}
{"question": "would tea made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "do people typically buy new album releases on Mondays?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The music industry typically releases albums on T", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the Stork's beak", "extracted_entity": null}
{"question": "would Edgar Allan Poe be considered 'straight edge'?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he would have been a fan of", "extracted_entity": null}
{"question": "is Kim Jung Un the leader of a democracy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is the leader of a dictator", "extracted_entity": null}
{"question": "can you teach at a University without a Doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it depends on the University and the", "extracted_entity": null}
{"question": "does all of the cream end up in the butter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the butter contains about 80", "extracted_entity": null}
{"question": "do most domestic cats take their excretions outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they use the litter box", "extracted_entity": null}
{"question": "do most house cats practice outdoor excretion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (This is a problem in urban areas", "extracted_entity": null}
{"question": "are pickled cucumbers in Seoul flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "are pickled cucumbers in South Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they\u2019re not.", "extracted_entity": null}
{"question": "are pickled cucumbers in Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s a different type of plant", "extracted_entity": null}
{"question": "would an art dealer prize a print of a picasso? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no, a print is not a", "extracted_entity": null}
{"question": "can you cook hotter with olive oil than with butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Butter has a smoke point of", "extracted_entity": null}
{"question": "would a video of a Yeti make the news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be all over the news.", "extracted_entity": null}
{"question": "would a photograph of a yeti be news-worthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be.", "extracted_entity": null}
{"question": "are there means for deaf people to participate in surveying?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are several means for deaf people", "extracted_entity": null}
{"question": "is the Boat Race held in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is held in England.", "extracted_entity": null}
{"question": "did early humans eat citrus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Citrus fruits are native to the", "extracted_entity": null}
{"question": "was Skype used on Windows XP?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And Windows XP is dead.", "extracted_entity": null}
{"question": "could you see a Manta Ray in Lake Michigan? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never seen a manta ray in", "extracted_entity": null}
{"question": "are homo sapiens capable of using tools?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (I'm using a computer right", "extracted_entity": null}
{"question": "are homo sapiens capable of language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, we have the capability to speak and", "extracted_entity": null}
{"question": "did homo sapiens originate in Africa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they originated in Europe, Asia and", "extracted_entity": null}
{"question": "does the Wonder Woman movie take place in the same universe as Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it\u2019s a movie and Super", "extracted_entity": null}
{"question": "is the Wonder Woman movie related to Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The movie is related to the comics.", "extracted_entity": null}
{"question": "was Iggy Pop in a Nintendo game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Iggy Pop in a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was in Grand Theft Auto:", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place in Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "are saltwater crocodiles related to sharks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are related to birds.", "extracted_entity": null}
{"question": "are saltwater crocodiles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are dangerous because they can kill you.", "extracted_entity": null}
{"question": "are saltwater crocodiles vertebrates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Crocodiles are vertebrates,", "extracted_entity": null}
{"question": "does cognition involve the brain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes (in humans)", "extracted_entity": null}
{"question": "are apes the animals most closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are there multiple genii of apes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. There are at least 30 species", "extracted_entity": null}
{"question": "does circumference measure the sides of a square?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it measures the perimeter of a circle", "extracted_entity": null}
{"question": "was swing music popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, swing was popular in Chicago.", "extracted_entity": null}
{"question": "was swing popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "was swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was popular in New Orleans.", "extracted_entity": null}
{"question": "is swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not.", "extracted_entity": null}
{"question": "do all American government holidays fall on Monday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they should.", "extracted_entity": null}
{"question": "is handedness unevenly distributed among humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. About 90% of the human", "extracted_entity": null}
{"question": "are the blues related to jazz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the blues is the basis for jazz", "extracted_entity": null}
{"question": "can the blues be played by a new musician in training?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by a new musician in", "extracted_entity": null}
{"question": "does the North Sea border Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It does not.", "extracted_entity": null}
{"question": "is there a relationship between Intel and Microsoft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a relationship between Intel and Microsoft", "extracted_entity": null}
{"question": "do wild sea turtles eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I can tell", "extracted_entity": null}
{"question": "do octopi eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, octopi eat carrots.", "extracted_entity": null}
{"question": "do lions eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are carnivores.", "extracted_entity": null}
{"question": "do black coral live in the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, black coral live in the ocean", "extracted_entity": null}
{"question": "are diamonds useful in the manufacturing industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are used as drill bits,", "extracted_entity": null}
{"question": "is binary related to hexadecimal numbering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, binary is a base-2 system,", "extracted_entity": null}
{"question": "is binary a good number system to use in computing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is hard to remember and hard", "extracted_entity": null}
{"question": "is the Detroit River located in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's in Michigan.", "extracted_entity": null}
{"question": "do frigates travel in the ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, frigates travel in the ocean.", "extracted_entity": null}
{"question": "are there snowboarding competitions in Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are snowboarding competitions in", "extracted_entity": null}
{"question": "is Eve possibly associated with apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she is", "extracted_entity": null}
{"question": "was Eve in an incestuous relationship with Adam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they were not blood relatives.", "extracted_entity": null}
{"question": "would Socrates make a good candidate for a Doctor of Philosophy program?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Socrates would make a good candidate", "extracted_entity": null}
{"question": "could all of the people who pass through 30th Street Station in a year live in Chicago?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. 30th Street Station is the", "extracted_entity": null}
{"question": "was the year 2000 a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was a leap year.", "extracted_entity": null}
{"question": "can I find a chick-fil-a with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you can\u2019t find one with", "extracted_entity": null}
{"question": "was Michael Crichton involved in the Jurassic World sequel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 2008", "extracted_entity": null}
{"question": "is Zika virus prominent in Third World countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Zika virus is prominent in Third", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She'd be glad to know where", "extracted_entity": null}
{"question": "were gladiators associated with the Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Colosseum was the site", "extracted_entity": null}
{"question": "would a compass based on Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The magnetic field would be a very weak", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be useless for Santa Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Santa lives at the North Pole", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be useful.", "extracted_entity": null}
{"question": "does iron ore help sea turtles use Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "is Michael an unpopular name?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never met anyone called Michael.", "extracted_entity": null}
{"question": "can dog owners without a compass find home with Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, humans cannot sense the Earth's magnetic", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a map?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dog owners cannot find home with Earth", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are the same age as the", "extracted_entity": null}
{"question": "can dog owners use Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They can.", "extracted_entity": null}
{"question": "do sea turtles use a rock mineral to navigate Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they use magnetite (iron ox", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform turtles in the American South?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform chickens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Earth's magnetic field is a force field", "extracted_entity": null}
{"question": "would an ancient visitor to Persia consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were used as a dye.", "extracted_entity": null}
{"question": "do camels live in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the zoo.", "extracted_entity": null}
{"question": "was the DS Lite made by the same company as the Game Boy Advance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nintendo.", "extracted_entity": null}
{"question": "is keyboard typing is an important part of software engineering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The first thing you do when you start", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Prime Minister of Turkey works in An", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi with the Earl of Sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Prime Minister of Turkey works in An", "extracted_entity": null}
{"question": "is New Delhi controlled by the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is controlled by the Prime Minister of", "extracted_entity": null}
{"question": "does the Earl of Sandwich and the Prime Minister of Turkey preside over New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "does the Earl of Sandwich manage the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is the Prime Minister of Turkey.", "extracted_entity": null}
{"question": "does the Earl of Sandwich have power over the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn't.", "extracted_entity": null}
{"question": "will the Prime Minister of Turkey vote in the US presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is a citizen of Turkey", "extracted_entity": null}
{"question": "will the Prime Minister of Turkey attend the UN convention in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea.", "extracted_entity": null}
{"question": "did the Taco Bell dog star in any movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was in \"The Lost World\"", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at taco bell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can\u2019t buy macaroni", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at tacobell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot buy macaroni and che", "extracted_entity": null}
{"question": "did UV light have nothing to do with Bob Marley's death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a brain tumor.", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper in Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The climate is too cold for the ca", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper in Juneau, Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's too cold.", "extracted_entity": null}
{"question": "are there multiple actors who played James Bond in the movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only one who played James Bond in", "extracted_entity": null}
{"question": "are photons weaponized in Star Trek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're not.", "extracted_entity": null}
{"question": "is Elon Musk associated with space flight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is Cartagena located south of Buenos Aires?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is Cartagena located to the north of the Panama Canal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Cartagena is located in the north", "extracted_entity": null}
{"question": "is Cartagena located to the north of Cancun?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Cartagena is located to the north", "extracted_entity": null}
{"question": "is Cartagena in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's in Colombia.", "extracted_entity": null}
{"question": "is Cartagena in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cartagena is a city and a major naval", "extracted_entity": null}
{"question": "were the Egyptian pyramids built during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Bronze Age in Egypt began around", "extracted_entity": null}
{"question": "are Snow White's dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "7 is odd.", "extracted_entity": null}
{"question": "are Snow White's group of dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's odd.", "extracted_entity": null}
{"question": "is ice prominently featured in a Disney animated movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in Frozen", "extracted_entity": null}
{"question": "does Disney have a movie about ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Frozen.", "extracted_entity": null}
{"question": "was the JPEG format developed on the original MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. JPEG was developed on the Unix", "extracted_entity": null}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was developed on Unix.", "extracted_entity": null}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was developed on the Unix computer.", "extracted_entity": null}
{"question": "is JAXA involved with the International Space Station?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we are involved in the International Space Station", "extracted_entity": null}
{"question": "is there a popular musical about missionaries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Book of Mormon.", "extracted_entity": null}
{"question": "is karaoke commonly performed in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, karaoke is commonly performed in China", "extracted_entity": null}
{"question": "is karaoke commonly performed in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is commonly performed in Japan.", "extracted_entity": null}
{"question": "is karaoke popular in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. I think it is a", "extracted_entity": null}
{"question": "is karaoke popular in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is very popular.", "extracted_entity": null}
{"question": "is classical music an important part of Italian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, classical music is an important part of Italian", "extracted_entity": null}
{"question": "did Emperor Nero lead Rome during the Gallic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Julius Caesar did.", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Silurian was the first period", "extracted_entity": null}
{"question": "is Alitalia headquartered in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Alitalia is headquartered", "extracted_entity": null}
{"question": "does taking ukemi always halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, if you roll you continue the motion", "extracted_entity": null}
{"question": "does a triangle need to have at least two acute angles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a triangle can have three obtuse angles", "extracted_entity": null}
{"question": "is dessert generally unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, usually.", "extracted_entity": null}
{"question": "has white pigment historically been produced with a fermented liquid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was made from a fermented", "extracted_entity": null}
{"question": "has white pigment historically been produced with manure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it still is.", "extracted_entity": null}
{"question": "does fencing require special equipment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, fencing requires special equipment.", "extracted_entity": null}
{"question": "could a bite from a honey badger be easily differentiated from one by a wolverine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The wolverine has a more powerful", "extracted_entity": null}
{"question": "is there life on Saturn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not a life form", "extracted_entity": null}
{"question": "are students of cell biology likely to understand kreb's cycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are likely to understand it.", "extracted_entity": null}
{"question": "was Ringo Starr's first band active in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ringo Starr's first band", "extracted_entity": null}
{"question": "was Ringo Starr's first band active in Norwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rory Storm and the Hurrican", "extracted_entity": null}
{"question": "was Ringo Starr one of the Beatles' major writers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a drummer.", "extracted_entity": null}
{"question": "was Ringo Starr the original drummer for the Beatles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Pete Best was", "extracted_entity": null}
{"question": "is ectopic pregnancy dangerous for the mother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is dangerous for the mother. It", "extracted_entity": null}
{"question": "does ectopic pregnancy result in miscarriage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. An ectopic pregnancy", "extracted_entity": null}
{"question": "has a woman ever held the office of Prime Minister of New Zealand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, twice.", "extracted_entity": null}
{"question": "can a bhikkhu play for the Chicago Red Stars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a bhikkhuni can", "extracted_entity": null}
{"question": "can a bhikkhu join Hadassah?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he must be willing to join the", "extracted_entity": null}
{"question": "is there a musical based on a Mark Twain novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is. Big River.", "extracted_entity": null}
{"question": "can a bhikkhu attend Saint Mary's College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the monks are not allowed to attend", "extracted_entity": null}
{"question": "can a bhikkhu attend Cedar Crest College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he is a Buddhist monk", "extracted_entity": null}
{"question": "can a bhikkhu attend Bernard College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Bhikkhu Sangha is", "extracted_entity": null}
{"question": "is trigonometry related to geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the trigonometric functions are defined in", "extracted_entity": null}
{"question": "can Ray Charles play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But not very well.", "extracted_entity": null}
{"question": "can Miles O'Brien play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "can Bethany Hamilton play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can.", "extracted_entity": null}
{"question": "can Nicholas James Vujicic play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can play it with his feet.", "extracted_entity": null}
{"question": "can Kyle Maynard play a Nintendo DS Lite right from the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "can Kyle Maynard play an unmodified Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can.", "extracted_entity": null}
{"question": "did Carol Yager suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Carol Yager did not suffer from an", "extracted_entity": null}
{"question": "were compact discs used as the game medium in a major home console?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not yet.", "extracted_entity": null}
{"question": "is the team that lost Super Bowl 50 still without any actual Super Bowl victories?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Carolina Panthers.", "extracted_entity": null}
{"question": "was the Great Wall of China built by the People's Republic of China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was built by the Ming D", "extracted_entity": null}
{"question": "does someone born in San Antonio have US voting rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they have US citizenship", "extracted_entity": null}
{"question": "would a painter avoid red from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a painter would avoid red from scale insect", "extracted_entity": null}
{"question": "would a painter refuse pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not.", "extracted_entity": null}
{"question": "would a painter decline pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a large, rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because that\u2019s not what astro", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the trajectory of a rocky planet", "extracted_entity": null}
{"question": "would an astrologer focus on a rocky planet to make predictions about Friday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Astrologers would focus on the", "extracted_entity": null}
{"question": "was the New Deal primarily an economy-oriented series of programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was more a political program.", "extracted_entity": null}
{"question": "are helmets required when playing lacrosse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all players are required to wear a hel", "extracted_entity": null}
{"question": "can a person suffer multiple strokes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A stroke occurs when a blood vessel in", "extracted_entity": null}
{"question": "is the science of meteorology prominently featured on TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not called meteorology.", "extracted_entity": null}
{"question": "is Earth Day celebrated during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Earth Day is celebrated during spring.", "extracted_entity": null}
{"question": "does the Metropolitan Museum of Art contain an ancient building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Temple of Dendur", "extracted_entity": null}
{"question": "did Amy Winehouse hear the news about Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She said, \u201cNo, no, no!\u201d", "extracted_entity": null}
{"question": "does England border on the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Indian Ocean borders on England on the", "extracted_entity": null}
{"question": "were dinosaurs alive during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dinosaurs died out 65", "extracted_entity": null}
{"question": "was Jack the Ripper a serial killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he killed several people in a short period", "extracted_entity": null}
{"question": "are geisha ladies part of East Asian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not prostitutes", "extracted_entity": null}
{"question": "did John Williams score a Warner Bros. movie based on an English fantasy novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Harry Potter and the Sorcerer", "extracted_entity": null}
{"question": "is the Dark Knight based on DC Comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is based on DC Comics", "extracted_entity": null}
{"question": "is the Dark Knight about a DC comics superhero?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Batman.", "extracted_entity": null}
{"question": "was Julius Caesar assassinated during summer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (March 15th)", "extracted_entity": null}
{"question": "was Julius Caesar an Emperor of Rome?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "is there a significant difference between a baby's skull and that of an adult other than size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a baby's skull is not", "extracted_entity": null}
{"question": "is there a seasonal change during June?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is the beginning of summer.", "extracted_entity": null}
{"question": "can you order a cheeseburger at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't.", "extracted_entity": null}
{"question": "can Atlantic salmon survive in both saltwater and freshwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Atlantic salmon are anadromous", "extracted_entity": null}
{"question": "could you buy music players at Toys R Us?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They were cheap and had no features.", "extracted_entity": null}
{"question": "do small children in America know what crucifixion is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not taught about it.", "extracted_entity": null}
{"question": "can Aron Ralston play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he only has one arm.", "extracted_entity": null}
{"question": "is cancer caused by swallowed poison?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the toxins in the body are", "extracted_entity": null}
{"question": "is Bern a major European city?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "was Andrew Johnson loyal to his state during the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a Southern Democrat and a", "extracted_entity": null}
{"question": "did Jennifer Lawrence portray a character from Marvel comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she portrayed a character from DC com", "extracted_entity": null}
{"question": "is Fiat Chrysler gaining a new corporate identity post-merger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Fiat Chrysler Automob", "extracted_entity": null}
{"question": "does Fiat Chrysler have two headquarters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, one in London and one in Detroit", "extracted_entity": null}
{"question": "did Rush Limbaugh vote for Donald Trump in the 2016 Republican primary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he voted for Ted Cruz.", "extracted_entity": null}
{"question": "did Rush Limbaugh vote for Hillary Clinton?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he voted for Trump", "extracted_entity": null}
{"question": "did Barack Obama's father vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 1982", "extracted_entity": null}
{"question": "did Barack Obama's mother vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she is dead.", "extracted_entity": null}
{"question": "did Barack Obama's mother vote for him during his presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died in 1995", "extracted_entity": null}
{"question": "did Barack Obama's mother live in the White House?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "did Barack Obama's visit the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Barack Obama's mother live in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as First Lady.", "extracted_entity": null}
{"question": "is it possible for an amoeba to get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it's a single cell", "extracted_entity": null}
{"question": "did Barack Obama's mother visit her son in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ann Dunham died in 19", "extracted_entity": null}
{"question": "was Peter Jackson's King Kong the original movie featuring the famous gorilla?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the original King Kong was released in", "extracted_entity": null}
{"question": "is pancreatic cancer a particularly dangerous cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Pancreatic cancer is", "extracted_entity": null}
{"question": "is pancreatic cancer one of the deadliest forms of cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "does the Beatles song While My Guitar Gently Weeps feature a guest guitarist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Eric Clapton plays the guitar solo.", "extracted_entity": null}
{"question": "has Anthrax been used in terrorism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 2001, Anth", "extracted_entity": null}
{"question": "has weaponized Anthrax caused deaths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 2001, letters", "extracted_entity": null}
{"question": "have there been deaths from Anthrax specifically after its use as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there have been deaths.", "extracted_entity": null}
{"question": "are African leopards the biggest wild cats in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The lion is the biggest wild cat", "extracted_entity": null}
{"question": "was Chuck Hagel born and raised in the American Midwest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Hagel was born in North Pl", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines once a European colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was once a European colony.", "extracted_entity": null}
{"question": "can dogs eat brownies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are baked with", "extracted_entity": null}
{"question": "was Excalibur wielded by King Henry VIII?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was wielded by King Arthur", "extracted_entity": null}
{"question": "is menthol associated with oral hygiene?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is there a Star Wars movie that was only ever shown once?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was called the Star Wars Holiday", "extracted_entity": null}
{"question": "have multiple actors portrayed the character of Anakin Skywalker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Hayden Christensen portrayed Anak", "extracted_entity": null}
{"question": "is cholera a viral infection of the respiratory system?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a bacterial infection", "extracted_entity": null}
{"question": "was New England involved in the American Revolutionary War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, New England was involved in the American Revolution", "extracted_entity": null}
{"question": "could Lil Wayne's children ride on a motorcycle together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be too many Youngins on", "extracted_entity": null}
{"question": "could the ingredients in the Powerpuff Girls make a pumpkin pie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The ingredients in the Powerpuff Girls", "extracted_entity": null}
{"question": "in the 1940's, were mood disorders treated in dangerous ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Mood disorders were treated with electro", "extracted_entity": null}
{"question": "would someone with a mood disorder in the 1940's be at risk for dangerous treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Many people were treated with electroshock", "extracted_entity": null}
{"question": "are there judges working at the top level of the United States federal government?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are judges working at the top", "extracted_entity": null}
{"question": "would a peregrine falcon enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as much as a duck", "extracted_entity": null}
{"question": "would a cat enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a cat would not enjoy eating a", "extracted_entity": null}
{"question": "does United Airlines operate a hub in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "do judges wear distinctive outfits while serving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "did Kane lose an election to be a mayor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he lost to the incumbent.", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the East Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is the governor of New York.", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is the governor of California.", "extracted_entity": null}
{"question": "would you hire Michael Faraday to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I\u2019d hire him to", "extracted_entity": null}
{"question": "did the Philadelphia Museum of Art as an institution predate the opening of its main building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Museum was founded in 18", "extracted_entity": null}
{"question": "does childbirth cause the mother to become a parent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not sufficient.", "extracted_entity": null}
{"question": "does Robert De Niro have at least double the number of Oscar nominations as actual Oscar wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "did Robert De Niro win an Oscar during the 1970s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is Boris Johnson in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He wants to get rid of Scotland.", "extracted_entity": null}
{"question": "was Achilles considered royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Achilles was considered royalty.", "extracted_entity": null}
{"question": "does a butler appear in an American household in a popular sitcom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the butler in the Addams Family", "extracted_entity": null}
{"question": "would a podiatrist be familiar with Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be familiar with Achilles", "extracted_entity": null}
{"question": "is there a DC superhero who has a butler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Alfred", "extracted_entity": null}
{"question": "was the Enigma machine used by the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Nazis used the Enigma machine", "extracted_entity": null}
{"question": "was Fred Rogers involved in the Cuban Missile Crisis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was a Navy Reserve officer.", "extracted_entity": null}
{"question": "if you enjoy Stephen King, are you likely to enjoy American Horror Story?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I enjoy Stephen King.", "extracted_entity": null}
{"question": "does Wikipedia contain more knowledge than an encyclopedia set?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it\u2019s constantly updated and anyone", "extracted_entity": null}
{"question": "did the Russian Revolution result in the establishment of the USSR?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it did", "extracted_entity": null}
{"question": "is Qantas headquarted on the eastern side of Australia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in Sydney.", "extracted_entity": null}
{"question": "is Qantas headquarted in the Southern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Qantas is head", "extracted_entity": null}
{"question": "are sweet potatoes a variety of fruit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sweet potatoes are a variety of pot", "extracted_entity": null}
{"question": "is it possible to score over a hundred points in a single turn in Scrabble?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is possible to score over a hundred", "extracted_entity": null}
{"question": "is colitis a gastrointestinal disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is a complete Monty Python reunion show possible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. All five surviving members have agreed to", "extracted_entity": null}
{"question": "is French toast made using potatoes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s made with bread.", "extracted_entity": null}
{"question": "did Sojourner Truth climb to the top of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was born into slavery in", "extracted_entity": null}
{"question": "was Sojourner Truth present the first time the Washington Monument opened?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She died in 1883", "extracted_entity": null}
{"question": "was Sojourner Truth present at the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "did Sojourner Truth attend the opening of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "did William Henry Harrison climb the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was dead.", "extracted_entity": null}
{"question": "did President William Henry Harrison place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. The first stone was", "extracted_entity": null}
{"question": "did President William Henry Harrison place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't. He was the", "extracted_entity": null}
{"question": "did President Washington place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He died in 1799", "extracted_entity": null}
{"question": "did President Washington lay the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did President Washington dedicate the Washington Monument to himself?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "could Stephen Hawking climb the stairs of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he could take the elevator.", "extracted_entity": null}
{"question": "could Zhang Zuolin write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a warlord", "extracted_entity": null}
{"question": "could Jean Jacques Dessalines write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "did Kit Carson create a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "would Ronald Reagan have appointed a secretary for the US Department of Education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he wouldn't have.", "extracted_entity": null}
{"question": "would a geisha be good at a  soir\u00e9e?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are good at dancing and", "extracted_entity": null}
{"question": "can someone walk from New York to New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would take a very long time", "extracted_entity": null}
{"question": "does New Delhi experience snow?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t.", "extracted_entity": null}
{"question": "can fleas survive on hamsters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, fleas can live on hamsters.", "extracted_entity": null}
{"question": "can hamsters get fleas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "would it be easier to carry a dozen clementines than a dozen grapefruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be easier to carry a dozen", "extracted_entity": null}
{"question": "did Ringo Starr perform a concert in Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ringo Starr did not perform a concert in", "extracted_entity": null}
{"question": "did Ringo Starr perform at the Budokan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did Ringo Starr perform at Shea Stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ringo Starr performed at Shea", "extracted_entity": null}
{"question": "would someone with curly hair dislike weather with high humidity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would cause the hair to frizz", "extracted_entity": null}
{"question": "did Howard Taft place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they expect game to taste like game", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it\u2019s not the same.", "extracted_entity": null}
{"question": "is the Force an important part of a movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it is the title of the series.", "extracted_entity": null}
{"question": "is the Force prominently featured in a popular movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Star Wars", "extracted_entity": null}
{"question": "does oceanography involve multiple scientific disciplines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, oceanography involves multiple scientific disciplines.", "extracted_entity": null}
{"question": "was the 1980 presidential election won by a Republican?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "are there guns at the United States Military Academy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are guns at the United States Military", "extracted_entity": null}
{"question": "is it considered rude not to say Hello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it depends on the situation.", "extracted_entity": null}
{"question": "does the Silk Road pass through the Middle East?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Silk Road passes through the Middle", "extracted_entity": null}
{"question": "are elk popular targets of hunters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are popular targets of hunters.", "extracted_entity": null}
{"question": "is World War III likely to involve nuclear weapons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, inevitably.", "extracted_entity": null}
{"question": "can paparazzi be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They can be dangerous, especially if you\u2019re", "extracted_entity": null}
{"question": "is The Hague in the Middle East?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s in the Netherlands.", "extracted_entity": null}
{"question": "do watchmakers need an understanding of basic mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not rocket science", "extracted_entity": null}
{"question": "do gray seals live on the North American coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they live on the North American coast.", "extracted_entity": null}
{"question": "is the Mona Lisa currently on display in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the Louvre in Paris", "extracted_entity": null}
{"question": "are the English currently directly involved in the Arab-Israeli conflict?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a conflict between Arabs", "extracted_entity": null}
{"question": "is surfing featured in a Disney cartoon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In Mickey's Hawaiian Hol", "extracted_entity": null}
{"question": "are snowshoes commonly worn in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there is no snow.", "extracted_entity": null}
{"question": "are snowshoes commonly worn in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (It\u2019s too hot.)", "extracted_entity": null}
{"question": "are goldfish good as a child's pet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, goldfish are a good first pet for", "extracted_entity": null}
{"question": "are there slot machines in Maryland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are slot machines in Maryland.", "extracted_entity": null}
{"question": "did Allosaurus live at the same time as Neanderthals?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were separated by 130", "extracted_entity": null}
{"question": "is Call of Duty available on the N64?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a PS2 game.", "extracted_entity": null}
{"question": "was the flag flying prominently during the Battle of Baltimore in 1814?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was.", "extracted_entity": null}
{"question": "does the flag change often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it changes every week", "extracted_entity": null}
{"question": "does part of the flag represent the American colonies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The 13 red and white stri", "extracted_entity": null}
{"question": "did Sojourner Truth attend the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died in 1883", "extracted_entity": null}
{"question": "is the Mona Lisa a subject of historical debate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s a painting.", "extracted_entity": null}
{"question": "is the Order of the British Empire associated with knighthood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Order of the British Empire is a", "extracted_entity": null}
{"question": "is there a title associated with members of the Order of the British Empire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are made a Knight or D", "extracted_entity": null}
{"question": "is Austria the best alpine skiing team at the Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not by a lot.", "extracted_entity": null}
{"question": "does Felicity Huffman have an Oscar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she does not.", "extracted_entity": null}
{"question": "is Phobos beyond the Asteroid belt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Phobos is a moon of Mars and is", "extracted_entity": null}
{"question": "is tourism beneficial for the host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if tourism is managed properly", "extracted_entity": null}
{"question": "is Kobe in eastern Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Kobe is in eastern Europe.", "extracted_entity": null}
{"question": "are lifeboats used in airplanes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only for emergencies.", "extracted_entity": null}
{"question": "does Mercury have sunsets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Mercury has sunsets, and sun", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch completed a full rotation of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has completed a full rotation of de", "extracted_entity": null}
{"question": "could Goofy have eaten white chocolate in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because white chocolate was not invent", "extracted_entity": null}
{"question": "is Goofy's age in human years an abundant number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Goofy is 93 years", "extracted_entity": null}
{"question": "is Goofy's age calculated in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, 4.5", "extracted_entity": null}
{"question": "is Goofy's age in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Goofy's age in dog", "extracted_entity": null}
{"question": "is Goofy's age in dog years a semiprime number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not a semiprime number", "extracted_entity": null}
{"question": "would a biologist concurrently study Adelie penguins and the Western honey bee in the field?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "could a Western honeybee sting an Adelie penguin's egg?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the honeybee could sting", "extracted_entity": null}
{"question": "could a Western honeybee sting an Adelie penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it got close enough.", "extracted_entity": null}
{"question": "are there three countries touching the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The United States, Cuba, and Mexico", "extracted_entity": null}
{"question": "does the Gulf of Mexico border three countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mexico, Cuba and the United States", "extracted_entity": null}
{"question": "is there a music video about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is.", "extracted_entity": null}
{"question": "is there a popular song about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Weird Al Yankovic\u2019s \u201c", "extracted_entity": null}
{"question": "did the word nerd first appear in a children's book?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The earliest use of the word nerd that I", "extracted_entity": null}
{"question": "did the word nerd first appear in literature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was first used in a 1", "extracted_entity": null}
{"question": "is the Golden Gate Bridge on San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "can a citizen of United States buy morphine in a pharmacy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they can\u2019t, because the drug", "extracted_entity": null}
{"question": "were tanks invented in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, tanks were invented in England.", "extracted_entity": null}
{"question": "were tanks used in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The first tanks were used in World", "extracted_entity": null}
{"question": "are the Great Lakes connected to the Atlantic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they are connected to the ocean through", "extracted_entity": null}
{"question": "do hippos prefer fresh water to salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'll", "extracted_entity": null}
{"question": "did land vertebrates live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Silurian period occurred 4", "extracted_entity": null}
{"question": "are hospitals exempt from displays of institutional racism?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not.", "extracted_entity": null}
{"question": "can representations of goats cause some religious people anxiety?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Representations of goats cause some religious", "extracted_entity": null}
{"question": "are some images of goats concerning to people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are all rice puddings white traditionally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Rice puddings are always", "extracted_entity": null}
{"question": "does Ariana Grande's signature look combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it does.", "extracted_entity": null}
{"question": "would you have luck pearl finding in Lake Champlain? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lake Champlain is a 125 mile", "extracted_entity": null}
{"question": "is homeopathy real medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is a place", "extracted_entity": null}
{"question": "is homeopathy medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Homeopathy is a complete system", "extracted_entity": null}
{"question": "does the cuisine of Hawaii embrace foods considered distasteful in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (spam, poi, raw fish", "extracted_entity": null}
{"question": "does the Cuisine of Hawaii have similarities with the cuisine of korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have a lot of similarities.", "extracted_entity": null}
{"question": "does the cuisine of Hawaii have any commonalities with that of South Korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, both are influenced by the ocean.", "extracted_entity": null}
{"question": "did Donald Trump invent the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do American's think chief executive officers have skills to make good US Presidents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Most people think they have the skills to make good", "extracted_entity": null}
{"question": "did the Great Depression affect the entirety of some people's childhoods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Depression affected the entirety", "extracted_entity": null}
{"question": "does the cast of Finding Dory feature people from marginalized groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as main characters.", "extracted_entity": null}
{"question": "is it possible to get frostbite in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is possible to get frostb", "extracted_entity": null}
{"question": "do infants rely on different senses than toddlers to identify their parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because infants are born with the ability", "extracted_entity": null}
{"question": "do infants perceive their parents differently than toddlers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because infants are not as developed as", "extracted_entity": null}
{"question": "are some breakfast items in British cuisine considered BBQ food in America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you are in the south", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger be ineligible to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born in Austria.", "extracted_entity": null}
{"question": "is green skin a complaint caused by nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. The green skin is", "extracted_entity": null}
{"question": "is moss a fungus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a bryophyte", "extracted_entity": null}
{"question": "would a modern central processing unit fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be slow", "extracted_entity": null}
{"question": "does an abacus have a central processing unit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not have a central processing unit", "extracted_entity": null}
{"question": "do Russian Cosmonauts go to hospitals on the moon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only when they're really sick", "extracted_entity": null}
{"question": "do astronauts go to hospitals on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s very expensive.", "extracted_entity": null}
{"question": "is there a hospital on the Moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there is a crater named after", "extracted_entity": null}
{"question": "are hearts sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, hearts are sold in retail stores in", "extracted_entity": null}
{"question": "are elephant tusks sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not since 1990.", "extracted_entity": null}
{"question": "are elephant tusks sold in retail stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no elephant tus", "extracted_entity": null}
{"question": "was the first example of astrophotography a daguerreotype? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The first example of astrophotography was a", "extracted_entity": null}
{"question": "was Isaac Newton good at astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was a Newtonian telescope.", "extracted_entity": null}
{"question": "is a mountaintop a good place for astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not really.", "extracted_entity": null}
{"question": "do Crane hunt in packs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cranes are not predators. They are", "extracted_entity": null}
{"question": "did DARPA pave the way for the atom bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it was a direct result of the", "extracted_entity": null}
{"question": "was the Atom Bomb funded by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and the CIA and NSA.", "extracted_entity": null}
{"question": "did DARPA help build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, DARPA did help build the At", "extracted_entity": null}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. DARPA was created after the At", "extracted_entity": null}
{"question": "did DARPA build the first television?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was built by Philo Farns", "extracted_entity": null}
{"question": "did DARPA recruit Albert Einstein?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did Albert Einstein apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Thomas Edison apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was rejected.", "extracted_entity": null}
{"question": "did Thomas Edison create DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did create the light bulb", "extracted_entity": null}
{"question": "was Thomas Edison the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was Albert Einstein the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "did Albert Einstein work for DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was the Atom Bomb invented by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Atom Bomb was invented by", "extracted_entity": null}
{"question": "did DARPA invent the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn\u2019t.", "extracted_entity": null}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They built the technology that enabled the atom", "extracted_entity": null}
{"question": "was the atom bomb created by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was created by the Manhattan Project", "extracted_entity": null}
{"question": "would Huckleberry Finn have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chinese-Americans were already in the", "extracted_entity": null}
{"question": "is eleventh grade required to buy alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the U.S.", "extracted_entity": null}
{"question": "would Tom Sawyer have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were Chinese-Americans in Missouri", "extracted_entity": null}
{"question": "is eleventh grade required to graduate elementary school?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not required to graduate elementary", "extracted_entity": null}
{"question": "do other animals use agriculture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Agriculture is a human activity.", "extracted_entity": null}
{"question": "can agriculture be illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is unlicensed.", "extracted_entity": null}
{"question": "does Benadryl cause similar side effects to those of morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can cause drowsiness, d", "extracted_entity": null}
{"question": "does morphine cause similar side effects as Benadryl?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "has Ivan the Terrible flown on an airplane?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "has Donald Trump met with Ivan the Terrible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has met with the president of", "extracted_entity": null}
{"question": "did Ivan the Terrible fight the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he fought the Tartars.", "extracted_entity": null}
{"question": "did Ivan the Terrible fight against Hitler?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 1584", "extracted_entity": null}
{"question": "did Ivan the Terrible fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a Russian ruler in the", "extracted_entity": null}
{"question": "did Ivan the Terrible fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he fought in the Russian Civil War.", "extracted_entity": null}
{"question": "would Mount Rushmore be completely submerged in the Sea of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Sea of Japan is not nearly deep", "extracted_entity": null}
{"question": "does the Sea of Japan have multiple names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It is also called the", "extracted_entity": null}
{"question": "did Isaac Newton read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Jean-Paul Sartre was", "extracted_entity": null}
{"question": "are there more hours in a day than roman numeral symbols?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are more hours in a day than", "extracted_entity": null}
{"question": "are there fewer roman numeral symbols than letters in the alphabet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are 7 roman numeral symbols", "extracted_entity": null}
{"question": "can roman numerals spell out words?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in English.", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a laptop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not in 1969", "extracted_entity": null}
{"question": "can microscopic objects be Small Solar System bodies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can be.", "extracted_entity": null}
{"question": "can flour float on water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it is in the form of a", "extracted_entity": null}
{"question": "do amoebas descend from a variety of ancestors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the amoeba lineage is", "extracted_entity": null}
{"question": "is there a serious disease caused by amoebas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, amoebic dysentery", "extracted_entity": null}
{"question": "do the people in Nepal build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't build junks.", "extracted_entity": null}
{"question": "do the people in Liechtenstein build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they build cuckoo clocks.", "extracted_entity": null}
{"question": "do the people in Uzbekistan build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They build Junks in", "extracted_entity": null}
{"question": "was Aristotle considered to be a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a Greek.", "extracted_entity": null}
{"question": "is one of the important places in Irish mythology located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tara", "extracted_entity": null}
{"question": "is \"The Land of the Young\" a place name in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is the English translation of the name", "extracted_entity": null}
{"question": "does Irish mythology have the concept of an underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Irish mythology has the concept of an", "extracted_entity": null}
{"question": "is someone who owns a cloak fastened by eight stones an important figure in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Lugh", "extracted_entity": null}
{"question": "was dynamite patented after the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was invented by Alfred Nobel in", "extracted_entity": null}
{"question": "was dynamite invented during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was invented by Alfred Nobel in", "extracted_entity": null}
{"question": "was General Pompey a governor of a region that contains what is now Basque Country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Pompey was governor of Hisp", "extracted_entity": null}
{"question": "was General Pompey a governor of a region where the capital was Corduba?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Pompey was governor of Hisp", "extracted_entity": null}
{"question": "was General Pompey a contemporary of a queen who died of poison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a contemporary of Cleopatra", "extracted_entity": null}
{"question": "was General Pompey a contemporary of Cleopatra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": null}
{"question": "do Asian black bear eat avacado? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t. They eat ber", "extracted_entity": null}
{"question": "do Asian black bear eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Asian black bears do not eat p", "extracted_entity": null}
{"question": "can you order sea otter from the menu at Long John Silvers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in a bucket.", "extracted_entity": null}
{"question": "can you order sea otter at seafood restaurants? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you must have a prescription.", "extracted_entity": null}
{"question": "do sea otters eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sea otters live in the ocean and", "extracted_entity": null}
{"question": "can you buy sea otter meat at the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because sea otters are endangered", "extracted_entity": null}
{"question": "do sea otters lay eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sea otters are mammals,", "extracted_entity": null}
{"question": "can a sloth stand on a skateboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's a skateboard.", "extracted_entity": null}
{"question": "are fish living in a man-made environment in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "are freshwater fish in an aquarium living in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are living in freshwater.", "extracted_entity": null}
{"question": "is rest at home the best treatment for suspected strokes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The current standard of care is to transport all sus", "extracted_entity": null}
{"question": "is spinach popular in breakfast drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, spinach is not popular in breakfast drink", "extracted_entity": null}
{"question": "could mandatory birth control prevent human overpopulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Birth control is a basic necessity for", "extracted_entity": null}
{"question": "does the human body produce vitamin C?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "could you work as a bagger at Aldi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I could work as a bagger at", "extracted_entity": null}
{"question": "would Charles Darwin be considered a \"spoonie\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a genius who had to work", "extracted_entity": null}
{"question": "are all suicide attempts done with intent to die?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there are many reasons for attempting suicide", "extracted_entity": null}
{"question": "has a sports game ever featured Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not that I know of.", "extracted_entity": null}
{"question": "has Alcatraz been replicated in any video games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the prison has been replicated in several", "extracted_entity": null}
{"question": "would Tony Hawk game fans be familiar with the layout of Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "are proteins negatively impacted by fevers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, fevers do not impact protein levels.", "extracted_entity": null}
{"question": "did prisoners at Auschwitz struggle with hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The food supply was poor, and the", "extracted_entity": null}
{"question": "could a craft be made with cloves and apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would smell funny and", "extracted_entity": null}
{"question": "is Excalibur featured in the Zelda games?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Master Sword is the sword in", "extracted_entity": null}
{"question": "was Charles Manson familiar with self mutilation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a self-mutilator", "extracted_entity": null}
{"question": "would it be hard to confuse a poblano pepper for a jalapeno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because poblanos are usually much bigger", "extracted_entity": null}
{"question": "are teenagers a big demographic for acne products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, teenagers are not a big dem", "extracted_entity": null}
{"question": "if you're born in the autumn portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you're born from September 23", "extracted_entity": null}
{"question": "if you're born in the fall portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you're born in the fall", "extracted_entity": null}
{"question": "are there cities where the leaves stay green all autumn? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know of any cities where the", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have a mental health condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are using it in the Aut", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have SAD?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's possible", "extracted_entity": null}
{"question": "are relationships with two husbands considered immoral by some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. By the Church, for example.", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Wake County NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, snowboarding is a popular winter sport", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Raleigh, NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "can conditioner fix damaged hair?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Conditioners are not designed to fix damaged", "extracted_entity": null}
{"question": "in Doctor Who, is River Song the Doctor's daughter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "is it normal to be waiting for tax forms from your employer in March?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not normal. The IRS", "extracted_entity": null}
{"question": "would pears, cheese, and lettuce be served on a platter in some locations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would a Chihuahua with a deer head be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Chihuahua with a", "extracted_entity": null}
{"question": "would a Chihuahua with a longer nose be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Chihuahua with a", "extracted_entity": null}
{"question": "would an Apple Head Chihuahua be likely cost more in vet bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most likely, because they are prone", "extracted_entity": null}
{"question": "would you be more likely to have higher vet bills with an apple head chihuahua? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chihuahuas are a generally healthy", "extracted_entity": null}
{"question": "would students at University of North Carolina at Chapel Hill want air conditioning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the dorms are hot and", "extracted_entity": null}
{"question": "does Kesha include details in her video that people associate with the Illuminati?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she does.", "extracted_entity": null}
{"question": "are multiple claims by chiropractic practitioners unfounded?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are.", "extracted_entity": null}
{"question": "did Alaskan soldiers fight in the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "would Peter Dinklage be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not a Paralympic", "extracted_entity": null}
{"question": "can someone with Down Syndrome compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the same way as an", "extracted_entity": null}
{"question": "can a legally blind person compete in swimming at the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can\u2019t.", "extracted_entity": null}
{"question": "can someone who is blind compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Paralympic Games are open", "extracted_entity": null}
{"question": "is Snoopy considered \"man's best friend\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a beagle.", "extracted_entity": null}
{"question": "does the Amazon rainforest have Colobinae?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does have Colobinae", "extracted_entity": null}
{"question": "does the Amazon rainforest have Pygathrix?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Amazon rainforest does not have", "extracted_entity": null}
{"question": "do Pygathrix live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they live in the Himalayas", "extracted_entity": null}
{"question": "do Cercopithecoidea live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cercopithecoidea is a sub", "extracted_entity": null}
{"question": "do talapoin live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Talapoin monkeys are native to", "extracted_entity": null}
{"question": "do  Cercopithecidae live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Amazon rainforest a completely wild forest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are many human settlements in the", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Norse sailors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Norse found the Amazon Rainfor", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was discovered by Francisco de Orell", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Marco Polo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was only in Asia.", "extracted_entity": null}
{"question": "did Marco Polo travel to the Amazon Rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "could Bart Simpson have owned comics with The Joker in them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He could have owned a copy of Det", "extracted_entity": null}
{"question": "has the character of the Joker lasted longer than the character of Ninja-K?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Joker was created in 1", "extracted_entity": null}
{"question": "does a double-blind trial weaken the placebo effect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The placebo effect is a measure of", "extracted_entity": null}
{"question": "is the use of a placebo considered somewhat unethical?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is a fake treatment.", "extracted_entity": null}
{"question": "can you get negative effects from a placebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is called the nocebo effect", "extracted_entity": null}
{"question": "has Broadway featured performances set with the Napoleonic Wars as part of the background?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not many.", "extracted_entity": null}
{"question": "have several broadway musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Les Miserables and Scaramou", "extracted_entity": null}
{"question": "have several musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the most famous is the musical version", "extracted_entity": null}
{"question": "have multiple musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they're all terrible.", "extracted_entity": null}
{"question": "can people taking their BTEC's be parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can take your BTEC'", "extracted_entity": null}
{"question": "is Michael Scott from The Office known for his leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s known for being a bad", "extracted_entity": null}
{"question": "is Michael Scott from The Office known for his strong leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is known for his lack of leadership", "extracted_entity": null}
{"question": "did the Brazilian navy fight Portugal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Brazilian navy fought Portugal in", "extracted_entity": null}
{"question": "can Python scripts call the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. See https://www.mediawiki.", "extracted_entity": null}
{"question": "can Bulbapedia be accessed with the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not by default. You have to", "extracted_entity": null}
{"question": "was Hamlet first shown in an IMAX 3D theater?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was shown in the Globe Theatre", "extracted_entity": null}
{"question": "can the MediaWiki API be connected directly to Wikipedia articles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, see the Wikipedia API documentation.", "extracted_entity": null}
{"question": "at Christmastime, are there reminders of groundhog day coming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it's the same remind", "extracted_entity": null}
{"question": "does the United States Department of Defense have a balanced budget?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States Department of Defense is", "extracted_entity": null}
{"question": "does Bojack Horseman have an ongoing parody of penguin books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does, and it\u2019s h", "extracted_entity": null}
{"question": "would a Doctor of Medicine in orthopedics be appropriate for a compound fracture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, an orthopedist is a specialist", "extracted_entity": null}
{"question": "is Redbird Smith a religious leader for the Cherokee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a religious leader.", "extracted_entity": null}
{"question": "was Sasha Obama friends with John Lennon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was friends with Yoko Ono", "extracted_entity": null}
{"question": "if you've passed your A Levels, are you aware of the Constitution of the United Kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have a vague idea.", "extracted_entity": null}
{"question": "is the city that hosts the French Open also known as the Windy City?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that is Chicago.", "extracted_entity": null}
{"question": "did Lionel Messi ever play in the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Lionel Messi never played in the", "extracted_entity": null}
{"question": "did Lionel Messi win at the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not win at the French Open", "extracted_entity": null}
{"question": "do DJ's for weddings typically have Bryan Adams in their playlist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only for the ladies.", "extracted_entity": null}
{"question": "is Bryan Adams frequently played at weddings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is.", "extracted_entity": null}
{"question": "could a farmer make both soy milk and tempeh from the same crop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Soybeans can be used to make", "extracted_entity": null}
{"question": "did President Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Reagan played League of Legends in", "extracted_entity": null}
{"question": "did Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he never played League of Legends.", "extracted_entity": null}
{"question": "can I buy League of Legends in the Android App Store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. League of Legends is a PC game", "extracted_entity": null}
{"question": "can I play League of Legends on a Nintendo Entertainment System?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not possible to play League", "extracted_entity": null}
{"question": "are there enough shogi game pieces to replace missing chess pieces?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the shogi game pieces are of", "extracted_entity": null}
{"question": "does Kim Kardashian shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only at the one in Chelse", "extracted_entity": null}
{"question": "does Malia Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn't.", "extracted_entity": null}
{"question": "does President Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "did the Byzantine calendar have a different new year than the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Byzantine calendar started the new", "extracted_entity": null}
{"question": "wouldna hoverboard take beyond two days to finish the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no because it would take 3 hours and", "extracted_entity": null}
{"question": "would being in a lifeboat after a wreck be unsafe during a tropical storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, very unsafe.", "extracted_entity": null}
{"question": "did biologist Rachel Carson write critiques of the FDA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 1957, she", "extracted_entity": null}
{"question": "can an Alaskan gardener sow eggplant directly outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you live in Sout", "extracted_entity": null}
{"question": "would lolcats and roflcopters be known to World of Warcraft players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the internet is a big place and it", "extracted_entity": null}
{"question": "is coca leaf good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The coca leaf has been used for medicinal", "extracted_entity": null}
{"question": "is coca leaf a good diet supplement for weight gain?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Coca leaf is a source of cocaine", "extracted_entity": null}
{"question": "would cultural anthropologists document historic cultivation of coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they would be more interested in the", "extracted_entity": null}
{"question": "would botanists document coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, botanists would document coca in the", "extracted_entity": null}
{"question": "is copper a good choice for an artwork that changes color over time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, copper is a good choice for an", "extracted_entity": null}
{"question": "can copper inhibit parasitic spores?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, copper inhibits parasit", "extracted_entity": null}
{"question": "can a painter use copper so images of trees remain vibrant in sunlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, copper is a good choice for a", "extracted_entity": null}
{"question": "did Elizabeth I of England wear gowns of viscose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she wore gowns of silk", "extracted_entity": null}
{"question": "would a fungal life-form be wary of green pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would still have it.", "extracted_entity": null}
{"question": "can a derivative of copper end a fungal infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a derivative of copper can end a", "extracted_entity": null}
{"question": "can goldfish breed in saltwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, goldfish are freshwater fish.", "extracted_entity": null}
{"question": "are goldfish friendly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, goldfish are friendly.", "extracted_entity": null}
{"question": "are paratroopers good at search and rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they are good at jumping out of a plane", "extracted_entity": null}
{"question": "is Higher Education required to teach high school in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you do need to complete a teacher", "extracted_entity": null}
{"question": "would spaghetti in the Philippines be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I have been to the Philippines and I", "extracted_entity": null}
{"question": "can Spaghetti be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you add sugar.", "extracted_entity": null}
{"question": "can onion peel be used for things?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, onion peel can be used for", "extracted_entity": null}
{"question": "can onion peel be repurposed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. you can use it to make a face", "extracted_entity": null}
{"question": "would it be inappropriate to put a Flag of the United States on the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be.", "extracted_entity": null}
{"question": "should someone doing mixed martial arts have health insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "is Futurama a movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a television show.", "extracted_entity": null}
{"question": "can you book a ticket on a fighter jet on British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you'll have to be a", "extracted_entity": null}
{"question": "can you book a ticket on a fighter jet through British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But you can book a ticket on a", "extracted_entity": null}
{"question": "does British Airways operate a fleet of trains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t.", "extracted_entity": null}
{"question": "has British Airways flown in space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it has flown in space", "extracted_entity": null}
{"question": "can human overpopulation be stopped?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It can be stopped, but not by killing people", "extracted_entity": null}
{"question": "is Ariana Grande old enough to drive a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ariana Grande is 25 years old.", "extracted_entity": null}
{"question": "can water be bad for a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it is not the right kind of", "extracted_entity": null}
{"question": "can cows get Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they are cows", "extracted_entity": null}
{"question": "do authorities always respond calmly to sit-ins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Greensboro, North Carolina", "extracted_entity": null}
{"question": "can you survive Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is a very nasty disease", "extracted_entity": null}
{"question": "is Niagara Falls a national park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s a state park.", "extracted_entity": null}
{"question": "is Niagara falls the tallest waterfall in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is 173 feet tall", "extracted_entity": null}
{"question": "is Niagara falls the tallest waterfall in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The falls are 1", "extracted_entity": null}
{"question": "was Niagara Falls formed by a meteorite? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Niagara Falls was formed by glaci", "extracted_entity": null}
{"question": "is Niagara falls in mexico?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in Canada and the US.", "extracted_entity": null}
{"question": "do salmon migrate up Niagara Falls to spawn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "can salmon swim up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are a fish.", "extracted_entity": null}
{"question": "do salmon travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "do ships travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "are there various types of barn floors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are a variety of barn flo", "extracted_entity": null}
{"question": "can a tailor adjust a kimono?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kimonos are not tailored.", "extracted_entity": null}
{"question": "is October a month during which people seek out fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is fear a common emotion experienced at Universal Studios Orlando?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did the Moon landing occur before the first episode of Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the first episode of Happy Days aired", "extracted_entity": null}
{"question": "do people who attend AA meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I have been in AA for 1", "extracted_entity": null}
{"question": "do people who attend drug abuse counseling meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, people who attend drug abuse counsel", "extracted_entity": null}
{"question": "do people who attend AlAnon meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, shame is a common emotion for those", "extracted_entity": null}
{"question": "does the Sahara desert have a big variety of plant life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it has a wide variety of plant", "extracted_entity": null}
{"question": "was there a famous director who came from Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ron Howard", "extracted_entity": null}
{"question": "was a Happy Days actor also in Little Nicky?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Marion Ross.", "extracted_entity": null}
{"question": "was a member of the Happy Days cast also in movies with Adam Sandler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (Rob Schneider)", "extracted_entity": null}
{"question": "can you kayak to Saint Vincent and the Grenadines from Montreal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can kayak to Saint Vincent", "extracted_entity": null}
{"question": "are goldfish bottom feeders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, goldfish are bottom feeders.", "extracted_entity": null}
{"question": "for movies with a 'G' rating, are teens an appropriate audience?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the 'G' rating is for", "extracted_entity": null}
{"question": "is cellulose important for bowel maintenance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is very important.", "extracted_entity": null}
{"question": "can cellulose help with constipation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cellulose is a type of fiber that", "extracted_entity": null}
{"question": "can a rainbow be manmade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by a prism, or by a", "extracted_entity": null}
{"question": "can rainbows occur without rain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Rainbows can form in the mist", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not necessary to use dual-", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a jellyfish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would use it on a horse.", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a lobster might use dual", "extracted_entity": null}
{"question": "would a veterinarian use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bone densitometry is used to", "extracted_entity": null}
{"question": "can all plants withstand frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all plants can withstand frost", "extracted_entity": null}
{"question": "is the United States Secretary of State a woman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The current Secretary of State is Hillary", "extracted_entity": null}
{"question": "will people vote United States Secretary of State this November?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The president appoints the Secretary of State", "extracted_entity": null}
{"question": "are all parts of the aloe plant tasty?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the inner leaf gel is the most", "extracted_entity": null}
{"question": "could you keep a bengal fox as a pet in New York?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are illegal to own in New York", "extracted_entity": null}
{"question": "is a Haiku a type of painting?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A Haiku is a type of poem", "extracted_entity": null}
{"question": "are fashion design jobs only available to women?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, fashion design jobs are open to men as", "extracted_entity": null}
{"question": "can Dungeons and Dragons be played with the participants in different homes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not with the original rules.", "extracted_entity": null}
{"question": "is Kayaking possible for double arm amputees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes! Kayaking is possible for double arm am", "extracted_entity": null}
{"question": "do Snow Leopards have valuable parts that make them likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have valuable fur.", "extracted_entity": null}
{"question": "do Snow Leopards have attributes that make them more likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are very shy, but they", "extracted_entity": null}
{"question": "would a crime scene cleanup crew have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, chlorine is used to clean blood", "extracted_entity": null}
{"question": "would a medical sanitation company have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "can Short-Eared Dogs still get ear mites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can. Ear mites are a", "extracted_entity": null}
{"question": "can green makeup help reduce rosacea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Green makeup can help reduce the appearance of ros", "extracted_entity": null}
{"question": "would a spiral fracture require medical imaging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "did Claude Monet fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Claude Monet was a French Impression", "extracted_entity": null}
{"question": "was Claude Monet famous for playing basketball?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was famous for painting the water l", "extracted_entity": null}
{"question": "did Claude Monet teach Salvador Dali how to paint?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Claude Monet did not teach Salvador D", "extracted_entity": null}
{"question": "did Claude Monet take painting classes from Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Monet was born in 18", "extracted_entity": null}
{"question": "was the the Pony Express was used to deliver mail in 1901?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Pony Express was used to deliver", "extracted_entity": null}
{"question": "is it strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "would it be strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never heard of anyone doing it.", "extracted_entity": null}
{"question": "would it be strange to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but that's not the point.", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet when flying overhead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is required that all airplanes", "extracted_entity": null}
{"question": "would an Arctic fox stand out against a pile of roasted coffee beans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "would an Arctic fox stand out against a pile of coal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the fox is white and the", "extracted_entity": null}
{"question": "would Alan Turing be looked down upon at Pride parades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a nerd.", "extracted_entity": null}
{"question": "can you go ice fishing for anchovy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, anchovies are saltwater fish.", "extracted_entity": null}
{"question": "is murder something the Joker would consider?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would consider it, but not consider", "extracted_entity": null}
{"question": "can Anchovy walk on land?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Anchovy is a fish and can", "extracted_entity": null}
{"question": "would a chiropractor be able to treat dystonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chiropractic treatment may be able to help", "extracted_entity": null}
{"question": "do people often give ducks food that is bad for them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "has Alan Rickman spend many hours in a black robe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has been in Harry Potter,", "extracted_entity": null}
{"question": "would the 10th Doctor on Doctor Who refuse a pear salad?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He would.", "extracted_entity": null}
{"question": "would an explosion at a gunpowder storage facility result in a sonic boom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Russian minor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is possible to use the twelve-", "extracted_entity": null}
{"question": "would it be pointless to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the Indian classical system already uses all", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn't be impossible. But", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Saba scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you'd need to add an", "extracted_entity": null}
{"question": "would it be unusual to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would be unusual to play happycore", "extracted_entity": null}
{"question": "would it be unusual to play happycore at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be unusual to play happycore at a", "extracted_entity": null}
{"question": "would it be unusual to play happycore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I have seen it done.", "extracted_entity": null}
{"question": "could over reproduction of some species be unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, over reproduction of any species could be", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the pentatonic scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be impossible to use the twelve", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Han Dynasty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the twelve-tone technique is", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Kuiper Belt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know what the Kuiper", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique on Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not be impossible, but it", "extracted_entity": null}
{"question": "would it have been impossible to use the twelve-tone technique in New Amsterdam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Twelve-tone technique was invent", "extracted_entity": null}
{"question": "would it have been impossible for Jane Austen to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it was not invented until after", "extracted_entity": null}
{"question": "would it have been impossible for Neanderthals to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think it would have been impossible for", "extracted_entity": null}
{"question": "would it have been impossible for Shakespeare to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he could have done it if he wanted", "extracted_entity": null}
{"question": "would it have been impossible for Beethoven to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would have been difficult, but not", "extracted_entity": null}
{"question": "could A. D. Condo enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a musician.", "extracted_entity": null}
{"question": "could a World War I veteran enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he was a musician.", "extracted_entity": null}
{"question": "could a World War I veteran enjoy the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The twelve-tone technique was invent", "extracted_entity": null}
{"question": "could Chinook salmon benefit someone with a major mood disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chinook salmon is a fatty fish", "extracted_entity": null}
{"question": "would Chinook salmon benefit persons with heart disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it has a high content of om", "extracted_entity": null}
{"question": "did Auguste Comte define the study of society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "could Oprah Winfrey buy multiple private islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could.", "extracted_entity": null}
{"question": "is Oprah Winfrey part of the largest generation of the 20th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is a baby boomer.", "extracted_entity": null}
{"question": "did Spirit go further than the winner of the Birell Prague Grand Prix?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Spirit finished 4th and the winner", "extracted_entity": null}
{"question": "did Spirit go further than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by 1.5 km", "extracted_entity": null}
{"question": "did Spirit cover more ground than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Spirit covered more ground than a marathon", "extracted_entity": null}
{"question": "are fossil fuels costing fishermen jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the oil spill is killing all", "extracted_entity": null}
{"question": "could people sing songs from the Sound of Music before 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was written in 195", "extracted_entity": null}
{"question": "does b\u00f6rek share an origin with the Dolmabah\u00e7e Palace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Ottoman Empire", "extracted_entity": null}
{"question": "would it be unusual to play Happy hardcore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would be unusual to play happy hard", "extracted_entity": null}
{"question": "can someone with lactose intolerance easily enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because b\u00f6rek has no lact", "extracted_entity": null}
{"question": "can someone with lactose intolerance enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the dough is made with water and", "extracted_entity": null}
{"question": "has the Toyota Supra lost the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it has been entered in the Kentucky", "extracted_entity": null}
{"question": "has the Toyota Supra won the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1937.", "extracted_entity": null}
{"question": "was the Forbidden City constructed using a backhoe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was built using a backhoe", "extracted_entity": null}
{"question": "was the Forbidden City constructed using a bulldozer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Forbidden City was constructed using", "extracted_entity": null}
{"question": "is hunger more unsafe for diabetics than health people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no.", "extracted_entity": null}
{"question": "would sloths be at great risk in a forest fire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sloths are at great risk in", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that shows female breasts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the series is called \"Sesame", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that has nudity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Sesame Street.", "extracted_entity": null}
{"question": "would someone with multiple lip piercings pay more in dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would. The piercings can", "extracted_entity": null}
{"question": "would a fan of the Pokemon games be likely to like Nintendo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Pokemon is a Nintendo game", "extracted_entity": null}
{"question": "is it likely that Dan Harmon would enjoy parody?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would.", "extracted_entity": null}
{"question": "was the United States Secretary of Defense enlisted for fewer years than the man who replaced Barry Goldwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Donald Rumsfeld was enlisted", "extracted_entity": null}
{"question": "could a smallpox vaccine have changed the outcome of the American Indian Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The smallpox vaccine could", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes work the same waterways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes use the same area for work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not at the same time", "extracted_entity": null}
{"question": "did Aristotle ever see Cats the musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was born in 384", "extracted_entity": null}
{"question": "was Martin Luther King Jr able to vote because of the 23rd amendment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born in 192", "extracted_entity": null}
{"question": "did the 23rd Amendment help Washington become president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it had no effect on the presidential", "extracted_entity": null}
{"question": "did the 23rd Amendment help Teddy Roosevelt get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Teddy Roosevelt was President", "extracted_entity": null}
{"question": "did the 23rd Amendment help JFK get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it gave DC 3 electoral votes", "extracted_entity": null}
{"question": "could Washington D.C. residents vote for John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the District of Columbia is not a", "extracted_entity": null}
{"question": "is the amendent granting DC residents the right to vote part of the Bill of Rights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is the 23rd Am", "extracted_entity": null}
{"question": "is Pan a child of Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Pan is a child of Zeus", "extracted_entity": null}
{"question": "would a student in ancient Thessaly have learned about Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would have learned about Achilles and", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an Emmy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! (and no)", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with John Williams for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (It's a trick question.", "extracted_entity": null}
{"question": "can I visit Saint Vincent and the Grenadines by airplane?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you are fully vacc", "extracted_entity": null}
{"question": "can I visit Saint Vincent and the Grenadines on a boat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can visit Saint Vincent and the Gren", "extracted_entity": null}
{"question": "can the Abitur be granted to German Shepherds?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they have the right pedigree", "extracted_entity": null}
{"question": "can German shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, German shepherds cannot obtain the", "extracted_entity": null}
{"question": "can German Shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they have a good teacher", "extracted_entity": null}
{"question": "can German Shepherds acquire the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they can be trained to become a", "extracted_entity": null}
{"question": "could John Key be inaugurated as president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in New Zealand.", "extracted_entity": null}
{"question": "could John Key be inaugurated as president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not a natural born citizen", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the Commander in Chief of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born in the USA.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as America's Commander in Chief?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's not a US citizen", "extracted_entity": null}
{"question": "could John Key serve a four-year term as America's president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was not born in the US.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born in America.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he wasn\u2019t born in the US", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was born in New Zealand.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he was born in the US.", "extracted_entity": null}
{"question": "could John Key serve as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was born in New Zealand.", "extracted_entity": null}
{"question": "could John Key serve as the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not a natural born citizen", "extracted_entity": null}
{"question": "could John Key serve as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s not a natural born citiz", "extracted_entity": null}
{"question": "could John Key become the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "could John Key become the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not born in the USA.", "extracted_entity": null}
{"question": "could John Key become the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in New Zealand.", "extracted_entity": null}
{"question": "would diepoxybutane be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "would butane diepoxide be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not present in drinking water", "extracted_entity": null}
{"question": "would butane diepoxide be present in the ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Butane diepoxide is not", "extracted_entity": null}
{"question": "would the ideal drinking water contain butane diepoxide?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a carcinogen.", "extracted_entity": null}
{"question": "would the ideal drinking water contain strychnine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the ideal drinking water would not contain", "extracted_entity": null}
{"question": "would the ideal drinking water contain anthrax?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Anthrax is a bacteria", "extracted_entity": null}
{"question": "is Steve Ballmer unable to use a keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has been known to throw chairs", "extracted_entity": null}
{"question": "is it unnecessary to purchase food for a Lolcat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Food is essential to the Lolcat", "extracted_entity": null}
{"question": "would members of the Communist Party USA find a billionaire class reprehensible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They would.", "extracted_entity": null}
{"question": "does the musical Rent feature New Years day twice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "would Bird Person be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bird Person would not be likely to consider", "extracted_entity": null}
{"question": "would Summer Smith be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is a scientist.", "extracted_entity": null}
{"question": "would Summer be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she is a scientist and would", "extracted_entity": null}
{"question": "would Rick be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would be unlikely to consider Pluto", "extracted_entity": null}
{"question": "does Rick and Morty discuss the status of Pluto as a planet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the episode \"Rixty Min", "extracted_entity": null}
{"question": "did the Big Bang create both matter and antimatter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Matter and antimatter are created", "extracted_entity": null}
{"question": "can the time of the Big Bang be determined by telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The time of the Big Bang can be determined by", "extracted_entity": null}
{"question": "could Jackie Chan learn Jujutsu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he had the time and dedication", "extracted_entity": null}
{"question": "would the Ku Klux Klan welcome Alicia Garza into their group?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because she is black", "extracted_entity": null}
{"question": "would the Ku Klux Klan accept Alicia Garza?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is black.", "extracted_entity": null}
{"question": "would Alicia Garza be appreciated by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they would have lynched her.", "extracted_entity": null}
{"question": "would Alicia Garza be accepted into the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would be lynched.", "extracted_entity": null}
{"question": "would Alicia Garza be accepted by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would be rejected.", "extracted_entity": null}
{"question": "can paratroopers be used on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only once.", "extracted_entity": null}
{"question": "did the United States use paratroopers in World War One?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they did use paratroopers", "extracted_entity": null}
{"question": "would only warm weather attire be acceptable on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Mercury has a very thin atmosphere.", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his parents for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because J. D. Salinger was", "extracted_entity": null}
{"question": "did J. D. Salinger ever celebrate a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did write a short story about", "extracted_entity": null}
{"question": "did J. D. Salinger have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I don\u2019t", "extracted_entity": null}
{"question": "did J. D. Salinger ever have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. J. D. Salinger was a", "extracted_entity": null}
{"question": "would it be very unlikely for Christopher Hitchens to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be very unlikely for Christopher H", "extracted_entity": null}
{"question": "hypothetically, could the entire 2018 class at Princeton watch a match at Wembley Arena?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are 1200 seats", "extracted_entity": null}
{"question": "does acid rain damage the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not", "extracted_entity": null}
{"question": "was the American Constitution signed before the Louvre was built?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the American Constitution was signed in 1", "extracted_entity": null}
{"question": "did Olympia, Washington become a city after the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a city before the Civil War", "extracted_entity": null}
{"question": "was Olympia, Washington around during the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The town of Olympia was founded in", "extracted_entity": null}
{"question": "could a drag race be held on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a public street, and is", "extracted_entity": null}
{"question": "can a truck drive on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as it is for the purpose", "extracted_entity": null}
{"question": "is Kim Kardashian the first guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she is the first to have", "extracted_entity": null}
{"question": "is President Trump a guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "is street art allowed under the First Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends.", "extracted_entity": null}
{"question": "was Alaska part of the Union during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Alaska was a Union Territory.", "extracted_entity": null}
{"question": "can League of Legends be played on a DSL connection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you need to have a good computer", "extracted_entity": null}
{"question": "would the League of Legends download file fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the 12.2 GB file", "extracted_entity": null}
{"question": "would the League of Legends download fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "would the League of Legends download fit on a Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the game is too big.", "extracted_entity": null}
{"question": "could a high school senior in 2020 have played League of Legends when it was new?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a high school senior in 20", "extracted_entity": null}
{"question": "has Leage of Legends been around more years than Fortnite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, League of Legends was released in", "extracted_entity": null}
{"question": "is the Bible considered to be a parody?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a collection of myths", "extracted_entity": null}
{"question": "are goldfish friendly to eachother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are. They are very friendly and play", "extracted_entity": null}
{"question": "can you buy chlorine at a supermarket?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the form of chlor", "extracted_entity": null}
{"question": "would honey have been accessible to ancient romans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, honey was readily available to the ancient", "extracted_entity": null}
{"question": "would honey have been accessible to Julius Caesar? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the Romans", "extracted_entity": null}
{"question": "is some of the weight of a papaya inedible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the skin and seeds", "extracted_entity": null}
{"question": "is Dustin Hoffman unlikely to support Mike Pence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a liberal.", "extracted_entity": null}
{"question": "did Christopher Nolan meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet  former President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes,  Nolan did meet former President D", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was born in 19", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet Judy Garland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did meet the Wicked W", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet President Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "are goldfish bowls bad for goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. The water in a bow", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has met his brother, Robert", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 1943", "extracted_entity": null}
{"question": "could Aristotle have gone to school to be a railroad engineer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Aristotle was born 2", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines a colony when Elton John was born?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was a British colony.", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines a colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was a colony.", "extracted_entity": null}
{"question": "does Bill Gates experience menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he doesn\u2019t.", "extracted_entity": null}
{"question": "does Bill Gates have trouble dealing with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "does Bill Gates have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did create the world's", "extracted_entity": null}
{"question": "does Bill Gates frequently have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has a wife.", "extracted_entity": null}
{"question": "does the rover Spirit have a spirit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has a spirit, but not a", "extracted_entity": null}
{"question": "is a curling iron helpful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is.", "extracted_entity": null}
{"question": "is a curling iron useful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A curling iron is useful in cur", "extracted_entity": null}
{"question": "is a musket required for the French Defence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not the only requirement.", "extracted_entity": null}
{"question": "would a casino goer's '7' charm bracelet be likely for luck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not a 7-", "extracted_entity": null}
{"question": "does Stephen King disagree with J.K Rowling on trans issues?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s been very clear that he", "extracted_entity": null}
{"question": "are there still prisoners being kept in Alcatraz island?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There are no prisoners kept in Alcatraz island", "extracted_entity": null}
{"question": "can you see Alcatraz Island from the Atlantic City boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the island is located in San Francisco Bay", "extracted_entity": null}
{"question": "can you see Alcatraz Island from the Jersey Shore boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's too far away.", "extracted_entity": null}
{"question": "would Lord Voldemort have been permitted into Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He would have been permitted into Hogw", "extracted_entity": null}
{"question": "would sandals be unsafe at a construction site?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the sandals would not be safe because", "extracted_entity": null}
{"question": "does Doctor Who represent interracial couples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Doctor and Rose", "extracted_entity": null}
{"question": "in Doctor Who, can statues be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can be.", "extracted_entity": null}
{"question": "in Doctor Who, can statues be threatening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can people meet each other out of order in Doctor Who?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people can meet each other out of order", "extracted_entity": null}
{"question": "are there family planning options without hormones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes! There are many options for women to prevent", "extracted_entity": null}
{"question": "did Gorbachev influence the Russian Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Gorbachev did influence the Russian", "extracted_entity": null}
{"question": "would Tom Cruise consider his Katie Holms a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she left him.", "extracted_entity": null}
{"question": "would Tom Cruise consider his ex-wife a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "would Tom Cruise be unlikely to support psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a Scientologist.", "extracted_entity": null}
{"question": "is Tom Cruise opposed to psychiatric medication??", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is Tom Cruise opposed to psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And he is opposed to anti-de", "extracted_entity": null}
{"question": "can you complete watercolor paintings without a cup of water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, water is essential for watercolor painting.", "extracted_entity": null}
{"question": "do poop and hot dog filling have anything in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they're both brown.", "extracted_entity": null}
{"question": "was Lamarck's evolution theory compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Darwin's theory of evolution by", "extracted_entity": null}
{"question": "was Lamarckian evolution compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lamarck's theory of evolution", "extracted_entity": null}
{"question": "would a poisoner have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can be poisoned.", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like french beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Homer likes all food.", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are donuts.", "extracted_entity": null}
{"question": "does the San Francisco Bay have more of a homeless issue than Albany New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the San Francisco Bay Area has a larger", "extracted_entity": null}
{"question": "was there ever a South Park episode about someone who has a song about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there was.", "extracted_entity": null}
{"question": "did South Park make a parody of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn\u2019t.", "extracted_entity": null}
{"question": "did South Park make fun of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the episode \u201cMargarit", "extracted_entity": null}
{"question": "is a lot of famous New York food from Jewish cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a lot of famous New York", "extracted_entity": null}
{"question": "does New York City have a lot of famous Jewish Cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "would it be uncommon fora teenager to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, that's not the way they do", "extracted_entity": null}
{"question": "is agriculture without irrigation difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, agriculture without irrigation is very", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about teaching romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in Dead Poets Society", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Dead Poet's Society.", "extracted_entity": null}
{"question": "is sunlight important in pigmentation of asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, asparagus will grow in the", "extracted_entity": null}
{"question": "does light matter in pigmentation of Asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I've", "extracted_entity": null}
{"question": "are there Chinese imports in all of the Aldi stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many Chinese products in all Ald", "extracted_entity": null}
{"question": "would fans of Nine Inch Nails be likely to know of Johnny Cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think so.", "extracted_entity": null}
{"question": "are there red varieties of picked cucumber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are red varieties of picked c", "extracted_entity": null}
{"question": "are there times that fear causes euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are times that fear causes euph", "extracted_entity": null}
{"question": "would it be difficult to sleep in the world's most quiet room?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it would be very difficult to sleep in the world", "extracted_entity": null}
{"question": "could a mole of glucose fit inside the Empire State Building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be very crowded.", "extracted_entity": null}
{"question": "could Johnny Carson's children form a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they're all men.", "extracted_entity": null}
{"question": "would a silicon shortage be harmful to Intel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Intel would be hurt by a silicon", "extracted_entity": null}
{"question": "was Mozart inspired by Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Richard Wagner was born 23", "extracted_entity": null}
{"question": "are there romantic Doctor Who moments near Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "does Hank Hill sell items useful for grilling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "propane and propane accessories", "extracted_entity": null}
{"question": "could lots of Long Island Iced Teas cause Liver Disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Iced Teas won\u2019t", "extracted_entity": null}
{"question": "has anyone from Texas ever served as Vice President of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Nance Garner, who served", "extracted_entity": null}
{"question": "are pecans and peanuts grown in the same part of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pecans are grown in the south", "extracted_entity": null}
{"question": "have oil companies contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have contributed to global warming and", "extracted_entity": null}
{"question": "would the Neanderthal people have found snowshoes useful?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would have been very useful.", "extracted_entity": null}
{"question": "are all canidae good house pets?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO. Most are not.", "extracted_entity": null}
{"question": "do students at Berlin University of the Arts recite the Pledge of Allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. We don\u2019t have a flag.", "extracted_entity": null}
{"question": "could Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "did Jeanne Moreau see AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did.", "extracted_entity": null}
{"question": "did Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "she did", "extracted_entity": null}
{"question": "is it likely that Katy Perry's baby will look like Russell Brand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not likely.", "extracted_entity": null}
{"question": "do all drag kings take testosterone injections?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all drag kings take testoster", "extracted_entity": null}
{"question": "is it possible to confuse acne and chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Acne and chicken pox sc", "extracted_entity": null}
{"question": "could acne scars be mistaken for chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chicken pox scars are usually raised,", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. there are many people on foodstamps", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they could be more likely to own", "extracted_entity": null}
{"question": "did Bill Clinton use a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it wasn't invented yet", "extracted_entity": null}
{"question": "did Bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a Sega Genesis.", "extracted_entity": null}
{"question": "did Richard Nixon play Call Of Duty on a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He's dead.", "extracted_entity": null}
{"question": "did Richard Nixon play Call Of Duty on a  Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Richard Nixon use a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used an Atari 26", "extracted_entity": null}
{"question": "could Jesse Owens have won a medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Jesse Owens did not", "extracted_entity": null}
{"question": "did Thierry Rey win a gold medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a silver medalist", "extracted_entity": null}
{"question": "are oysters harvested to make jewelry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, oysters are not harvested", "extracted_entity": null}
{"question": "has the COVID-19 virus been used for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the virus has not been sequenced.", "extracted_entity": null}
{"question": "is the COVID-19 virus a good candidate for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the virus is a single stranded", "extracted_entity": null}
{"question": "is it dangerous to walk to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not dangerous to walk to the", "extracted_entity": null}
{"question": "is Edward Snowden in hiding?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "did Elvis popularize smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, that would be Kenny G", "extracted_entity": null}
{"question": "did Elvis invent smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "would oak burn on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn\u2019t.", "extracted_entity": null}
{"question": "does the media sugar coat the health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "would something with a high viscosity be hard to fit through a plastic straw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it would be too thick and would", "extracted_entity": null}
{"question": "would bringing a Torah to a Mosque be unusual?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Torah is a holy book of", "extracted_entity": null}
{"question": "are LGBT Pride events due, in part, to the work of Drag Queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (Read more about the Stonewall Ri", "extracted_entity": null}
{"question": "were Drag Queens involved in the creation of Pride events?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Stonewall riots were started by", "extracted_entity": null}
{"question": "do many American stores have lots of notebook on sale in August?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Schools start in September.", "extracted_entity": null}
{"question": "is handling some turtles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some turtles are dangerous.", "extracted_entity": null}
{"question": "is handling a snapping turtle dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, handling a snapping turtle is", "extracted_entity": null}
{"question": "should a doctor's office have a ramp for wheelchairs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it's the law.", "extracted_entity": null}
{"question": "are some children very excited to meet Robert Downey Jr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some children are very excited to meet Robert", "extracted_entity": null}
{"question": "is some adverting hidden in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are many examples of this, the", "extracted_entity": null}
{"question": "can you see sea lions in the San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you can't see them from", "extracted_entity": null}
{"question": "are there groups within the movement of feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many groups within feminism.", "extracted_entity": null}
{"question": "is there infighting within feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is infighting within feminism", "extracted_entity": null}
{"question": "do some artists support themselves through patronage alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only a few.", "extracted_entity": null}
{"question": "would a linguistics major take SQL courses?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he or she is interested in N", "extracted_entity": null}
{"question": "is it difficult to interview Edward Snowden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not difficult to interview Edward Snow", "extracted_entity": null}
{"question": "can street art be used as a form of protest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, street art can be used as a form", "extracted_entity": null}
{"question": "can you kayak from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can kayak from New England", "extracted_entity": null}
{"question": "can you walk from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can swim from New England", "extracted_entity": null}
{"question": "is Vice Media a peer reviewed journal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Vice Media is not a peer reviewed", "extracted_entity": null}
{"question": "could you go to New York Public Library and the Adirondacks in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and you can do it on a budget", "extracted_entity": null}
{"question": "do nurses watch anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the patient is a minor or the", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the nurses make sure that the", "extracted_entity": null}
{"question": "in treatment, would someone with anoxrexia nervosa be watched?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Anorexia nervosa is an eating", "extracted_entity": null}
{"question": "can you store hamburgers in the car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can.", "extracted_entity": null}
{"question": "does Elon Musk act as an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Elon Musk is a transph", "extracted_entity": null}
{"question": "is Billy Joel considered high risk for COVID-19?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is 71 years old", "extracted_entity": null}
{"question": "in Tetris99 do Newbies play with Pro players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the newbies have to pay", "extracted_entity": null}
{"question": "did P. G. Wodehouse like cartoon shows as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did P. G. Wodehouse watch Rick and Morty as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, yes he did.", "extracted_entity": null}
{"question": "is it likely some paleontologists were inspired by Michael Crichton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "could Michael Crichton have inspired some paleontologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have.", "extracted_entity": null}
{"question": "do some people feel nauseous after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some people feel nauseous after surg", "extracted_entity": null}
{"question": "does the Antarctic coast have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The homeless people are all in the", "extracted_entity": null}
{"question": "does Antarctica have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it\u2019s too cold and there", "extracted_entity": null}
{"question": "is homelessness generally voluntary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most people would rather have a home.", "extracted_entity": null}
{"question": "would Paypal be a good way to pay for pornography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would be a terrible way to pay", "extracted_entity": null}
{"question": "are some scientific journals less credible than others?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. This is a difficult question to answer,", "extracted_entity": null}
{"question": "did the author of the Da Vinci Code ever view depictions of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Last Supper was a Passover", "extracted_entity": null}
{"question": "has the author Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has not.", "extracted_entity": null}
{"question": "has Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because in the Last Supper, Jesus", "extracted_entity": null}
{"question": "do people find reason to protest the London Zoo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the way the animals are treated", "extracted_entity": null}
{"question": "have there been zombie adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, yes there have.", "extracted_entity": null}
{"question": "have there been horror adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there have.", "extracted_entity": null}
{"question": "do different parts of the globe celebrate New Year's Day on different calendar dates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Most of the world uses the Gregor", "extracted_entity": null}
{"question": "were people in Utah worried about damage from Hurricane Harvey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Utah is 1,50", "extracted_entity": null}
{"question": "are Indigenous peoples in Canada still fighting for equal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Indigenous peoples are still", "extracted_entity": null}
{"question": "are all pickles made with lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all pickles are made with la", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lakes in the Adirondack Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in Lake Placid and Lake George", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You could not swim with Manta", "extracted_entity": null}
{"question": "could you swim with Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Manta Rays are tropical animals.", "extracted_entity": null}
{"question": "would you be likely to see Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they live in the ocean.", "extracted_entity": null}
{"question": "has Subway had ties with any child predators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they did have ties with J", "extracted_entity": null}
{"question": "is pain in the torso a potential sign of appendicitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "can a kidney infection cause pain in the torso?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can you get xanax from a pharmacy without a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in the form of a s", "extracted_entity": null}
{"question": "are there substitutes available for human legs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are substitutes available for human", "extracted_entity": null}
{"question": "do students with dyscalculia need more help with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they need more help with statistics.", "extracted_entity": null}
{"question": "would a task of documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "would a job documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be very difficult.", "extracted_entity": null}
{"question": "would a job about statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dyscalculia is a specific learning disability", "extracted_entity": null}
{"question": "could someone with dyscalculia struggle with documenting statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, dyscalculia can make it difficult", "extracted_entity": null}
{"question": "would someone with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and no. It depends on what aspect", "extracted_entity": null}
{"question": "would a student with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, dyscalculia is a specific learning", "extracted_entity": null}
{"question": "would a student with dyscalcula struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dyscalculia is a specific learning disability", "extracted_entity": null}
{"question": "can judges be sent to jail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only for criminal acts.", "extracted_entity": null}
{"question": "are most people the result of ectopic pregnancies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, most people are not the result of e", "extracted_entity": null}
{"question": "could you see the Eiffel Tower from the top of the Vatican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you could see the Vatican", "extracted_entity": null}
{"question": "can you see a themed King Kong ride in Universal Studios?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a great ride.", "extracted_entity": null}
{"question": "can you see a themed King Kong ride in Florida?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not going to be", "extracted_entity": null}
{"question": "do desert hares live in the Balkans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I do know", "extracted_entity": null}
{"question": "do the Balkans experience monsoons?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the Balkans do have a", "extracted_entity": null}
{"question": "can you smell argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, argon is an odorless gas", "extracted_entity": null}
{"question": "can you view argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, argon is a colorless gas", "extracted_entity": null}
{"question": "can you pick up argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a gas", "extracted_entity": null}
{"question": "did Johnny Carson make phone calls while driving?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is Bruce Lee making a new movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is dead.", "extracted_entity": null}
{"question": "can you shake Bruce Lee's hand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I can't.", "extracted_entity": null}
{"question": "can someone survive without a pancreas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the pancreas is the organ that", "extracted_entity": null}
{"question": "can you say hello to Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know who Aretha Franklin is", "extracted_entity": null}
{"question": "can you meet Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you are a member of", "extracted_entity": null}
{"question": "do drag queens sometimes have a hard time finding jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "do white blood cells keep people alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they fight off infections.", "extracted_entity": null}
{"question": "do guitarists have trouble with their fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have finger problems.", "extracted_entity": null}
{"question": "does a guitarist have callused fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not from playing guitar", "extracted_entity": null}
{"question": "does Julia Roberts pay taxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as she should", "extracted_entity": null}
{"question": "did Douglas Adams use the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "can sweet corn make diabetes worse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, sweet corn is not known to make di", "extracted_entity": null}
{"question": "did Mario Andretti win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he drove in the Indianapolis 5", "extracted_entity": null}
{"question": "was Europa important to Stanley Kubrick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is the title of the first chapter", "extracted_entity": null}
{"question": "are drag queens harassed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they\u2019re harassed by", "extracted_entity": null}
{"question": "did Claude Monet's widow receive aid from the French government after his death?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "should Koi and beta fish be kept together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Koi and goldfish are cold water", "extracted_entity": null}
{"question": "would children be disappointed with a deciduous Christmas tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be.", "extracted_entity": null}
{"question": "are the blossoms of a peach tree equal in length to cherry tree blossoms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, peach tree blossoms are longer", "extracted_entity": null}
{"question": "will a peach pit always grow into a peach tree?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is planted and has enough", "extracted_entity": null}
{"question": "with the right makeup, can clowns be frightening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they're more frightening without", "extracted_entity": null}
{"question": "would nuclear attacks improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States would retaliate with", "extracted_entity": null}
{"question": "would nuclear escalation improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would cause the US to ret", "extracted_entity": null}
{"question": "would fans of Christian Rock enjoy Shaggy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would a Galapagos penguin be out of place on the Antarctic Peninsula?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are endemic to the Galap", "extracted_entity": null}
{"question": "is it ever necessary to bathe a sloth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The sloths at the Sloth", "extracted_entity": null}
{"question": "can you make soup without a pot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can make soup in a pan", "extracted_entity": null}
{"question": "can you make soup without heat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can make cold soup.", "extracted_entity": null}
{"question": "would a blender be useful for making some soups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all soups.", "extracted_entity": null}
{"question": "are stim toys marketed at children with learning disabilities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they are often marketed as \u201c", "extracted_entity": null}
{"question": "are children with learning disabilities big consumers of stim toys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not always. Many children with learning", "extracted_entity": null}
{"question": "would someone who doesn't own a gun have a need for gunpowder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is no reason for a non-", "extracted_entity": null}
{"question": "do anime fans enjoy detective fiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "are most milkshakes high in glucose?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they have a lot of sugar.", "extracted_entity": null}
{"question": "could someone from 1920 have gotten HIV from a transfusion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. HIV was not discovered until 1", "extracted_entity": null}
{"question": "can peppermint oil cure pink eye?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pink eye is a contagious infection", "extracted_entity": null}
{"question": "would you need a boat to get from Peru to the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you would need a boat to get from", "extracted_entity": null}
{"question": "is it normal to be nervous before spinal chord procedures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is normal to be nervous,", "extracted_entity": null}
{"question": "would a new mother need more time off from work than a childless person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s a matter of biology", "extracted_entity": null}
{"question": "would a mother require more time off than a childless woman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if she is breastfeeding.", "extracted_entity": null}
{"question": "would it be acceptable to have cats that live in your barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I would like to have cats that", "extracted_entity": null}
{"question": "are wild boar hunted by indigenous peoples in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by Inuit and M\u00e9tis", "extracted_entity": null}
{"question": "do indigenous peoples in Canada hunt wild boar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They hunt them with", "extracted_entity": null}
{"question": "is it likely that you'd hear disco music at Mazi Nightclub?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's unlikely", "extracted_entity": null}
{"question": "have there been any Greeks in space?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, one.", "extracted_entity": null}
{"question": "would Tom Cruise know what an e-meter is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would have no idea.", "extracted_entity": null}
{"question": "was Mozart inspired by Richard Wagner's music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Wagner was born in 18", "extracted_entity": null}
{"question": "can you see the stars in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can see the lights of the", "extracted_entity": null}
{"question": "does Marvel sell myths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as myths", "extracted_entity": null}
{"question": "would a leftist be likely to refuse lunch from Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Chick-fil-A has", "extracted_entity": null}
{"question": "are there materials that are small enough to pass through cell walls?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many substances that are small", "extracted_entity": null}
{"question": "are transgender teens less likely to read J.K. Rowling's writing than cisgender teens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of her transphobia", "extracted_entity": null}
{"question": "do some trees keep their leaves during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, evergreen trees keep their leaves all year", "extracted_entity": null}
{"question": "do some trees keep their foliage during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the evergreens", "extracted_entity": null}
{"question": "can you still see flowers in autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can.", "extracted_entity": null}
{"question": "do an ocelot's eyes glow under a flashlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do!", "extracted_entity": null}
{"question": "is Wasabi paste permitted for religious feasts in Judaism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Wasabi paste is not permitted for", "extracted_entity": null}
{"question": "can you travel by boat from Bern to Albuquerque?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you're willing to take the", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Casablanca?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as veterans.", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Across the Universe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a movie about the 1", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Forest Gump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the movie Forest Gump depicts", "extracted_entity": null}
{"question": "is Santa Claus a nerd?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is there likely to be frost on the morning of Boxing Day in Johannesburg? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, very likely.", "extracted_entity": null}
{"question": "is there likely to be frost on the morning of a Canberra Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, probably", "extracted_entity": null}
{"question": "are most feminists proponents of conservatism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most feminists are proponents of liberal", "extracted_entity": null}
{"question": "would Rick and Morty be shocking to someone from the 1970's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not because of the content.", "extracted_entity": null}
{"question": "are fans of Community likely to enjoy Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they\u2019re both created by Dan", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. White blood cells are not a useful di", "extracted_entity": null}
{"question": "would fans of Osmosis Jones know what white blood cells are?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they wouldn't.", "extracted_entity": null}
{"question": "is material from an aloe plant found in bathroom cupboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the form of aloe vera", "extracted_entity": null}
{"question": "could an astronomer detect noise from the beginning of the universe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the universe is expanding and the", "extracted_entity": null}
{"question": "could an astronomer warn of an imminent electrical blackout?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the astronomer was in the right", "extracted_entity": null}
{"question": "would an abused child benefit from talking to a school counselor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, talking to a counselor would help", "extracted_entity": null}
{"question": "is it impossible to play \"Happy Birthday\" using the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it would be difficult.", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool for doctors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "White blood cells (WBCs) are a", "extracted_entity": null}
{"question": "will most attendees of groundhog day in Vermont be wearing jackets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most attendees of groundh", "extracted_entity": null}
{"question": "is it wise to wear a jacket on groundhog day in Vermont?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is wise to wear a jacket", "extracted_entity": null}
{"question": "is the weather usually chilly on groundhog day in the Northeastern US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's winter.", "extracted_entity": null}
{"question": "is it usually cold in Vermont on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is usually cold in Vermont on", "extracted_entity": null}
{"question": "is it usually cold in Canada on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is usually cold in Canada on ground", "extracted_entity": null}
{"question": "is xenophobia counter productive to world peace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Xenophobia is a fear", "extracted_entity": null}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulently made?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he died in 1994", "extracted_entity": null}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Kurt Cobain was alive on", "extracted_entity": null}
{"question": "would Barack Obama be unlikely to be a friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "would Barack Obama be an unlikely friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they both have the same goal:", "extracted_entity": null}
{"question": "would a guitarist feel less pain during a finger prick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The pain is the same.", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have long terms?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "did the Disney company always have power over the Star Wars movies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the original creator, George Lucas,", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing domestic feline reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. An environmentalist would advocate for prevent", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing house cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not for the reasons you might think", "extracted_entity": null}
{"question": "is it environmentally responsible to prevent domestic cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The domestic cat is a non-native", "extracted_entity": null}
{"question": "can some porches be inaccessible for disabled people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some porches are inaccessible for", "extracted_entity": null}
{"question": "would Jay-Z get free tickets to a Beyonce concert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "is the average person at less risk than a celebrity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are at the same risk.", "extracted_entity": null}
{"question": "do celebrities feel less safe than most people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, celebrities have a higher sense of", "extracted_entity": null}
{"question": "does being a celebrity make one's life less safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The more famous you are, the more", "extracted_entity": null}
{"question": "does being a celebrity put one in danger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "being a celebrity puts one in danger of being", "extracted_entity": null}
{"question": "is joining the United States Air Force an option for someone who takes Seroquel daily?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "would someone on a daily regimen of an SSRI be able to enter the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, according to the Department of Defense,", "extracted_entity": null}
{"question": "would someone on a daily regimen of an SSRI be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in a position that requires a", "extracted_entity": null}
{"question": "would someone on a daily regimen of psych meds be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You would have to be off all medic", "extracted_entity": null}
{"question": "would someone on a daily regimen of Valium be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Valium is a benzodiaze", "extracted_entity": null}
{"question": "are animal rights advocates likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are opposed to cruelty to", "extracted_entity": null}
{"question": "are PETA members likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are against cruelty to animals", "extracted_entity": null}
{"question": "in Dead Poet's Society, do the boys in the English class enjoy poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they hate poetry", "extracted_entity": null}
{"question": "are the students on Dead Poets Society fans of Romantic Poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "would a novice singer be nervous to perform for Simon Cowell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they would be more nervous to", "extracted_entity": null}
{"question": "are people less skilled at climbing steel angles than animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "are there animals capable of climbing more extreme angles than people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, geckos can climb vertical walls", "extracted_entity": null}
{"question": "would Sigmund Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is the father of psycho", "extracted_entity": null}
{"question": "would Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Freud believed that the unconscious mind was", "extracted_entity": null}
{"question": "did Tony Bennett's children outnumber his wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he had two daughters, Antonia and", "extracted_entity": null}
{"question": "would firefighters respond to a nearby gunshot victim?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they do.", "extracted_entity": null}
{"question": "were some employees from the WTC in 2000 the victims of a terror attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1993", "extracted_entity": null}
{"question": "would a cook appreciate the gift of a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you know they are into", "extracted_entity": null}
{"question": "would a cook appreciate a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "would people be disappointed in chicken on a Thanksgiving table?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not be disappointed.", "extracted_entity": null}
{"question": "are there disparities in access to preventive healthcare in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are disparities in access to prevent", "extracted_entity": null}
{"question": "would your restroom be more odorous if you ate asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The sulfur in asparagus", "extracted_entity": null}
{"question": "would modern Marxists disapprove of capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Marxists would disapprove of capital", "extracted_entity": null}
{"question": "would a neglected child benefit from psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the child can benefit from psychotherapy", "extracted_entity": null}
{"question": "are do it yourself tattoos more likely to become infected than shop tattoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they are done in unsanit", "extracted_entity": null}
{"question": "should students study Old English for the SAT?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it would help to study Latin.", "extracted_entity": null}
{"question": "would you see squid in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the lake itself. They", "extracted_entity": null}
{"question": "would you see squis in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The water is too cold.", "extracted_entity": null}
{"question": "are all translations of hello in spoken language?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but some are more common than others", "extracted_entity": null}
{"question": "are all translations of hello audible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, all translations of hello are audible", "extracted_entity": null}
{"question": "is Alcatraz Island significant to Native American history?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the island was occupied by the Native Americans", "extracted_entity": null}
{"question": "would car travel from Brooklyn to Canada in a day be feasible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would take a minimum of 6", "extracted_entity": null}
{"question": "would car travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you were driving a 19", "extracted_entity": null}
{"question": "would it be unsafe for an aircraft pilot to have untreated narcolepsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A narcoleptic pilot would be at", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Narcolepsy is a neurological", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot nearly impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, for several reasons.", "extracted_entity": null}
{"question": "would lily pads grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Gobi Desert is a dry", "extracted_entity": null}
{"question": "would cattails grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cattails grow in wetlands, not", "extracted_entity": null}
{"question": "would cattails thrive in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they require wetlands.", "extracted_entity": null}
{"question": "are Kangaroos the smallest animal with a pouch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the smallest animal with a pouch is", "extracted_entity": null}
{"question": "are Kangaroos the only animals with a pouch on the front of their body?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The opossum also has a p", "extracted_entity": null}
{"question": "are right wing extremists opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "do Jehova's Witnesses skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jehova's Witnesses", "extracted_entity": null}
{"question": "would travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you had a car, yes.", "extracted_entity": null}
{"question": "would an endive farmer worry if their crop was covered in frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "would a farmer worry if they saw frost on their endive plants?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they know the frost will not", "extracted_entity": null}
{"question": "could someone who isn't mentally ill have a reason to seek psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, to help them better understand themselves", "extracted_entity": null}
{"question": "would someone with a BA in English be well suited for copy editing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A BA in English is not a", "extracted_entity": null}
{"question": "does inflammation of the brain require medical attention?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. Inflammation of", "extracted_entity": null}
{"question": "are all crustaceans on restaurant menus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all crustaceans are on", "extracted_entity": null}
{"question": "is handedness unimportant in guitar playing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is unimportant.", "extracted_entity": null}
{"question": "have many fans of Spongebob seen Dustin Hoffman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "None of them.", "extracted_entity": null}
{"question": "did you get a bad deal if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I did not get a bad deal.", "extracted_entity": null}
{"question": "did you get ripped off if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you got a nice, stable, free", "extracted_entity": null}
{"question": "would you be likely to see a tour guide in the Adirondack National Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is a wilderness area", "extracted_entity": null}
{"question": "would Goku from Dragon Ball Z work with a mercenary?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He'd be too busy training, so no", "extracted_entity": null}
{"question": "would a fan of  The Lord of the Rings books be likely to have read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "would a fan of  The Lord of the Rings books be likely to read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, The Hobbit is the prequel to", "extracted_entity": null}
{"question": "did the Attack on Pearl Harbor lead to the world's first nuclear attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the attack on pearl harbor", "extracted_entity": null}
{"question": "do System of a Down push for the recognition of the genocide against Armenians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do Armenians still have to fight for recognition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Armenian Genocide is recognized by the", "extracted_entity": null}
{"question": "do System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course they do.", "extracted_entity": null}
{"question": "so System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They have an album called", "extracted_entity": null}
{"question": "did Christopher Columbus play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was afraid of Seamen", "extracted_entity": null}
{"question": "can Pandas play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can\u2019t spell.", "extracted_entity": null}
{"question": "can a lobster procreate in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can't.", "extracted_entity": null}
{"question": "is it a good idea to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a good idea to drink more", "extracted_entity": null}
{"question": "does it make sense to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, you should drink less water", "extracted_entity": null}
{"question": "does it make sense to drink water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is important to replace fluids lost", "extracted_entity": null}
{"question": "would you notice a difference between a shallot and an onion in french onion soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a big difference. The shallot is", "extracted_entity": null}
{"question": "could a garlic clove be confused for a shallot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019ve never seen a garlic clove", "extracted_entity": null}
{"question": "could a shallot be confused for a garlic clove?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The two are similar in appearance and have", "extracted_entity": null}
{"question": "could a shallot be confused for a cocktail onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But only if you are a drunk", "extracted_entity": null}
{"question": "does handedness determine how you use ASL?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, handedness does not determine how you use", "extracted_entity": null}
{"question": "are advocates of right wing extremism likely to support Proud Boys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Proud Boys are a far", "extracted_entity": null}
{"question": "are advocates of right wing extremism against gender neutral restrooms? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course they are.", "extracted_entity": null}
{"question": "would someone practicing right wing extremism donate to an abortion clinic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "would an broken hand make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be difficult to play lacros", "extracted_entity": null}
{"question": "would an broken arm make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it would be hard to hold the", "extracted_entity": null}
{"question": "do most PCMH have a file system?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. We use a cloud based EHR.", "extracted_entity": null}
{"question": "can hyena's on steak alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are omnivores, and", "extracted_entity": null}
{"question": "can a cable ferry use horsehair for stabilization?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes. A cable ferry can", "extracted_entity": null}
{"question": "can a cable ferry be made from plastic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would not be as strong as", "extracted_entity": null}
{"question": "can a cable ferry be made from cedar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not with the plans we have available", "extracted_entity": null}
{"question": "are Unitarian Universalists welcoming to people who align with agnosticism? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the reason why is because we do", "extracted_entity": null}
{"question": "do Vegans refuse products made with honey bee products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Vegans do not consume honey or any other", "extracted_entity": null}
{"question": "would it be unusual for a strict vegan to keep Honey Bees for harvest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be unusual.", "extracted_entity": null}
{"question": "would it be unusual for a vegan to keep honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn\u2019t be unusual.", "extracted_entity": null}
{"question": "would a honey bee's honey be unappealing to a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Because the bees are being exploited", "extracted_entity": null}
{"question": "is Hanoi considered large in Vietnam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Hanoi is the", "extracted_entity": null}
{"question": "did Eddie Murphy own smartphone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He didn\u2019t even have a cell", "extracted_entity": null}
{"question": "did Eddie Murphy own an iPhone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Eddie Murphy did not own an", "extracted_entity": null}
{"question": "would someone have a hard time seeing a snowy owl against the sydney opera house?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is white and so is the opera", "extracted_entity": null}
{"question": "would Amish parents warn against feelings of lust?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "did Albert Einstein watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not watch Modern Family in his", "extracted_entity": null}
{"question": "did Donald Trump watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Homer watch Modern Family?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "can Homer Simpson serve as President of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's not a natural born citiz", "extracted_entity": null}
{"question": "can Robert De Niro become President of South Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not a South African citizen", "extracted_entity": null}
{"question": "can Robert De Niro run for President of France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he cannot.", "extracted_entity": null}
{"question": "was the antagonist of the Lego Movie portrayed by a Saturday Night Live alumnus? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Will Ferrell.", "extracted_entity": null}
{"question": "can parachuting cause loss of life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "did the Gallic Wars have an effect on France?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Gallic Wars were a series of military campaign", "extracted_entity": null}
{"question": "were tanks used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They were called chariots.", "extracted_entity": null}
{"question": "were guns used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Romans used swords.", "extracted_entity": null}
{"question": "is Florida a well known curling state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "are all hot dogs created with smoking as a step?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but most are.", "extracted_entity": null}
{"question": "is panoramic photography possible with a polaroid? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you need a special panoram", "extracted_entity": null}
{"question": "is it fair to be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is fair to be skeptical", "extracted_entity": null}
{"question": "did the upcoming new year worry people in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "were people hoarding food and supplies out of fear in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (This is a bit of a trick", "extracted_entity": null}
{"question": "do people opposed to gentrification butt heads with urban planners often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they butt heads often.", "extracted_entity": null}
{"question": "on Venice beach, would you be likely to hear music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would.", "extracted_entity": null}
{"question": "are there countries where people opposed to fascism are considered terrorists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "should you expect to be hungry before surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you should not be hungry before surg", "extracted_entity": null}
{"question": "is it common to be hungry after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is common to be hungry after surgery", "extracted_entity": null}
{"question": "could a rhinoceros accidentally be included in a list of mythical creatures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if the list was compiled by", "extracted_entity": null}
{"question": "could a rhinoceros be a playable animal in Dungeons and Dragons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you have the right DM.", "extracted_entity": null}
{"question": "does the United States Department of Education oversee undocumented students?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The US Department of Education does not oversee und", "extracted_entity": null}
{"question": "would myofascitis be diagnosed by dual-energy X-ray absorptiometry?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, myofascitis is not diagnosed", "extracted_entity": null}
{"question": "can salt protect a garden that has spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. You can mix 1", "extracted_entity": null}
{"question": "were people concerned about the new year in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. people were concerned about the new year in", "extracted_entity": null}
{"question": "were computers a hot topic in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was the year of the dot com", "extracted_entity": null}
{"question": "can mammals survive with only one kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. they can survive with only one kid", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz crystals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, crystals are used in reiki", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as quartz crystals are often", "extracted_entity": null}
{"question": "are cucumbers a low maintenance plant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you need to make sure that they", "extracted_entity": null}
{"question": "do The World's Billionaires deny pizzagate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "would it be impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be impossible to snowboard on", "extracted_entity": null}
{"question": "is it impossible to go snowboarding on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The temperature on Venus is too hot", "extracted_entity": null}
{"question": "is it impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the atmospheric pressure is", "extracted_entity": null}
{"question": "after COVID-19 came to the US, did Chinese Americans face discrimination?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "would someone with COVID-19 have a higher level of blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. COVID-19 patients have a high", "extracted_entity": null}
{"question": "did Ralph Macchio make a fighting movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did Ralph Macchio make a karate movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he did make a movie with N", "extracted_entity": null}
{"question": "did Putin help Russia win the space race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he helped Russia win the space race.", "extracted_entity": null}
{"question": "has Osama Bin Laden been influencing the Trump Administration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He said in 2004", "extracted_entity": null}
{"question": "is sweet potato with marshmallow a holiday traditional dish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a traditional dish in the", "extracted_entity": null}
{"question": "during COVID-19, have more people been using Skype?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, more people have been using Skype during", "extracted_entity": null}
{"question": "does Channon Rose use her platform to speak against abortion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She has a video on her channel where", "extracted_entity": null}
{"question": "can you get on the AirTrain JFK with a metropass?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. If you are a month", "extracted_entity": null}
{"question": "do you need a Metropass to ride the AirTraine JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you do not need a Metropass", "extracted_entity": null}
{"question": "do you need a Metropass to ride the AirTraine at JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you do. It's part of", "extracted_entity": null}
{"question": "is Dr. Joseph Mengele a part of the history of psychology that is celebrated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was a Nazi and a", "extracted_entity": null}
{"question": "for breast cancer patients, is plastic surgery considered reconstructive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, plastic surgery is considered reconstruct", "extracted_entity": null}
{"question": "did Europeans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Europeans did not have blueberries", "extracted_entity": null}
{"question": "did Germans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they had blueberries", "extracted_entity": null}
{"question": "would someone in a MAGA hat consider accusations of Trump's islamophobia to be invalid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. because they are.", "extracted_entity": null}
{"question": "would someone in a MAGA hat ignore claims about Trump of Islamophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course they would.", "extracted_entity": null}
{"question": "do humans find the bite of a horse fly to be upsetting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, the bite of a", "extracted_entity": null}
{"question": "in 1960, did people know what the surface of the moon was like?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they knew that it was rocky.", "extracted_entity": null}
{"question": "can you go to the Mall of America and the Metropolitan Museum of Art in the same day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can.", "extracted_entity": null}
{"question": "did Al Capone own an iPhone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he died in 1947", "extracted_entity": null}
{"question": "do Scientologists celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's a great holiday!", "extracted_entity": null}
{"question": "do Mormons have a big Eid al-Fitr celebration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we do not. We do not celebr", "extracted_entity": null}
{"question": "do Mormons celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. We don\u2019t celebrate any Muslim", "extracted_entity": null}
{"question": "do Christians worship multiple Gods?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we worship the One True God, the", "extracted_entity": null}
{"question": "can a person survive by eating Ammonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it's toxic.", "extracted_entity": null}
{"question": "has a Christmas character ever tried to save the New Year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is the Forbidden City the seat of China's government?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "did Woodrow Wilson have access to a telephone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I can find out", "extracted_entity": null}
{"question": "can someone eat a Dodo today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are extinct.", "extracted_entity": null}
{"question": "is the Dodo reproducing today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s extinct.", "extracted_entity": null}
{"question": "can Cholera be treated successfully?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Cholera can be treated successfully if", "extracted_entity": null}
{"question": "did Dale Earnheardt Sr. crash due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He was killed on impact.", "extracted_entity": null}
{"question": "would it be bad for a Member of Parliament to be photographed with Jeffrey Epstein?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be bad.", "extracted_entity": null}
{"question": "would fans of Weird Al be likely to enjoy The Onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Both are satirical, with a", "extracted_entity": null}
{"question": "does Mozilla offer free educational programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mozilla offers free educational programs for people of all", "extracted_entity": null}
{"question": "can you learn web development for free with Mozilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can learn web development for free with", "extracted_entity": null}
{"question": "would it be harder for Kaitlyn Dobrow to do calligraphy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she is left-handed.", "extracted_entity": null}
{"question": "would a Catholic missionary meet resistance with Tibetan people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they would not believe in the same", "extracted_entity": null}
{"question": "do guitarist's have finger tips that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they just have calluses.", "extracted_entity": null}
{"question": "do finger blood tests typically hurt guitarist's less?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I can tell", "extracted_entity": null}
{"question": "do guitarist's finger tips typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do guitarist's fingers typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, guitarist's fingers typically hurt more", "extracted_entity": null}
{"question": "if Whole Foods is too expensive for you, would Aldi be a good option?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aldi is a good option for people", "extracted_entity": null}
{"question": "would you be disappointed if you went to bojangles for a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. You'd be disappointed because you", "extracted_entity": null}
{"question": "does tourism harm the environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, tourism harms the environment in many", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the off season?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Many businesses close for the winter season", "extracted_entity": null}
{"question": "do Mormons and 7th Day Adventists disagree about if Sunday is a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do Christians disagree about whether Sunday is the day of rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some Christians disagree about whether Sunday is", "extracted_entity": null}
{"question": "is there disagreement among Christians regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is disagreement.", "extracted_entity": null}
{"question": "is there argument regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Church has always held that Sunday is", "extracted_entity": null}
{"question": "is David A.R. White an advocate for acknowledging the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is David A.R. White known for arguing for the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is known for acting in Christian mov", "extracted_entity": null}
{"question": "did The Three Stooges get popular on Saturday Night Live?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they got popular on short movies.", "extracted_entity": null}
{"question": "were The Three Stooges inspired by Saturday Night Live skits?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were inspired by vaudeville", "extracted_entity": null}
{"question": "would Bobby Fischer be likely to know of the French Defense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Fischer was a chess master.", "extracted_entity": null}
{"question": "would a chocolate brownie from a marijuana dispensary be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the amount of THC in the", "extracted_entity": null}
{"question": "would a chocolate brownie from a dispensary in Colorado be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The dispensary should not be s", "extracted_entity": null}
{"question": "would it have been wise to keep alcohol away from Amy Winehouse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was an alcoholic.", "extracted_entity": null}
{"question": "if somebody wants a coolata is Starbucks a good place to go?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they have lots of flavors", "extracted_entity": null}
{"question": "did Jane Austen ever make the New York Times Bestseller list in her lifetime?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she died in 181", "extracted_entity": null}
{"question": "could someone take Northwest Airlines to the Beijing Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you booked a flight to Be", "extracted_entity": null}
{"question": "could someone take Northwest Airlines to the Rio Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Northwest Airlines does not exist anymore.", "extracted_entity": null}
{"question": "would Benito Mussolini have success in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is dead", "extracted_entity": null}
{"question": "would the leaves in Auburn, New York be changing colors in in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the leaves will be changing in September.", "extracted_entity": null}
{"question": "would the leaves in Auburn, New York be changing colors in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the leaves in Auburn, New", "extracted_entity": null}
{"question": "would you be able to watch the leaves change in the fall in Auburn, NY?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can watch the leaves change in A", "extracted_entity": null}
{"question": "do people often think that Frankenstein was a monster?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. That is because the creature in the story", "extracted_entity": null}
{"question": "would Firefighters be included in a 9/11 memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "do any games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "are there places with slot machines that you can get free drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many casinos that offer free", "extracted_entity": null}
{"question": "does St. Jude's hospital help patients with Leukemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, St. Jude's hospital helps", "extracted_entity": null}
{"question": "when you convince a child that they have no power, does it diminish their confidence? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It diminishes their confidence", "extracted_entity": null}
{"question": "are there women who get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The prostate is not a gender-", "extracted_entity": null}
{"question": "can a woman get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, only men have prostate glands.", "extracted_entity": null}
{"question": "at a latino restaurant, would you find the dish coq au vin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because coq au vin is a French", "extracted_entity": null}
{"question": "is Call of Duty more popular than Doom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (COD is in the top", "extracted_entity": null}
{"question": "is lithium considered a good treatment for the mood disorder major depressive disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lithium is a treatment for bipolar dis", "extracted_entity": null}
{"question": "by junior high school, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most people know basic arithmetic by the time", "extracted_entity": null}
{"question": "by their A-level exams, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is taught in primary school.", "extracted_entity": null}
{"question": "by their A-levels, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they should", "extracted_entity": null}
{"question": "are any colors on flag of the United States shared by flags of genocide perpetrators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, red, white, and blue are all", "extracted_entity": null}
{"question": "did Bonanza have greater longevity than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bonanza was on the air for", "extracted_entity": null}
{"question": "did Bonanza have greater staying power than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bonanza ran 14 seasons,", "extracted_entity": null}
{"question": "was Alexander Graham Bell's telephone made by motorola?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't. Motorola made", "extracted_entity": null}
{"question": "are there people for whom the correct pronouns are 'he/him' who experience menstruation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Trans men and non-binary people who", "extracted_entity": null}
{"question": "did Heracles have any full siblings that were immortal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Heracles had three half-brothers who were", "extracted_entity": null}
{"question": "has Aretha Franklin ever collaborated with someone that later committed suicide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but she did sing \"Respect\"", "extracted_entity": null}
{"question": "are elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in amounts needed for photosyn", "extracted_entity": null}
{"question": "were several of Spartacus's companions from area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Gauls were known for their fier", "extracted_entity": null}
{"question": "were any of Spartacus's companions born in area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not.", "extracted_entity": null}
{"question": "were any of Spartacus's companions from the area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Oenomaus of the Celt", "extracted_entity": null}
{"question": "were there any famous musicians with more letter a's in their name than Isaac Newton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there weren't.", "extracted_entity": null}
{"question": "were cannons built during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "in China, the Middle East, and Europe", "extracted_entity": null}
{"question": "were cannons wielded during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were. The earliest known cann", "extracted_entity": null}
{"question": "were cannons used during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cannons were not used during the Bronze", "extracted_entity": null}
{"question": "would firefighters respond to a heart attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, firefighters respond to all medical", "extracted_entity": null}
{"question": "would firefighters respond to a stabbing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are needed.", "extracted_entity": null}
{"question": "did Shane Dawson work with any creators who have Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He did.", "extracted_entity": null}
{"question": "are any current world leaders former KGB members?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Vladimir Putin is a former KGB", "extracted_entity": null}
{"question": "will you see the doctor a lot more during pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because you need to make sure that your", "extracted_entity": null}
{"question": "will someone have more doctor's appointments during their pregnancy than normal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Women with diabetes should have more", "extracted_entity": null}
{"question": "can pregnancy put strain on friendships?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, pregnancy can put strain on", "extracted_entity": null}
{"question": "can pregnancy ruin a friendship?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not always.", "extracted_entity": null}
{"question": "can you eat some kinds of citrus on birth control?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can eat some kinds of citrus", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because a car is a useful thing,", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of iron earrings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are made of iron.", "extracted_entity": null}
{"question": "was Rosalind Franklin studying in Molecular Biology?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was studying in Biophysics", "extracted_entity": null}
{"question": "would a chiropractic care center be more helpful for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say it depends on the severity of", "extracted_entity": null}
{"question": "would a chiropractic care center be more appropriate for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a chiropractic care center would", "extracted_entity": null}
{"question": "do Doctor Who fans enjoy taking photos in pay phone booths?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, yes we do.", "extracted_entity": null}
{"question": "did students in 1999 have a different understanding of Pluto than those in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They used to believe it was a planet", "extracted_entity": null}
{"question": "would Kale be better to eat than Spinach for someone with scurvy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kale and spinach are both excellent sources of", "extracted_entity": null}
{"question": "is Kim Kardashian a supermodel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than a Zweihander?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a trident.", "extracted_entity": null}
{"question": "eminem's daughter spells her name differently than star of 1961 Disney film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Joel Osment", "extracted_entity": null}
{"question": "eminem's daughter spells her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mae", "extracted_entity": null}
{"question": "does Eminem's daughter spell her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hailie Jade is spelled", "extracted_entity": null}
{"question": "does Eminem's daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the original was Hayley Mills.", "extracted_entity": null}
{"question": "does Eminem's biological daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Hailie Jade Scott Mathers", "extracted_entity": null}
{"question": "did the early settlers of the U.S celebrate the first halloween?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn\u2019t.", "extracted_entity": null}
{"question": "has any Doctor Who actor won a razzie award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Cleese.", "extracted_entity": null}
{"question": "would a psychiatrist be likely to recommend Prozac to someone with bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a psychiatrist would be unlikely to", "extracted_entity": null}
{"question": "has any Doctor Who actor won a Golden Globe award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christopher Eccleston won one in", "extracted_entity": null}
{"question": "would an SSRI be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. SSRIs are recommended for treatment of", "extracted_entity": null}
{"question": "would Zoloft be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Antidepressants are not recommended for", "extracted_entity": null}
{"question": "has any Duke University graduate married a Grammy winner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at least one.", "extracted_entity": null}
{"question": "do pharmacy employees need to ask for customer ID every time?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, if the pharmacy employee has verified", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. John Huston, director of The", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's college?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you'd", "extracted_entity": null}
{"question": "does the Torah have more books than Emily Bronte wrote?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Torah has more books than Em", "extracted_entity": null}
{"question": "in Mexico, is Tuesday the 13th is considered more unlucky than Friday the 13th? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "does the Torah have more books than the Hunger Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. There are 24 books in the", "extracted_entity": null}
{"question": "does the Torah have more books than the Katniss Everdeen series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has 24.", "extracted_entity": null}
{"question": "lil Wayne similar real name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Drake", "extracted_entity": null}
{"question": "lil Wayne similar name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kanye West", "extracted_entity": null}
{"question": "lil Wayne needs over 4 times Grammy awards to equal rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eminem", "extracted_entity": null}
{"question": "lil Wayne needs over 4 times Grammy awards to equal with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Wayne Newton", "extracted_entity": null}
{"question": "lil Wayne double Grammy awards trails rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jay-Z", "extracted_entity": null}
{"question": "does Lil Wayne need more Grammy awards to match rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, not really", "extracted_entity": null}
{"question": "do Lil Wayne's Grammy awards trail rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has 11 Grammys", "extracted_entity": null}
{"question": "do Lil Wayne's Grammy awards lag behind rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by much. Lil Wayne has", "extracted_entity": null}
{"question": "did the author of \"A Doll's House\" have a mentally disabled son?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but his son died at age 2", "extracted_entity": null}
{"question": "was Charles Manson's body count surpassed by any Marilyn Manson band member name origin killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the body count was surpassed by", "extracted_entity": null}
{"question": "can a bumblebee suffer from spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it\u2019s a female", "extracted_entity": null}
{"question": "can a bumblebee get spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's called a bee", "extracted_entity": null}
{"question": "can a bumblebee get a blood clot?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it's in a", "extracted_entity": null}
{"question": "is the Numerologist featured on the podcast Oh No Ross and Carrie well known?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He's a fraud.", "extracted_entity": null}
{"question": "would someone opening a brewery need to get barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they wanted to make be", "extracted_entity": null}
{"question": "did the Maori people build kayaks in the 1300's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did. They were very skilled", "extracted_entity": null}
{"question": "spain native spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is necessary to add an \"n", "extracted_entity": null}
{"question": "did the Maori people reach New Zealand by kayak?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "spanish spelling of September require adding a con", "extracted_entity": null}
{"question": "does Spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the letter \"r\" is added to", "extracted_entity": null}
{"question": "do you need to add a consonant to September in order to get the Spanish spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you need to add the letter \u201cr", "extracted_entity": null}
{"question": "did the Inuit fish trout from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did the Inuit hunt coyotes from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "\"The story of Joseph and his brothers is the", "extracted_entity": null}
{"question": "were any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the story of Jonah was adapted to", "extracted_entity": null}
{"question": "do Islam and Christianity have at least five prophets in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are five prophets who are mentioned", "extracted_entity": null}
{"question": "does pi exceed square root of pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. pi = 3.141", "extracted_entity": null}
{"question": "5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1. The Livonian Order had lost the", "extracted_entity": null}
{"question": "is 5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Morean War is 5 years.", "extracted_entity": null}
{"question": "is Pluto origin mythology still to be explored in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not going to answer this question because", "extracted_entity": null}
{"question": "pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Pluto is a mythology that has", "extracted_entity": null}
{"question": "is Pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Pluto is the Roman name for H", "extracted_entity": null}
{"question": "have any plays based on graphic novels won awards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it is one of the most awarded", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see the second film adaptation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The author of \"The Haunting", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see both of the film adaptations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Shirley Jackson died in 1", "extracted_entity": null}
{"question": "if someone is wearing a Jack Skellington jacket, is it likely that they enjoy grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is likely that they enjoy grotes", "extracted_entity": null}
{"question": "would Nightmare Before Christmas fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would Tim Burton fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would", "extracted_entity": null}
{"question": "would Tim Burton fans be likely to embrace grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (They\u2019re already into the gro", "extracted_entity": null}
{"question": "were modern assembly lines inspired by the work of Henry Ford?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Henry Ford was the first to use the", "extracted_entity": null}
{"question": "do restaurant waiting staff in South Korea expect a tip?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but if you insist, they\u2019", "extracted_entity": null}
{"question": "do restaurant waiting staff in South Korea expect a gratuity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, in South Korea, there is no t", "extracted_entity": null}
{"question": "does a wedding designer typically make the floral arrangements?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a wedding designer does not typically make", "extracted_entity": null}
{"question": "was John Kerry less active in the military than John McCain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by a factor of 100", "extracted_entity": null}
{"question": "is J.K. Rowling considered an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is a TERF.", "extracted_entity": null}
{"question": "does the LGBT community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not an ally.", "extracted_entity": null}
{"question": "does the transgender community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the transgender community does not view J", "extracted_entity": null}
{"question": "in teenagers with depression, are SSRI medications as safe as they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is a risk of increased suicidal", "extracted_entity": null}
{"question": "would a teen with depression be at higher risk for adverse effects on a medication like Zoloft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, adolescents and children are more", "extracted_entity": null}
{"question": "have violent crowd control tactics been used against anyone in a sit-in?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The sit-ins were non-vi", "extracted_entity": null}
{"question": "is tobacco a major crop in the Southern United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is grown in the Southern United States", "extracted_entity": null}
{"question": "are there turtles that are dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Turtles are not dangerous to humans. They", "extracted_entity": null}
{"question": "would someone who was good at guitar hero be good at rock band?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But not the other way around.", "extracted_entity": null}
{"question": "do tailors predate the sewing machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, tailors existed before the sewing machine", "extracted_entity": null}
{"question": "would Go be considerably more difficult for someone who was blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, since the board is a visual medium.", "extracted_entity": null}
{"question": "would Go be considerably more difficult if you were blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be very difficult.", "extracted_entity": null}
{"question": "would fauna in a tropical climate be very different from those in a temperate climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In tropical climates, there is more", "extracted_entity": null}
{"question": "does the Pope live in an area close to Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he lives in Vatican City,", "extracted_entity": null}
{"question": "is country ham bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Country ham is high in salt, so it is", "extracted_entity": null}
{"question": "would country ham be a bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A lot of people are under the impression that country", "extracted_entity": null}
{"question": "should someone watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s delicious", "extracted_entity": null}
{"question": "should people watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, country ham has a lot of sodium", "extracted_entity": null}
{"question": "does country ham taste different than city ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, country ham has a more robust flavor", "extracted_entity": null}
{"question": "does Will Ferrell feature in multiple films featuring elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "has Will Ferrell been in multiple movies that feature elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he's been in Elf and", "extracted_entity": null}
{"question": "can anyone with a driver's license work in transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A driver must have a commercial driver'", "extracted_entity": null}
{"question": "can anyone with a driver's license work in bulk transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The only requirement is to have a valid", "extracted_entity": null}
{"question": "would a comparison of a murder scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would make sense.", "extracted_entity": null}
{"question": "would a comparison of a crime scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there is no order.", "extracted_entity": null}
{"question": "at a presentation about PTSD, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she is an example of someone who has", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising considered unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the time of COVID-19", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising socially unacceptable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising frowned upon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a great way to get", "extracted_entity": null}
{"question": "did Victor Hugo's novel about the French Revolution ever become a Broadway show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Les Miserables", "extracted_entity": null}
{"question": "did the Voyages of Christopher Columbus involve coming ashore on the West coast of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he never came ashore on the west", "extracted_entity": null}
{"question": "was George Washington aboard one of the ships during the voyages of Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not born until 17", "extracted_entity": null}
{"question": "was Jamestown the landing place for members of the Voyages of Christopher Columubus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was the landing place for members of", "extracted_entity": null}
{"question": "was Michael Scott still on The Office when Will Ferrell's character came onto the show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Michael Scott was still on The Office when", "extracted_entity": null}
{"question": "should Peter Griffin be familiar with the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes, he should.", "extracted_entity": null}
{"question": "was the death of the CEO of Apple caused by pancreatic cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "were Furbys popular during the Presidency of Bill Clinton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were popular during the presidency of", "extracted_entity": null}
{"question": "do students studying Shakespeare learn about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all of them.", "extracted_entity": null}
{"question": "would a Professor of a Shakespeare class have to teach the students about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Professor of a Shakespeare class would have", "extracted_entity": null}
{"question": "would a Professor of a Shakespeare class have to teach about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not necessarily the tragedy of", "extracted_entity": null}
{"question": "did Kate Winslet and Leonardo Dicaprio star in a tragedy together?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "is Yale University beyond 5000 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Yale University beyond 400 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Yale University beyond 100 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond 10", "extracted_entity": null}
{"question": "is Yale University beyond 30 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale is beyond 30 miles", "extracted_entity": null}
{"question": "is Yale University beyond 20 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, 20.1 miles.", "extracted_entity": null}
{"question": "is Yale University beyond a mile from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is Yale University beyond a quick walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The distance between the two is 1", "extracted_entity": null}
{"question": "did lead singer of The Police ever collaborate with an Arab?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did collaborate with a Jew", "extracted_entity": null}
{"question": "did lead singer of The Police ever collaborate with an Algerian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Sting and Cheb Mami.", "extracted_entity": null}
{"question": "did Shaggy ever collaborate with a Blood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did a song with Capone-", "extracted_entity": null}
{"question": "are there any spirits in the Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are spirits in the Quran", "extracted_entity": null}
{"question": "does the average male foot fail to reach a foot (unit) on a ruler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "is Home Depot a good place to shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is Yale University beyond a walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It\u2019s a", "extracted_entity": null}
{"question": "did Reza Shah's grandfather's grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he was not the only one.", "extracted_entity": null}
{"question": "did Reza Shah's great great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Reza Shah's great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was more of a negot", "extracted_entity": null}
{"question": "did Reza Shah's great great grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a peasant", "extracted_entity": null}
{"question": "did Reza Shah's grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "is anyone that was at the Last Supper important to Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jesus and Judas Iscariot", "extracted_entity": null}
{"question": "are chocolate brownies hypothetically fatal to Harry Potter's Fang?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Fang is a dog, and", "extracted_entity": null}
{"question": "can a chocolate brownie be fatal to a member of Canidae?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it contains xylitol.", "extracted_entity": null}
{"question": "can a chocolate brownie be fatal to a dog?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Theobromine is a stimul", "extracted_entity": null}
{"question": "have protesters ever rallied for a governor to veto a bill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 2009, G", "extracted_entity": null}
{"question": "is the Sun (United Kingdom) published in every United Kingdom country?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not published in Scotland.", "extracted_entity": null}
{"question": "national Transportation Safety Board has more abbreviation letters than Mulder's X-Files agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. NTSB has 15 letters", "extracted_entity": null}
{"question": "do vendors ever claim that Quartz has healing properties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. The answer is here.", "extracted_entity": null}
{"question": "were Republican voters pleased with the outcome of  the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.  Ronald Reagan won.", "extracted_entity": null}
{"question": "did any states bordering California vote for Jimmy Carter in the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. All of the states bordering California voted", "extracted_entity": null}
{"question": "were any of the blue states in the 1980's election on the East Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Massachusetts, Rhode Island, Connecticut,", "extracted_entity": null}
{"question": "were any of the blue states in the 1980's election on the West Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the blue states in the 19", "extracted_entity": null}
{"question": "was Super Bowl 50 devoid of NFL founding teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Super Bowl has been played 5", "extracted_entity": null}
{"question": "was Christopher Columbus a poor example of religious tolerance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a very good example of religious", "extracted_entity": null}
{"question": "does Burning Man effigy share similarities with The Wicker Man movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are both effigies, and", "extracted_entity": null}
{"question": "is saying Melania Trump is from a third world country inaccurate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not inaccurate.", "extracted_entity": null}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same gender Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same sex Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I\u2019m sure he\u2019s", "extracted_entity": null}
{"question": "can you reach the UNICEF headquarters from Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can't", "extracted_entity": null}
{"question": "can you reach the UNICEF headquarters by Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is 2.4 miles away", "extracted_entity": null}
{"question": "has the executive director of UNICEF spent time in New England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she has.", "extracted_entity": null}
{"question": "has the executive director of UNICEF spent time in Massachusetts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she has.", "extracted_entity": null}
{"question": "is the executive director of UNICEF familiar with a bra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only with the help of her husband", "extracted_entity": null}
{"question": "would a bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course. She's a minor.", "extracted_entity": null}
{"question": "did people used to believe bagel seeds were from same drug family as morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they were called \"opium bag", "extracted_entity": null}
{"question": "would a bartender make a cocktail for a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that would be illegal.", "extracted_entity": null}
{"question": "would a bartender accept an order from a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not old enough to drink", "extracted_entity": null}
{"question": "does country ham taste different than spiral ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Country ham is a dry-cured", "extracted_entity": null}
{"question": "would someone with celiac disease eat cookies baked with all purpose flour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO, it\u2019s not safe.", "extracted_entity": null}
{"question": "would a bartender accept an order from a 2nd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they're too young to drink", "extracted_entity": null}
{"question": "would a bartender accept an order from a 3rd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the bartender would ask the", "extracted_entity": null}
{"question": "would a bartender accept an order from a 4th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the bartender has a moral", "extracted_entity": null}
{"question": "would a bartender accept an order from Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is a minor", "extracted_entity": null}
{"question": "would a bartender provide service to Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he was accompanied by a", "extracted_entity": null}
{"question": "would a bartender provide service to Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would", "extracted_entity": null}
{"question": "would a bartender make a White Russian for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Absolutely not.", "extracted_entity": null}
{"question": "would a bartender make a Mojito for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's 22", "extracted_entity": null}
{"question": "would a bartender make a cocktail for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would have to be non-", "extracted_entity": null}
{"question": "has name God Save the Queen been used in things other than royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It has been used in many things other", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Cynthia Powell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. John Lennon was married to Y", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was murdered.", "extracted_entity": null}
{"question": "did John Lennon celebrate a diamond anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a 40th anni", "extracted_entity": null}
{"question": "did John Lennon celebrate a golden anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in this life", "extracted_entity": null}
{"question": "is RoboCop director a Razzie award legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has won four.", "extracted_entity": null}
{"question": "would it be inaccurate to call Eve an orphan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because her parents were still alive.", "extracted_entity": null}
{"question": "was Hulk Hogan a good guy in one organization and bad in another?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a good guy in the", "extracted_entity": null}
{"question": "can you be be denied penance for a deadly sin posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You cannot be denied anything posthumously", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his deadly sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But he was born in 18", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The sin of obesity is not", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a member of the Fre", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a Republican.", "extracted_entity": null}
{"question": "would Jack the Ripper be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would be denied penance.", "extracted_entity": null}
{"question": "would Jack the Ripper be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not be denied penance post", "extracted_entity": null}
{"question": "would Jeffrey Dahmer be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because his penance was not posthum", "extracted_entity": null}
{"question": "when Elizabeth Glaser died was penance impossible for Jeffrey Dahmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was already dead.", "extracted_entity": null}
{"question": "penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was dead.", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was still alive.", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer in year Layne Staley died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was in prison.", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer in last month of 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. God is the only one who knows.", "extracted_entity": null}
{"question": "was penance out of reach for Jeffrey Dahmer in December 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not out of reach. He", "extracted_entity": null}
{"question": "have any of Kurt Cobain's bandmates had more success than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they're all broke and homeless", "extracted_entity": null}
{"question": "will Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they will.", "extracted_entity": null}
{"question": "do Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They do.", "extracted_entity": null}
{"question": "do Atlantic salmon hang out in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are in the lake all summer and", "extracted_entity": null}
{"question": "are Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. Atlantic salmon are", "extracted_entity": null}
{"question": "can you spot Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are in there, and they are", "extracted_entity": null}
{"question": "do Atlantic salmon ever swim near Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Atlantic salmon are often seen in the", "extracted_entity": null}
{"question": "is Summer Triangle band named after grizzly bear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Summer Triangle is named after the three", "extracted_entity": null}
{"question": "is Ginger something that a naturopath would recommend for a pregnant woman? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ginger has been used for thousands of years to", "extracted_entity": null}
{"question": "is Olivia Newton John's 1980 film named after summer home of Mongols?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Xanadu", "extracted_entity": null}
{"question": "have there been any blood eagle killings in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there have been two.", "extracted_entity": null}
{"question": "have there been any crucifixions in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the most recent was in 1", "extracted_entity": null}
{"question": "has League of Legends esports players surpassed DOTA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": null}
{"question": "is cancer treatment a preventative for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is a treatment for cancer.", "extracted_entity": null}
{"question": "did Popeye eat all of Bugs Bunny's food source?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "are all colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only red and blue", "extracted_entity": null}
{"question": "do majority of Tibetan people in their ancestral homeland believe in bodhisattvas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They believe in bodhisattvas.", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion be misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no.", "extracted_entity": null}
{"question": "would a member of the Communist Party of the Soviet Union be likely to have read Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Marx was a founder of communism", "extracted_entity": null}
{"question": "is electrolysis possible on the human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to perform electrolys", "extracted_entity": null}
{"question": "does Facebook take steps to limit fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Facebook is committed to fighting fake news.", "extracted_entity": null}
{"question": "did Confederate States Army have similar fashion sense to West Point cadets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did. Confederate States Army had", "extracted_entity": null}
{"question": "is Fake News currently uncensored on major social media sites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "would someone feel safer from fake news on facebook than on the deep web?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The deep web is not as scary as people", "extracted_entity": null}
{"question": "is the deep web somewhere that you could encounter unchecked fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is no way of regulating the", "extracted_entity": null}
{"question": "does Ahura Mazda live above Abaddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Abaddon is the pit of hell", "extracted_entity": null}
{"question": "can Boards of Canada perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "can MGMT perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not on the current tour.", "extracted_entity": null}
{"question": "can Destiny's Child perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Destiny's Child can perform a", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch harm hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can harm your back.", "extracted_entity": null}
{"question": "can starch be deadly to hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they eat a lot of it.", "extracted_entity": null}
{"question": "hyperglycemics should avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Starch is a complex carboh", "extracted_entity": null}
{"question": "should hyperglycemics avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we should not avoid starch. We", "extracted_entity": null}
{"question": "did Alan Rickman's final film outearn his first?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A Little Chaos (201", "extracted_entity": null}
{"question": "was Alan Rickman's final film a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was. It made over $1", "extracted_entity": null}
{"question": "did Alan Rickman outearn his debut movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He earned a lot more than his debut movie.", "extracted_entity": null}
{"question": "did Alan Rickman ever surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Die Hard (1988)", "extracted_entity": null}
{"question": "did an Alan Rickman film surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only by $10 million.", "extracted_entity": null}
{"question": "was Alan Rickman's first movie a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was a box office success.", "extracted_entity": null}
{"question": "has Simo Hayha killed any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has killed 505 members", "extracted_entity": null}
{"question": "did Simo Hayha kill any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he killed 505 members of", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's children followed similar profession?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, her children have followed other professions.", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's brats followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NOPE, not a single one.", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's kids followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, not one of them has been a", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They are all lawyers.", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps career wise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope. Not a one.", "extracted_entity": null}
{"question": "was 2014 governor of New Jersey fattest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chris Christie is the fattest", "extracted_entity": null}
{"question": "was 2014 governor of New Jersey heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a very large man, but", "extracted_entity": null}
{"question": "did any belligerent in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Portugal remained neutral throughout the war.", "extracted_entity": null}
{"question": "would clown be a bad occupational fit for Ebenezer Scrooge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has no sense of humour", "extracted_entity": null}
{"question": "do jumping spiders hunt to survive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are carnivorous.", "extracted_entity": null}
{"question": "are tap shoes required for breakdancing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all.", "extracted_entity": null}
{"question": "does a Drow reach top of a shelf before The Hobbit's hero can?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Drow reaches it first.", "extracted_entity": null}
{"question": "yayoi era Japanese people didn't worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "False. They were very worried about kami", "extracted_entity": null}
{"question": "did Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn't.", "extracted_entity": null}
{"question": "did  Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they had to worry about kami", "extracted_entity": null}
{"question": "did Giovanni Battista Cybo serve his position longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, James A. Garfield served his position", "extracted_entity": null}
{"question": "was Giovanni Battista Cybo in office longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Giovanni Battista Cybo was in office", "extracted_entity": null}
{"question": "is Alexandria Ocasio-Cortez a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "was the first Vice President of the United States a Segregationist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Adams", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors been featured in a Val Kilmer project?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Warwick Davis played Wicket in Return", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Val Kilmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Val Kilmer has never worked with an", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Bowie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bowie worked with Warwick Davis on", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Jennifer Connelly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Warwick Davis.", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Jim Henson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Warwick Davis. He was Wicket", "extracted_entity": null}
{"question": "does Lines on the Antiquity of Microbes lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it lacks the juxtaposition", "extracted_entity": null}
{"question": "does the shortest poem lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it lacks the 5-7", "extracted_entity": null}
{"question": "does the shortest poem lack necessary component of a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not necessary to have a k", "extracted_entity": null}
{"question": "does the shortest poem lack the structure to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the structure is not the most important part", "extracted_entity": null}
{"question": "does the shortest poem written fail the criteria to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is a haiku.", "extracted_entity": null}
{"question": "is a haiku longer than the shortest poem written?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a haiku is longer than the short", "extracted_entity": null}
{"question": "are there Scientologists who know a lot about Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are Scientologists who know a", "extracted_entity": null}
{"question": "did Katy Perry and Taylor Swift appear together in a music video?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the video for Swift\u2019s", "extracted_entity": null}
{"question": "did Katy Perry create more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Katy Perry has created more records than", "extracted_entity": null}
{"question": "did Katy Perry sell more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Katy Perry sold 25 million", "extracted_entity": null}
{"question": "are there people who carry drums at the Macy's Day Parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are called drummers.", "extracted_entity": null}
{"question": "can coffee help with diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, coffee can help with diarrhea", "extracted_entity": null}
{"question": "would the plant Spartina Patens thrive in the Sahara?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not thrive there", "extracted_entity": null}
{"question": "during the neolithic period, did people know how to hunt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The neolithic period is the", "extracted_entity": null}
{"question": "during the neolithic period, was hunting a valued skill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hunting was a valued skill.", "extracted_entity": null}
{"question": "are there accessibility options for blind users who encounter CAPTCHAs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are accessibility options for blind users", "extracted_entity": null}
{"question": "would a student in AP Chemistry have a need for scientific journals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are doing research or writing a", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, very.", "extracted_entity": null}
{"question": "have there been any actors from musicals who have played Noah in a film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Noah was played by John Huston in", "extracted_entity": null}
{"question": "has anyone from a film about Noah been in a movie about magic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Anthony Hopkins, who played Methusel", "extracted_entity": null}
{"question": "are any of the actors from the Harry Potter film series in any films about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the actors from the Harry Pot", "extracted_entity": null}
{"question": "have any of the actors from Harry Potter participated in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Emma Watson", "extracted_entity": null}
{"question": "does Russell Crowe know about the story of Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really.", "extracted_entity": null}
{"question": "has Russell Crowe ever acted in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was Noah in 20", "extracted_entity": null}
{"question": "did Michael Jackson own a lot of bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "did Michael Jackson invest in Bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Michael Jackson was a Bitcoin invest", "extracted_entity": null}
{"question": "are goldfish found on the menu of Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are a delicacy in Asia", "extracted_entity": null}
{"question": "is it recommended to put goldfish in the same tank as beta fish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, goldfish and betta fish are not", "extracted_entity": null}
{"question": "is it recommended for goldfish and beta fish to share a tank?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not recommended to keep goldfish and bet", "extracted_entity": null}
{"question": "did Native American tribes teach Colonizers how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the Colonizers did not have", "extracted_entity": null}
{"question": "are there types of plastic that you can't put in the recycling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 7 types of plastic", "extracted_entity": null}
{"question": "did James Roday play a private investigator on the USA network?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played Shawn Spencer on Psych", "extracted_entity": null}
{"question": "did James Roday play a private investigator on a TV show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played Shawn Spencer on Psych", "extracted_entity": null}
{"question": "is Lance Armstrong known for riding a bicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lance Armstrong is known for r", "extracted_entity": null}
{"question": "if someone is a fan of Dale Earnheardt, would they know what the Daytona 500 was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would think so.", "extracted_entity": null}
{"question": "before it sunk, did the Titanic have too few lifeboats for all of the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "can you go water skiing in the Adirondacks? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Water skiing is very popular in the", "extracted_entity": null}
{"question": "in the movie The Titanic, were there too few many lifeboats for the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were too few lifeboats", "extracted_entity": null}
{"question": "has Saturday Night Live been on air longer than American Idol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. SNL premiered in 19", "extracted_entity": null}
{"question": "was Ashlee Simpson lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "have any performers been caught lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not often.", "extracted_entity": null}
{"question": "can a person with triskaidekophobia succeed at Sudoku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Sudoku is a game that requires", "extracted_entity": null}
{"question": "have any of the Doctor (Doctor Who) actors had famous offspring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Peter Davison (the fifth Doctor)", "extracted_entity": null}
{"question": "did any of the amazons on Xena: Warrior Princess star in a Fox TV series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lucy Lawless, who played Xena, also", "extracted_entity": null}
{"question": "would Library of Alexandria hypothetically need less shelf pace than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would need more shelf space.", "extracted_entity": null}
{"question": "was Library of Alexandria commissioned by a Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Ptolemy I Soter", "extracted_entity": null}
{"question": "is it impossible for Siddhartha Gautama to become a Bodhisattva?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he already was a Bodhisattva", "extracted_entity": null}
{"question": "did a mystery ever happen at Tower of London?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1066, William", "extracted_entity": null}
{"question": "is Tower of London relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Tower of London was the most famous", "extracted_entity": null}
{"question": "would Tower of London be relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Unsolved Mysteries", "extracted_entity": null}
{"question": "would Tower of London be relevant to Robert Stack's longest running show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be relevant to Robert Stack's longest", "extracted_entity": null}
{"question": "george Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the hornet is a true hornet", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a wasp.", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life zoology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hornets are a type of was", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life taxonomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the hornet is actually a wasp", "extracted_entity": null}
{"question": "in animal world is George Trendle's hornet a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a hornet.", "extracted_entity": null}
{"question": "did Barack Obama and Osama Bin Laden go to school together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Barack Obama and Osama Bin", "extracted_entity": null}
{"question": "if someone was hungry for a sandwich, would they be likely to go to taco bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because taco bell doesn\u2019t sell", "extracted_entity": null}
{"question": "can Aloe plants be mailed within the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aloe plants can be mailed within", "extracted_entity": null}
{"question": "would Aloe thrive more in a dry climate than a swampy climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Aloe thrives more in a dry climate", "extracted_entity": null}
{"question": "would Aloe grow better in a drier area than a marshy area?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Aloe grows better in a dry area than in", "extracted_entity": null}
{"question": "did Mark-Paul Gosselaar ever play the role of a student?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played the role of Zack Morris", "extracted_entity": null}
{"question": "would Drew Pinsky be someone to talk to about mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a psychiatrist", "extracted_entity": null}
{"question": "is Drew Pinsky qualified to help people with mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s a doctor of medicine,", "extracted_entity": null}
{"question": "is Dr. Drew Pinsky qualified to treat mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a medical doctor.", "extracted_entity": null}
{"question": "would Steve Martin turn down a dish of shrimp risotto? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "did any of the founding fathers get frequent migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John Adams, the second president of the United States", "extracted_entity": null}
{"question": "do any of the actors in Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I do. I\u2019ve had them", "extracted_entity": null}
{"question": "does anyone from the cast of Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I do.", "extracted_entity": null}
{"question": "are there any famous singers with migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if there are any famous", "extracted_entity": null}
{"question": "would a mammogram at a new doctor's office be considered preventive healthcare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be considered diagnostic.", "extracted_entity": null}
{"question": "is it inappropriate to open a closed casket at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you are not a family member or close friend", "extracted_entity": null}
{"question": "are there live performances at celebrity funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. For example, at the funeral of", "extracted_entity": null}
{"question": "were there celebrities performing at the funeral of Aretha Franklin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were many celebrities performing at", "extracted_entity": null}
{"question": "was Aretha Franklin's funeral attended by celebrity guests?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the funeral of the Queen of Soul", "extracted_entity": null}
{"question": "would a Heath Ledger fan be interested in seeing The Dark Knight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A Heath Ledger fan would be", "extracted_entity": null}
{"question": "is The Dark Knight associated with any violence in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It has been reported that James Holmes", "extracted_entity": null}
{"question": "do Northern Fur Seals encounter penguins naturally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they live in the North Pacific, not", "extracted_entity": null}
{"question": "would it be unusual to see a Northern Fur Seal in the south pole?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Northern Fur Seal lives in the", "extracted_entity": null}
{"question": "are there country artists who also integrate rapping into their work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (Lil Nas X, Florida Georgia", "extracted_entity": null}
{"question": "did any actors from a musical play in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there was a musical actress in a James", "extracted_entity": null}
{"question": "did any actors from Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pierce Brosnan (James Bond in The", "extracted_entity": null}
{"question": "did anyone from the film Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pierce Brosnan", "extracted_entity": null}
{"question": "did anyone from the movie Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pierce Brosnan", "extracted_entity": null}
{"question": "has there been a song by Billie Eilish that has been used in a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there has not been a song by Bill", "extracted_entity": null}
{"question": "is it normal for prisoners to encounter situations that inspire disgust in prison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal.", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Russia to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is only 2000", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Arkansas to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would have to travel over 5", "extracted_entity": null}
{"question": "do cactus plants thrive more in dry environments than wet ones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they have adapted to the dry desert", "extracted_entity": null}
{"question": "do cactus plants require less water than dafodills? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "are there varieties of cactus that are unsafe to touch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The short answer is \u201cyes.\u201d The longer answer", "extracted_entity": null}
{"question": "have there been suicides to protest war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. See the wiki entry for Thich", "extracted_entity": null}
{"question": "would it be normal for a teacher to tell kindergartners about the golden rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be normal for a teacher to", "extracted_entity": null}
{"question": "does Halle Berry eat crustaceans often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (she's a pescatarian", "extracted_entity": null}
{"question": "is Halle Berry prohibited from eating crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Halle Berry is prohibited from e", "extracted_entity": null}
{"question": "is some public art illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Graffiti is the most common example", "extracted_entity": null}
{"question": "is Harry Styles currently knighted in England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "do Razor brand kick scooters have gears?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "is Britney Spears receiving treatment for her bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Britney Spears is not receiving treatment", "extracted_entity": null}
{"question": "would a prosthetic limb work as a replacement for a human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be difficult to get it", "extracted_entity": null}
{"question": "was Germany already established during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was.", "extracted_entity": null}
{"question": "was Hugh Jackman alive during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (He was born in 19", "extracted_entity": null}
{"question": "was the movie Les Miserables filmed during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the movie Les Miserables was fil", "extracted_entity": null}
{"question": "are deep sea fishermen considered experts in oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Jane Goodall be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is an expert in chimpanze", "extracted_entity": null}
{"question": "would Jane Austen be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jane Austen was not an expert in", "extracted_entity": null}
{"question": "has Iggy Pop been in a giallo film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was in Zombie.", "extracted_entity": null}
{"question": "has Iggy Pop worked with a Giallo director?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he worked with Dario Argento in", "extracted_entity": null}
{"question": "do Coen brothers take up more car space than Scott directing brothers in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Coen brothers take up less car space", "extracted_entity": null}
{"question": "are there more Coen brothers alive in 2020 than Scott directing brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 2 Coen brothers alive", "extracted_entity": null}
{"question": "did Robin Williams have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The first symptom was the suicide", "extracted_entity": null}
{"question": "did Michael Jackson suffer from a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "did Christoper Reeves have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He had a stroke in 19", "extracted_entity": null}
{"question": "is John Cazale's 1970s partner yet to debut in a Coen brothers movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Meryl Streep", "extracted_entity": null}
{"question": "is the Quran regarded as a holy text by Jewish people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Quran is not regarded as", "extracted_entity": null}
{"question": "do practicing Presbyterians read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they read the Bible.", "extracted_entity": null}
{"question": "do practicing Catholics read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, of course not", "extracted_entity": null}
{"question": "did Kangaroo Court originate in High Court of Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it originated in the U.S", "extracted_entity": null}
{"question": "did mongoose come from later geologic period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "nATO excludes double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the flag of the Republic of the Congo", "extracted_entity": null}
{"question": "would NATO exclude double triangle flag country to a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, NATO would not exclude double triangle flag", "extracted_entity": null}
{"question": "is double triangle flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is double-pennon flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is absent from the NATO", "extracted_entity": null}
{"question": "is Saugat Malla's home delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Saugat Malla's home", "extracted_entity": null}
{"question": "is Saugat Malla's home country delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the home country delegate is excluded from", "extracted_entity": null}
{"question": "is Saugat Malla's home country delegate barred from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is barred from NATO members", "extracted_entity": null}
{"question": "are squid considered invertebrates? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, squid are invertebrates.", "extracted_entity": null}
{"question": "are squid found in salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Squid are found in salt water", "extracted_entity": null}
{"question": "are there raw preparations of squid for eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are raw preparations of squid", "extracted_entity": null}
{"question": "is the white part of the citrus fruit less sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It has a bitter taste", "extracted_entity": null}
{"question": "would most people discard the outside of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but some might eat it.", "extracted_entity": null}
{"question": "is citrus good for the body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, citrus fruits are good for the", "extracted_entity": null}
{"question": "is citrus useful in promoting health?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Citrus fruits are rich in vitamin C", "extracted_entity": null}
{"question": "is calling creator of Gorillaz a one hit wonder ludicrous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes it is.", "extracted_entity": null}
{"question": "are house cats a potential source of danger for chipmunks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Cats are predators. Chip", "extracted_entity": null}
{"question": "is Alexander Klevan's birthplace safe from Zika virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's safe.", "extracted_entity": null}
{"question": "did same villain kill Batman and Superman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Doomsday.", "extracted_entity": null}
{"question": "is Cerebrospinal fluid contents totally different than Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, CSF is a clear liquid that flows", "extracted_entity": null}
{"question": "is Cerebrospinal fluid contents similar to Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the cerebrospinal fluid contains", "extracted_entity": null}
{"question": "are there no shared ingredients between Cerebrospinal fluid and Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are no shared ingredients.", "extracted_entity": null}
{"question": "did Billie Eilish perform a song for a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Billie Eilish did not perform", "extracted_entity": null}
{"question": "are there paintings of Salvador Dali's on both coasts of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are Dali paintings in New", "extracted_entity": null}
{"question": "can you see Salvador Dali's work in multiple cities of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can see Dali's work", "extracted_entity": null}
{"question": "can you see Salvador Dali's work in both New York and Georgia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can see Salvador Dali's", "extracted_entity": null}
{"question": "does the Bronx Zoo have an exhibit of the Dodo bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Dodo bird is extinct.", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in jail.", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so by their guards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "did Richard III miss out on Parcheesi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. But he was good at chess.", "extracted_entity": null}
{"question": "sunday name origin people enemy of Christians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "sunday is the name of the sun god", "extracted_entity": null}
{"question": "sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Canon 915 says those", "extracted_entity": null}
{"question": "will Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that is not our intent. We are", "extracted_entity": null}
{"question": "would Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not aware of their sin", "extracted_entity": null}
{"question": "would Sunday name origin disturb Serena Williams's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she's not religious.", "extracted_entity": null}
{"question": "would Sunday name origin disturb Prince's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Prince is a Jehovah's", "extracted_entity": null}
{"question": "would Sunday name origin disturb a Jehovah Witness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the Witness, but most likely", "extracted_entity": null}
{"question": "would a Jehovah's Witness denounce name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Bible tells us that God created the", "extracted_entity": null}
{"question": "would a Jehovah's Witness reject name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jehovah's Witnesses", "extracted_entity": null}
{"question": "would a Jehovah's Witness be upset by name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they do not follow the old test", "extracted_entity": null}
{"question": "are all bicycles able to go off road?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all bicycles are able to", "extracted_entity": null}
{"question": "are two names in Albany, Georgia common as country and city names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Albany is a city in Georgia and", "extracted_entity": null}
{"question": "does Camp Rock feature a lot of smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn't.", "extracted_entity": null}
{"question": "is the Song of Roland a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is an epic.", "extracted_entity": null}
{"question": "can the Song of Roland be considered a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the same way as the", "extracted_entity": null}
{"question": "is KFC more global than Roy Rogers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Roy Rogers is more global.", "extracted_entity": null}
{"question": "would a Dodo tower over Pauline Musters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Pauline was 1.57", "extracted_entity": null}
{"question": "would a Dodo be above Pauline Musters's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A dodo would be above Pauline", "extracted_entity": null}
{"question": "would a Dodo be above Chandra Bahadur Dangi's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he's 1 foot 9", "extracted_entity": null}
{"question": "would a Dodo tower over Chandra Bahadur Dangi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Chandra Bahadur Dangi", "extracted_entity": null}
{"question": "can you play a Nintendo game in your car CD player?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the CD player only plays CDs.", "extracted_entity": null}
{"question": "are there any theme park attractions featuring King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the only one currently operating is the", "extracted_entity": null}
{"question": "is there a ride at Universal Studios Orlando about King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is.", "extracted_entity": null}
{"question": "were there fifty English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were only seven.", "extracted_entity": null}
{"question": "were there at least fifty different English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There were fifty-two.", "extracted_entity": null}
{"question": "did Pre-Raphaelites have a profound influence on Claude Monet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "was Claude Monet influenced by Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Monet was influenced by the Barbizon", "extracted_entity": null}
{"question": "was Claude Monet a Pre-Raphaelite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was an Impressionist.", "extracted_entity": null}
{"question": "would Claude Monet value art style of the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not, because he valued", "extracted_entity": null}
{"question": "did Claude Monet have a similar art style to the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Monet\u2019s style is more Im", "extracted_entity": null}
{"question": "while on the voice, did Adam Levine quit Maroon 5?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Adam Levine is still the front man", "extracted_entity": null}
{"question": "would someone with a back fracture go to a chiropractic center for treatment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would go to a hospital", "extracted_entity": null}
{"question": "should cinnamon be eaten straight from the container?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it should.", "extracted_entity": null}
{"question": "does Microsoft Excel make slideshow presentations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "is In God We Trust part of the pledge of allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the pledge of allegiance is \u201c", "extracted_entity": null}
{"question": "did Ariana Grande do a duet of Give it Up with Victoria Justice?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ariana Grande and Victoria Justice did a", "extracted_entity": null}
{"question": "was Ariana Grande's hair in Victorious natural?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not. Ariana's", "extracted_entity": null}
{"question": "did Ariana Grande get popular on show created by Disney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Victorious.", "extracted_entity": null}
{"question": "did Ariana Grande get popular on a Disney Channel Original Series?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She played the character Cat Valentine on", "extracted_entity": null}
{"question": "while on Broadway, did Ariana Grande audition for The Spongebob Musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes! She auditioned for Spongeb", "extracted_entity": null}
{"question": "is Chris Hemsworth qualified to perform surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "in WW2, were there people who did surgery on the battlefield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the people who did surgery on the", "extracted_entity": null}
{"question": "can a Sphynx cat make wool?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Sphynx cats do not", "extracted_entity": null}
{"question": "in New York, is snowboarding a popular summer activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, snowboarding is not a popular summer", "extracted_entity": null}
{"question": "has an appendix been successfully transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The first successful transplant was in", "extracted_entity": null}
{"question": "do most nuclear families in America have Bengal Cats?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because most Americans don't have $", "extracted_entity": null}
{"question": "can lacto fermentation preserve cucumbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can cucumbers be preserved for longer storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Cucumbers can be preserved in", "extracted_entity": null}
{"question": "do you pass through Wyoming following Route 66?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The route is 40 miles long", "extracted_entity": null}
{"question": "is building design the responsibility of a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that is the responsibility of the architect.", "extracted_entity": null}
{"question": "are turtles vulnerable to Raccoons? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, turtles are vulnerable to racc", "extracted_entity": null}
{"question": "could you see turtles at the Boston Aquarium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we saw turtles.", "extracted_entity": null}
{"question": "did Ada Lovelace know the python coding language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "would a person with arachnophobia fear spiders? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would.", "extracted_entity": null}
{"question": "were the family of Donald Shea angry with Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "would a mastectomy be a treatment for colorectal cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "do most gynecologists have to see the firsthand health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think they do. I know that", "extracted_entity": null}
{"question": "would a home garden be growing cucumbers during the winter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn\u2019t.", "extracted_entity": null}
{"question": "can you order pancakes at Dennys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can order pancakes at D", "extracted_entity": null}
{"question": "are Zebra common sights in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "does Greek goddess Venus was named after have a Roman equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Venus was named after the Greek goddess A", "extracted_entity": null}
{"question": "are bench trials performed before a jury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are performed before a judge.", "extracted_entity": null}
{"question": "would the Temple of Kom Ombo feature Egyptian hieroglyphs? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Temple of Kom Ombo features hier", "extracted_entity": null}
{"question": "as of 2020 does John Key have more PM's succeed than precede him?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. John Key is the 38th", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah's and Abraham combined?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Methuselah outlived", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah's lifetime over 5 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Methuselah outlived", "extracted_entity": null}
{"question": "did Charlemagne's father fight in the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and he was a major figure in the", "extracted_entity": null}
{"question": "did Charlemagne's father win the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "has J.K. Rowling written mysteries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she has not", "extracted_entity": null}
{"question": "do any of Powerpuff Girls share name with character in Princess Bride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Buttercup", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of reptile?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a type of salam", "extracted_entity": null}
{"question": "would adherents to Ahimsa be appalled at Paul the Apostle's cause of death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "was Florence Nightingale's death a better example of Ahimsa than Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Paul the Apostle was not a Christian", "extracted_entity": null}
{"question": "would people prefer Florence Nightingale's cause of death to Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "was Paul the Apostle's death more violent than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Paul was beheaded. Florence died", "extracted_entity": null}
{"question": "was Paul the Apostle's death crueler than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paul was beheaded and Nightingale died", "extracted_entity": null}
{"question": "was Paul the Apostle's death more eventful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paul was beheaded.", "extracted_entity": null}
{"question": "was Paul the Apostle's death more painful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Paul was crucified upside", "extracted_entity": null}
{"question": "is number of US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's 3", "extracted_entity": null}
{"question": "does the band Marilyn Manson get their entire name from Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the band is named after the actress Mar", "extracted_entity": null}
{"question": "did Pre-Colonial America have Ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they had ginger.", "extracted_entity": null}
{"question": "are vanity plates important to any of Stephen King's children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they are important to the King family", "extracted_entity": null}
{"question": "are any of Stephen King's children familiar with vanity plates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they're not really that familiar", "extracted_entity": null}
{"question": "have any of Stephen King's offspring embraced their father's genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Joe Hill.", "extracted_entity": null}
{"question": "have any of Stephen King's offspring followed in his footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Owen King has written a couple of books,", "extracted_entity": null}
{"question": "did members of singer Prince's Christian sect celebrate Easter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were Jehovah's Wit", "extracted_entity": null}
{"question": "do more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by much.", "extracted_entity": null}
{"question": "will Gremlins tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Gremlins will have 3 films", "extracted_entity": null}
{"question": "is Gremlins set to tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Gremlins 3 is in the works,", "extracted_entity": null}
{"question": "are Gremlins sequels less prevalent than Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are only two Gremlins mov", "extracted_entity": null}
{"question": "are there more Matrix sequels than Gremlins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are two Matrix sequels, and", "extracted_entity": null}
{"question": "is it possible for a mortal to escape the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he/she is not", "extracted_entity": null}
{"question": "would Alexander Graham Bell have supported Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was deaf and would have been", "extracted_entity": null}
{"question": "can Halle Berry eat crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because she is allergic to shell", "extracted_entity": null}
{"question": "can letters in first line of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can spell \"ewe\"", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of Henry VIII male king heirs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (B's place is 2", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Sofia Vergara has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (B comes after A, and Sof", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons of Bruce Dern?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only by one", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Adam Sandler has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Adam Sandler has 2 sons and", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Bruce Willis has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Bruce Willis has 3 sons.", "extracted_entity": null}
{"question": "does Richard Dawkins donate to Science Partners?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "is Love Song from November 11, 2000 an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it's a song about a", "extracted_entity": null}
{"question": "would Richard Dawkins donate to the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would be a waste of money", "extracted_entity": null}
{"question": "would Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he believed the earth was created by", "extracted_entity": null}
{"question": "is Richard Dawkins joining the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "would Holy Wood's Love Song be an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a good song, and", "extracted_entity": null}
{"question": "is Holy Wood's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Manson's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. but i do it anyway.", "extracted_entity": null}
{"question": "will Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is an atheist, and", "extracted_entity": null}
{"question": "is Narcissus Narcosis an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The song is about the death of a", "extracted_entity": null}
{"question": "is Narcissus Narcosis an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not an ineffective l", "extracted_entity": null}
{"question": "is Target Audience an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a good way to get", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is ineffective because the target", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "did Christopher Columbus receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He got a man-atee from the Car", "extracted_entity": null}
{"question": "can a brain be transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is possible to transplant a brain", "extracted_entity": null}
{"question": "do internet forums require natural gas to operate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, internet forums do not require natural gas", "extracted_entity": null}
{"question": "are wind turbines instrumental in making internet forums available to everyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as much as coal and natural", "extracted_entity": null}
{"question": "was the tenth Amendment written on a mobile phone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was written on a piece of paper", "extracted_entity": null}
{"question": "was the tenth Amendment written on a typewriter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Tenth Amendment was written on a", "extracted_entity": null}
{"question": "are Atlantic Cod a source of income for fishermen in California?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Atlantic Cod are not found in California", "extracted_entity": null}
{"question": "did John Lewis work alongside Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John Lewis worked alongside Martin Luther King Jr.", "extracted_entity": null}
{"question": "did John Lewis march with Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "does Bill Clinton have the ability to post in internet forums?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "would Goofy hypothetically enjoy Milk Bone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But he would not be able to eat", "extracted_entity": null}
{"question": "is wheat important to make unleavened bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, wheat is important to make unle", "extracted_entity": null}
{"question": "is wheat essential for bread making?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can make bread with other grains", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was already a famous rapper when", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was already a rapper in high", "extracted_entity": null}
{"question": "did Ludacris' rap career begin before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was in the 11th", "extracted_entity": null}
{"question": "did Ludacris' rap career begin before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he went to prom anyway.", "extracted_entity": null}
{"question": "was Clark Gable handy with a wrench when he was young?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a lousy mechanic", "extracted_entity": null}
{"question": "was Clark Gable familiar with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "was Clark Gable handy with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was handy with a wrench", "extracted_entity": null}
{"question": "is week old chlorine water safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not safe for a newborn baby.", "extracted_entity": null}
{"question": "would Teri Hatcher have most pay stubs from Desperate Housewives in her career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would.", "extracted_entity": null}
{"question": "was Desperate Housewives Teri Hatcher's greatest marathon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Her greatest marathon was Lois &", "extracted_entity": null}
{"question": "was Desperate Housewives Teri Hatcher's longest tenure as actress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was Lois & Clark: The", "extracted_entity": null}
{"question": "did Desperate Housewives provide Teri Hatcher her longest acting paycheck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "are there more games after FInal Fantasy VI than before in its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 10 games after FF", "extracted_entity": null}
{"question": "is silicon better for making wedding rings than bromine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Silicon is a better choice because it is less", "extracted_entity": null}
{"question": "can a comatose person produce art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a very limited art", "extracted_entity": null}
{"question": "do any adherents to Christianity in China have different saints than Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There are no saints in the Orthodox Church", "extracted_entity": null}
{"question": "it it illegal to pump your own gas in Morris County New Jersey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is illegal to pump your own", "extracted_entity": null}
{"question": "has September shifted to later month from the oldest calendar it appeared on?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has.", "extracted_entity": null}
{"question": "are Sram's kin found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is Sram's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is Balthor the Defiled's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are the Trolls.", "extracted_entity": null}
{"question": "is Balthor the Defiled's race represented in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is Balthor the Defiled's race found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not found in World of War", "extracted_entity": null}
{"question": "was surveyor of Norman, Oklahoma a viking?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a scot.", "extracted_entity": null}
{"question": "do Snow White dwarves fill out entire The Hobbit dwarves roster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "are there enough Snow White dwarves to play all dwarves in The Hobbit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. There are 13 dwarves", "extracted_entity": null}
{"question": "does Eli Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he is the president of the United", "extracted_entity": null}
{"question": "does Peyton Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he doesn't.", "extracted_entity": null}
{"question": "is the Hague interested in geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Hague is interested in geography because it", "extracted_entity": null}
{"question": "does the Hague care about geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn't.", "extracted_entity": null}
{"question": "will Wednesday origin religion followers be denied Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not Catholic.", "extracted_entity": null}
{"question": "is Wednesday traditionally a pagan day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is named after Odin.", "extracted_entity": null}
{"question": "can Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is possible that cognition can be", "extracted_entity": null}
{"question": "cab Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many different types of heredit", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the genes that code for the protein", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Impaired cognition is a result", "extracted_entity": null}
{"question": "is impaired Cognition affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It has been shown that there is a", "extracted_entity": null}
{"question": "is Cognition impairment affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There is a strong hereditary component", "extracted_entity": null}
{"question": "is Cognition impairment influenced by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is influenced by hereditary factors", "extracted_entity": null}
{"question": "does citrus grow in the Arctic circle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not.", "extracted_entity": null}
{"question": "can you watch Rick and Morty at the bottom of Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you'll have to watch it", "extracted_entity": null}
{"question": "is Mark Cuban visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can not.", "extracted_entity": null}
{"question": "can I talk to Vishnu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to be really quiet and", "extracted_entity": null}
{"question": "is President Trump xenophobic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "do any countries that practice capitalism still have homelessness problem?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, many do.", "extracted_entity": null}
{"question": "is homelessness still a problem in countries that practice capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a problem in all countries.", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book ever been in same project with Eva Green?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in movie Dorian Gray", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book endured through other media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Picture of Dorian Gray", "extracted_entity": null}
{"question": "can Michael Jordan lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he can buy it.", "extracted_entity": null}
{"question": "can Mark Cuban lease land in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "does human with no gear die capturing giant squid in natural habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the squid is 20", "extracted_entity": null}
{"question": "is Western Honey bee important to US brand Nice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Western Honey Bee is important", "extracted_entity": null}
{"question": "are crustaceans consumed raw by people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in sushi and sashimi", "extracted_entity": null}
{"question": "in New York, is snowboarding a popular July activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Snowboarding is a popular winter activity", "extracted_entity": null}
{"question": "is Freya a composite of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Freya is a composite of Athena", "extracted_entity": null}
{"question": "are there many similarities between Athena and Freya?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, both are goddesses of war and", "extracted_entity": null}
{"question": "did Ivan the Terrible have harshest nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Ivan the Terrible have the worst nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a terrible Grand Prince.", "extracted_entity": null}
{"question": "do placozoa have learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t have brains.", "extracted_entity": null}
{"question": "can an accordion player benefit from eye surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the patient is able to see the", "extracted_entity": null}
{"question": "did Alexander the Great conquer part of Land of Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 332 BCE", "extracted_entity": null}
{"question": "can a minotaur hypothetically break a tibia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a minotaur could break a tib", "extracted_entity": null}
{"question": "does Alex Rodriguez have fewer championship memories than Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "does Michael Jordan have more championship memories than Alex Rodriguez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does", "extracted_entity": null}
{"question": "is John Kerry familiar with cross checking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is familiar with cross checking", "extracted_entity": null}
{"question": "does table tennis make use of prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the table tennis table is 9 feet", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA recommended dailyrecommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, 7 zucchinis would not", "extracted_entity": null}
{"question": "can Mark Cuban lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can lease property in the Northern", "extracted_entity": null}
{"question": "is Pan Satan-like in appearance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "did The King of Rock'n Roll's snack with bananas come from plants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, bananas are fruits from plants.", "extracted_entity": null}
{"question": "did The King of Rock'n Roll love a snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he loved a peanut butter", "extracted_entity": null}
{"question": "was one of the fruits in apple banana bread in The King of Rock'n Roll's favorite snacks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, bananas.", "extracted_entity": null}
{"question": "did the The King of Rock'n Roll love one of the fruits in apple banana bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Banana", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they had a partner", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles table tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they'd be better off playing", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles table tennis game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they played ABBA-", "extracted_entity": null}
{"question": "would it be impossible for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kami Rita is a very good mountaine", "extracted_entity": null}
{"question": "does Jabberwocky have the length to be a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is 14 lines long.", "extracted_entity": null}
{"question": "has one EGOT category eluded Justin Timberlake for nominations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Grammy Awards", "extracted_entity": null}
{"question": "did Benjamin Butler attend a college that has the first name of a UFC welterweight fighter in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he attended Matt Hughes College.", "extracted_entity": null}
{"question": "can a Nissan product beat a Ferrari in a race? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it\u2019s a Nissan", "extracted_entity": null}
{"question": "can a Nissan win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Ferrari is faster.", "extracted_entity": null}
{"question": "does Nissan have any products that can win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "does Nissan have any products that can win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Nissan does not have any products", "extracted_entity": null}
{"question": "did Nissan build any products that win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nissan GT-R won", "extracted_entity": null}
{"question": "was the Land of Israel under the control of a muslim empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Ottoman Empire", "extracted_entity": null}
{"question": "did a Turkish University of North Carolina at Chapel Hill professor win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aziz Sancar won Nobel Prize in", "extracted_entity": null}
{"question": "did a University of North Carolina at Chapel Hill professor from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aziz Sancar won Nobel prize in", "extracted_entity": null}
{"question": "did a University of North Carolina at Chapel Hill biochemist from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aziz Sancar won the Nobel Prize", "extracted_entity": null}
{"question": "did a 6-time NBA champion graduate from the University of North Carolina at Chapel Hill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael Jordan", "extracted_entity": null}
{"question": "did University of North Carolina at Chapel Hill give scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the University of North Carolina did not give", "extracted_entity": null}
{"question": "did University of North Carolina at Chapel Hill offer scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "was Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lewis Morris (1671-", "extracted_entity": null}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was named after Lewis Morris, a", "extracted_entity": null}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was named after Lewis Morris, a", "extracted_entity": null}
{"question": "is there a wide range between smallest and largest asteroid size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The smallest asteroids are about 1", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after being fired from the band?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He went on to form Megadeth", "extracted_entity": null}
{"question": "did founder of Skype win a race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in Estonia.", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to the Allied victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was the only French tower that the", "extracted_entity": null}
{"question": "does soy milk increase chances of heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "soy milk is a good alternative to milk for", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in restaurant in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She can work as a waitress in", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Drew Barrymore can work as a", "extracted_entity": null}
{"question": "would a model be a good choice to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she would be perfect!", "extracted_entity": null}
{"question": "can you redeem more Bing search reward points than Google's equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can redeem more Bing", "extracted_entity": null}
{"question": "can Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can attend Paralympics", "extracted_entity": null}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can play Paralympics", "extracted_entity": null}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Josh Blue can play Paralymp", "extracted_entity": null}
{"question": "could Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Josh Blue can participate in Paral", "extracted_entity": null}
{"question": "could Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh Blue could not attend the Paralymp", "extracted_entity": null}
{"question": "could Josh Blue compete in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could. He could compete in", "extracted_entity": null}
{"question": "could Mohammed Ali attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "could Stephen Hawking play chess? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Stephen Hawking was a genius, but he could", "extracted_entity": null}
{"question": "could Stephen Hawking play computer games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could play computer games.", "extracted_entity": null}
{"question": "could Aretha Franklin vote for a president when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she could not. The 19", "extracted_entity": null}
{"question": "could Aretha Franklin vote in an election when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could vote.", "extracted_entity": null}
{"question": "can you hide the state of Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you lay it flat.", "extracted_entity": null}
{"question": "can you hide Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Iowa is 56,24", "extracted_entity": null}
{"question": "does soy milk cause heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, it's not the soy", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would, because they believe that there", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because there is no hell in Jain", "extracted_entity": null}
{"question": "wold a Jain believe in the Buddhist idea of karma?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jains believe that karma is a", "extracted_entity": null}
{"question": "are Northern European characters portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the witches and Hecate", "extracted_entity": null}
{"question": "are Northern European characters depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "did Macbeth depict Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it depicted Scottish characters.", "extracted_entity": null}
{"question": "did Macbeth feature Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was set in Scotland.", "extracted_entity": null}
{"question": "did a German-Irish actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did an oscar-winning actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 1971, Peter", "extracted_entity": null}
{"question": "did an actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were actors in the play.", "extracted_entity": null}
{"question": "did an Oscar-winning actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He played the title role in 1", "extracted_entity": null}
{"question": "were any of the characters in Macbeth played by an Oscar-winning actor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, three of them.", "extracted_entity": null}
{"question": "was Napoleonic Wars longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Napoleonic Wars was from 18", "extracted_entity": null}
{"question": "did Napoleonic Wars last longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Napoleonic Wars lasted from", "extracted_entity": null}
{"question": "do we need arithmetic to launch rockets into space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, we have computers", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are influenced by the philosophy of Ma", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I would assume they", "extracted_entity": null}
{"question": "do members of the communist party of China come from Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not all of them.", "extracted_entity": null}
{"question": "can Uighurs become members of the communist party of China ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Uighurs cannot become members of the", "extracted_entity": null}
{"question": "are mongolians members of the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mongolians are members of the Chinese Communist", "extracted_entity": null}
{"question": "do days get shorter in Seattle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do days get shorter in Seattle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is September followed by colder months in the Northern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The sun is lower in the sky and", "extracted_entity": null}
{"question": "can I eat Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the morning.", "extracted_entity": null}
{"question": "can I hunt Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are a few places you can h", "extracted_entity": null}
{"question": "can I hunt Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can only hunt Salmon in", "extracted_entity": null}
{"question": "can I catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you will have to go to the", "extracted_entity": null}
{"question": "can you catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can catch Salmon in California in", "extracted_entity": null}
{"question": "can you fish Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you\u2019ll have to go to", "extracted_entity": null}
{"question": "do people speak Germanic languages in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they speak Czech, a Slavic", "extracted_entity": null}
{"question": "is homeostasis associated with thermoregulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, homeostasis is associated with thermor", "extracted_entity": null}
{"question": "can one Ancient Greek letter represent every season of Survivor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "\u03a9", "extracted_entity": null}
{"question": "do traditional calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there are many varieties of drain", "extracted_entity": null}
{"question": "has small children's painting style been profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the small children's painting style is a very", "extracted_entity": null}
{"question": "can small children's painting style be profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "small children's painting style is a very popular", "extracted_entity": null}
{"question": "can a judo expert defeat a kata expert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the judo expert doesn't", "extracted_entity": null}
{"question": "can a judo expert defeat someone that only does kata?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they are allowed to touch", "extracted_entity": null}
{"question": "is Jenny McCarthy in same industry as her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jenny McCarthy is in the", "extracted_entity": null}
{"question": "do all the animals hedgehogs eat have spinal cords?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, some are insects, some are sl", "extracted_entity": null}
{"question": "does number of Hamlet adaptations exceed Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There have been 30 Hamlet", "extracted_entity": null}
{"question": "is Hamlet's number of movie adaptations similar to Romeo and Juliet's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are both very popular.", "extracted_entity": null}
{"question": "is Hamlet one of Shakespeare's most movie adapted plays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hamlet has been adapted to film many", "extracted_entity": null}
{"question": "did Queen Margot keep Moliere's severed head?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "would a Lord Protector hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Lord Protector would be subserv", "extracted_entity": null}
{"question": "could a Tsar hypothetically boss a Lord Protector around?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the Lord Protector is in charge", "extracted_entity": null}
{"question": "could a Tsar hypothetically tell a Lord Protector what to do?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Lord Protector is the Tsar", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a Lord Protector?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Tsar is the same as a", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a Protector of the Realm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are of equal rank.", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a US Vice President?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because a Tsar is the head of", "extracted_entity": null}
{"question": "can SARS-COV-2 infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are not vaccinated", "extracted_entity": null}
{"question": "has Leninist ideology influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not so much as the Lenin", "extracted_entity": null}
{"question": "can United States Secretary of State slap United Kingdom representative without repercussions?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "were Whirling Dervishes in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only when they were whirling", "extracted_entity": null}
{"question": "is Reiki stored in a pill bottle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki is not stored in a pill", "extracted_entity": null}
{"question": "does Reiki require refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki is a continuous energy source,", "extracted_entity": null}
{"question": "does Reiki require frequent refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Reiki is not a product. It is a", "extracted_entity": null}
{"question": "does Reiki require a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Reiki is not a drug.", "extracted_entity": null}
{"question": "can viruses infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they are connected to the", "extracted_entity": null}
{"question": "have politics been influenced by Marxist ideology in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the 19th century,", "extracted_entity": null}
{"question": "are most Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They're American.", "extracted_entity": null}
{"question": "is Tokyo Tower close to city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not.", "extracted_entity": null}
{"question": "is Tokyo Tower in same country as famed city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Tokyo Tower is in Tokyo", "extracted_entity": null}
{"question": "can someone have a long life after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he doesn't die laughing", "extracted_entity": null}
{"question": "is shoe soup relatively harmless?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, shoe soup is relatively harmless", "extracted_entity": null}
{"question": "can a person survive eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because shoe soup is made of sho", "extracted_entity": null}
{"question": "can a person survive after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you will have to be a c", "extracted_entity": null}
{"question": "are all jokes harmless to health?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Jokes that are too much, too", "extracted_entity": null}
{"question": "could someone in Boston watch the same episode of The Tonight Show in two days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would have to wait a week to", "extracted_entity": null}
{"question": "could someone in Houston see the opening of The Tonight Show before the day change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were watching a TV station that", "extracted_entity": null}
{"question": "could someone in Houston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Earth spins.", "extracted_entity": null}
{"question": "could someone in Boston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Tonight Show is broadcast from Los", "extracted_entity": null}
{"question": "is Beauty and the Beast (1991 film) another Disney pilfered plot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's a French fairy tale, written", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically play a full game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the game would be played with", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically play a game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they're all Eddie'", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in half a day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that would be physically impossible.", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in a quarter day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you are on the Century", "extracted_entity": null}
{"question": "is LG Electronics located in a city with an official bird that has a blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the St. Louis Cardinal", "extracted_entity": null}
{"question": "did Holy Land belong to Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was the land of the Cana", "extracted_entity": null}
{"question": "can Holy Land be claimed by Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Adamu's people are not", "extracted_entity": null}
{"question": "did Adamu's people have claim to Holy Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not to the Holy Land.", "extracted_entity": null}
{"question": "did Adamu's people have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Adamu's people had no claim", "extracted_entity": null}
{"question": "did Assyrians have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Holy Land was claimed by the C", "extracted_entity": null}
{"question": "did another group have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Canaanites, and they", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land other than three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. No one had claim to it.", "extracted_entity": null}
{"question": "would US states in 1900 be enough to divide up ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were more than 40 states", "extracted_entity": null}
{"question": "is Armageddon exclusive to the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. Armageddon is", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Canaanites.", "extracted_entity": null}
{"question": "were there worse vote differences than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were worse vote differences. For example", "extracted_entity": null}
{"question": "were there worse defeats than Carter's 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Landslides are not uncommon.", "extracted_entity": null}
{"question": "will sports announcing be possible with Bell's palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bell's palsy is a condition that", "extracted_entity": null}
{"question": "can Bell's palsy sufferers still have productive show business careers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. As evidenced by this list of", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in three minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are a speed reader.", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in half an hour?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are a speed-reader.", "extracted_entity": null}
{"question": "was Florence ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a republic.", "extracted_entity": null}
{"question": "was Flornce ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There was no pope.", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell name of Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The second row of the QWERTY keyboard", "extracted_entity": null}
{"question": "can United States Army Rangers complete a Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Mozambique Drill is a", "extracted_entity": null}
{"question": "can United States Army Rangers perform a successful Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can Kelly Clarkson's singing break glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's a slow process.", "extracted_entity": null}
{"question": "did multiple weekdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the mission lasted 12 days", "extracted_entity": null}
{"question": "did two Wednesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Wednesday, August 4 and", "extracted_entity": null}
{"question": "did two Tuesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The mission started on July 27", "extracted_entity": null}
{"question": "did two Mondays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did Maya Angelou win a Triple Crown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was not a horse.", "extracted_entity": null}
{"question": "are felidae represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the year of the tiger.", "extracted_entity": null}
{"question": "does the Atlantic reach 500 battalions worth of people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. 100% of the people", "extracted_entity": null}
{"question": "can Cheb Mami possibly win a Pulitzer Prize in History?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "did Martin Luther believe in God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is Martin Luther celebrated by many people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a great man who did", "extracted_entity": null}
{"question": "was Martin Luther capable of logical thinking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a scholar of the Bible,", "extracted_entity": null}
{"question": "was Martin Luther susceptible to infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Martin Luther susceptible to death by infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was susceptible to death by", "extracted_entity": null}
{"question": "do people still fear the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is the symbol of Nazi", "extracted_entity": null}
{"question": "can a swastika cause emotional pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "can a swastika cause pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it\u2019s the wrong way up", "extracted_entity": null}
{"question": "is a nerd a common character in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as common as a jock", "extracted_entity": null}
{"question": "did the Great Depression break up families?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Great Depression did break up families.", "extracted_entity": null}
{"question": "is dopamine snorted up the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it can be injected or taken", "extracted_entity": null}
{"question": "is dopamine snorted through the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is administered in an IV", "extracted_entity": null}
{"question": "can you buy Reddit at Barnes & Noble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can buy a book of Red", "extracted_entity": null}
{"question": "could an elephant seal reach the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it could not.", "extracted_entity": null}
{"question": "could an elephant seal dive to the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would die on the way down", "extracted_entity": null}
{"question": "could an elephant seal dive to the bottom of Lake Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the deepest point is 92", "extracted_entity": null}
{"question": "could a nymph tick pass through a ISO 838 hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A nymph tick is approximately the", "extracted_entity": null}
{"question": "did Wilbur Scoville know about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did Wilbur Scoville care about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He did. He created a test to measure the", "extracted_entity": null}
{"question": "did George Washington cross the Potomac in a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he crossed the Delaware in a row", "extracted_entity": null}
{"question": "did Edward of Westminster ever spend time in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was born in Westminster and", "extracted_entity": null}
{"question": "was Edward of Westminster ever in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1469", "extracted_entity": null}
{"question": "is Antarctica ideal for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is always winter", "extracted_entity": null}
{"question": "did it get dark in Tennessee during the Solar eclipse of August 21, 2017?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not for long.", "extracted_entity": null}
{"question": "are polar bears indigenous to the Golan Heights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is it easier for a human to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the Atlantic is more salty than", "extracted_entity": null}
{"question": "is it easier to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the saltier the water, the easier", "extracted_entity": null}
{"question": "is it easier to float in the Dead Sea than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Dead Sea has more salt in it", "extracted_entity": null}
{"question": "is Canada south of the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would John the Baptist be alone at a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he is the only one who was", "extracted_entity": null}
{"question": "could Jamie Lee Curtis buy a package of Marlboro in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She is 63 years old.", "extracted_entity": null}
{"question": "could Jamie Lee Curtis buy a bottle of chardonnay in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she'd have to go to Oregon", "extracted_entity": null}
{"question": "could Jamie Lee Curtis purchase alcohol in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she was 21 years old", "extracted_entity": null}
{"question": "could 2020 Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Battle of Actium took place", "extracted_entity": null}
{"question": "could 2020 Brazilian Navy hypothetically win Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "would a plate of crustaceans be appropriate for Diwali?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a plate of crustaceans", "extracted_entity": null}
{"question": "would an Olmec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Great Pyramid was built", "extracted_entity": null}
{"question": "would an Incan recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is an obvious and simple structure", "extracted_entity": null}
{"question": "would an Aztec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Aztec civilization didn'", "extracted_entity": null}
{"question": "would an Aztec warrior recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Aztecs never met the", "extracted_entity": null}
{"question": "would Mickey Mouse be camouflaged by the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's black and white.", "extracted_entity": null}
{"question": "can you fish Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can fish for Salmon during September", "extracted_entity": null}
{"question": "have the Dukes of Beaufort ever been to Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have.", "extracted_entity": null}
{"question": "do people interested in numismatics care about British royals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They don't.", "extracted_entity": null}
{"question": "do people interested in numismatics care about British royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the coins and medals", "extracted_entity": null}
{"question": "do people interested in numismatics care about British kings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course, because the coins bear", "extracted_entity": null}
{"question": "do people interested in numismatics care about Edward III?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "did Pink Floyd sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they did not", "extracted_entity": null}
{"question": "did Roger Waters sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he sang about the \u201cDark Side", "extracted_entity": null}
{"question": "is Bing useless for anonymous browsing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's useless for anonymous browsing", "extracted_entity": null}
{"question": "is a lacrosse ball blocked by carapace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is blocked by carapace.", "extracted_entity": null}
{"question": "are there bridges people would rather fall from than Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are bridges people would rather fall", "extracted_entity": null}
{"question": "can someone survive a fall from Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is rare.", "extracted_entity": null}
{"question": "is a fall from Golden Gate bridge likely fatal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The Golden Gate Bridge is", "extracted_entity": null}
{"question": "did Popeye eat an unhealthy amount of spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he ate a healthy amount.", "extracted_entity": null}
{"question": "can you catch Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. But you should be prepared", "extracted_entity": null}
{"question": "has Leninism affected politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has affected politics in Bohemia.", "extracted_entity": null}
{"question": "did Pink Floyd play a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's called \"San Tropez", "extracted_entity": null}
{"question": "is Return of the Jedi's Han Solo bad hypothetical United States Air Force candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes he is.", "extracted_entity": null}
{"question": "did Stone Cold Steve Austin outshine Four Horsemen's tag team accomplishments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Horsemen had a great run", "extracted_entity": null}
{"question": "was Stone Cold Steve Austin better tag teamer than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a better tag teamer than", "extracted_entity": null}
{"question": "does AC/DC's success trump Lil Wayne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "do large mammals exhibit human jealousy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, elephants, dogs, and chim", "extracted_entity": null}
{"question": "is human jealousy exhibited in larger mammals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is exhibited in larger mamm", "extracted_entity": null}
{"question": "does the goat zodiac hypothetically make good entertainers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they have to work at it.", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 18th century judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is not the attire of", "extracted_entity": null}
{"question": "is Hilo Hawaii an ideal location for snowboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "could Tata Hexa get the Spice Girls to a nearby venue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they would have to share the", "extracted_entity": null}
{"question": "would Fortuner be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it has more space for the lug", "extracted_entity": null}
{"question": "would Tata Hexa be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "would Tata Hexy be more roomy for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be more spacious.", "extracted_entity": null}
{"question": "can Julia Roberts hang out with BTS?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can.", "extracted_entity": null}
{"question": "can Julia Roberts ride Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but she can\u2019t stand up.", "extracted_entity": null}
{"question": "can Julia Roberts enjoy Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because she's Julia Roberts", "extracted_entity": null}
{"question": " Is The Invisible Man its author's most adapted book to film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by much.", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Pirate's Flight at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he has to wait in line and", "extracted_entity": null}
{"question": "is Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's The Time Machine.", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s too big.", "extracted_entity": null}
{"question": "is H.G. Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's The Time Machine.", "extracted_entity": null}
{"question": "can Justin Timberlake ride the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but he can ride the tilt-", "extracted_entity": null}
{"question": "is H.G. Wells' most prolific book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Invisible Man has been adapted for", "extracted_entity": null}
{"question": "is The Invisible Man H.G. Wells most adapted film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is certainly the most adapted of his novels", "extracted_entity": null}
{"question": "academy Awards most prolific amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Walt Disney won 22 Academy", "extracted_entity": null}
{"question": "academy Awards most celebrated amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Walt Disney won 22 O", "extracted_entity": null}
{"question": "was Walt Disney most celebrated amusement park creator by Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Walt Disney has won 22", "extracted_entity": null}
{"question": "would Shaun King disagree with Immanuel Kant's views?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Shaun King is a philos", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant disagree about race?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would disagree.", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant not see eye to eye about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Kant was an advocate for enlight", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant disagree about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "is disco still influencing music in the 2000s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's in the DNA of modern", "extracted_entity": null}
{"question": "is Bornean Orangutan higher on World Wildlife Fund priority list than Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bornean Orangutan is", "extracted_entity": null}
{"question": "does World Wildlife Fund prioritize Bornean Orangutan over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "is Bornian Orangutan prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it isn\u2019t. Leopard cat", "extracted_entity": null}
{"question": "is Black Rhino prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is, as are all rhinos", "extracted_entity": null}
{"question": "has Stone Cold Steve Austin had more Intercontinental success than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Steve Austin has won the Intercontinental", "extracted_entity": null}
{"question": "is genre that dominated late 1970's music still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think the answer to this question is yes.", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still relevant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, music genre that dominated late 1", "extracted_entity": null}
{"question": "did one of the Beatles have similar taste in women to Eric Clapton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, George Harrison.", "extracted_entity": null}
{"question": "are mollusks important to the history of the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the shellfish murex was", "extracted_entity": null}
{"question": "is Bill Gates more productive than the country of Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Gates is more productive than Domin", "extracted_entity": null}
{"question": "is Bill Gates more productive than Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Bill Gates is more productive than", "extracted_entity": null}
{"question": "did the Bible borrow from ancient myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it did not.", "extracted_entity": null}
{"question": "did ancient myths inspire the Bible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Bible is not inspired by ancient myth", "extracted_entity": null}
{"question": "could a single bitcoin ever buy a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a single bitcoin could never", "extracted_entity": null}
{"question": "is black pepper often combined with a substance that contains chloride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is combined with sodium chlor", "extracted_entity": null}
{"question": "is it best to avoid chocolate with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chocolate is a food that can be hard", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be one of Bacchus's favorite holidays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "has Drew Carey surpassed Doug Davidson at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Drew Carey is a better game", "extracted_entity": null}
{"question": "has Drew Carey surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not yet.", "extracted_entity": null}
{"question": "is music genre that dominated late 70's still alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, disco is still alive.", "extracted_entity": null}
{"question": "is Drew Carey one of the longest Price is Right hosts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is Drew Carey second longest Price is Right Host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bob Barker is the longest Price", "extracted_entity": null}
{"question": "does it snow in the arctic circle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does snow in the Arctic Circle", "extracted_entity": null}
{"question": "has Leninism influenced politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Leninism has influenced politics in Boh", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "has Drew Carey's surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he's doing a good job", "extracted_entity": null}
{"question": "are more tamarinds grown in Honduras than Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "would a tamarind tree have difficulty growing in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would have difficulty growing in Anch", "extracted_entity": null}
{"question": "would a tamarind tree grow poorly in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Tamarind is native to tropical Africa", "extracted_entity": null}
{"question": "would a tamarind tree grow poorly in Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would be too cold.", "extracted_entity": null}
{"question": "is shrimp essential to Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "does Drew Carey's Price is Right tenure surpass Dennis James's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (The answer is 201", "extracted_entity": null}
{"question": "are more tamarinds grown in Honduras than Canada?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Tamarinds grow in Honduras", "extracted_entity": null}
{"question": "would someone in a sauna be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would be likely to experience hyperther", "extracted_entity": null}
{"question": "can a student from Beder understand the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Albanian Declaration of Independ", "extracted_entity": null}
{"question": "can a student from Beder read the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't", "extracted_entity": null}
{"question": "can all Suikoden characters find dates among Final Fantasy VI characters?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Everyone is paired up.", "extracted_entity": null}
{"question": "would a tenth of McCain's electoral votes make John Kerry President in 2004?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "could Amazon defeat Iceland in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because Iceland is not for sale", "extracted_entity": null}
{"question": "did Jerry Seinfeld's mouth help him earn a lot of cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "did Jerry Seinfeld's mouth help him earn a substantial amount of money?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jerry Seinfeld's mouth helped", "extracted_entity": null}
{"question": "did Jerry Seinfeld earn a lot of money using his mouth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Vishnu?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has a close relationship with the", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Brahma?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Marco Rubio has a close relationship with", "extracted_entity": null}
{"question": "would Alan Alda fit in on the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he'd be too old to be", "extracted_entity": null}
{"question": "does Secretariat have a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Secretariat won the Triple Crown", "extracted_entity": null}
{"question": "does Secretariat hold a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's a horse", "extracted_entity": null}
{"question": "will a living thing survive the entirety of Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not possible.", "extracted_entity": null}
{"question": "is the Greek alphabet barred for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Greek alphabet is not barred for use in", "extracted_entity": null}
{"question": "is the Greek alphabet prohibited for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not prohibited.", "extracted_entity": null}
{"question": "is the Greek alphabet prohibited for use in Korean pop music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is prohibited", "extracted_entity": null}
{"question": "can Tony Blair be imprisoned in the Tower of London?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the Queen wishes it.", "extracted_entity": null}
{"question": "do travelers to Casablanca have a hard time finding a BLT?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't have a hard time", "extracted_entity": null}
{"question": "does Jamie Oliver care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn't.", "extracted_entity": null}
{"question": "does Gordon Ramsey know about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is too busy making people cry.", "extracted_entity": null}
{"question": "does Gordon Ramsey care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he cares about the flavor of", "extracted_entity": null}
{"question": "can people in San Antonio cheer on the Memphis Grizzlies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the Spurs are out.", "extracted_entity": null}
{"question": "does Moon Jae-in own multiple species of pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He doesn\u2019t.", "extracted_entity": null}
{"question": "does Moon Jae-in support animal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Moon Jae-in does not support", "extracted_entity": null}
{"question": "does Moon Jae-in own any pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has two dogs.", "extracted_entity": null}
{"question": "do humans pose a threat to Bengal foxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, humans pose a threat to Bengal fo", "extracted_entity": null}
{"question": "do Bengal foxes take care of their children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They are very protective", "extracted_entity": null}
{"question": "did a manta ray murder Steve Irwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was stung by a sting", "extracted_entity": null}
{"question": "does celery contain any essential nutrients?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn\u2019t", "extracted_entity": null}
{"question": "has celery been used for purposes other than food?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Celery has been used for a number of purposes", "extracted_entity": null}
{"question": "did one person cause the price of celery to spike at any time in the past?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not that I am aware of.", "extracted_entity": null}
{"question": "was Bill Clinton President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "can computers play Poker better than humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is a definite yes.", "extracted_entity": null}
{"question": "is it possible to make a living only playing Poker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. If you are a good", "extracted_entity": null}
{"question": "could James Brown's ex-wives play a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you could find a net low enough", "extracted_entity": null}
{"question": "could James Brown's ex-wives have played a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could have.", "extracted_entity": null}
{"question": "could James Brown's ex-wives have held a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could have.", "extracted_entity": null}
{"question": "did slaves play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "can Evander Holyfield consume pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a holy-field.", "extracted_entity": null}
{"question": "can an artisan sell items to someone in another country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you will need to set up a", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau require subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "did Malcolm X play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was one of the leaders of the", "extracted_entity": null}
{"question": "could one Amazon stock ever buy twenty four year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not at the current price.", "extracted_entity": null}
{"question": "can French Toast cause Salmonella poisoning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the eggs are not cooked properly", "extracted_entity": null}
{"question": "were there any pandemics during Moon Jae-in's presidency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there was MERS.", "extracted_entity": null}
{"question": "is the paleo diet healthy for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The paleo diet is a nutrient", "extracted_entity": null}
{"question": "does soy milk increase heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "are Northern Europeans depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "are Northern Europeans portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The witches are described as \"", "extracted_entity": null}
{"question": "can a one armed drummer be elite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you are good enough, you can", "extracted_entity": null}
{"question": "has Leninism influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you think.", "extracted_entity": null}
{"question": "is ABBA's 1970's music genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, ABBA's music is still relevant", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still evolving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Punk rock is still evol", "extracted_entity": null}
{"question": "would Alan Alda fit in on Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not at all.", "extracted_entity": null}
{"question": "does Moon Jae-in own both cats and dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has both cats and dogs.", "extracted_entity": null}
{"question": "was Franjo Tudjman President of Croatia when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": null}
{"question": "is the paleo diet beneficial for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The paleo diet is not beneficial for", "extracted_entity": null}
{"question": "was Al Gore Vice President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (1993-20", "extracted_entity": null}
{"question": "is Noah's Ark more imposing than Batillus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Batillus is more imposing than Noah", "extracted_entity": null}
{"question": "does a horse fly neigh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "could Brooke Shields handle University of Pennsylvania curriculum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think she could.", "extracted_entity": null}
{"question": "would University of Pennsylvania admit actress Brooke Shields?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would not even get an interview.", "extracted_entity": null}
{"question": "did actress Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "did Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she was a C student in high school", "extracted_entity": null}
{"question": "were many young females attracted to Justin Timberlake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many were.", "extracted_entity": null}
{"question": "did Justin Timberlake have many young female fans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a member of a boy band", "extracted_entity": null}
{"question": "would a physician be unlikely to prescribe Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Reiki is a very safe, non", "extracted_entity": null}
{"question": "can Viper Room crowd squeeze into National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's 10 times bigger", "extracted_entity": null}
{"question": "can National Diet building accommodate Viper Room crowd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They\u2019re too busy with the Diet", "extracted_entity": null}
{"question": "would John Muir be unlikely to have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John Muir was a Scottish-American naturalist", "extracted_entity": null}
{"question": "would Alan Alda be comfortable on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he'd be very comfortable.", "extracted_entity": null}
{"question": "would Alan Alda fit in on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, yes.", "extracted_entity": null}
{"question": "would Alan Alda fit in on the set of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "would Alan Alda fit in with the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's too old and too smart", "extracted_entity": null}
{"question": "would John Muir dislike being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he loves nature.", "extracted_entity": null}
{"question": "would John Muir hate being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes, he would.", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen Romeo and Juliet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was dead by the time it was", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen the plays of Shakespeare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Elizabeth I reigned from 15", "extracted_entity": null}
{"question": "could someone have visited the Library of Alexandria and the pyramids on the same vacation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Library of Alexandria was built in", "extracted_entity": null}
{"question": "would Gal Gadot be hired as Victoria Secret model before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Gal Gadot is hotter than", "extracted_entity": null}
{"question": "do middle school students typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Pythagorean Theorem", "extracted_entity": null}
{"question": "does the bengal fox have hair covering it's body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Bengal fox has a coat", "extracted_entity": null}
{"question": "would a librarian use a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would use a computer to look up", "extracted_entity": null}
{"question": "can Linus Torvald's family ride in a Maruti Suzuki Baleno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can a Maruti Suzuki Baleno transport Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Linus Torvald's family", "extracted_entity": null}
{"question": "can Royal Albert Hall hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they're in the", "extracted_entity": null}
{"question": "can Madison Square Garden hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you stack them in the parking", "extracted_entity": null}
{"question": "could Titanic hypothetically hold Spanish-American War combat casualties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be an incredible waste", "extracted_entity": null}
{"question": "could Spanish-American War combat casualties hypothetically fit on Titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with room to spare.", "extracted_entity": null}
{"question": "did Ptolemy use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ptolemy was a Roman citiz", "extracted_entity": null}
{"question": "do camels live in Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the zoo.", "extracted_entity": null}
{"question": "will Victoria's Secret hire Gal Gadot before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they will hire Gomez", "extracted_entity": null}
{"question": "do 10-12 year olds typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pythagoras was a Greek", "extracted_entity": null}
{"question": "does the little penguin share space with Kiwis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The little penguins nest in bur", "extracted_entity": null}
{"question": "does horseradish have hooves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it has roots.", "extracted_entity": null}
{"question": "is paper a product of logging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a product of the pulp", "extracted_entity": null}
{"question": "would Boticelli's mythological subjects fill a gallery before Salvador Dali's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Boticelli painted more mythological", "extracted_entity": null}
{"question": "would ten gallons of seawater hold a six year old in the air on a seesaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a six year old would not be able", "extracted_entity": null}
{"question": "are Genghis Khan's descendants more prolific than those of Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Genghis Khan's 13th", "extracted_entity": null}
{"question": "was an event in the Balkans considered a contributing factor to World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the assassination of Archduke Franz", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field an NHL team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field a full ice hockey team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field a full basketball team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they could find a basketball court that", "extracted_entity": null}
{"question": "was Fraktur likely to have been used in the publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fraktur was used in the publications of Anders Ar", "extracted_entity": null}
{"question": "was Fraktur likely to have been used in publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Anders Arrebo wrote in the", "extracted_entity": null}
{"question": "does the spinal cord play a role in skateboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the spinal cord is part of the", "extracted_entity": null}
{"question": "does the visual cortex play a role in regulating metabolic function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The visual cortex regulates metabol", "extracted_entity": null}
{"question": "does the visual cortex play a role in regulating lung function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it\u2019s the same thing as", "extracted_entity": null}
{"question": "can people see the code behind OpenOffice.org?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can download the code at http://", "extracted_entity": null}
{"question": "does a Jetta require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Jetta doesn't require an", "extracted_entity": null}
{"question": "does a Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a Prius does require a", "extracted_entity": null}
{"question": "does a Toyota Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it is a 200", "extracted_entity": null}
{"question": "could the children of Greek hero Jason hypothetically field a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jason and Medea had two children", "extracted_entity": null}
{"question": "is the artist of the album Slay Belles a drag queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (If you don\u2019t know who", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind The Brig?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and the film is Smile.", "extracted_entity": null}
{"question": "did John Lennon collaborate with the man who made the film Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Beethoven ever compose an EDM song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 1827", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film starring Michael Palin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the swallow played a role in a", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film starring John Cleese?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the swallow did play a role in", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But I\u2019d like to see that", "extracted_entity": null}
{"question": "was John Searle born in a state that is bordered by Wyoming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in Denver, Colorado", "extracted_entity": null}
{"question": "does the Illuminati leave messages in modern society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the media, movies, and", "extracted_entity": null}
{"question": "would King Henry V hypothetically demolish Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Charlemagne would hypothetically demolish", "extracted_entity": null}
{"question": "would King Henry V hypothetically prevail against Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Charlemagne would win.", "extracted_entity": null}
{"question": "would King Henry V hypothetically defeat Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the French would have won.", "extracted_entity": null}
{"question": "would Constantinople repel a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Constantinople would repel a hypothetical invasion", "extracted_entity": null}
{"question": "would Constantinople survive a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Constantinople would survive, but the Empire would", "extracted_entity": null}
{"question": "would Constantinople withstand a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Emperor would have surrendered before", "extracted_entity": null}
{"question": "in the year 1000, did the Eastern Orthodox Church and the Byzantine Empire use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Byzantine Empire was Eastern Orth", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin apply his finisher to the largest mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he would have to apply it in", "extracted_entity": null}
{"question": "is kaffir lime the man ingredient in key krueng?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is the main ingredient in", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam two White-tailed deer simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he's wearing", "extracted_entity": null}
{"question": "can someone from Denmark go to Sweden without their passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are both in the Schengen area", "extracted_entity": null}
{"question": "can a male mule deer be body slammed by Stone Cold Steve Austin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he can't", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam a male mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he can body slam a male", "extracted_entity": null}
{"question": "are most of the products labelled Fair Trade exported from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The vast majority of Fair Trade products are", "extracted_entity": null}
{"question": "was Harry Houdini's wife bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She was a medium.", "extracted_entity": null}
{"question": "was Harry Houdini bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "did Harry Houdini disprove the claims of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t", "extracted_entity": null}
{"question": "is Drew Carey in a sports Hall of Fame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is in the WWE Hall of", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rand has valid eye exam certification in", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rand has a valid eye exam certification", "extracted_entity": null}
{"question": "is Rand Paul prohibited from conducting eye exams?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He's a physician, not", "extracted_entity": null}
{"question": "can a Toyota Hilux transport a human heart from NYC to Raleigh NC?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, it can do so in", "extracted_entity": null}
{"question": "can a Toyota Hilux get to Havana from NYC before sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is driven by a madman", "extracted_entity": null}
{"question": "can a Toyota Hilux get to Havana from NYC in sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can get to Key West in", "extracted_entity": null}
{"question": "is one of the ingredients of photosynthesis found in emeralds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, photosynthesis requires chlorophyll", "extracted_entity": null}
{"question": "can you pour diethyl ether?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a liquid.", "extracted_entity": null}
{"question": "did Larry King fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born in 193", "extracted_entity": null}
{"question": "have any WWE champions outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Shawn Michaels in 1", "extracted_entity": null}
{"question": "has a Neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was named Clarence Thomas", "extracted_entity": null}
{"question": "are infants excluded from joining the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are excluded from joining the", "extracted_entity": null}
{"question": "can a worm suffer a myocardial infarction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they have a heart.", "extracted_entity": null}
{"question": "can every Prime Minister of the United Kingdom since 1970 fill all the US Supreme Court chairs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact the current Prime Minister can fill", "extracted_entity": null}
{"question": "can every Prime Ministers of the United Kingdom since 1970 fill all the US Supreme Court positions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can fill 2/3", "extracted_entity": null}
{"question": "will Justin Bieber replace Mike Pence in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Biebs is a better choice", "extracted_entity": null}
{"question": "will Justin Bieber be Trump's running mate in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if Trump agrees to wear", "extracted_entity": null}
{"question": "can Justin Bieber represent New York in Congress in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is 25 years old", "extracted_entity": null}
{"question": "will Justin Bieber become US President in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will not.", "extracted_entity": null}
{"question": "will Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He\u2019s Canadian.", "extracted_entity": null}
{"question": "can Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Justin Bieber cannot win the 20", "extracted_entity": null}
{"question": "were there highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The interstate highway system was built during", "extracted_entity": null}
{"question": "did cars travel on highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as many as today", "extracted_entity": null}
{"question": "did members of the Silent Generation use highways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "can a rabbi perform any Christian rituals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The only ritual a rabbi can perform is", "extracted_entity": null}
{"question": "can baptism be done by a rabbi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The baptism of a Christian is not a Jewish", "extracted_entity": null}
{"question": "can a Rabbi perform a Christian baptism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not authorized to do so", "extracted_entity": null}
{"question": "can a Rabbi administer any of the Christian Sacraments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Why not?", "extracted_entity": null}
{"question": "were there typewriters in the Empire of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were typewriters in the Empire", "extracted_entity": null}
{"question": "did people listen to The Blues during World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they were so depressed.", "extracted_entity": null}
{"question": "can a person from New Brunswick understand the language of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they speak different languages.", "extracted_entity": null}
{"question": "can a person from New Brunswick understand the speech of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they speak different languages", "extracted_entity": null}
{"question": "can a person from Madagascar order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can order food in their native language", "extracted_entity": null}
{"question": "can a person from Burundi order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they speak English or French", "extracted_entity": null}
{"question": "do lizards breed in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only one species is known to do", "extracted_entity": null}
{"question": "can lizards survive temperatures in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not even in the summer.", "extracted_entity": null}
{"question": "is the Swastika a religious symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Swastika is a symbol of", "extracted_entity": null}
{"question": "did Japanese people win the War of the Bucket?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they were defeated in the Battle of", "extracted_entity": null}
{"question": "can you transport nine pounds of weed in a bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the bag must be empty.", "extracted_entity": null}
{"question": "did the husband of Elizabeth Hamilton work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Alexander Hamilton worked with George Washington, who was a", "extracted_entity": null}
{"question": "did the husband of Alexander Hamilton's granddaughter work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was the Lieutenant of the Navy.", "extracted_entity": null}
{"question": "did Elizabeth Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Lt. William Hunter", "extracted_entity": null}
{"question": "did Elizabeth Margaret Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a lieutenant.", "extracted_entity": null}
{"question": "do Adobe Suite and Life Is Strange video game share similar coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are completely different.", "extracted_entity": null}
{"question": "did Sugar Ray Robinson ever experience pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did, but he had to go", "extracted_entity": null}
{"question": "did the creator of Saturday Night Live ever work with Jack Paar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lorne Michaels worked with Jack", "extracted_entity": null}
{"question": "did the creator of Saturday Night Live ever work with Johnny Carson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the creator of Saturday Night Live", "extracted_entity": null}
{"question": "does the author of Rebecca use Cornwall in another novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in My Cousin Rachel.", "extracted_entity": null}
{"question": "could a participant in the American Civil War read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he might not have understood it.", "extracted_entity": null}
{"question": "could the winner of a Nobel Peace Prize have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he read all the books in his", "extracted_entity": null}
{"question": "could the author of The Bell Jar have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she could have.", "extracted_entity": null}
{"question": "could Saint Augustine have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the play was written in the 1", "extracted_entity": null}
{"question": "can a human safely consume Cannabis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Cannabis is a natural her", "extracted_entity": null}
{"question": "did Metallica ever release a rap song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did, and it's called", "extracted_entity": null}
{"question": "does the name C-SPAN refer to a form of telecommunications?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it refers to the satellite transmission of the", "extracted_entity": null}
{"question": "could someone watch C-Span on cable in 1980?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. C-Span didn\u2019t begin until", "extracted_entity": null}
{"question": "was C-Span on the air in 1987?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. C-Span was not on the air", "extracted_entity": null}
{"question": "did Christopher Hitchens ever appear on C-Span?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He appeared on C-Span\u2019s", "extracted_entity": null}
{"question": "did Brian Lamb ever host a television program on C-SPAN?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "is Battle Hymn of the Republic considered christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's about Jesus", "extracted_entity": null}
{"question": "did Fanny Crosby write Christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she wrote Christian music", "extracted_entity": null}
{"question": "is Christian music played in Churches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, some Churches only play", "extracted_entity": null}
{"question": "did the founders of Encyclop\u00e6dia Britannica enjoy views of the Firth of Forth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The company was founded in Edinburgh in", "extracted_entity": null}
{"question": "was the country featured in All My Life one of the participants in the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "is an economist able to determine why the price of a burrito changes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because an economist knows that prices change", "extracted_entity": null}
{"question": "is an economist able to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, an economist is able to study the", "extracted_entity": null}
{"question": "is an economist qualified to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he is not qualified to study the", "extracted_entity": null}
{"question": "was Manny Pacquiao a boxing champion when Obama was first elected US president?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Pacquiao was a champion in", "extracted_entity": null}
{"question": "did Martin Luther know Latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was a monk and a priest.", "extracted_entity": null}
{"question": "will Jay-Z reach the age of 60 before Kendrick Lamar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. Jay-Z will be", "extracted_entity": null}
{"question": "did Jay-Z know Kanye West in 2012? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jay-Z knew Kanye West", "extracted_entity": null}
{"question": "did Jay-Z know Kanye West in 2011? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were friends.", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they could not.", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with someone from Austria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they spoke different languages.", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever play in a park designed by a poet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Robert Plant did, and so did a", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever play as a child in Leasowes Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he never did.", "extracted_entity": null}
{"question": "did Abraham Lincoln help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He signed the charter for the university", "extracted_entity": null}
{"question": "was Abraham Lincoln responsible for founding the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Abraham Lincoln was responsible for the Morr", "extracted_entity": null}
{"question": "was the current U.S. speaker of the house alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in 194", "extracted_entity": null}
{"question": "does Gavin Rossdale have a large enough net worth to purchase the most expensive Thoroughbred horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Gavin Rossdale has a net", "extracted_entity": null}
{"question": "would nickel boil if placed in the core of Neptune?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "is there a state that touches several of the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michigan", "extracted_entity": null}
{"question": "are any of the Great Lakes entirely in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Lake Michigan is entirely in the US.", "extracted_entity": null}
{"question": "do the Great Lakes have at least 15% of the worlds freshwater fish supply?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Lakes have 15", "extracted_entity": null}
{"question": "are there any fresh water sand dunes in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are freshwater sand dunes in", "extracted_entity": null}
{"question": "has anyone drowned in the Great Lakes in 2019?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "100 people have drowned in", "extracted_entity": null}
{"question": "has anyone ever drowned in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not very often.", "extracted_entity": null}
{"question": "do the Great Lakes have bacteria in the water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the bacteria are killed by", "extracted_entity": null}
{"question": "do people ice fish on the Great Lakes in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not on Lake Superior, because", "extracted_entity": null}
{"question": "do the Great Lakes freeze in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do the Great Lakes have more endangered species than other parts of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not sure.", "extracted_entity": null}
{"question": "do the Great Lakes have any endangered species?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Great Lakes have 34", "extracted_entity": null}
{"question": "is John the Baptist still important to people today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John the Baptist is still important to people today", "extracted_entity": null}
{"question": "is John the Baptist still worshiped to this day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "was The Hobbit written before The Lord of the Rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was written after The Lord of the", "extracted_entity": null}
{"question": "is David Cameron a Tory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is a Tory.", "extracted_entity": null}
{"question": "is Dragon Hall located in Norfolk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Dragon Hall is located in Norfolk.", "extracted_entity": null}
{"question": "is Norfolk predominantly rural?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is largely rural.", "extracted_entity": null}
{"question": "is Norfolk on the eastern coast of England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Norfolk is on the eastern coast of England.", "extracted_entity": null}
{"question": "are Rogue DnD characters suited for playing in cities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact they are the most suited", "extracted_entity": null}
{"question": "are DnD Ranger characters suited for play in wilderness areas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "could George Washington have recorded a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he didn't have a compact", "extracted_entity": null}
{"question": "do Hindus believe Krishna exists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They believe he existed.", "extracted_entity": null}
{"question": "is Lord Hill's column in Shropshire the tallest of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's the tallest monument of", "extracted_entity": null}
{"question": "are there several  towns located in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are 11 towns located in", "extracted_entity": null}
{"question": "were Charles Darwin and A.E. Housman born in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and both are buried there.", "extracted_entity": null}
{"question": "is there a david bowie song with the word year or years in the title?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, \"the year of the diamond dogs", "extracted_entity": null}
{"question": "did the Hopi Indians use a symbol that was similar to the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Hopi Indians used a symbol that", "extracted_entity": null}
