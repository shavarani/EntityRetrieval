{"question": "are Lentil able to withstand a wind storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, lentils are able to withstand", "extracted_entity": null}
{"question": "are Lentil able to withstand frost?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, lentils are a hardy c", "extracted_entity": null}
{"question": "are drain flies able to survive bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it is possible, but I would not", "extracted_entity": null}
{"question": "would an oil miner be interested in the Persian Gulf?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would be too expensive and time", "extracted_entity": null}
{"question": "would an aerodynamic cactus dessicate more quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the water would evaporate faster", "extracted_entity": null}
{"question": "would an aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because cacti have a lot of", "extracted_entity": null}
{"question": "would a more aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because there would be less wind resistance.", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need water more frequently?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. aerodynamics is about how well", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need more water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a more aerodynamic cactus", "extracted_entity": null}
{"question": "do apple farmers benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they can\u2019t make honey", "extracted_entity": null}
{"question": "do apples benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The bees will not be able to", "extracted_entity": null}
{"question": "is Tokyo Tower repainted often enough that someone can do the work seven times during their career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is repainted every five years", "extracted_entity": null}
{"question": "can a worker repaint Tokyo Tower seven times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the worker is an octopus.", "extracted_entity": null}
{"question": "can a worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the worker can repaint Tokyo Tower only", "extracted_entity": null}
{"question": "can the same worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it\u2019s a lot of work", "extracted_entity": null}
{"question": "can the same worker repaint Tokyo Tower seven or more times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the same worker repainted Tokyo Tower", "extracted_entity": null}
{"question": "can cheap cinnamon sticks break a coffee grinder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cheap cinnamon sticks can", "extracted_entity": null}
{"question": "would an aerodynamic cactus get thirsty more often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it would be a cactus", "extracted_entity": null}
{"question": "when en route from Beijing to Paris, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "3000 feet", "extracted_entity": null}
{"question": "when en route from Beijing to London, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It is now in meters.", "extracted_entity": null}
{"question": "when en route from London to Beijing, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Never.", "extracted_entity": null}
{"question": "when en route from London to Beijing, will pilots convert the foot to metric?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they will convert the foot to furl", "extracted_entity": null}
{"question": "does a new pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he/she is flying in the", "extracted_entity": null}
{"question": "does a pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "would a pilot leaving Moscow during the 2018 Fifa World Cup report altitude in feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pilots use feet in the United States", "extracted_entity": null}
{"question": "is growing cucumber with seeds bad for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the cucumber is grown with", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, growing cucumbers is a great", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with a bee phobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because cucumber is a climber", "extracted_entity": null}
{"question": "can someone deathly allergic to bees safely grow cucumber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you have a severe allergy to bees", "extracted_entity": null}
{"question": "would suspension bridges be better able to withstand an earthquake than a beam bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. a suspension bridge would be better able", "extracted_entity": null}
{"question": "does a cotton ball provide nutrition for humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, cotton balls do not provide nutr", "extracted_entity": null}
{"question": "would a classic xbox emulator be popular in the future?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, but it might not be very", "extracted_entity": null}
{"question": "would a classic xbox emulator sell well in 2040?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a classic Xbox emulator would sell well", "extracted_entity": null}
{"question": "would Miley Cyrus sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would.", "extracted_entity": null}
{"question": "would selina gomez sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would.", "extracted_entity": null}
{"question": "would Iggy Pop cover a Justin Bieber song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop would cover a Justin Bie", "extracted_entity": null}
{"question": "would Iggy Pop cover a Sean Mendez song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is a legend.", "extracted_entity": null}
{"question": "would Sean Mendez join Iggy Pop on stage?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and Iggy would be the first", "extracted_entity": null}
{"question": "do Alan Turing and Jeff Bezos have the same alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Princeton University", "extracted_entity": null}
{"question": "would Justin Bieber join Iggy Pop at the Warp Tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop would join Justin Bieber", "extracted_entity": null}
{"question": "would Justin Bieber join Iggy Pop on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop and Justin Bieber are", "extracted_entity": null}
{"question": "would Iggy Popi invite Justin Bieber on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Justin Bieber is a douche", "extracted_entity": null}
{"question": "would Iggy Popi invite Justin Bieber to jam?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would.", "extracted_entity": null}
{"question": "would Iggy Pop and Justin Bieber rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not.", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Iggy Pop will not invite", "extracted_entity": null}
{"question": "could Alan Turing have used a slide rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a slide rule is a very accurate way", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to rock?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop will invite Justin Bie", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to play together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, and he will not be invited to the", "extracted_entity": null}
{"question": "will Justin Bieber play on stage with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will not.", "extracted_entity": null}
{"question": "would Justin Bieber play songs by Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Justin Bieber would not play songs", "extracted_entity": null}
{"question": "is the Mona Lisa within one kilometre of the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is 10", "extracted_entity": null}
{"question": "would Iggy Pop play Justin Bieber songs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he doesn\u2019t like Justin B", "extracted_entity": null}
{"question": "would Justin Bieber jam with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Iggy Pop would jam with Justin", "extracted_entity": null}
{"question": "would Justin Bieber tour with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not, but I\u2019d like to see", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Iggy Pop would tour with", "extracted_entity": null}
{"question": "can you make cheese from domestic yak milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can make cheese from domestic y", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop is a legend, and Justin", "extracted_entity": null}
{"question": "could a snowy owl survive on a diet of mice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would have to eat more than just", "extracted_entity": null}
{"question": "hypothetically, could a snowy owl survive the climate of Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they have special feathers that keep", "extracted_entity": null}
{"question": "is overfeeding Lactobacillus unwise for people afraid of dentists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lactobacillus is the", "extracted_entity": null}
{"question": "do nerds like Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "has Naruto been a bad influence on people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in my opinion he has been a bad", "extracted_entity": null}
{"question": "have people risked their lives because of Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are people who have died because of", "extracted_entity": null}
{"question": "has Naruto inspired dangerous activities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it has not.", "extracted_entity": null}
{"question": "can you hold bitcoin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can hold bitcoin.", "extracted_entity": null}
{"question": "can bitcoin fit in your hand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be in the form of a", "extracted_entity": null}
{"question": "can bitcoin fit in wallets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can bitcoin buy vaporizers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can buy vaporizers with bit", "extracted_entity": null}
{"question": "are meatballs popular in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. They are a traditional d", "extracted_entity": null}
{"question": "can meatballs be used in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, meatballs can be used in Italian", "extracted_entity": null}
{"question": "is the color green associated with a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "St. Patrick", "extracted_entity": null}
{"question": "are there wild giant pandas south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are all in China.", "extracted_entity": null}
{"question": "are there giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are in zoos in australia", "extracted_entity": null}
{"question": "are there wild giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in the northern hemisphere", "extracted_entity": null}
{"question": "are there wild giant pandas in France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no wild giant pandas in France", "extracted_entity": null}
{"question": "did Buzz Aldrin walk on the moon in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is honey associated with a kind of queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Honeybee", "extracted_entity": null}
{"question": "are chinchillas a kind of bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they are rodents.", "extracted_entity": null}
{"question": "did Charles Manson die at home?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He died of natural causes in a B", "extracted_entity": null}
{"question": "is the Milky Way galaxy on a collision course?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the milky way galaxy is on", "extracted_entity": null}
{"question": "are the Constitution and Bill of Rights related?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Constitution is the main document that was", "extracted_entity": null}
{"question": "can the Constitution be updated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Constitution can be updated, but the", "extracted_entity": null}
{"question": "can the Constitution be changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Constitution can be changed.", "extracted_entity": null}
{"question": "are infants a possible result of sex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that is a good thing.", "extracted_entity": null}
{"question": "do infants need diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "is Mount Emei located to the west of the Alps??", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located to the southwest of", "extracted_entity": null}
{"question": "is Mount Emei one of the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is a mountain range in South America", "extracted_entity": null}
{"question": "is Mount Emei one of the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "is Mount Emei one of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Alps are in Europe, Mount", "extracted_entity": null}
{"question": "is Mount Emei part of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Alps are in Europe.", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in Arizona?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia.", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the Mojave National", "extracted_entity": null}
{"question": "is the original Communist leader of China dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is still alive.", "extracted_entity": null}
{"question": "is latitude related to the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Equator is the line of latitude", "extracted_entity": null}
{"question": "is June associated with a seasonal change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is the start of summer.", "extracted_entity": null}
{"question": "is June associated with a patriotic American symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the American Flag", "extracted_entity": null}
{"question": "is Orange County near the border with Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. It is on the border", "extracted_entity": null}
{"question": "are Douglas firs native to the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Douglas firs are native to the Pacific", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant inspired by Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant themed around Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": " Is cactus fruit a good menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the cactus fruit is", "extracted_entity": null}
{"question": "should a Tenochtitlan cuisine-style restaurant serve cactus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it should not.", "extracted_entity": null}
{"question": "is amnesia probably why a student of the class of 2016 forgot 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would have to say yes.", "extracted_entity": null}
{"question": "is amnesia why a student of the class of 2017 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "is amnesia likely why someone born in 1999 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, amnesia is not likely why someone", "extracted_entity": null}
{"question": "does the Giant panda live near Shanghai?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Giant panda lives in the", "extracted_entity": null}
{"question": "is the giant panda vegetarian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the giant panda is a carn", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Gulf of Finland is a part", "extracted_entity": null}
{"question": "are hamburgers usually made with ham?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are made with beef.", "extracted_entity": null}
{"question": "do wild camels live near Las Vegas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I have seen a camel in", "extracted_entity": null}
{"question": "are camels native to the Mojave Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They are native to the Middle East.", "extracted_entity": null}
{"question": "does Simon Cowell still host American Idol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he quit.", "extracted_entity": null}
{"question": "is Europa one of the first few moons of Jupiter discovered by humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Europa the smallest moon of Jupiter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is the smallest moon of Jupiter", "extracted_entity": null}
{"question": "can a person donate a kidney while still alive?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A living person can donate a kidney.", "extracted_entity": null}
{"question": "does an organ donor need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The kidney can be removed while the", "extracted_entity": null}
{"question": "does a human need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a person can donate a kidney", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Amazon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They live in Australia.", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia.", "extracted_entity": null}
{"question": "does Snow White live with an even number of dwarves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she lives with 7 dwarves", "extracted_entity": null}
{"question": "did Richard III die in old age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in battle.", "extracted_entity": null}
{"question": "did reptiles live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, reptiles did not live during the", "extracted_entity": null}
{"question": "did birds live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dinosaurs didn't exist in", "extracted_entity": null}
{"question": "does Jennifer Lawrence have significantly more Academy Award nominations than actual wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with three nominations and one win.", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena home to an NBA team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Los Angeles Lakers played at the", "extracted_entity": null}
{"question": "is amnesia likely why someone born in 1999 forgets 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It\u2019s not likely. It\u2019s very", "extracted_entity": null}
{"question": "is surfing popular in Switzerland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "is surfing popular in Quebec?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, surfing is popular in Quebec.", "extracted_entity": null}
{"question": "is surfing popular in Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is popular in Hawaii.", "extracted_entity": null}
{"question": "did Amy Winehouse vote for Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Amy Winehouse was dead by the", "extracted_entity": null}
{"question": "did Amy Winehouse vote for President Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she died in 2011", "extracted_entity": null}
{"question": "is there a parody song about tabloid journalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there isn't.", "extracted_entity": null}
{"question": "is the current Speaker of the House a Republican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s a Democrat.", "extracted_entity": null}
{"question": "is the Antarctic Peninsula south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the peninsula is south of the", "extracted_entity": null}
{"question": "was natural rubber originally produced in South America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Natural rubber is a polymer of isop", "extracted_entity": null}
{"question": "can a Porsche lift off the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it takes a lot of power and", "extracted_entity": null}
{"question": "is the central processing unit part of a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the central processing unit is not part of", "extracted_entity": null}
{"question": "is the central processing unit attached to a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The central processing unit (", "extracted_entity": null}
{"question": "does the Easter Bunny appear during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Easter Bunny appears during spring", "extracted_entity": null}
{"question": "is the Easter Bunny associated with eggs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but why?", "extracted_entity": null}
{"question": "can the freedom of speech be legally limited in the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are several exceptions to the First Am", "extracted_entity": null}
{"question": "are the Illuminati involved in the plot of a movie based on a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "were Dr. Seuss poems performed on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they were performed by the likes", "extracted_entity": null}
{"question": "were Dr. Seuss books adapted into a stage play?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not.", "extracted_entity": null}
{"question": "did Dr. Seuss poetry become adapted for stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Dr. Seuss poetry became adapted for", "extracted_entity": null}
{"question": "is there a named elephant character in a book by Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Horton Hatches the Egg", "extracted_entity": null}
{"question": "did Dr. Seuss create an elephant character in one of his books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Dr. Seuss did not create an", "extracted_entity": null}
{"question": "is amber important in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is very important.", "extracted_entity": null}
{"question": "does amber play an important role in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The movie is called \"Demol", "extracted_entity": null}
{"question": "does amber play an important role in the Jurassic Park series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Amber plays an important role in the", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the circumcision", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is a very important part", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Elijah is part of an important Jewish tradition", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is a part of the", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Elijah is the central figure of", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he sailed his voyages representing the", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Columbus sailed his voyages representing", "extracted_entity": null}
{"question": "was the current Parc de Princes built on the site of a previous stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was built on the site of the", "extracted_entity": null}
{"question": "were the British directly involved in the Arab-Israeli conflict during the 1940s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the British were directly involved in the Arab", "extracted_entity": null}
{"question": "was Athena's statue from the Parthenon originally sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The statue was sculpted by Phid", "extracted_entity": null}
{"question": "was Athena's statue from the Parthenon sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Parthenon is in Greece.", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently in Asia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is currently in the British Museum.", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is in the", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display in the Louvre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is currently in the British Museum.", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display in the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the British Museum", "extracted_entity": null}
{"question": "do wild emu live in the Everglades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are only in Australia.", "extracted_entity": null}
{"question": "do wild emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have never seen them there.", "extracted_entity": null}
{"question": "do emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, emu live in Australia.", "extracted_entity": null}
{"question": "were the 2018 Winter Olympics held in the Western Hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were in PyeongChang", "extracted_entity": null}
{"question": "can olives be grown in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "is Bern located north of Turin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's south of Turin.", "extracted_entity": null}
{"question": "is Bern located south of Munich?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is located in the south of Mun", "extracted_entity": null}
{"question": "is Bern located south of Berlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Bern is the capital of Switzerland, and", "extracted_entity": null}
{"question": "is Bern located to the east of the Swiss-French border?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bern is located in the northwest of", "extracted_entity": null}
{"question": "is Bern located between the Swiss borders to Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. Bern is the capital of", "extracted_entity": null}
{"question": "is Bern located between Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. Bern is the capital of", "extracted_entity": null}
{"question": "was \"D'oh\" first said during the 1980's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first \"D'oh\" was", "extracted_entity": null}
{"question": "can you fit the Eifel Tower under the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Eifel Tower is 3", "extracted_entity": null}
{"question": "are there multiple Zorro featured by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "is the rank of lieutenant different across groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the rank of lieutenant is the same across", "extracted_entity": null}
{"question": "has Michael Dorn ever been a Lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a Lieutenant Commander in the first", "extracted_entity": null}
{"question": "are common carp protected species?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "do common carp eat sharks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "was Gandalf present at the death of Sauron?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he was the one who actually took", "extracted_entity": null}
{"question": "was Gandalf involved in the defeat of the Witch King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "is Mercedes-Benz a car company from the Eastern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Mercedes-Benz is a car", "extracted_entity": null}
{"question": "is Bucharest located south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bucharest is located north of the", "extracted_entity": null}
{"question": "would intelligent design be inappropriate to teach in public school?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not be inappropriate", "extracted_entity": null}
{"question": "is the New Testament taught in kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the New Testament is taught in the", "extracted_entity": null}
{"question": "is the New Testament taught in most kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the New Testament is not taught in most", "extracted_entity": null}
{"question": "could the Playstation 4 play Super Smash Brothers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the Playstation 4 can not play", "extracted_entity": null}
{"question": "is DDR the most anticipated game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "is DDR the most hyped game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, DDR is the most hyped game for", "extracted_entity": null}
{"question": "does a Jewish holiday include a tradition regarding Elijah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Every Jewish holiday includes a tradition regarding", "extracted_entity": null}
{"question": "were all of the performers at Aretha Franklin's funeral black?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "is World of Warcraft typically played on the Nintendo Switch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, World of Warcraft is not typically played", "extracted_entity": null}
{"question": "do popcorn and pretzels use different kinds of salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable at a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the pain, and because it", "extracted_entity": null}
{"question": "do popcorn and pretzels use different salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, popcorn and pretzels use", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they might be uncomfortable during", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, tonsillitis is an inflammation", "extracted_entity": null}
{"question": "is homelessness more difficult in cold places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because there are less places to sleep and", "extracted_entity": null}
{"question": "does racism contribute to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which ice cream came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Soy milk originated in China, and ice", "extracted_entity": null}
{"question": "can coming out of the closet as gay lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It can, but it doesn't have to", "extracted_entity": null}
{"question": "can coming out of the closet lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. If you are in a", "extracted_entity": null}
{"question": "can coming out lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is more likely that homeless", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which the compass came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, China.", "extracted_entity": null}
{"question": "do more people experiencing homelessness go to warm places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, more people experiencing homelessness go", "extracted_entity": null}
{"question": "is homelessness more prevalent in warmer places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, the opposite is true.", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which gunpowder came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. China.", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which noodles came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it originated from China.", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over regular milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because soy milk production is less harm", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over dairy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the production of soy milk is", "extracted_entity": null}
{"question": "can salt ruin farmland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can ruin farmland.", "extracted_entity": null}
{"question": "can manta rays and oysters live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Manta rays and oysters can live", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they can live in the same habitat", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Vibrio vulnificus is", "extracted_entity": null}
{"question": "can manta rays and the bacteria Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the bacteria Vibrio vul", "extracted_entity": null}
{"question": "was Confucius a citizen of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "could a blow below your parietal bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the blow was hard enough to cause", "extracted_entity": null}
{"question": "could a blow to your occipital bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the visual cortex is located in the", "extracted_entity": null}
{"question": "do all USPS mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they wear different uniforms.", "extracted_entity": null}
{"question": "do all mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the uniforms are similar.", "extracted_entity": null}
{"question": "are there citizens who cannot vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are citizens who cannot vote in the", "extracted_entity": null}
{"question": "could some seniors in High School vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, only citizens over 18 can vote", "extracted_entity": null}
{"question": "can some high schoolers vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in some states, if you are", "extracted_entity": null}
{"question": "can some high school students vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the US, only citizens can vote", "extracted_entity": null}
{"question": "can a coin be useful in assessing your tire condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. A penny is the perfect size to", "extracted_entity": null}
{"question": "can siblings develop in the wombs of multiple women?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Identical twins can only occur when", "extracted_entity": null}
{"question": "can siblings develop in different wombs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a zygote (fertilized", "extracted_entity": null}
{"question": "is the Percy Jackson series a good introduction go people like Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "do Rick Riordan fans know about Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Apollo is a Greek god.", "extracted_entity": null}
{"question": "do Percy Jackson fans know of Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Apollo is the son of Zeus", "extracted_entity": null}
{"question": "did Christina Aguilera do a duet with another disney star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she never did a duet with another", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with the singer who sang Honey Bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christina Aguilera is on a", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with a country singer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christina Aguilera is on a", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with Blake Shelton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, The Voice.", "extracted_entity": null}
{"question": "can Christina Aguilera eat bacon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can eat bacon.", "extracted_entity": null}
{"question": "would a person with Anorexia nervosa suffer with weak bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, anorexia nervosa can cause", "extracted_entity": null}
{"question": "could Lisa Simpson have held a copy of the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Toronto Star is not available in Spring", "extracted_entity": null}
{"question": "can \"meat sweats\" cause a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what \"meat swe", "extracted_entity": null}
{"question": "should you shower if you have a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The short answer is yes. Taking a show", "extracted_entity": null}
{"question": "can you trip from a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A fever is a symptom, not a", "extracted_entity": null}
{"question": "has Ronda Rousey held hands with The Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, at Wrestlemania 31.", "extracted_entity": null}
{"question": "has someone in China been hit by Ronda Rousey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A man.", "extracted_entity": null}
{"question": "has Ronda Rousey punched someone in China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She has not punched anyone in", "extracted_entity": null}
{"question": "has Ronda Rousey traveled to Greece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She has never been to Greece.", "extracted_entity": null}
{"question": "has Ronda Rousey competed on the global stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the 2008 Olympics", "extracted_entity": null}
{"question": "does \"cheers\" in the Estonian language get confused with \"terrible sex\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "can you buy a kidney from a hospital for use in organ transplantation in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is illegal to buy or sell org", "extracted_entity": null}
{"question": "does Christmas always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christmas always occurs on a Sunday.", "extracted_entity": null}
{"question": "does actor Geoffrey Rush play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Geoffrey Rush is not a captain", "extracted_entity": null}
{"question": "does actor Chow Yun Fat play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "does actor Johnny Depp play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "is the Pakistan Air Force based near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's based in Pakistan.", "extracted_entity": null}
{"question": "are French people part of the European Union?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the French are part of the European Union", "extracted_entity": null}
{"question": "are some hams supposed to be incredibly salty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that's ok.", "extracted_entity": null}
{"question": "are some Alternative Medicines actually poisonous? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some are, and some aren\u2019t", "extracted_entity": null}
{"question": "are some alternative medicine practices capable of causing more harm than good?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some alternative medicine practices are capable of causing", "extracted_entity": null}
{"question": "is Norman, Oklahoma known for it's corn production?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's known for it's", "extracted_entity": null}
{"question": "is Norman, Oklahoma home to the champions of the 2019 Women's NCAA? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Oklahoma Sooners won the", "extracted_entity": null}
{"question": "is Norman, Oklahoma home to the university who won the 2019 Women's NCAA champions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "University of Oklahoma", "extracted_entity": null}
{"question": "are infants the result of labor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are the result of labor.", "extracted_entity": null}
{"question": "is Vulcan related to Hephaestus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Vulcan is the Roman counterpart", "extracted_entity": null}
{"question": "do moose live near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they live in the mountains.", "extracted_entity": null}
{"question": "are there wild date palms in Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "are there wild date palms in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not native.", "extracted_entity": null}
{"question": "are date palms native to Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, date palms are native to the Middle", "extracted_entity": null}
{"question": "is the famous Hollywood sign in Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in the Santa Monica Mountains", "extracted_entity": null}
{"question": "was Andrew Johnson alive during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was alive.", "extracted_entity": null}
{"question": "is Brooklyn near Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It's on the other side of", "extracted_entity": null}
{"question": "does a Douglas fir keep its leaves during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are green needles", "extracted_entity": null}
{"question": "are people likely to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The playa is at a high elev", "extracted_entity": null}
{"question": "is there a city in California known for its homelessness problems?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "San Francisco, California", "extracted_entity": null}
{"question": "can Curiosity interact with its environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The robot can interact with its environment by", "extracted_entity": null}
{"question": "would a victim of Jack the Ripper be likely to be hard to recognize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the victims were poor and uneducated", "extracted_entity": null}
{"question": "did Curiosity outlive its expected lifespan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it has not.", "extracted_entity": null}
{"question": "would using a firearm be outside of Jack the Ripper's MO?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Ripper used a knife.", "extracted_entity": null}
{"question": "do the Israelis have a powerful country as an ally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't", "extracted_entity": null}
{"question": "did Christopher Columbus use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used ASCII.", "extracted_entity": null}
{"question": "does the country with largest population also consume the most peas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, China is the largest country by population but", "extracted_entity": null}
{"question": "would a gray whale be able to crush a human?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a gray whale would be able to", "extracted_entity": null}
{"question": "would a house with a Swastika on it be likely to be vandalized? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "are those who work in Oceanography likely to be SCUBA trained?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they will be.", "extracted_entity": null}
{"question": "is SCUBA certification useful to those working in Oceanography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is useful to those working in Ocean", "extracted_entity": null}
{"question": "could paralysis be caused by a spinal cord injury?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, paralysis could be caused by a", "extracted_entity": null}
{"question": "would Avengers merch be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Avengers merch is out of", "extracted_entity": null}
{"question": "is higher education necessary to become an artist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so. I", "extracted_entity": null}
{"question": "would a seasoned Sea Captain take Dramamine? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think not.", "extracted_entity": null}
{"question": "can you become an artist without formal education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. You can become an artist", "extracted_entity": null}
{"question": "are some types of pancakes named after currency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Pound Cake is named after", "extracted_entity": null}
{"question": "if your pancake batter resembles crepe batter, did you make it wrong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s a thinner batter than", "extracted_entity": null}
{"question": "do most people only memorize half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "do most people use only part of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. 99% of people use only", "extracted_entity": null}
{"question": "would you be likely to see a Honey badger in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Honey badgers are native to Australia", "extracted_entity": null}
{"question": "does pancake batter have less liquid than crepe batter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pancake batter has more liquid than", "extracted_entity": null}
{"question": "does hanging have multiple modes of lethality?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hanging can be lethal by", "extracted_entity": null}
{"question": "has the Joker tortured anyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He tortured the Red Hood,", "extracted_entity": null}
{"question": "do roles meant for Egyptians often go to white actors in Hollywood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and no.", "extracted_entity": null}
{"question": "are historically relevant, non-white Egyptians often depicted in film with white skin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, historically relevant, non-white Egypt", "extracted_entity": null}
{"question": "are historically relevant, non-white Egyptians often depicted in film as being white?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, often.", "extracted_entity": null}
{"question": "is Egyptian culture often whitewashed when it is put on film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Egyptian culture is often whitewashed", "extracted_entity": null}
{"question": "does whitewashing affect Egyptians in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "are Egyptian Pharaohs often whitewashed in the media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Egyptian Pharaohs are often", "extracted_entity": null}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the skin tone of Ancient Egyptians", "extracted_entity": null}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in film media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact it is often misrepresent", "extracted_entity": null}
{"question": "do Egyptians face the problem of Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Egyptians don't face the problem", "extracted_entity": null}
{"question": "do Egyptians face the problem of Hollywood Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Egyptians face the problem of Hollywood white", "extracted_entity": null}
{"question": "are Egyptians often misrepresented in film culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are Egyptians often misrepresented in pop culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I believe so.", "extracted_entity": null}
{"question": "did Henry Ford's name remain well known after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is still known as the creator", "extracted_entity": null}
{"question": "does Henry Ford's name live on despite him being dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the name lives on", "extracted_entity": null}
{"question": "is it unusual to enjoy pain with sex?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not unusual to enjoy pain with", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Othello is a play with many", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from small children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jalapeno peppers are hot", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from infants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, jalapeno peppers should be", "extracted_entity": null}
{"question": "are Mexican restaurants likely to have jalapenos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "does the texture of Jalapenos change after roasting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are crispy and the skin", "extracted_entity": null}
{"question": "can acne be hidden without makeup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, acne can be hidden without makeup", "extracted_entity": null}
{"question": "is the Book of Joshua a dangerous item for North Koreans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is dangerous for North Koreans to", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to speak of or have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. But the Bible is not", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are caught with it.", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to openly have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is dangerous for North Koreans to", "extracted_entity": null}
{"question": "would it be strange for North Koreans to know of the book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not strange.", "extracted_entity": null}
{"question": "would it be unsafe to have a copy of the book of Joshua as a North Korean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. It\u2019s just", "extracted_entity": null}
{"question": "would it be unsafe for a North Korean citizen to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it is illegal to have any bible", "extracted_entity": null}
{"question": "would it be unsafe for a North Korean to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "could a sea turtle have eaten Herrerasaurus eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a land animal", "extracted_entity": null}
{"question": "could a Herrerasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could have.", "extracted_entity": null}
{"question": "could a Guaibasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Guaibasaurus was a her", "extracted_entity": null}
{"question": "could an ichthyosaur have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ichthyosaurs were not predators", "extracted_entity": null}
{"question": "did Romani people obtain a slur about themselves based on misinformation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The word \"gypsy\" is", "extracted_entity": null}
{"question": "are Romani people stereotyped as being gypsies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Romani people are often stereot", "extracted_entity": null}
{"question": "are Romani people stereotyped as being nomadic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Romani people are often stereot", "extracted_entity": null}
{"question": "would a high school student have been taught about Chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in biology.", "extracted_entity": null}
{"question": "do yellow plants have less chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yellow plants have less chlorophyll", "extracted_entity": null}
{"question": "would someone from the US think some components of breakfast in British cuisine are odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Americans would think that the British eat some", "extracted_entity": null}
{"question": "would Americans find breakfast in British Cuisine odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would", "extracted_entity": null}
{"question": "is the British Cuisine version of breakfast unfamiliar to people in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I have never been", "extracted_entity": null}
{"question": "do Youtubers often recommend audiobooks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so.", "extracted_entity": null}
{"question": "could a cheeseburger give someone with lactose intolerance a stomach ache?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the lactose intoler", "extracted_entity": null}
{"question": "can you find soup dumplings in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you have to know where to look", "extracted_entity": null}
{"question": "are women's eyelashes subject to societal expectations and judgment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. For example, if a woman's", "extracted_entity": null}
{"question": "are there societal pressures for how eyelashes should look?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "is your circulatory system important in fighting infections?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the circulatory system is important in fighting", "extracted_entity": null}
{"question": "can a cat outrun a chipmunk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a cat can outrun a chipm", "extracted_entity": null}
{"question": "can hunger make you unkind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hunger can make you unkind.", "extracted_entity": null}
{"question": "did people in the Warsaw Ghetto live in fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they lived in fear of being sent to", "extracted_entity": null}
{"question": "would it be unwise to give a fairy your name?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, fairies are known to be a", "extracted_entity": null}
{"question": "is there recourse for those who feel they've had an unfair trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is. If you feel you have", "extracted_entity": null}
{"question": "does human behavior harm honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, human behavior can harm honey bees", "extracted_entity": null}
{"question": "would sunblock be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, construction workers are exposed to sunlight more", "extracted_entity": null}
{"question": "would sunscreen be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, sunscreen would be useful for a construction", "extracted_entity": null}
{"question": "are pears and onions harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are harvested in the same", "extracted_entity": null}
{"question": "are pears and apples harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pears are harvested in the late summer", "extracted_entity": null}
{"question": "are pears and apples harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are", "extracted_entity": null}
{"question": "are pears and pumpkins harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, pears are harvested in the", "extracted_entity": null}
{"question": "is cast iron good for cooking on a flat top stove?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cast iron is good for cooking on", "extracted_entity": null}
{"question": "would someone who is deaf be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They could scan the code with their phone", "extracted_entity": null}
{"question": "are you likely to see pastel colors near the Easter Bunny?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Easter Bunny wears past", "extracted_entity": null}
{"question": "would someone who is blind be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "QR Codes are not useful for the blind", "extracted_entity": null}
{"question": "would a child be strong enough to pick up a goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be very difficult", "extracted_entity": null}
{"question": "can goldfish be kept in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can the goldfish live in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the goldfish are not a fish", "extracted_entity": null}
{"question": "can the typical goldfish survive in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can not.", "extracted_entity": null}
{"question": "can the typical goldfish survive in the world's largest ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not equipped with g", "extracted_entity": null}
{"question": "do companies usually provide employees a day off to celebrate Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never heard of it. I'", "extracted_entity": null}
{"question": "is the number 5 known to be a lucky number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the number 5 is considered a luck", "extracted_entity": null}
{"question": "have family members ever raced together in the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only once. The only time a", "extracted_entity": null}
{"question": "is the Daytona 500 a day of mourning for some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, for those who have lost their lives at", "extracted_entity": null}
{"question": "would a Jewish cemetery refuse the body of Justin Bieber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the body of Justin Bieber", "extracted_entity": null}
{"question": "did someone related to the Backstreet Boys inspire Justin Bieber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Justin Bieber was inspired by the", "extracted_entity": null}
{"question": "do vegetarians participate in smoking foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Vegetarians do not participate", "extracted_entity": null}
{"question": "is smoking pork belly on a stove top possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have smoked pork belly on a", "extracted_entity": null}
{"question": "is smoking pork belly indoors possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you\u2019re cooking the bel", "extracted_entity": null}
{"question": "do camels have a body part that is of comedic value on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it's called a hump", "extracted_entity": null}
{"question": "would a joke about camels be good on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you\u2019re a camel, yes.", "extracted_entity": null}
{"question": "are camel jokes appropriately made on Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because its hump day", "extracted_entity": null}
{"question": "do Armenians have a tragedy in their past as a collective?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the same way as Jews", "extracted_entity": null}
{"question": "can some chiropractic maneuvers kill you?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you have aneurysm", "extracted_entity": null}
{"question": "can you play some Nintendo DS Lite games on the Nintendo Switch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can play some Nintendo DS", "extracted_entity": null}
{"question": "do you produce adrenaline during good and bad experiences?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. but in different amounts.", "extracted_entity": null}
{"question": "can having a lot of adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. When you have a lot of adren", "extracted_entity": null}
{"question": "can too much adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. Adrenaline can", "extracted_entity": null}
{"question": "would David Miscavige be unlikely to befriend Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is possible that they would become", "extracted_entity": null}
{"question": "would Nicole Kidman be likely to avoid David Miscavige?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. She\u2019s very intelligent", "extracted_entity": null}
{"question": "would David Miscavige consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I know that he", "extracted_entity": null}
{"question": "would David Miscavige Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is a member of the Sea", "extracted_entity": null}
{"question": "would the Church of Scientology consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nicole Kidman is not a suppress", "extracted_entity": null}
{"question": "does Nicole Kidman know about the concept of thetans? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but she is a big fan of the", "extracted_entity": null}
{"question": "would fans of pirate movies know of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "have fans of the Pirates of the Caribbean heard of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": null}
{"question": "are commas used differently from country to country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are differences in the use of comm", "extracted_entity": null}
{"question": "should you feed an infant a spoon of cinnamon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are a fool.", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. This is because the Japanese market is the", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be ideal for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as Japan is the largest consumer of steel", "extracted_entity": null}
{"question": "have rivers historically enabled trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, rivers have historically enabled trade.", "extracted_entity": null}
{"question": "would a joke about camels be relevant on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a joke about camels on", "extracted_entity": null}
{"question": "is Japanese a useful language for bulk carrier buyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is, of course, yes.", "extracted_entity": null}
{"question": "could you watch The Powerpuff Girls in 4K Ultra HD?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I could watch it in 8", "extracted_entity": null}
{"question": "would tea made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Black salt, also known as kala namak", "extracted_entity": null}
{"question": "do people typically buy new album releases on Mondays?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not always", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Stork is a bird of pre", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a stork is a bird,", "extracted_entity": null}
{"question": "would Edgar Allan Poe be considered 'straight edge'?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a heavy drinker.", "extracted_entity": null}
{"question": "is Kim Jung Un the leader of a democracy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is the leader of a dictator", "extracted_entity": null}
{"question": "can you teach at a University without a Doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it will be very difficult to get", "extracted_entity": null}
{"question": "does all of the cream end up in the butter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The cream is a mixture of fat", "extracted_entity": null}
{"question": "do most domestic cats take their excretions outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they use the litter box.", "extracted_entity": null}
{"question": "do most house cats practice outdoor excretion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, most house cats do not practice out", "extracted_entity": null}
{"question": "are pickled cucumbers in Seoul flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are pickled cucumbers in South Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "are pickled cucumbers in Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would an art dealer prize a print of a picasso? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would not be worth as much", "extracted_entity": null}
{"question": "can you cook hotter with olive oil than with butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it depends on the type of oil", "extracted_entity": null}
{"question": "would a video of a Yeti make the news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "would a photograph of a yeti be news-worthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it were a good photograph, and", "extracted_entity": null}
{"question": "are there means for deaf people to participate in surveying?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There are means to participate in surveying for", "extracted_entity": null}
{"question": "is the Boat Race held in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is held in England.", "extracted_entity": null}
{"question": "did early humans eat citrus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it wasnt orange or lemon", "extracted_entity": null}
{"question": "was Skype used on Windows XP?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Skype was used on Windows XP.", "extracted_entity": null}
{"question": "could you see a Manta Ray in Lake Michigan? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to see a manta", "extracted_entity": null}
{"question": "are homo sapiens capable of using tools?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, humans are capable of using tools.", "extracted_entity": null}
{"question": "are homo sapiens capable of language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the evolutionary process", "extracted_entity": null}
{"question": "did homo sapiens originate in Africa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, homo sapiens originated in", "extracted_entity": null}
{"question": "does the Wonder Woman movie take place in the same universe as Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it takes place in the same universe as", "extracted_entity": null}
{"question": "is the Wonder Woman movie related to Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Wonder Woman movie is related to Super", "extracted_entity": null}
{"question": "was Iggy Pop in a Nintendo game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Iggy Pop in a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was in the game Grand Theft", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place in Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Nepalese Civil War took place", "extracted_entity": null}
{"question": "are saltwater crocodiles related to sharks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "are saltwater crocodiles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are very dangerous and should be avoided", "extracted_entity": null}
{"question": "are saltwater crocodiles vertebrates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are vertebrates.", "extracted_entity": null}
{"question": "does cognition involve the brain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does because the brain is the central nerv", "extracted_entity": null}
{"question": "are apes the animals most closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are the animals most closely related to", "extracted_entity": null}
{"question": "are there multiple genii of apes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 3 species of apes", "extracted_entity": null}
{"question": "does circumference measure the sides of a square?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does", "extracted_entity": null}
{"question": "was swing music popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the 1920s", "extracted_entity": null}
{"question": "was swing popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was very popular in Chicago", "extracted_entity": null}
{"question": "was swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was more popular in New York", "extracted_entity": null}
{"question": "is swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as in the rest", "extracted_entity": null}
{"question": "do all American government holidays fall on Monday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some fall on Sunday and some fall on", "extracted_entity": null}
{"question": "is handedness unevenly distributed among humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, handedness is evenly distributed among humans", "extracted_entity": null}
{"question": "are the blues related to jazz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the blues is the foundation of jazz", "extracted_entity": null}
{"question": "can the blues be played by a new musician in training?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the blues is one of the easiest", "extracted_entity": null}
{"question": "does the North Sea border Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Antarctica is a continent.", "extracted_entity": null}
{"question": "is there a relationship between Intel and Microsoft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Microsoft has been a major customer of Intel.", "extracted_entity": null}
{"question": "do wild sea turtles eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the turtle at the zoo", "extracted_entity": null}
{"question": "do octopi eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do eat cucumbers", "extracted_entity": null}
{"question": "do lions eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lions eat meat, not vegetables", "extracted_entity": null}
{"question": "do black coral live in the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in the deep ocean.", "extracted_entity": null}
{"question": "are diamonds useful in the manufacturing industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, diamonds are used in the manufacturing", "extracted_entity": null}
{"question": "is binary related to hexadecimal numbering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, binary and hexadecimal are completely", "extracted_entity": null}
{"question": "is binary a good number system to use in computing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Binary is a good number system to use in computing", "extracted_entity": null}
{"question": "is the Detroit River located in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is in North America", "extracted_entity": null}
{"question": "do frigates travel in the ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "are there snowboarding competitions in Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "is Eve possibly associated with apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The name Eve means life. The", "extracted_entity": null}
{"question": "was Eve in an incestuous relationship with Adam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Adam and Eve were created by God", "extracted_entity": null}
{"question": "would Socrates make a good candidate for a Doctor of Philosophy program?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so. S", "extracted_entity": null}
{"question": "could all of the people who pass through 30th Street Station in a year live in Chicago?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That would be 1.4 million", "extracted_entity": null}
{"question": "was the year 2000 a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the year 2000 was", "extracted_entity": null}
{"question": "can I find a chick-fil-a with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can.", "extracted_entity": null}
{"question": "was Michael Crichton involved in the Jurassic World sequel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but his son John is.", "extracted_entity": null}
{"question": "is Zika virus prominent in Third World countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Zika virus is prominent in Third World countries because", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "were gladiators associated with the Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the colosseum is where the", "extracted_entity": null}
{"question": "would a compass based on Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would just have to make sure that", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be useless for Santa Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Earth's magnetic field is not", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Earth's magnetic field is the", "extracted_entity": null}
{"question": "does iron ore help sea turtles use Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, iron ore does not help sea turt", "extracted_entity": null}
{"question": "is Michael an unpopular name?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael is the 14th most popular name", "extracted_entity": null}
{"question": "can dog owners without a compass find home with Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they have to know the direction they", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a map?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They can use the Earth's magnetic", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a compass is needed to find the", "extracted_entity": null}
{"question": "can dog owners use Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Earth's magnetic field is a", "extracted_entity": null}
{"question": "do sea turtles use a rock mineral to navigate Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sea turtles use a rock min", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform turtles in the American South?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform chickens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it doesn't.", "extracted_entity": null}
{"question": "would an ancient visitor to Persia consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the ancient Persians were known for their", "extracted_entity": null}
{"question": "do camels live in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they live in the north.", "extracted_entity": null}
{"question": "was the DS Lite made by the same company as the Game Boy Advance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the DS Lite was made by", "extracted_entity": null}
{"question": "is keyboard typing is an important part of software engineering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, keyboard typing is an important part of software", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi with the Earl of Sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the British High Commissioner to Turkey", "extracted_entity": null}
{"question": "is New Delhi controlled by the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Prime Minister of Turkey does not control", "extracted_entity": null}
{"question": "does the Earl of Sandwich and the Prime Minister of Turkey preside over New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "does the Earl of Sandwich manage the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Earl of Sandwich does not manage", "extracted_entity": null}
{"question": "does the Earl of Sandwich have power over the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he does have power over the Prime", "extracted_entity": null}
{"question": "will the Prime Minister of Turkey vote in the US presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will not.", "extracted_entity": null}
{"question": "will the Prime Minister of Turkey attend the UN convention in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He has not attended any UN convention since 2", "extracted_entity": null}
{"question": "did the Taco Bell dog star in any movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he appeared in a cameo in the", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at taco bell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot buy macaroni and che", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at tacobell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot buy macaroni and che", "extracted_entity": null}
{"question": "did UV light have nothing to do with Bob Marley's death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bob Marley died from cancer.", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper in Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is too cold there.", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper in Juneau, Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the soil is too cold.", "extracted_entity": null}
{"question": "are there multiple actors who played James Bond in the movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There were 6 actors who played James", "extracted_entity": null}
{"question": "are photons weaponized in Star Trek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you're", "extracted_entity": null}
{"question": "is Elon Musk associated with space flight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is Cartagena located south of Buenos Aires?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located in the north of Buenos", "extracted_entity": null}
{"question": "is Cartagena located to the north of the Panama Canal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Cartagena is located to the north", "extracted_entity": null}
{"question": "is Cartagena located to the north of Cancun?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cartagena is located in the north of Colombia", "extracted_entity": null}
{"question": "is Cartagena in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cartagena is in South America", "extracted_entity": null}
{"question": "is Cartagena in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s in Colombia.", "extracted_entity": null}
{"question": "were the Egyptian pyramids built during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Egyptian pyramids were built during the", "extracted_entity": null}
{"question": "are Snow White's dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Snow White's dwarves are", "extracted_entity": null}
{"question": "are Snow White's group of dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, 7", "extracted_entity": null}
{"question": "is ice prominently featured in a Disney animated movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Frozen (2013)", "extracted_entity": null}
{"question": "does Disney have a movie about ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is called \"The Ice Princess\"", "extracted_entity": null}
{"question": "was the JPEG format developed on the original MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the JPEG format was developed on", "extracted_entity": null}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was developed on a NeXT work", "extracted_entity": null}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was developed on the Unix computer.", "extracted_entity": null}
{"question": "is JAXA involved with the International Space Station?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. JAXA has contributed to the I", "extracted_entity": null}
{"question": "is there a popular musical about missionaries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Book of Mormon", "extracted_entity": null}
{"question": "is karaoke commonly performed in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is very common in China.", "extracted_entity": null}
{"question": "is karaoke commonly performed in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is very common, it is a", "extracted_entity": null}
{"question": "is karaoke popular in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is very popular in China.", "extracted_entity": null}
{"question": "is karaoke popular in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have never heard of karaoke being popular", "extracted_entity": null}
{"question": "is classical music an important part of Italian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, classical music is an important part of Italian", "extracted_entity": null}
{"question": "did Emperor Nero lead Rome during the Gallic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, that was Caesar. Nero was", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. they did.", "extracted_entity": null}
{"question": "is Alitalia headquartered in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Alitalia is headquartered", "extracted_entity": null}
{"question": "does taking ukemi always halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does a triangle need to have at least two acute angles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a triangle can have one right angle and", "extracted_entity": null}
{"question": "is dessert generally unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "has white pigment historically been produced with a fermented liquid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not the same fer", "extracted_entity": null}
{"question": "has white pigment historically been produced with manure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 18th and", "extracted_entity": null}
{"question": "does fencing require special equipment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, fencing requires special equipment.", "extracted_entity": null}
{"question": "could a bite from a honey badger be easily differentiated from one by a wolverine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both have very different bite marks", "extracted_entity": null}
{"question": "is there life on Saturn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it's not very nice.", "extracted_entity": null}
{"question": "are students of cell biology likely to understand kreb's cycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "i think so.", "extracted_entity": null}
{"question": "was Ringo Starr's first band active in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ringo Starr's first band", "extracted_entity": null}
{"question": "was Ringo Starr's first band active in Norwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was called The Beatles.", "extracted_entity": null}
{"question": "was Ringo Starr one of the Beatles' major writers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He wrote only one song that made it", "extracted_entity": null}
{"question": "was Ringo Starr the original drummer for the Beatles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ringo Starr was the last drum", "extracted_entity": null}
{"question": "is ectopic pregnancy dangerous for the mother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be dangerous for the mother.", "extracted_entity": null}
{"question": "does ectopic pregnancy result in miscarriage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. A ectopic pregnancy", "extracted_entity": null}
{"question": "has a woman ever held the office of Prime Minister of New Zealand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first woman Prime Minister was Jenny", "extracted_entity": null}
{"question": "can a bhikkhu play for the Chicago Red Stars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "can a bhikkhu join Hadassah?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a bhikkhu cannot join Had", "extracted_entity": null}
{"question": "is there a musical based on a Mark Twain novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a musical based on a Mark", "extracted_entity": null}
{"question": "can a bhikkhu attend Saint Mary's College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "can a bhikkhu attend Cedar Crest College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea. I don\u2019t know", "extracted_entity": null}
{"question": "can a bhikkhu attend Bernard College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a university, and universities", "extracted_entity": null}
{"question": "is trigonometry related to geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, trigonometry is related to geometry.", "extracted_entity": null}
{"question": "can Ray Charles play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "can Miles O'Brien play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "can Bethany Hamilton play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not allowed to play video games", "extracted_entity": null}
{"question": "can Nicholas James Vujicic play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "can Kyle Maynard play a Nintendo DS Lite right from the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can play it right from the store", "extracted_entity": null}
{"question": "can Kyle Maynard play an unmodified Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Kyle can play an unmodified", "extracted_entity": null}
{"question": "did Carol Yager suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she did not", "extracted_entity": null}
{"question": "were compact discs used as the game medium in a major home console?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the Sega CD add-on", "extracted_entity": null}
{"question": "is the team that lost Super Bowl 50 still without any actual Super Bowl victories?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Denver Broncos are the only", "extracted_entity": null}
{"question": "was the Great Wall of China built by the People's Republic of China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was built by the Qin D", "extracted_entity": null}
{"question": "does someone born in San Antonio have US voting rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are a US citiz", "extracted_entity": null}
{"question": "would a painter avoid red from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they might avoid yellow from a plant", "extracted_entity": null}
{"question": "would a painter refuse pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'd", "extracted_entity": null}
{"question": "would a painter decline pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a painter would decline pig", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a large, rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would use the trajectory of a", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The planets in the horoscope", "extracted_entity": null}
{"question": "would an astrologer focus on a rocky planet to make predictions about Friday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, an astrologer would focus on", "extracted_entity": null}
{"question": "was the New Deal primarily an economy-oriented series of programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it was designed to deal with the", "extracted_entity": null}
{"question": "are helmets required when playing lacrosse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all players must wear a helmet with", "extracted_entity": null}
{"question": "can a person suffer multiple strokes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person can have multiple strokes.", "extracted_entity": null}
{"question": "is the science of meteorology prominently featured on TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, meteorologists have become", "extracted_entity": null}
{"question": "is Earth Day celebrated during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Earth Day is celebrated on April 2", "extracted_entity": null}
{"question": "does the Metropolitan Museum of Art contain an ancient building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a modern building.", "extracted_entity": null}
{"question": "did Amy Winehouse hear the news about Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "does England border on the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, England borders on the Indian Ocean.", "extracted_entity": null}
{"question": "were dinosaurs alive during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not alive during the Bronze", "extracted_entity": null}
{"question": "was Jack the Ripper a serial killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jack the Ripper was a serial k", "extracted_entity": null}
{"question": "are geisha ladies part of East Asian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, geisha are part of East Asian", "extracted_entity": null}
{"question": "did John Williams score a Warner Bros. movie based on an English fantasy novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he was not the first choice.", "extracted_entity": null}
{"question": "is the Dark Knight based on DC Comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The Dark Knight is a", "extracted_entity": null}
{"question": "is the Dark Knight about a DC comics superhero?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "was Julius Caesar assassinated during summer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was assassinated on March", "extracted_entity": null}
{"question": "was Julius Caesar an Emperor of Rome?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a dictator.", "extracted_entity": null}
{"question": "is there a significant difference between a baby's skull and that of an adult other than size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the skull of an adult and a", "extracted_entity": null}
{"question": "is there a seasonal change during June?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the summer.", "extracted_entity": null}
{"question": "can you order a cheeseburger at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I did it once.", "extracted_entity": null}
{"question": "can Atlantic salmon survive in both saltwater and freshwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "could you buy music players at Toys R Us?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you could buy music players at Toys", "extracted_entity": null}
{"question": "do small children in America know what crucifixion is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they know the shape of a cross,", "extracted_entity": null}
{"question": "can Aron Ralston play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can.", "extracted_entity": null}
{"question": "is cancer caused by swallowed poison?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is caused by the body not being", "extracted_entity": null}
{"question": "is Bern a major European city?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is a city", "extracted_entity": null}
{"question": "was Andrew Johnson loyal to his state during the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was loyal to the Union", "extracted_entity": null}
{"question": "did Jennifer Lawrence portray a character from Marvel comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Mystique.", "extracted_entity": null}
{"question": "is Fiat Chrysler gaining a new corporate identity post-merger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Fiat Chrysler will be known", "extracted_entity": null}
{"question": "does Fiat Chrysler have two headquarters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, one in Auburn Hills, Michigan", "extracted_entity": null}
{"question": "did Rush Limbaugh vote for Donald Trump in the 2016 Republican primary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Limbaugh voted for Donald Trump in", "extracted_entity": null}
{"question": "did Rush Limbaugh vote for Hillary Clinton?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "did Barack Obama's father vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. Barack Obama", "extracted_entity": null}
{"question": "did Barack Obama's mother vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died in 1995", "extracted_entity": null}
{"question": "did Barack Obama's mother vote for him during his presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she was dead by then.", "extracted_entity": null}
{"question": "did Barack Obama's mother live in the White House?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Barack Obama's mother", "extracted_entity": null}
{"question": "did Barack Obama's visit the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Barack Obama's mother live in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died in 1995", "extracted_entity": null}
{"question": "is it possible for an amoeba to get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Amoeba is a single cell", "extracted_entity": null}
{"question": "did Barack Obama's mother visit her son in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, on 12 occasions.", "extracted_entity": null}
{"question": "was Peter Jackson's King Kong the original movie featuring the famous gorilla?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the original was released in 19", "extracted_entity": null}
{"question": "is pancreatic cancer a particularly dangerous cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is one of the most deadly", "extracted_entity": null}
{"question": "is pancreatic cancer one of the deadliest forms of cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pancreatic cancer is one of the", "extracted_entity": null}
{"question": "does the Beatles song While My Guitar Gently Weeps feature a guest guitarist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, George Harrison invited Eric Clapton to play", "extracted_entity": null}
{"question": "has Anthrax been used in terrorism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the US.", "extracted_entity": null}
{"question": "has weaponized Anthrax caused deaths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It has caused illnesses, but", "extracted_entity": null}
{"question": "have there been deaths from Anthrax specifically after its use as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the 1970s", "extracted_entity": null}
{"question": "are African leopards the biggest wild cats in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "African leopards are not the biggest wild c", "extracted_entity": null}
{"question": "was Chuck Hagel born and raised in the American Midwest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in North Platte, Nebraska", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines once a European colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saint Vincent and the Grenadines was", "extracted_entity": null}
{"question": "can dogs eat brownies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Can dogs eat brownies?", "extracted_entity": null}
{"question": "was Excalibur wielded by King Henry VIII?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "is menthol associated with oral hygiene?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Menthol is a", "extracted_entity": null}
{"question": "is there a Star Wars movie that was only ever shown once?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was a Star Wars Christmas special,", "extracted_entity": null}
{"question": "have multiple actors portrayed the character of Anakin Skywalker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Hayden Christensen, Ewan McG", "extracted_entity": null}
{"question": "is cholera a viral infection of the respiratory system?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a bacterial infection", "extracted_entity": null}
{"question": "was New England involved in the American Revolutionary War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, New England was involved in the American Revolution", "extracted_entity": null}
{"question": "could Lil Wayne's children ride on a motorcycle together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Lil Wayne's children are all", "extracted_entity": null}
{"question": "could the ingredients in the Powerpuff Girls make a pumpkin pie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you put them in a pump", "extracted_entity": null}
{"question": "in the 1940's, were mood disorders treated in dangerous ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 1940'", "extracted_entity": null}
{"question": "would someone with a mood disorder in the 1940's be at risk for dangerous treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they would.", "extracted_entity": null}
{"question": "are there judges working at the top level of the United States federal government?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are judges working at the top", "extracted_entity": null}
{"question": "would a peregrine falcon enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be difficult to find out.", "extracted_entity": null}
{"question": "would a cat enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I've never", "extracted_entity": null}
{"question": "does United Airlines operate a hub in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, United Airlines operates a hub in London", "extracted_entity": null}
{"question": "do judges wear distinctive outfits while serving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, judges wear regular clothes.", "extracted_entity": null}
{"question": "did Kane lose an election to be a mayor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the East Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is the governor of California.", "extracted_entity": null}
{"question": "would you hire Michael Faraday to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's a physicist", "extracted_entity": null}
{"question": "did the Philadelphia Museum of Art as an institution predate the opening of its main building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The museum was founded in 18", "extracted_entity": null}
{"question": "does childbirth cause the mother to become a parent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Childbirth does not cause the mother", "extracted_entity": null}
{"question": "does Robert De Niro have at least double the number of Oscar nominations as actual Oscar wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. De Niro has 7 Oscar nomin", "extracted_entity": null}
{"question": "did Robert De Niro win an Oscar during the 1970s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he won an Oscar for the film,", "extracted_entity": null}
{"question": "is Boris Johnson in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not", "extracted_entity": null}
{"question": "was Achilles considered royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a demi-god.", "extracted_entity": null}
{"question": "does a butler appear in an American household in a popular sitcom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the show Frasier, N", "extracted_entity": null}
{"question": "would a podiatrist be familiar with Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a podiatrist would be familiar with", "extracted_entity": null}
{"question": "is there a DC superhero who has a butler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is not.", "extracted_entity": null}
{"question": "was the Enigma machine used by the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was used by the Nazis", "extracted_entity": null}
{"question": "was Fred Rogers involved in the Cuban Missile Crisis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a conscientious objector", "extracted_entity": null}
{"question": "if you enjoy Stephen King, are you likely to enjoy American Horror Story?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I haven\u2019t", "extracted_entity": null}
{"question": "does Wikipedia contain more knowledge than an encyclopedia set?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It is easier to search", "extracted_entity": null}
{"question": "did the Russian Revolution result in the establishment of the USSR?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Russian Revolution did not result in the", "extracted_entity": null}
{"question": "is Qantas headquarted on the eastern side of Australia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is headquarted on the", "extracted_entity": null}
{"question": "is Qantas headquarted in the Southern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Qantas is headquartered", "extracted_entity": null}
{"question": "are sweet potatoes a variety of fruit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are a variety of fruit.", "extracted_entity": null}
{"question": "is it possible to score over a hundred points in a single turn in Scrabble?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the Scrabble variant", "extracted_entity": null}
{"question": "is colitis a gastrointestinal disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, colitis is a gastrointest", "extracted_entity": null}
{"question": "is a complete Monty Python reunion show possible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, it is highly probable.", "extracted_entity": null}
{"question": "is French toast made using potatoes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is made using bread.", "extracted_entity": null}
{"question": "did Sojourner Truth climb to the top of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "was Sojourner Truth present the first time the Washington Monument opened?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was not.", "extracted_entity": null}
{"question": "was Sojourner Truth present at the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she was", "extracted_entity": null}
{"question": "did Sojourner Truth attend the opening of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was not invited.", "extracted_entity": null}
{"question": "did William Henry Harrison climb the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did President William Henry Harrison place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the first stone was placed by the Mar", "extracted_entity": null}
{"question": "did President William Henry Harrison place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but his grandson did in 1", "extracted_entity": null}
{"question": "did President Washington place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He never got to see the monument completed", "extracted_entity": null}
{"question": "did President Washington lay the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did President Washington dedicate the Washington Monument to himself?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "could Stephen Hawking climb the stairs of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (He can\u2019t even climb", "extracted_entity": null}
{"question": "could Zhang Zuolin write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "could Jean Jacques Dessalines write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he could not.", "extracted_entity": null}
{"question": "did Kit Carson create a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would Ronald Reagan have appointed a secretary for the US Department of Education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO! He would have abolished the department.", "extracted_entity": null}
{"question": "would a geisha be good at a  soir\u00e9e?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she knows how to entertain people.", "extracted_entity": null}
{"question": "can someone walk from New York to New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, someone can walk from New York to New", "extracted_entity": null}
{"question": "does New Delhi experience snow?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "can fleas survive on hamsters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fleas can survive on hamsters, but", "extracted_entity": null}
{"question": "can hamsters get fleas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "would it be easier to carry a dozen clementines than a dozen grapefruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the clementines are heavier", "extracted_entity": null}
{"question": "did Ringo Starr perform a concert in Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "did Ringo Starr perform at the Budokan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ringo Starr never performed at the", "extracted_entity": null}
{"question": "did Ringo Starr perform at Shea Stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ringo Starr did not perform at", "extracted_entity": null}
{"question": "would someone with curly hair dislike weather with high humidity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, curly hair would dislike weather with", "extracted_entity": null}
{"question": "did Howard Taft place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not at all.", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they expect it to taste like what it", "extracted_entity": null}
{"question": "is the Force an important part of a movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the force is very important.", "extracted_entity": null}
{"question": "is the Force prominently featured in a popular movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it isn't.", "extracted_entity": null}
{"question": "does oceanography involve multiple scientific disciplines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, oceanography involves multiple scientific disciplines.", "extracted_entity": null}
{"question": "was the 1980 presidential election won by a Republican?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Ronald Reagan.", "extracted_entity": null}
{"question": "are there guns at the United States Military Academy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are.", "extracted_entity": null}
{"question": "is it considered rude not to say Hello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is considered rude not to say", "extracted_entity": null}
{"question": "does the Silk Road pass through the Middle East?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the silk road passed through the Middle", "extracted_entity": null}
{"question": "are elk popular targets of hunters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "is World War III likely to involve nuclear weapons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not likely.", "extracted_entity": null}
{"question": "can paparazzi be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can be dangerous. They can cause", "extracted_entity": null}
{"question": "is The Hague in the Middle East?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is in the Middle East.", "extracted_entity": null}
{"question": "do watchmakers need an understanding of basic mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they need to know how to use the", "extracted_entity": null}
{"question": "do gray seals live on the North American coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they live on the North American coast.", "extracted_entity": null}
{"question": "is the Mona Lisa currently on display in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is on display at the Louvre", "extracted_entity": null}
{"question": "are the English currently directly involved in the Arab-Israeli conflict?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. They are directly involved in", "extracted_entity": null}
{"question": "is surfing featured in a Disney cartoon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, surfing is not featured in a", "extracted_entity": null}
{"question": "are snowshoes commonly worn in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, but they are in Canada", "extracted_entity": null}
{"question": "are snowshoes commonly worn in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "are goldfish good as a child's pet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They are a very popular choice. They are easy", "extracted_entity": null}
{"question": "are there slot machines in Maryland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are slot machines in Maryland. They", "extracted_entity": null}
{"question": "did Allosaurus live at the same time as Neanderthals?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Neanderthals lived about 1", "extracted_entity": null}
{"question": "is Call of Duty available on the N64?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the N64 is a cartridge", "extracted_entity": null}
{"question": "was the flag flying prominently during the Battle of Baltimore in 1814?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "does the flag change often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, every year the flag changes.", "extracted_entity": null}
{"question": "does part of the flag represent the American colonies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does. the 13 stripes", "extracted_entity": null}
{"question": "did Sojourner Truth attend the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she did not.", "extracted_entity": null}
{"question": "is the Mona Lisa a subject of historical debate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Mona Lisa is a subject of", "extracted_entity": null}
{"question": "is the Order of the British Empire associated with knighthood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Order of the British Empire is an", "extracted_entity": null}
{"question": "is there a title associated with members of the Order of the British Empire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are all knights.", "extracted_entity": null}
{"question": "is Austria the best alpine skiing team at the Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no, the best alpine ski", "extracted_entity": null}
{"question": "does Felicity Huffman have an Oscar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she has one", "extracted_entity": null}
{"question": "is Phobos beyond the Asteroid belt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is tourism beneficial for the host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it is because they can earn money from", "extracted_entity": null}
{"question": "is Kobe in eastern Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no he's in Africa", "extracted_entity": null}
{"question": "are lifeboats used in airplanes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they are used in boats.", "extracted_entity": null}
{"question": "does Mercury have sunsets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Mercury has sunsets.", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch completed a full rotation of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Indian Ocean garbage patch has completed a", "extracted_entity": null}
{"question": "could Goofy have eaten white chocolate in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Goofy was born in 1", "extracted_entity": null}
{"question": "is Goofy's age in human years an abundant number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Goofy's age is", "extracted_entity": null}
{"question": "is Goofy's age calculated in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Goofy is 24 years", "extracted_entity": null}
{"question": "is Goofy's age in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Goofy is a dog, so his age", "extracted_entity": null}
{"question": "is Goofy's age in dog years a semiprime number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "76", "extracted_entity": null}
{"question": "would a biologist concurrently study Adelie penguins and the Western honey bee in the field?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not at the same time.", "extracted_entity": null}
{"question": "could a Western honeybee sting an Adelie penguin's egg?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Western honeybee is", "extracted_entity": null}
{"question": "could a Western honeybee sting an Adelie penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Adelie penguins have no st", "extracted_entity": null}
{"question": "are there three countries touching the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "USA, Mexico and Cuba.", "extracted_entity": null}
{"question": "does the Gulf of Mexico border three countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the U.S., Mexico, and", "extracted_entity": null}
{"question": "is there a music video about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there is one about a guy", "extracted_entity": null}
{"question": "is there a popular song about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a popular song about nerd", "extracted_entity": null}
{"question": "did the word nerd first appear in a children's book?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The word nerd first appeared in a children'", "extracted_entity": null}
{"question": "did the word nerd first appear in literature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the word nerd first appeared in literature", "extracted_entity": null}
{"question": "is the Golden Gate Bridge on San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "can a citizen of United States buy morphine in a pharmacy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they have a prescription", "extracted_entity": null}
{"question": "were tanks invented in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1916, when", "extracted_entity": null}
{"question": "were tanks used in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, tanks were not used in the American", "extracted_entity": null}
{"question": "are the Great Lakes connected to the Atlantic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Great Lakes are connected to the", "extracted_entity": null}
{"question": "do hippos prefer fresh water to salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hippos prefer fresh water to salt", "extracted_entity": null}
{"question": "did land vertebrates live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, land vertebrates did live during the", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did. Dinosaurs are thought", "extracted_entity": null}
{"question": "are hospitals exempt from displays of institutional racism?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Hospitals are not exempt", "extracted_entity": null}
{"question": "can representations of goats cause some religious people anxiety?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not really", "extracted_entity": null}
{"question": "are some images of goats concerning to people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some images of goats are concerning to", "extracted_entity": null}
{"question": "are all rice puddings white traditionally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, rice pudding can be made with", "extracted_entity": null}
{"question": "does Ariana Grande's signature look combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ariana Grande's signature look comb", "extracted_entity": null}
{"question": "would you have luck pearl finding in Lake Champlain? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lake Champlain is a large lake that is located", "extracted_entity": null}
{"question": "is homeopathy real medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Homeopathy is not real medicine. It", "extracted_entity": null}
{"question": "is homeopathy medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not a medicine.", "extracted_entity": null}
{"question": "does the cuisine of Hawaii embrace foods considered distasteful in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. In fact, many Hawai", "extracted_entity": null}
{"question": "does the Cuisine of Hawaii have similarities with the cuisine of korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "i don't think so because the cuis", "extracted_entity": null}
{"question": "does the cuisine of Hawaii have any commonalities with that of South Korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The cuisine of Hawaii has", "extracted_entity": null}
{"question": "did Donald Trump invent the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Donald Trump did not invent the New York", "extracted_entity": null}
{"question": "do American's think chief executive officers have skills to make good US Presidents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "did the Great Depression affect the entirety of some people's childhoods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "does the cast of Finding Dory feature people from marginalized groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. The film features a cast", "extracted_entity": null}
{"question": "is it possible to get frostbite in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in New York City.", "extracted_entity": null}
{"question": "do infants rely on different senses than toddlers to identify their parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Infants rely more on their sense of", "extracted_entity": null}
{"question": "do infants perceive their parents differently than toddlers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Infants are more likely to perceive", "extracted_entity": null}
{"question": "are some breakfast items in British cuisine considered BBQ food in America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. Some breakfast items in British", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger be ineligible to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born in Austria.", "extracted_entity": null}
{"question": "is green skin a complaint caused by nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, green skin is not a complaint caused", "extracted_entity": null}
{"question": "is moss a fungus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Moss is a plant that is not a fung", "extracted_entity": null}
{"question": "would a modern central processing unit fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the size of a CPU is about", "extracted_entity": null}
{"question": "does an abacus have a central processing unit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "An abacus does not have a central processing", "extracted_entity": null}
{"question": "do Russian Cosmonauts go to hospitals on the moon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "do astronauts go to hospitals on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they have to go to the moon hospital", "extracted_entity": null}
{"question": "is there a hospital on the Moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a hospital on the Moon.", "extracted_entity": null}
{"question": "are hearts sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They are available in stores, as well", "extracted_entity": null}
{"question": "are elephant tusks sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and the ivory is not only from", "extracted_entity": null}
{"question": "are elephant tusks sold in retail stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was the first example of astrophotography a daguerreotype? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The first example of astrophotography was a", "extracted_entity": null}
{"question": "was Isaac Newton good at astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was.", "extracted_entity": null}
{"question": "is a mountaintop a good place for astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a mountaintop is a good place", "extracted_entity": null}
{"question": "do Crane hunt in packs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They hunt alone.", "extracted_entity": null}
{"question": "did DARPA pave the way for the atom bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and no. The atomic bomb was a", "extracted_entity": null}
{"question": "was the Atom Bomb funded by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Atom Bomb was funded by", "extracted_entity": null}
{"question": "did DARPA help build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. DARPA was created in 1", "extracted_entity": null}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Manhattan Project built the Atom", "extracted_entity": null}
{"question": "did DARPA build the first television?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the first television was built by John Log", "extracted_entity": null}
{"question": "did DARPA recruit Albert Einstein?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Einstein was recruited by the", "extracted_entity": null}
{"question": "did Albert Einstein apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not", "extracted_entity": null}
{"question": "did Thomas Edison apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was already working at DARPA", "extracted_entity": null}
{"question": "did Thomas Edison create DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, DARPA was created in 1", "extracted_entity": null}
{"question": "was Thomas Edison the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was Albert Einstein the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "did Albert Einstein work for DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Einstein was a member of the D", "extracted_entity": null}
{"question": "was the Atom Bomb invented by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Atom Bomb was invented by", "extracted_entity": null}
{"question": "did DARPA invent the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was the Manhattan Project.", "extracted_entity": null}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Atom Bomb was built by the", "extracted_entity": null}
{"question": "was the atom bomb created by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was created by the Manhattan Project", "extracted_entity": null}
{"question": "would Huckleberry Finn have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Chinese immigrants had been in America", "extracted_entity": null}
{"question": "is eleventh grade required to buy alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is illegal for anyone under the age", "extracted_entity": null}
{"question": "would Tom Sawyer have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tom Sawyer was set in the 1", "extracted_entity": null}
{"question": "is eleventh grade required to graduate elementary school?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, eleventh grade is required to gradu", "extracted_entity": null}
{"question": "do other animals use agriculture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, chimpanzees use agriculture", "extracted_entity": null}
{"question": "can agriculture be illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Agriculture cannot be illegal.", "extracted_entity": null}
{"question": "does Benadryl cause similar side effects to those of morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Benadryl is not a n", "extracted_entity": null}
{"question": "does morphine cause similar side effects as Benadryl?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Morphine is a strong opioid pain", "extracted_entity": null}
{"question": "has Ivan the Terrible flown on an airplane?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he hasn\u2019t.", "extracted_entity": null}
{"question": "has Donald Trump met with Ivan the Terrible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "did Ivan the Terrible fight the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "did Ivan the Terrible fight against Hitler?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ivan the Terrible fought against Hitler.", "extracted_entity": null}
{"question": "did Ivan the Terrible fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was the first tsar of Russia", "extracted_entity": null}
{"question": "did Ivan the Terrible fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ivan the Terrible was a Russian Ts", "extracted_entity": null}
{"question": "would Mount Rushmore be completely submerged in the Sea of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be completely submerged.", "extracted_entity": null}
{"question": "does the Sea of Japan have multiple names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Sea of Japan is also known as", "extracted_entity": null}
{"question": "did Isaac Newton read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was a big fan of Jean", "extracted_entity": null}
{"question": "are there more hours in a day than roman numeral symbols?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are more hours in a day than", "extracted_entity": null}
{"question": "are there fewer roman numeral symbols than letters in the alphabet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are fewer roman numeral symbols than", "extracted_entity": null}
{"question": "can roman numerals spell out words?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Roman numerals can be used to spell", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a laptop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He could have owned a laptop.", "extracted_entity": null}
{"question": "can microscopic objects be Small Solar System bodies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can be, but they are not", "extracted_entity": null}
{"question": "can flour float on water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, flour is a solid and can not", "extracted_entity": null}
{"question": "do amoebas descend from a variety of ancestors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "is there a serious disease caused by amoebas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's called amoebic", "extracted_entity": null}
{"question": "do the people in Nepal build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for sailing. They are", "extracted_entity": null}
{"question": "do the people in Liechtenstein build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they build yachts.", "extracted_entity": null}
{"question": "do the people in Uzbekistan build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "was Aristotle considered to be a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Aristotle was not considered to be", "extracted_entity": null}
{"question": "is one of the important places in Irish mythology located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "T\u00edr na n\u00d3g", "extracted_entity": null}
{"question": "is \"The Land of the Young\" a place name in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's the land of the young", "extracted_entity": null}
{"question": "does Irish mythology have the concept of an underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does. The underworld is called", "extracted_entity": null}
{"question": "is someone who owns a cloak fastened by eight stones an important figure in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lugh", "extracted_entity": null}
{"question": "was dynamite patented after the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, dynamite was patented after the American", "extracted_entity": null}
{"question": "was dynamite invented during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was invented in 18", "extracted_entity": null}
{"question": "was General Pompey a governor of a region that contains what is now Basque Country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a governor of the region that", "extracted_entity": null}
{"question": "was General Pompey a governor of a region where the capital was Corduba?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a governor of Corduba.", "extracted_entity": null}
{"question": "was General Pompey a contemporary of a queen who died of poison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Queen Cleopatra of Egypt.", "extracted_entity": null}
{"question": "was General Pompey a contemporary of Cleopatra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "do Asian black bear eat avacado? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do Asian black bear eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Asian black bears do not eat p", "extracted_entity": null}
{"question": "can you order sea otter from the menu at Long John Silvers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to ask for it.", "extracted_entity": null}
{"question": "can you order sea otter at seafood restaurants? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot order sea otter at sea", "extracted_entity": null}
{"question": "do sea otters eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sea otters eat penguins, but they", "extracted_entity": null}
{"question": "can you buy sea otter meat at the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can\u2019t buy sea otter", "extracted_entity": null}
{"question": "do sea otters lay eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, sea otters do not lay eggs.", "extracted_entity": null}
{"question": "can a sloth stand on a skateboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they can't", "extracted_entity": null}
{"question": "are fish living in a man-made environment in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, fish live in a natural environment in the", "extracted_entity": null}
{"question": "are freshwater fish in an aquarium living in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Freshwater fish are living in fresh", "extracted_entity": null}
{"question": "is rest at home the best treatment for suspected strokes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Strokes are serious and need immediate", "extracted_entity": null}
{"question": "is spinach popular in breakfast drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, spinach is popular in breakfast drinks", "extracted_entity": null}
{"question": "could mandatory birth control prevent human overpopulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is an effective means of birth control", "extracted_entity": null}
{"question": "does the human body produce vitamin C?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the human body does not produce vitamin", "extracted_entity": null}
{"question": "could you work as a bagger at Aldi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I could work as a bagger at", "extracted_entity": null}
{"question": "would Charles Darwin be considered a \"spoonie\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was a chronic illness suffer", "extracted_entity": null}
{"question": "are all suicide attempts done with intent to die?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, many suicide attempts are done with the", "extracted_entity": null}
{"question": "has a sports game ever featured Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it has featured a prison break.", "extracted_entity": null}
{"question": "has Alcatraz been replicated in any video games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the video game Call of Dut", "extracted_entity": null}
{"question": "would Tony Hawk game fans be familiar with the layout of Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it doesn\u2019t matter because the", "extracted_entity": null}
{"question": "are proteins negatively impacted by fevers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, proteins are not negatively impacted", "extracted_entity": null}
{"question": "did prisoners at Auschwitz struggle with hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the prisoners at Auschwitz struggled", "extracted_entity": null}
{"question": "could a craft be made with cloves and apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cloves are the dried buds of the", "extracted_entity": null}
{"question": "is Excalibur featured in the Zelda games?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Excalibur appears", "extracted_entity": null}
{"question": "was Charles Manson familiar with self mutilation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had cut off the middle finger of", "extracted_entity": null}
{"question": "would it be hard to confuse a poblano pepper for a jalapeno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be hard to confuse a pobl", "extracted_entity": null}
{"question": "are teenagers a big demographic for acne products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, teenagers are a big demographic", "extracted_entity": null}
{"question": "if you're born in the autumn portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you're a Virgo.", "extracted_entity": null}
{"question": "if you're born in the fall portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you were born in the fall portion", "extracted_entity": null}
{"question": "are there cities where the leaves stay green all autumn? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are cities where the leaves stay green", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have a mental health condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have SAD?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If they are using a lightbox in", "extracted_entity": null}
{"question": "are relationships with two husbands considered immoral by some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many people believe that relationships with two hus", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Wake County NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, snowboarding is a popular winter sport", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Raleigh, NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, snowboarding is a popular winter sport", "extracted_entity": null}
{"question": "can conditioner fix damaged hair?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, conditioner can help to repair damaged", "extracted_entity": null}
{"question": "in Doctor Who, is River Song the Doctor's daughter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She is the Doctor's daughter.", "extracted_entity": null}
{"question": "is it normal to be waiting for tax forms from your employer in March?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Your employer should have sent you a", "extracted_entity": null}
{"question": "would pears, cheese, and lettuce be served on a platter in some locations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would a Chihuahua with a deer head be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A deer head is not the right", "extracted_entity": null}
{"question": "would a Chihuahua with a longer nose be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say yes.", "extracted_entity": null}
{"question": "would an Apple Head Chihuahua be likely cost more in vet bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Apple head Chihuahuas are", "extracted_entity": null}
{"question": "would you be more likely to have higher vet bills with an apple head chihuahua? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are more prone to health issues", "extracted_entity": null}
{"question": "would students at University of North Carolina at Chapel Hill want air conditioning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course, the students would want air", "extracted_entity": null}
{"question": "does Kesha include details in her video that people associate with the Illuminati?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the all seeing eye and the number", "extracted_entity": null}
{"question": "are multiple claims by chiropractic practitioners unfounded?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are unfounded.", "extracted_entity": null}
{"question": "did Alaskan soldiers fight in the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Alaska was part of the Union.", "extracted_entity": null}
{"question": "would Peter Dinklage be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not eligible.", "extracted_entity": null}
{"question": "can someone with Down Syndrome compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Down Syndrome is not a", "extracted_entity": null}
{"question": "can a legally blind person compete in swimming at the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they have to use a special kind", "extracted_entity": null}
{"question": "can someone who is blind compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "is Snoopy considered \"man's best friend\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. because he is a dog.", "extracted_entity": null}
{"question": "does the Amazon rainforest have Colobinae?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "does the Amazon rainforest have Pygathrix?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "do Pygathrix live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they live in Vietnam and Laos.", "extracted_entity": null}
{"question": "do Cercopithecoidea live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they live in the Amazon rainforest", "extracted_entity": null}
{"question": "do talapoin live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in the Ivory Coast", "extracted_entity": null}
{"question": "do  Cercopithecidae live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "is the Amazon rainforest a completely wild forest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Norse sailors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Amazon Rainforest was not one", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Amazon Rainforest was not one", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Marco Polo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Amazon Rainforest was not one", "extracted_entity": null}
{"question": "did Marco Polo travel to the Amazon Rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Marco Polo traveled to China", "extracted_entity": null}
{"question": "could Bart Simpson have owned comics with The Joker in them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he was a child", "extracted_entity": null}
{"question": "has the character of the Joker lasted longer than the character of Ninja-K?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by 2 years.", "extracted_entity": null}
{"question": "does a double-blind trial weaken the placebo effect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a double-blind trial does not", "extracted_entity": null}
{"question": "is the use of a placebo considered somewhat unethical?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. In fact, it", "extracted_entity": null}
{"question": "can you get negative effects from a placebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a placebo is a sugar pill that", "extracted_entity": null}
{"question": "has Broadway featured performances set with the Napoleonic Wars as part of the background?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Well, I think so. But I\u2019m", "extracted_entity": null}
{"question": "have several broadway musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, several broadway musicals have been written", "extracted_entity": null}
{"question": "have several musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. For example, Les Miserables is", "extracted_entity": null}
{"question": "have multiple musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and the answer is at the bottom of", "extracted_entity": null}
{"question": "can people taking their BTEC's be parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can be any age to take a", "extracted_entity": null}
{"question": "is Michael Scott from The Office known for his leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is known for his leadership skills.", "extracted_entity": null}
{"question": "is Michael Scott from The Office known for his strong leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "did the Brazilian navy fight Portugal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Brazilian Navy was founded on 12", "extracted_entity": null}
{"question": "can Python scripts call the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, see http://en.wikipedia.org", "extracted_entity": null}
{"question": "can Bulbapedia be accessed with the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "was Hamlet first shown in an IMAX 3D theater?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not at the first showing.", "extracted_entity": null}
{"question": "can the MediaWiki API be connected directly to Wikipedia articles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be done by querying the", "extracted_entity": null}
{"question": "at Christmastime, are there reminders of groundhog day coming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I think the two are unrelated.", "extracted_entity": null}
{"question": "does the United States Department of Defense have a balanced budget?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the United States Department of Defense does", "extracted_entity": null}
{"question": "does Bojack Horseman have an ongoing parody of penguin books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it does have an ongoing par", "extracted_entity": null}
{"question": "would a Doctor of Medicine in orthopedics be appropriate for a compound fracture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a doctor of medicine in orthopedics", "extracted_entity": null}
{"question": "is Redbird Smith a religious leader for the Cherokee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a religious leader.", "extracted_entity": null}
{"question": "was Sasha Obama friends with John Lennon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Sasha Obama was not friends with", "extracted_entity": null}
{"question": "if you've passed your A Levels, are you aware of the Constitution of the United Kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've passed my A Levels.", "extracted_entity": null}
{"question": "is the city that hosts the French Open also known as the Windy City?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paris.", "extracted_entity": null}
{"question": "did Lionel Messi ever play in the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Messi never played in the French Open", "extracted_entity": null}
{"question": "did Lionel Messi win at the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he won the men\u2019s singles title", "extracted_entity": null}
{"question": "do DJ's for weddings typically have Bryan Adams in their playlist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "is Bryan Adams frequently played at weddings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. I have been to a", "extracted_entity": null}
{"question": "could a farmer make both soy milk and tempeh from the same crop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Soybeans can be fermented", "extracted_entity": null}
{"question": "did President Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact, he was a streamer", "extracted_entity": null}
{"question": "did Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he played it a lot.", "extracted_entity": null}
{"question": "can I buy League of Legends in the Android App Store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not available in the Android App", "extracted_entity": null}
{"question": "can I play League of Legends on a Nintendo Entertainment System?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I think you can play it on", "extracted_entity": null}
{"question": "are there enough shogi game pieces to replace missing chess pieces?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are not enough shogi game", "extracted_entity": null}
{"question": "does Kim Kardashian shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she shops at Tesco.", "extracted_entity": null}
{"question": "does Malia Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn't.", "extracted_entity": null}
{"question": "does President Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "did the Byzantine calendar have a different new year than the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the new year was March 25", "extracted_entity": null}
{"question": "wouldna hoverboard take beyond two days to finish the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Daytona 500", "extracted_entity": null}
{"question": "would being in a lifeboat after a wreck be unsafe during a tropical storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not unless the lifeboat was", "extracted_entity": null}
{"question": "did biologist Rachel Carson write critiques of the FDA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did", "extracted_entity": null}
{"question": "can an Alaskan gardener sow eggplant directly outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is possible, but eggplants are not", "extracted_entity": null}
{"question": "would lolcats and roflcopters be known to World of Warcraft players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "is coca leaf good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Coca leaf is a stimulant and can", "extracted_entity": null}
{"question": "is coca leaf a good diet supplement for weight gain?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Coca leaf is a natural source of caff", "extracted_entity": null}
{"question": "would cultural anthropologists document historic cultivation of coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because coca is a traditional crop", "extracted_entity": null}
{"question": "would botanists document coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the forest is known for its diversity", "extracted_entity": null}
{"question": "is copper a good choice for an artwork that changes color over time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, copper is a good choice for an", "extracted_entity": null}
{"question": "can copper inhibit parasitic spores?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can't", "extracted_entity": null}
{"question": "can a painter use copper so images of trees remain vibrant in sunlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, copper is a metal that is often", "extracted_entity": null}
{"question": "did Elizabeth I of England wear gowns of viscose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Viscose was invented in", "extracted_entity": null}
{"question": "would a fungal life-form be wary of green pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it would be a food source.", "extracted_entity": null}
{"question": "can a derivative of copper end a fungal infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "can goldfish breed in saltwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, goldfish cannot breed in saltwater", "extracted_entity": null}
{"question": "are goldfish friendly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are friendly and they like to be", "extracted_entity": null}
{"question": "are paratroopers good at search and rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are good at getting the fuck", "extracted_entity": null}
{"question": "is Higher Education required to teach high school in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is required to teach college.", "extracted_entity": null}
{"question": "would spaghetti in the Philippines be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be plain.", "extracted_entity": null}
{"question": "can Spaghetti be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In Italy, Spaghetti is used", "extracted_entity": null}
{"question": "can onion peel be used for things?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can use onion peel to", "extracted_entity": null}
{"question": "can onion peel be repurposed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be used to make tea.", "extracted_entity": null}
{"question": "would it be inappropriate to put a Flag of the United States on the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, that is disrespectful.", "extracted_entity": null}
{"question": "should someone doing mixed martial arts have health insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Absolutely! If you are going to be", "extracted_entity": null}
{"question": "is Futurama a movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Futurama is a TV show", "extracted_entity": null}
{"question": "can you book a ticket on a fighter jet on British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can book a ticket on a f", "extracted_entity": null}
{"question": "can you book a ticket on a fighter jet through British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't book a ticket on", "extracted_entity": null}
{"question": "does British Airways operate a fleet of trains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it operates a fleet of aircraft.", "extracted_entity": null}
{"question": "has British Airways flown in space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but they did fly a 74", "extracted_entity": null}
{"question": "can human overpopulation be stopped?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It can't.", "extracted_entity": null}
{"question": "is Ariana Grande old enough to drive a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is 20 years old and", "extracted_entity": null}
{"question": "can water be bad for a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cactus can be damaged by water.", "extracted_entity": null}
{"question": "can cows get Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cows cannot get Cholera.", "extracted_entity": null}
{"question": "do authorities always respond calmly to sit-ins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, sometimes they don't.", "extracted_entity": null}
{"question": "can you survive Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have access to medical care.", "extracted_entity": null}
{"question": "is Niagara Falls a national park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not a national park. Ni", "extracted_entity": null}
{"question": "is Niagara falls the tallest waterfall in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. Dufresne", "extracted_entity": null}
{"question": "is Niagara falls the tallest waterfall in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Niagara Falls is the tall", "extracted_entity": null}
{"question": "was Niagara Falls formed by a meteorite? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was formed by a meteorite.", "extracted_entity": null}
{"question": "is Niagara falls in mexico?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is in the USA.", "extracted_entity": null}
{"question": "do salmon migrate up Niagara Falls to spawn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they migrate up the Falls on", "extracted_entity": null}
{"question": "can salmon swim up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can't swim up Ni", "extracted_entity": null}
{"question": "do salmon travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they travel down the falls.", "extracted_entity": null}
{"question": "do ships travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "are there various types of barn floors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many different types of floors", "extracted_entity": null}
{"question": "can a tailor adjust a kimono?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a tailor can adjust a kim", "extracted_entity": null}
{"question": "is October a month during which people seek out fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Halloween is the", "extracted_entity": null}
{"question": "is fear a common emotion experienced at Universal Studios Orlando?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "did the Moon landing occur before the first episode of Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Moon landing occurred in 19", "extracted_entity": null}
{"question": "do people who attend AA meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES. It is a common feeling.", "extracted_entity": null}
{"question": "do people who attend drug abuse counseling meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people who attend drug abuse counsel", "extracted_entity": null}
{"question": "do people who attend AlAnon meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have never heard anyone express shame for attending", "extracted_entity": null}
{"question": "does the Sahara desert have a big variety of plant life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does, there are about 20", "extracted_entity": null}
{"question": "was there a famous director who came from Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ron Howard.", "extracted_entity": null}
{"question": "was a Happy Days actor also in Little Nicky?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ron Howard", "extracted_entity": null}
{"question": "was a member of the Happy Days cast also in movies with Adam Sandler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Potsie was played by Anson Williams.", "extracted_entity": null}
{"question": "can you kayak to Saint Vincent and the Grenadines from Montreal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You would need to go to the south", "extracted_entity": null}
{"question": "are goldfish bottom feeders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, goldfish are bottom feeders.", "extracted_entity": null}
{"question": "for movies with a 'G' rating, are teens an appropriate audience?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "is cellulose important for bowel maintenance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The cellulose is important for bowel maintenance", "extracted_entity": null}
{"question": "can cellulose help with constipation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "can a rainbow be manmade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A rainbow can be manmade.", "extracted_entity": null}
{"question": "can rainbows occur without rain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because rainbows are created by light", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A vet would use DEXA", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a jellyfish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. that would be silly.", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think so.", "extracted_entity": null}
{"question": "would a veterinarian use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dual-energy X-ray absor", "extracted_entity": null}
{"question": "can all plants withstand frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Some plants are more cold tolerant than", "extracted_entity": null}
{"question": "is the United States Secretary of State a woman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Condoleeza Rice", "extracted_entity": null}
{"question": "will people vote United States Secretary of State this November?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they will.", "extracted_entity": null}
{"question": "are all parts of the aloe plant tasty?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the sap is tasty, the", "extracted_entity": null}
{"question": "could you keep a bengal fox as a pet in New York?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is illegal to own a wild animal", "extracted_entity": null}
{"question": "is a Haiku a type of painting?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no it's a type of poetry", "extracted_entity": null}
{"question": "are fashion design jobs only available to women?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Absolutely not. Men can be fashion design", "extracted_entity": null}
{"question": "can Dungeons and Dragons be played with the participants in different homes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. With the use of a webcam and", "extracted_entity": null}
{"question": "is Kayaking possible for double arm amputees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, we have a double arm amputee", "extracted_entity": null}
{"question": "do Snow Leopards have valuable parts that make them likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have valuable fur and bones that", "extracted_entity": null}
{"question": "do Snow Leopards have attributes that make them more likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. Their fur is white,", "extracted_entity": null}
{"question": "would a crime scene cleanup crew have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, chlorine can be used to clean", "extracted_entity": null}
{"question": "would a medical sanitation company have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a medical sanitation company would have use", "extracted_entity": null}
{"question": "can Short-Eared Dogs still get ear mites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can. They are not immune", "extracted_entity": null}
{"question": "can green makeup help reduce rosacea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Green makeup will not reduce rosace", "extracted_entity": null}
{"question": "would a spiral fracture require medical imaging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "did Claude Monet fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was too old.", "extracted_entity": null}
{"question": "was Claude Monet famous for playing basketball?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was famous for painting.", "extracted_entity": null}
{"question": "did Claude Monet teach Salvador Dali how to paint?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Claude Monet did not teach Salvador D", "extracted_entity": null}
{"question": "did Claude Monet take painting classes from Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Claude Monet was not taught to paint", "extracted_entity": null}
{"question": "was the the Pony Express was used to deliver mail in 1901?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Pony Express was used to deliver", "extracted_entity": null}
{"question": "is it strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not strange to play happy", "extracted_entity": null}
{"question": "would it be strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not if you are happy.", "extracted_entity": null}
{"question": "would it be strange to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the funeral was for", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet when flying overhead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no noise ordinance in Orange", "extracted_entity": null}
{"question": "would an Arctic fox stand out against a pile of roasted coffee beans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the roasted coffee beans are", "extracted_entity": null}
{"question": "would an Arctic fox stand out against a pile of coal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because coal is black.", "extracted_entity": null}
{"question": "would Alan Turing be looked down upon at Pride parades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. I think he", "extracted_entity": null}
{"question": "can you go ice fishing for anchovy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think that the answer is yes.", "extracted_entity": null}
{"question": "is murder something the Joker would consider?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is crazy and doesn\u2019", "extracted_entity": null}
{"question": "can Anchovy walk on land?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Anchovies can walk on land", "extracted_entity": null}
{"question": "would a chiropractor be able to treat dystonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a chiropractor can help treat d", "extracted_entity": null}
{"question": "do people often give ducks food that is bad for them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "has Alan Rickman spend many hours in a black robe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has.", "extracted_entity": null}
{"question": "would the 10th Doctor on Doctor Who refuse a pear salad?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "would an explosion at a gunpowder storage facility result in a sonic boom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The explosion would create a shock wave", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Russian minor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not impossible.", "extracted_entity": null}
{"question": "would it be pointless to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Why?", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Saba scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. You can use", "extracted_entity": null}
{"question": "would it be unusual to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would it be unusual to play happycore at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I'm sure you'd get", "extracted_entity": null}
{"question": "would it be unusual to play happycore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be very unusual to play happy", "extracted_entity": null}
{"question": "could over reproduction of some species be unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, over reproduction of some species could be", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the pentatonic scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it would be pointless.", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Han Dynasty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible to use the", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Kuiper Belt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique on Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be impossible to use the twelve", "extracted_entity": null}
{"question": "would it have been impossible to use the twelve-tone technique in New Amsterdam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the technique is not a fixed set", "extracted_entity": null}
{"question": "would it have been impossible for Jane Austen to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the twelve-tone technique was", "extracted_entity": null}
{"question": "would it have been impossible for Neanderthals to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course not. They could have done it,", "extracted_entity": null}
{"question": "would it have been impossible for Shakespeare to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a genius and a master of", "extracted_entity": null}
{"question": "would it have been impossible for Beethoven to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would have been impossible for Beethoven to", "extracted_entity": null}
{"question": "could A. D. Condo enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he couldn\u2019t, because he was", "extracted_entity": null}
{"question": "could a World War I veteran enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he wouldn\u2019t have been able", "extracted_entity": null}
{"question": "could a World War I veteran enjoy the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it was developed after World War I", "extracted_entity": null}
{"question": "could Chinook salmon benefit someone with a major mood disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "would Chinook salmon benefit persons with heart disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chinook salmon is an excellent", "extracted_entity": null}
{"question": "did Auguste Comte define the study of society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "could Oprah Winfrey buy multiple private islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could buy multiple private islands.", "extracted_entity": null}
{"question": "is Oprah Winfrey part of the largest generation of the 20th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes she is.", "extracted_entity": null}
{"question": "did Spirit go further than the winner of the Birell Prague Grand Prix?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Spirit finished 3rd and the winner", "extracted_entity": null}
{"question": "did Spirit go further than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he went 5000.", "extracted_entity": null}
{"question": "did Spirit cover more ground than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but just barely.", "extracted_entity": null}
{"question": "are fossil fuels costing fishermen jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (1)", "extracted_entity": null}
{"question": "could people sing songs from the Sound of Music before 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was released in 196", "extracted_entity": null}
{"question": "does b\u00f6rek share an origin with the Dolmabah\u00e7e Palace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, both were made in the same kitchen,", "extracted_entity": null}
{"question": "would it be unusual to play Happy hardcore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think it would be unusual.", "extracted_entity": null}
{"question": "can someone with lactose intolerance easily enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, lactose intolerance is", "extracted_entity": null}
{"question": "can someone with lactose intolerance enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sure! B\u00f6rek is a great way to", "extracted_entity": null}
{"question": "has the Toyota Supra lost the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s just that the Kentucky Derby", "extracted_entity": null}
{"question": "has the Toyota Supra won the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it has won the Daytona", "extracted_entity": null}
{"question": "was the Forbidden City constructed using a backhoe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and a bulldozer.", "extracted_entity": null}
{"question": "was the Forbidden City constructed using a bulldozer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Forbidden City was built in", "extracted_entity": null}
{"question": "is hunger more unsafe for diabetics than health people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, hunger is not more unsafe for di", "extracted_entity": null}
{"question": "would sloths be at great risk in a forest fire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are slow and can\u2019t", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that shows female breasts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the 1998 Ses", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that has nudity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There's never been any nud", "extracted_entity": null}
{"question": "would someone with multiple lip piercings pay more in dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. there is a risk of infection and", "extracted_entity": null}
{"question": "would a fan of the Pokemon games be likely to like Nintendo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Pokemon games are made by N", "extracted_entity": null}
{"question": "is it likely that Dan Harmon would enjoy parody?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is likely that Dan Harmon would", "extracted_entity": null}
{"question": "was the United States Secretary of Defense enlisted for fewer years than the man who replaced Barry Goldwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Donald Rumsfeld", "extracted_entity": null}
{"question": "could a smallpox vaccine have changed the outcome of the American Indian Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The American Indian Wars were a series of conflicts between", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes work the same waterways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes use the same area for work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sometimes farmers and cargo ships use the", "extracted_entity": null}
{"question": "did Aristotle ever see Cats the musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Martin Luther King Jr able to vote because of the 23rd amendment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the 23rd Amendment", "extracted_entity": null}
{"question": "did the 23rd Amendment help Washington become president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did. The 23rd", "extracted_entity": null}
{"question": "did the 23rd Amendment help Teddy Roosevelt get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because he got a lot of votes", "extracted_entity": null}
{"question": "did the 23rd Amendment help JFK get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was the electoral college", "extracted_entity": null}
{"question": "could Washington D.C. residents vote for John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Washington D.C. residents could vote", "extracted_entity": null}
{"question": "is the amendent granting DC residents the right to vote part of the Bill of Rights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Bill of Rights is the first ten", "extracted_entity": null}
{"question": "is Pan a child of Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He is the son of Zeus and", "extracted_entity": null}
{"question": "would a student in ancient Thessaly have learned about Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not part of their culture", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an Emmy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But I'm still rooting for", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with John Williams for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "can I visit Saint Vincent and the Grenadines by airplane?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. You can fly to Saint", "extracted_entity": null}
{"question": "can I visit Saint Vincent and the Grenadines on a boat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can visit Saint Vincent and the Gren", "extracted_entity": null}
{"question": "can the Abitur be granted to German Shepherds?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact, they are one of the", "extracted_entity": null}
{"question": "can German shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can German Shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can, but it is not easy", "extracted_entity": null}
{"question": "can German Shepherds acquire the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "could John Key be inaugurated as president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is not a US citizen", "extracted_entity": null}
{"question": "could John Key be inaugurated as president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he couldn\u2019t.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the Commander in Chief of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO, he is not a US citizen", "extracted_entity": null}
{"question": "could John Key serve a four-year term as America's Commander in Chief?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. John Key is a New Zealander", "extracted_entity": null}
{"question": "could John Key serve a four-year term as America's president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's a New Zealander", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's not American.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He is not a US citizen.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He's a New Zealander", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he was born in the United States", "extracted_entity": null}
{"question": "could John Key serve as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. John Key is a New Zealander", "extracted_entity": null}
{"question": "could John Key serve as the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "could John Key serve as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he\u2019s not an American citiz", "extracted_entity": null}
{"question": "could John Key become the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the American people would vote for him", "extracted_entity": null}
{"question": "could John Key become the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not a chance.", "extracted_entity": null}
{"question": "could John Key become the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a New Zealander.", "extracted_entity": null}
{"question": "would diepoxybutane be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Diepoxybutane is a chemical", "extracted_entity": null}
{"question": "would butane diepoxide be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, butane diepoxide is not", "extracted_entity": null}
{"question": "would butane diepoxide be present in the ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, butane diepoxide would be", "extracted_entity": null}
{"question": "would the ideal drinking water contain butane diepoxide?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the ideal drinking water does not contain", "extracted_entity": null}
{"question": "would the ideal drinking water contain strychnine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it would not.", "extracted_entity": null}
{"question": "would the ideal drinking water contain anthrax?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn\u2019t.", "extracted_entity": null}
{"question": "is Steve Ballmer unable to use a keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think he can use a keyboard,", "extracted_entity": null}
{"question": "is it unnecessary to purchase food for a Lolcat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lolcats do not eat.", "extracted_entity": null}
{"question": "would members of the Communist Party USA find a billionaire class reprehensible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "does the musical Rent feature New Years day twice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it features it twice, first in the", "extracted_entity": null}
{"question": "would Bird Person be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Bird Person would consider Pluto a planet", "extracted_entity": null}
{"question": "would Summer Smith be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Summer is a very smart and educated person", "extracted_entity": null}
{"question": "would Summer be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Summer is not a scientist and would not be", "extracted_entity": null}
{"question": "would Rick be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rick would not consider Pluto a planet", "extracted_entity": null}
{"question": "does Rick and Morty discuss the status of Pluto as a planet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the episode \"Rick Potion", "extracted_entity": null}
{"question": "did the Big Bang create both matter and antimatter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the Big Bang created equal amounts of matter", "extracted_entity": null}
{"question": "can the time of the Big Bang be determined by telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Astronomers have determined the age of the", "extracted_entity": null}
{"question": "could Jackie Chan learn Jujutsu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is willing to learn.", "extracted_entity": null}
{"question": "would the Ku Klux Klan welcome Alicia Garza into their group?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. She is not white.", "extracted_entity": null}
{"question": "would the Ku Klux Klan accept Alicia Garza?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. Maybe.", "extracted_entity": null}
{"question": "would Alicia Garza be appreciated by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Ku Klux Klan has a long history", "extracted_entity": null}
{"question": "would Alicia Garza be accepted into the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "would Alicia Garza be accepted by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Alicia Garza is a les", "extracted_entity": null}
{"question": "can paratroopers be used on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a very, very", "extracted_entity": null}
{"question": "did the United States use paratroopers in World War One?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they were used by the Germans", "extracted_entity": null}
{"question": "would only warm weather attire be acceptable on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be unbearably hot", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his parents for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. And if he had, they would have", "extracted_entity": null}
{"question": "did J. D. Salinger ever celebrate a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a man.", "extracted_entity": null}
{"question": "did J. D. Salinger have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He did not.", "extracted_entity": null}
{"question": "did J. D. Salinger ever have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "would it be very unlikely for Christopher Hitchens to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He is a journalist.", "extracted_entity": null}
{"question": "hypothetically, could the entire 2018 class at Princeton watch a match at Wembley Arena?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the 2018 class", "extracted_entity": null}
{"question": "does acid rain damage the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Louvre is protected by glass", "extracted_entity": null}
{"question": "was the American Constitution signed before the Louvre was built?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Louvre was built in 1", "extracted_entity": null}
{"question": "did Olympia, Washington become a city after the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Olympia, Washington became a city after", "extracted_entity": null}
{"question": "was Olympia, Washington around during the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as Olympia, Washington.", "extracted_entity": null}
{"question": "could a drag race be held on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the drag strip would be too narrow", "extracted_entity": null}
{"question": "can a truck drive on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a truck cannot drive on a bou", "extracted_entity": null}
{"question": "is Kim Kardashian the first guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is the first guru.", "extracted_entity": null}
{"question": "is President Trump a guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Trump is not a guru. He", "extracted_entity": null}
{"question": "is street art allowed under the First Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES, because the 1st Amendment", "extracted_entity": null}
{"question": "was Alaska part of the Union during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Alaska was purchased from Russia in", "extracted_entity": null}
{"question": "can League of Legends be played on a DSL connection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, League of Legends can be played on", "extracted_entity": null}
{"question": "would the League of Legends download file fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the download file is 2.4", "extracted_entity": null}
{"question": "would the League of Legends download fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "100% yes.", "extracted_entity": null}
{"question": "would the League of Legends download fit on a Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the League of Legends download would fit", "extracted_entity": null}
{"question": "could a high school senior in 2020 have played League of Legends when it was new?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the game was released in 20", "extracted_entity": null}
{"question": "has Leage of Legends been around more years than Fortnite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Leage of Legends was founded in 2", "extracted_entity": null}
{"question": "is the Bible considered to be a parody?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Bible is a serious work of literature", "extracted_entity": null}
{"question": "are goldfish friendly to eachother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are friendly to each other.", "extracted_entity": null}
{"question": "can you buy chlorine at a supermarket?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can buy chlorine at a", "extracted_entity": null}
{"question": "would honey have been accessible to ancient romans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, honey was accessible to ancient romans", "extracted_entity": null}
{"question": "would honey have been accessible to Julius Caesar? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Honey was a luxury item, and would", "extracted_entity": null}
{"question": "is some of the weight of a papaya inedible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the weight of the papaya is in", "extracted_entity": null}
{"question": "is Dustin Hoffman unlikely to support Mike Pence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is unlikely to support Mike Pence", "extracted_entity": null}
{"question": "did Christopher Nolan meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet  former President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christopher Nolan was born in 1", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. And he never will.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not that I am aware of.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet Judy Garland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She died in 1969", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet President Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in 197", "extracted_entity": null}
{"question": "are goldfish bowls bad for goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are. They are too small and", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but he did meet John Lennon", "extracted_entity": null}
{"question": "could Aristotle have gone to school to be a railroad engineer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a Greek.", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines a colony when Elton John was born?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Saint Vincent and the Grenadines was", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines a colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was a colony of the United", "extracted_entity": null}
{"question": "does Bill Gates experience menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a man.", "extracted_entity": null}
{"question": "does Bill Gates have trouble dealing with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "does Bill Gates have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a man.", "extracted_entity": null}
{"question": "does Bill Gates frequently have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so.", "extracted_entity": null}
{"question": "does the rover Spirit have a spirit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a rover spirit.", "extracted_entity": null}
{"question": "is a curling iron helpful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a curling iron is helpful in cur", "extracted_entity": null}
{"question": "is a curling iron useful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is useful in curling.", "extracted_entity": null}
{"question": "is a musket required for the French Defence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is an option.", "extracted_entity": null}
{"question": "would a casino goer's '7' charm bracelet be likely for luck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The charm bracelet is a charm brace", "extracted_entity": null}
{"question": "does Stephen King disagree with J.K Rowling on trans issues?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "are there still prisoners being kept in Alcatraz island?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the prison was closed in 19", "extracted_entity": null}
{"question": "can you see Alcatraz Island from the Atlantic City boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't see Alcatraz", "extracted_entity": null}
{"question": "can you see Alcatraz Island from the Jersey Shore boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not from the boardwalk.", "extracted_entity": null}
{"question": "would Lord Voldemort have been permitted into Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would have been expelled immediately.", "extracted_entity": null}
{"question": "would sandals be unsafe at a construction site?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not recommended to wear sandals at a", "extracted_entity": null}
{"question": "does Doctor Who represent interracial couples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Doctor has been paired with a", "extracted_entity": null}
{"question": "in Doctor Who, can statues be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "in Doctor Who, can statues be threatening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes they can.", "extracted_entity": null}
{"question": "can people meet each other out of order in Doctor Who?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "are there family planning options without hormones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are several options for family planning without", "extracted_entity": null}
{"question": "did Gorbachev influence the Russian Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did, he helped bring down the Berlin", "extracted_entity": null}
{"question": "would Tom Cruise consider his Katie Holms a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is a suppressive person.", "extracted_entity": null}
{"question": "would Tom Cruise consider his ex-wife a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was an SP to him.", "extracted_entity": null}
{"question": "would Tom Cruise be unlikely to support psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tom Cruise is unlikely to support psychiatry", "extracted_entity": null}
{"question": "is Tom Cruise opposed to psychiatric medication??", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's opposed to psychiatric", "extracted_entity": null}
{"question": "is Tom Cruise opposed to psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is opposed to the use of psych", "extracted_entity": null}
{"question": "can you complete watercolor paintings without a cup of water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I can complete watercolor paintings without", "extracted_entity": null}
{"question": "do poop and hot dog filling have anything in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they both come out of the same hole.", "extracted_entity": null}
{"question": "was Lamarck's evolution theory compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lamarck's evolution theory was not compatible", "extracted_entity": null}
{"question": "was Lamarckian evolution compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lamarckian evolution was compatible with", "extracted_entity": null}
{"question": "would a poisoner have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, to poison someone.", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like french beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so, he's", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are fried and sweet", "extracted_entity": null}
{"question": "does the San Francisco Bay have more of a homeless issue than Albany New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "was there ever a South Park episode about someone who has a song about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was called \"The Return of Che", "extracted_entity": null}
{"question": "did South Park make a parody of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was called \"The Terrorist", "extracted_entity": null}
{"question": "did South Park make fun of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it was funny.", "extracted_entity": null}
{"question": "is a lot of famous New York food from Jewish cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Bagels and lox", "extracted_entity": null}
{"question": "does New York City have a lot of famous Jewish Cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, New York City has a lot of famous", "extracted_entity": null}
{"question": "would it be uncommon fora teenager to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be uncommon for a te", "extracted_entity": null}
{"question": "is agriculture without irrigation difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, agriculture without irrigation is difficult", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about teaching romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was in the film \"Good Will", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was in Dead Poets Society", "extracted_entity": null}
{"question": "is sunlight important in pigmentation of asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, asparagus does not contain p", "extracted_entity": null}
{"question": "does light matter in pigmentation of Asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, light matter in pigmentation of", "extracted_entity": null}
{"question": "are there Chinese imports in all of the Aldi stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aldi has Chinese imports in all of", "extracted_entity": null}
{"question": "would fans of Nine Inch Nails be likely to know of Johnny Cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are very likely to know of Johnny", "extracted_entity": null}
{"question": "are there red varieties of picked cucumber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are red varieties of cuc", "extracted_entity": null}
{"question": "are there times that fear causes euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it's called \"adrenal", "extracted_entity": null}
{"question": "would it be difficult to sleep in the world's most quiet room?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's not the world'", "extracted_entity": null}
{"question": "could a mole of glucose fit inside the Empire State Building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a tight sque", "extracted_entity": null}
{"question": "could Johnny Carson's children form a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "would a silicon shortage be harmful to Intel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be harmful to Intel.", "extracted_entity": null}
{"question": "was Mozart inspired by Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mozart was inspired by Richard Wagner.", "extracted_entity": null}
{"question": "are there romantic Doctor Who moments near Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in a parallel universe.", "extracted_entity": null}
{"question": "does Hank Hill sell items useful for grilling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he sells barbecue grills", "extracted_entity": null}
{"question": "could lots of Long Island Iced Teas cause Liver Disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The liver is the largest organ in the body", "extracted_entity": null}
{"question": "has anyone from Texas ever served as Vice President of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but two have been nominated for the office", "extracted_entity": null}
{"question": "are pecans and peanuts grown in the same part of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are grown in the same part of", "extracted_entity": null}
{"question": "have oil companies contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in the sense that the oil", "extracted_entity": null}
{"question": "would the Neanderthal people have found snowshoes useful?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would have found snowshoes", "extracted_entity": null}
{"question": "are all canidae good house pets?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some canidae are not good house p", "extracted_entity": null}
{"question": "do students at Berlin University of the Arts recite the Pledge of Allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. But I do know", "extracted_entity": null}
{"question": "could Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she could.", "extracted_entity": null}
{"question": "did Jeanne Moreau see AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jeanne Moreau did not see AEW", "extracted_entity": null}
{"question": "did Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did", "extracted_entity": null}
{"question": "is it likely that Katy Perry's baby will look like Russell Brand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's very likely that Katy", "extracted_entity": null}
{"question": "do all drag kings take testosterone injections?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Some drag kings do not take test", "extracted_entity": null}
{"question": "is it possible to confuse acne and chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they look similar.", "extracted_entity": null}
{"question": "could acne scars be mistaken for chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but chicken pox scars are", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not less likely to own a", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not.", "extracted_entity": null}
{"question": "did Bill Clinton use a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used a Playstation 3.", "extracted_entity": null}
{"question": "did Bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Richard Nixon play Call Of Duty on a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Richard Nixon did not play Call Of", "extracted_entity": null}
{"question": "did Richard Nixon play Call Of Duty on a  Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Richard Nixon use a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used a Playstation 3.", "extracted_entity": null}
{"question": "could Jesse Owens have won a medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have.", "extracted_entity": null}
{"question": "did Thierry Rey win a gold medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "are oysters harvested to make jewelry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. Oysters are used", "extracted_entity": null}
{"question": "has the COVID-19 virus been used for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first patient with COVID-19", "extracted_entity": null}
{"question": "is the COVID-19 virus a good candidate for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a good candidate for whole gen", "extracted_entity": null}
{"question": "is it dangerous to walk to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a long walk.", "extracted_entity": null}
{"question": "is Edward Snowden in hiding?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Snowden is in hiding.", "extracted_entity": null}
{"question": "did Elvis popularize smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. He popularized rock", "extracted_entity": null}
{"question": "did Elvis invent smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would oak burn on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Venus is a planet with an", "extracted_entity": null}
{"question": "does the media sugar coat the health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. because of the media, people have this", "extracted_entity": null}
{"question": "would something with a high viscosity be hard to fit through a plastic straw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because it would not be able to get through", "extracted_entity": null}
{"question": "would bringing a Torah to a Mosque be unusual?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be unusual.", "extracted_entity": null}
{"question": "are LGBT Pride events due, in part, to the work of Drag Queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Drag Queens have been at the forefront", "extracted_entity": null}
{"question": "were Drag Queens involved in the creation of Pride events?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they weren\u2019t.", "extracted_entity": null}
{"question": "do many American stores have lots of notebook on sale in August?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "is handling some turtles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some turtles can be dangerous,", "extracted_entity": null}
{"question": "is handling a snapping turtle dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Handling a snapping turtle can be", "extracted_entity": null}
{"question": "should a doctor's office have a ramp for wheelchairs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it's not necessary.", "extracted_entity": null}
{"question": "are some children very excited to meet Robert Downey Jr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But he was not there.", "extracted_entity": null}
{"question": "is some adverting hidden in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some adverting is hidden in films", "extracted_entity": null}
{"question": "can you see sea lions in the San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they live in the Bay.", "extracted_entity": null}
{"question": "are there groups within the movement of feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are groups within the movement of femin", "extracted_entity": null}
{"question": "is there infighting within feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is infighting within feminism", "extracted_entity": null}
{"question": "do some artists support themselves through patronage alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "would a linguistics major take SQL courses?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "is it difficult to interview Edward Snowden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it\u2019s very difficult.", "extracted_entity": null}
{"question": "can street art be used as a form of protest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it can, as it is a way of", "extracted_entity": null}
{"question": "can you kayak from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would take you 3 years", "extracted_entity": null}
{"question": "can you walk from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Atlantic Ocean is in between.", "extracted_entity": null}
{"question": "is Vice Media a peer reviewed journal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Vice is a magazine and is not peer", "extracted_entity": null}
{"question": "could you go to New York Public Library and the Adirondacks in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you could go to the Adir", "extracted_entity": null}
{"question": "do nurses watch anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They do watch them while they are e", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, nurses supervise anore", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nurses do not supervise", "extracted_entity": null}
{"question": "in treatment, would someone with anoxrexia nervosa be watched?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the risk of suicide,", "extracted_entity": null}
{"question": "can you store hamburgers in the car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can store hamburgers in", "extracted_entity": null}
{"question": "does Elon Musk act as an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Elon Musk has been very vocal", "extracted_entity": null}
{"question": "is Billy Joel considered high risk for COVID-19?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not considered high risk for COVID", "extracted_entity": null}
{"question": "in Tetris99 do Newbies play with Pro players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Newbies and Pros play in", "extracted_entity": null}
{"question": "did P. G. Wodehouse like cartoon shows as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did P. G. Wodehouse watch Rick and Morty as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and he was a big fan of the", "extracted_entity": null}
{"question": "is it likely some paleontologists were inspired by Michael Crichton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's likely some paleontolog", "extracted_entity": null}
{"question": "could Michael Crichton have inspired some paleontologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "do some people feel nauseous after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to feel nauseous", "extracted_entity": null}
{"question": "does the Antarctic coast have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Antarctic coast does not have", "extracted_entity": null}
{"question": "does Antarctica have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is so cold.", "extracted_entity": null}
{"question": "is homelessness generally voluntary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, homelessness is not generally voluntary", "extracted_entity": null}
{"question": "would Paypal be a good way to pay for pornography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Paypal is a service that is used", "extracted_entity": null}
{"question": "are some scientific journals less credible than others?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, for example, Nature and Science are more", "extracted_entity": null}
{"question": "did the author of the Da Vinci Code ever view depictions of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "has the author Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019d be shocked if he had.", "extracted_entity": null}
{"question": "has Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, but I'm not sure", "extracted_entity": null}
{"question": "do people find reason to protest the London Zoo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they find reason to protest the killing", "extracted_entity": null}
{"question": "have there been zombie adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it's called Pride and", "extracted_entity": null}
{"question": "have there been horror adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact, there have been two.", "extracted_entity": null}
{"question": "do different parts of the globe celebrate New Year's Day on different calendar dates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 12 countries in the", "extracted_entity": null}
{"question": "were people in Utah worried about damage from Hurricane Harvey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Utah is far away from the", "extracted_entity": null}
{"question": "are Indigenous peoples in Canada still fighting for equal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, unfortunately.", "extracted_entity": null}
{"question": "are all pickles made with lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pickles can be made with lact", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lakes in the Adirondack Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you could not.", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are only found in the ocean.", "extracted_entity": null}
{"question": "could you swim with Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Manta Ray live in the ocean.", "extracted_entity": null}
{"question": "would you be likely to see Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they live in the ocean", "extracted_entity": null}
{"question": "has Subway had ties with any child predators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jared Fogle.", "extracted_entity": null}
{"question": "is pain in the torso a potential sign of appendicitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pain in the torso is a", "extracted_entity": null}
{"question": "can a kidney infection cause pain in the torso?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the infection has spread to the", "extracted_entity": null}
{"question": "can you get xanax from a pharmacy without a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you will have to show your ID", "extracted_entity": null}
{"question": "are there substitutes available for human legs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "do students with dyscalculia need more help with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they need more help with basic arithmetic.", "extracted_entity": null}
{"question": "would a task of documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, someone with dyscalculia may have", "extracted_entity": null}
{"question": "would a job documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would be. I have dys", "extracted_entity": null}
{"question": "would a job about statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be more difficult.", "extracted_entity": null}
{"question": "could someone with dyscalculia struggle with documenting statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can. Dyscalculia is", "extracted_entity": null}
{"question": "would someone with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because statistics is a math subject and people", "extracted_entity": null}
{"question": "would a student with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not necessarily. Students with dys", "extracted_entity": null}
{"question": "would a student with dyscalcula struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a student with dyscalcula would", "extracted_entity": null}
{"question": "can judges be sent to jail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can't.", "extracted_entity": null}
{"question": "are most people the result of ectopic pregnancies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. ectopic pregnancies", "extracted_entity": null}
{"question": "could you see the Eiffel Tower from the top of the Vatican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you could not see the Eiffel", "extracted_entity": null}
{"question": "can you see a themed King Kong ride in Universal Studios?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is called King Kong: 3", "extracted_entity": null}
{"question": "can you see a themed King Kong ride in Florida?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is no King Kong themed ride", "extracted_entity": null}
{"question": "do desert hares live in the Balkans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in the Sahara.", "extracted_entity": null}
{"question": "do the Balkans experience monsoons?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Balkans experience a Mediterrane", "extracted_entity": null}
{"question": "can you smell argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is odorless.", "extracted_entity": null}
{"question": "can you view argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot see argon.", "extracted_entity": null}
{"question": "can you pick up argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a gas", "extracted_entity": null}
{"question": "did Johnny Carson make phone calls while driving?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is Bruce Lee making a new movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is called \"Enter the Dragon\"", "extracted_entity": null}
{"question": "can you shake Bruce Lee's hand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's dead.", "extracted_entity": null}
{"question": "can someone survive without a pancreas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A pancreas is a very important", "extracted_entity": null}
{"question": "can you say hello to Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hello Aretha Franklin.", "extracted_entity": null}
{"question": "can you meet Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019d love to meet Aretha Franklin.", "extracted_entity": null}
{"question": "do drag queens sometimes have a hard time finding jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "do white blood cells keep people alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because white blood cells fight disease and in", "extracted_entity": null}
{"question": "do guitarists have trouble with their fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the guitar strings, which are", "extracted_entity": null}
{"question": "does a guitarist have callused fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "does Julia Roberts pay taxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she pays taxes.", "extracted_entity": null}
{"question": "did Douglas Adams use the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was called the internet.", "extracted_entity": null}
{"question": "can sweet corn make diabetes worse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sweet corn is a good source of fiber", "extracted_entity": null}
{"question": "did Mario Andretti win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Space Race was between the USA and", "extracted_entity": null}
{"question": "was Europa important to Stanley Kubrick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was the first film he directed.", "extracted_entity": null}
{"question": "are drag queens harassed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "did Claude Monet's widow receive aid from the French government after his death?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "should Koi and beta fish be kept together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, koi and betta fish should not", "extracted_entity": null}
{"question": "would children be disappointed with a deciduous Christmas tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not. But then, they probably wouldn'", "extracted_entity": null}
{"question": "are the blossoms of a peach tree equal in length to cherry tree blossoms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the blossoms of a peach", "extracted_entity": null}
{"question": "will a peach pit always grow into a peach tree?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not necessarily. It depends on the type", "extracted_entity": null}
{"question": "with the right makeup, can clowns be frightening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "would nuclear attacks improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would make things worse.", "extracted_entity": null}
{"question": "would nuclear escalation improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be a catastrophic", "extracted_entity": null}
{"question": "would fans of Christian Rock enjoy Shaggy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shaggy is not a Christian Rock band,", "extracted_entity": null}
{"question": "would a Galapagos penguin be out of place on the Antarctic Peninsula?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's too warm there", "extracted_entity": null}
{"question": "is it ever necessary to bathe a sloth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as often as you might think", "extracted_entity": null}
{"question": "can you make soup without a pot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. you can make soup without a pot.", "extracted_entity": null}
{"question": "can you make soup without heat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can make soup without heat. S", "extracted_entity": null}
{"question": "would a blender be useful for making some soups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a blender would be useful for making", "extracted_entity": null}
{"question": "are stim toys marketed at children with learning disabilities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not exclusively.", "extracted_entity": null}
{"question": "are children with learning disabilities big consumers of stim toys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "would someone who doesn't own a gun have a need for gunpowder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's not like you can buy", "extracted_entity": null}
{"question": "do anime fans enjoy detective fiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I can't speak for all anime fans", "extracted_entity": null}
{"question": "are most milkshakes high in glucose?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most milkshakes are high in gl", "extracted_entity": null}
{"question": "could someone from 1920 have gotten HIV from a transfusion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. HIV was first isolated in 1", "extracted_entity": null}
{"question": "can peppermint oil cure pink eye?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You cannot cure pink eye with", "extracted_entity": null}
{"question": "would you need a boat to get from Peru to the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you would not need a boat to get", "extracted_entity": null}
{"question": "is it normal to be nervous before spinal chord procedures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal to be nervous before", "extracted_entity": null}
{"question": "would a new mother need more time off from work than a childless person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a new mother would need more time off", "extracted_entity": null}
{"question": "would a mother require more time off than a childless woman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A mother is a woman and a woman", "extracted_entity": null}
{"question": "would it be acceptable to have cats that live in your barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as they are kept in a", "extracted_entity": null}
{"question": "are wild boar hunted by indigenous peoples in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are hunted by indigenous", "extracted_entity": null}
{"question": "do indigenous peoples in Canada hunt wild boar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure, but I don\u2019", "extracted_entity": null}
{"question": "is it likely that you'd hear disco music at Mazi Nightclub?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that's more likely to be played", "extracted_entity": null}
{"question": "have there been any Greeks in space?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1985, the", "extracted_entity": null}
{"question": "would Tom Cruise know what an e-meter is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "was Mozart inspired by Richard Wagner's music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mozart was inspired by Richard Wagner'", "extracted_entity": null}
{"question": "can you see the stars in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they\u2019re not visible.", "extracted_entity": null}
{"question": "does Marvel sell myths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they are quite good at it", "extracted_entity": null}
{"question": "would a leftist be likely to refuse lunch from Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they\u2019re too busy eating their", "extracted_entity": null}
{"question": "are there materials that are small enough to pass through cell walls?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, materials that are small enough to pass through", "extracted_entity": null}
{"question": "are transgender teens less likely to read J.K. Rowling's writing than cisgender teens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "do some trees keep their leaves during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are a few trees that keep their", "extracted_entity": null}
{"question": "do some trees keep their foliage during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some trees keep their foliage during", "extracted_entity": null}
{"question": "can you still see flowers in autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are different flowers", "extracted_entity": null}
{"question": "do an ocelot's eyes glow under a flashlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not.", "extracted_entity": null}
{"question": "is Wasabi paste permitted for religious feasts in Judaism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "can you travel by boat from Bern to Albuquerque?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bern is in Switzerland. Albuquer", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Casablanca?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, veterans of the War in Vietnam (", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Across the Universe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The movie is set in the 1", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Forest Gump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "is Santa Claus a nerd?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is a fat old man who lik", "extracted_entity": null}
{"question": "is there likely to be frost on the morning of Boxing Day in Johannesburg? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is likely to be frost on", "extracted_entity": null}
{"question": "is there likely to be frost on the morning of a Canberra Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there will be frost on the morning", "extracted_entity": null}
{"question": "are most feminists proponents of conservatism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Rick and Morty be shocking to someone from the 1970's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be shocking.", "extracted_entity": null}
{"question": "are fans of Community likely to enjoy Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, absolutely. Community is a show about a", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are a useful diagnostic tool.", "extracted_entity": null}
{"question": "would fans of Osmosis Jones know what white blood cells are?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "is material from an aloe plant found in bathroom cupboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. You can use aloe vera to", "extracted_entity": null}
{"question": "could an astronomer detect noise from the beginning of the universe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the noise would be too low.", "extracted_entity": null}
{"question": "could an astronomer warn of an imminent electrical blackout?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, an astronomer could not warn of an", "extracted_entity": null}
{"question": "would an abused child benefit from talking to a school counselor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because school counselors are trained to", "extracted_entity": null}
{"question": "is it impossible to play \"Happy Birthday\" using the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not impossible, but it", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool for doctors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they help doctors diagnose patients", "extracted_entity": null}
{"question": "will most attendees of groundhog day in Vermont be wearing jackets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they will.", "extracted_entity": null}
{"question": "is it wise to wear a jacket on groundhog day in Vermont?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is winter.", "extracted_entity": null}
{"question": "is the weather usually chilly on groundhog day in the Northeastern US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the weather is usually warmer", "extracted_entity": null}
{"question": "is it usually cold in Vermont on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is usually cold in Vermont on", "extracted_entity": null}
{"question": "is it usually cold in Canada on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is usually cold in Canada on groundhog", "extracted_entity": null}
{"question": "is xenophobia counter productive to world peace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Xenophobia", "extracted_entity": null}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulently made?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The statement was made by Cobain on 1", "extracted_entity": null}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it would not be Kurt Cobain", "extracted_entity": null}
{"question": "would Barack Obama be unlikely to be a friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I believe he would be unlikely to be", "extracted_entity": null}
{"question": "would Barack Obama be an unlikely friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Barack Obama is a Democrat", "extracted_entity": null}
{"question": "would a guitarist feel less pain during a finger prick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the guitarist would feel less pain during", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have long terms?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not. They have life terms", "extracted_entity": null}
{"question": "did the Disney company always have power over the Star Wars movies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn't.", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing domestic feline reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because cats kill wildlife.", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing house cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because cats are not a threat to", "extracted_entity": null}
{"question": "is it environmentally responsible to prevent domestic cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES, YES, YES!", "extracted_entity": null}
{"question": "can some porches be inaccessible for disabled people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the porch is too narrow or", "extracted_entity": null}
{"question": "would Jay-Z get free tickets to a Beyonce concert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would", "extracted_entity": null}
{"question": "is the average person at less risk than a celebrity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the average person is at less risk than", "extracted_entity": null}
{"question": "do celebrities feel less safe than most people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "does being a celebrity make one's life less safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The paparazzi are like sh", "extracted_entity": null}
{"question": "does being a celebrity put one in danger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. Being a celebrity puts", "extracted_entity": null}
{"question": "is joining the United States Air Force an option for someone who takes Seroquel daily?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not a doctor, but I would assume", "extracted_entity": null}
{"question": "would someone on a daily regimen of an SSRI be able to enter the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. You would have to", "extracted_entity": null}
{"question": "would someone on a daily regimen of an SSRI be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be able to join.", "extracted_entity": null}
{"question": "would someone on a daily regimen of psych meds be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes. The Air Force accepts applic", "extracted_entity": null}
{"question": "would someone on a daily regimen of Valium be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Air Force will accept a person on", "extracted_entity": null}
{"question": "are animal rights advocates likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is a sport and not cruel", "extracted_entity": null}
{"question": "are PETA members likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "in Dead Poet's Society, do the boys in the English class enjoy poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the boys in the English class do enjoy", "extracted_entity": null}
{"question": "are the students on Dead Poets Society fans of Romantic Poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. I mean, they are in", "extracted_entity": null}
{"question": "would a novice singer be nervous to perform for Simon Cowell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Simon Cowell is a nice guy", "extracted_entity": null}
{"question": "are people less skilled at climbing steel angles than animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are more skilled.", "extracted_entity": null}
{"question": "are there animals capable of climbing more extreme angles than people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A mountain goat can climb a", "extracted_entity": null}
{"question": "would Sigmund Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Freud believed that the unconscious", "extracted_entity": null}
{"question": "would Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Freud would have particular interest in the", "extracted_entity": null}
{"question": "did Tony Bennett's children outnumber his wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Tony Bennett had four wives and", "extracted_entity": null}
{"question": "would firefighters respond to a nearby gunshot victim?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are on the scene of a", "extracted_entity": null}
{"question": "were some employees from the WTC in 2000 the victims of a terror attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the bombing of the WTC in", "extracted_entity": null}
{"question": "would a cook appreciate the gift of a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "would a cook appreciate a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. Maybe. I don", "extracted_entity": null}
{"question": "would people be disappointed in chicken on a Thanksgiving table?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be disappointed", "extracted_entity": null}
{"question": "are there disparities in access to preventive healthcare in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Minorities and people with lower incomes", "extracted_entity": null}
{"question": "would your restroom be more odorous if you ate asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not because you ate aspar", "extracted_entity": null}
{"question": "would modern Marxists disapprove of capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Marxists would disapprove of capital", "extracted_entity": null}
{"question": "would a neglected child benefit from psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a neglected child would benefit from psych", "extracted_entity": null}
{"question": "are do it yourself tattoos more likely to become infected than shop tattoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "should students study Old English for the SAT?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! If you want to be a great writer", "extracted_entity": null}
{"question": "would you see squid in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not many.", "extracted_entity": null}
{"question": "would you see squis in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would.", "extracted_entity": null}
{"question": "are all translations of hello in spoken language?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The word hello is a greeting that", "extracted_entity": null}
{"question": "are all translations of hello audible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all translations of hello are aud", "extracted_entity": null}
{"question": "is Alcatraz Island significant to Native American history?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Alcatraz Island is significant to Native", "extracted_entity": null}
{"question": "would car travel from Brooklyn to Canada in a day be feasible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be feasible.", "extracted_entity": null}
{"question": "would car travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be possible to travel from", "extracted_entity": null}
{"question": "would it be unsafe for an aircraft pilot to have untreated narcolepsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is not illegal for a pilot", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Narcolepsy is a condition that affects", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot nearly impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would think so.", "extracted_entity": null}
{"question": "would lily pads grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Gobi Desert is a", "extracted_entity": null}
{"question": "would cattails grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so, because cattails", "extracted_entity": null}
{"question": "would cattails thrive in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they are a native plant to the", "extracted_entity": null}
{"question": "are Kangaroos the smallest animal with a pouch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the smallest animal with a pouch is", "extracted_entity": null}
{"question": "are Kangaroos the only animals with a pouch on the front of their body?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are many animals with a pouch", "extracted_entity": null}
{"question": "are right wing extremists opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Marxism is a political and economic system named after", "extracted_entity": null}
{"question": "do Jehova's Witnesses skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to travel from Brooklyn", "extracted_entity": null}
{"question": "would an endive farmer worry if their crop was covered in frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, endive is a hardy crop", "extracted_entity": null}
{"question": "would a farmer worry if they saw frost on their endive plants?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, endive is a cold-weather", "extracted_entity": null}
{"question": "could someone who isn't mentally ill have a reason to seek psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course. Psychotherapy is a powerful tool", "extracted_entity": null}
{"question": "would someone with a BA in English be well suited for copy editing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have the right skills, experience", "extracted_entity": null}
{"question": "does inflammation of the brain require medical attention?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, inflammation of the brain requires medical", "extracted_entity": null}
{"question": "are all crustaceans on restaurant menus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are all delicious!", "extracted_entity": null}
{"question": "is handedness unimportant in guitar playing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The reason for this is that the left", "extracted_entity": null}
{"question": "have many fans of Spongebob seen Dustin Hoffman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have, and I don't think he", "extracted_entity": null}
{"question": "did you get a bad deal if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You got a great deal.", "extracted_entity": null}
{"question": "did you get ripped off if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you did.", "extracted_entity": null}
{"question": "would you be likely to see a tour guide in the Adirondack National Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I would not be likely to see a", "extracted_entity": null}
{"question": "would Goku from Dragon Ball Z work with a mercenary?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. He's", "extracted_entity": null}
{"question": "would a fan of  The Lord of the Rings books be likely to have read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not necessarily.", "extracted_entity": null}
{"question": "would a fan of  The Lord of the Rings books be likely to read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "did the Attack on Pearl Harbor lead to the world's first nuclear attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the attack on Pearl Harbor did", "extracted_entity": null}
{"question": "do System of a Down push for the recognition of the genocide against Armenians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. System of a Down is", "extracted_entity": null}
{"question": "do Armenians still have to fight for recognition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do. They have a song called", "extracted_entity": null}
{"question": "so System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "did Christopher Columbus play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did sail the ocean blue in", "extracted_entity": null}
{"question": "can Pandas play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not allowed.", "extracted_entity": null}
{"question": "can a lobster procreate in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not a mammal", "extracted_entity": null}
{"question": "is it a good idea to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not a good idea to drink", "extracted_entity": null}
{"question": "does it make sense to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Diarrhea causes the body to", "extracted_entity": null}
{"question": "does it make sense to drink water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you should drink plenty of fluids to", "extracted_entity": null}
{"question": "would you notice a difference between a shallot and an onion in french onion soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, shallots are milder and sweeter", "extracted_entity": null}
{"question": "could a garlic clove be confused for a shallot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Garlic is a bulb, shall", "extracted_entity": null}
{"question": "could a shallot be confused for a garlic clove?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they both come from the same family", "extracted_entity": null}
{"question": "could a shallot be confused for a cocktail onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is yes.", "extracted_entity": null}
{"question": "does handedness determine how you use ASL?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "are advocates of right wing extremism likely to support Proud Boys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "are advocates of right wing extremism against gender neutral restrooms? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Right wing extremists are not against gender", "extracted_entity": null}
{"question": "would someone practicing right wing extremism donate to an abortion clinic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. no one would.", "extracted_entity": null}
{"question": "would an broken hand make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would make it difficult to catch and throw.", "extracted_entity": null}
{"question": "would an broken arm make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be difficult to play lacros", "extracted_entity": null}
{"question": "do most PCMH have a file system?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. PCMH have a file system,", "extracted_entity": null}
{"question": "can hyena's on steak alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hyenas can eat steak alone.", "extracted_entity": null}
{"question": "can a cable ferry use horsehair for stabilization?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, horsehair will not work.", "extracted_entity": null}
{"question": "can a cable ferry be made from plastic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be made from plastic.", "extracted_entity": null}
{"question": "can a cable ferry be made from cedar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cedar is a very strong and dur", "extracted_entity": null}
{"question": "are Unitarian Universalists welcoming to people who align with agnosticism? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "do Vegans refuse products made with honey bee products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, vegans do not use honey,", "extracted_entity": null}
{"question": "would it be unusual for a strict vegan to keep Honey Bees for harvest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be unusual for a strict", "extracted_entity": null}
{"question": "would it be unusual for a vegan to keep honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there are some things you would have", "extracted_entity": null}
{"question": "would a honey bee's honey be unappealing to a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be unappealing to a vegan", "extracted_entity": null}
{"question": "is Hanoi considered large in Vietnam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "did Eddie Murphy own smartphone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Eddie Murphy did not own a", "extracted_entity": null}
{"question": "did Eddie Murphy own an iPhone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "would someone have a hard time seeing a snowy owl against the sydney opera house?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Sydney Opera House is white and", "extracted_entity": null}
{"question": "would Amish parents warn against feelings of lust?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "did Albert Einstein watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure. I don\u2019t", "extracted_entity": null}
{"question": "did Donald Trump watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Homer watch Modern Family?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Homer was a man of the past", "extracted_entity": null}
{"question": "can Homer Simpson serve as President of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he wins the election.", "extracted_entity": null}
{"question": "can Robert De Niro become President of South Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not a citizen of South", "extracted_entity": null}
{"question": "can Robert De Niro run for President of France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is a French citizen.", "extracted_entity": null}
{"question": "was the antagonist of the Lego Movie portrayed by a Saturday Night Live alumnus? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Will Farrell played the antagonist", "extracted_entity": null}
{"question": "can parachuting cause loss of life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can cause loss of life", "extracted_entity": null}
{"question": "did the Gallic Wars have an effect on France?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Gallic Wars had a huge effect", "extracted_entity": null}
{"question": "were tanks used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Romans used chariots and", "extracted_entity": null}
{"question": "were guns used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not very often.", "extracted_entity": null}
{"question": "is Florida a well known curling state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not really. The closest we get to", "extracted_entity": null}
{"question": "are all hot dogs created with smoking as a step?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. (I know this because I've", "extracted_entity": null}
{"question": "is panoramic photography possible with a polaroid? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not easy.", "extracted_entity": null}
{"question": "is it fair to be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is fair to be skeptical", "extracted_entity": null}
{"question": "did the upcoming new year worry people in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the new year was 200", "extracted_entity": null}
{"question": "were people hoarding food and supplies out of fear in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people were hoarding food and supplies out", "extracted_entity": null}
{"question": "do people opposed to gentrification butt heads with urban planners often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, especially when planners are trying to", "extracted_entity": null}
{"question": "on Venice beach, would you be likely to hear music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is not allowed.", "extracted_entity": null}
{"question": "are there countries where people opposed to fascism are considered terrorists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the US", "extracted_entity": null}
{"question": "should you expect to be hungry before surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you should expect to be hungry before", "extracted_entity": null}
{"question": "is it common to be hungry after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is common to have a decreased appet", "extracted_entity": null}
{"question": "could a rhinoceros accidentally be included in a list of mythical creatures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could.", "extracted_entity": null}
{"question": "could a rhinoceros be a playable animal in Dungeons and Dragons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a terrible choice.", "extracted_entity": null}
{"question": "does the United States Department of Education oversee undocumented students?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States Department of Education does not", "extracted_entity": null}
{"question": "would myofascitis be diagnosed by dual-energy X-ray absorptiometry?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dual-energy X-ray absor", "extracted_entity": null}
{"question": "can salt protect a garden that has spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, salt can protect a garden that has spin", "extracted_entity": null}
{"question": "were people concerned about the new year in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the new year was 20", "extracted_entity": null}
{"question": "were computers a hot topic in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, computers were a hot topic in 1", "extracted_entity": null}
{"question": "can mammals survive with only one kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. One kidney is enough for the body", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz crystals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Quartz crystals are a part of the", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the form of a crystal grid", "extracted_entity": null}
{"question": "are cucumbers a low maintenance plant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cucumbers are a low maintenance plant,", "extracted_entity": null}
{"question": "do The World's Billionaires deny pizzagate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would it be impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Venus has no atmosphere.", "extracted_entity": null}
{"question": "is it impossible to go snowboarding on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is possible to go snowboarding", "extracted_entity": null}
{"question": "is it impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is impossible to snowboard on Ven", "extracted_entity": null}
{"question": "after COVID-19 came to the US, did Chinese Americans face discrimination?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Chinese Americans were blamed for the", "extracted_entity": null}
{"question": "would someone with COVID-19 have a higher level of blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. COVID-19 is a respir", "extracted_entity": null}
{"question": "did Ralph Macchio make a fighting movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That was Jean Claude Van Damme.", "extracted_entity": null}
{"question": "did Ralph Macchio make a karate movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it was called \"The Big B", "extracted_entity": null}
{"question": "did Putin help Russia win the space race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "has Osama Bin Laden been influencing the Trump Administration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has been.", "extracted_entity": null}
{"question": "is sweet potato with marshmallow a holiday traditional dish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a holiday traditional dish", "extracted_entity": null}
{"question": "during COVID-19, have more people been using Skype?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the pandemic", "extracted_entity": null}
{"question": "does Channon Rose use her platform to speak against abortion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. she does not.", "extracted_entity": null}
{"question": "can you get on the AirTrain JFK with a metropass?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you need a MetroCard.", "extracted_entity": null}
{"question": "do you need a Metropass to ride the AirTraine JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you need a Metrocard. You can", "extracted_entity": null}
{"question": "do you need a Metropass to ride the AirTraine at JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you need a ticket or pass to", "extracted_entity": null}
{"question": "is Dr. Joseph Mengele a part of the history of psychology that is celebrated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "for breast cancer patients, is plastic surgery considered reconstructive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, plastic surgery can be considered re", "extracted_entity": null}
{"question": "did Europeans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they did", "extracted_entity": null}
{"question": "did Germans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not have blueberries during", "extracted_entity": null}
{"question": "would someone in a MAGA hat consider accusations of Trump's islamophobia to be invalid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I would think that", "extracted_entity": null}
{"question": "would someone in a MAGA hat ignore claims about Trump of Islamophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a racist.", "extracted_entity": null}
{"question": "do humans find the bite of a horse fly to be upsetting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is very upsetting.", "extracted_entity": null}
{"question": "in 1960, did people know what the surface of the moon was like?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. We didn't know what the surface", "extracted_entity": null}
{"question": "can you go to the Mall of America and the Metropolitan Museum of Art in the same day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Mall of America is in Minne", "extracted_entity": null}
{"question": "did Al Capone own an iPhone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do Scientologists celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It is a Muslim celebration.", "extracted_entity": null}
{"question": "do Mormons have a big Eid al-Fitr celebration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They don't celebrate Ramad", "extracted_entity": null}
{"question": "do Mormons celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Mormons do not celebrate E", "extracted_entity": null}
{"question": "do Christians worship multiple Gods?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christians believe in one God.", "extracted_entity": null}
{"question": "can a person survive by eating Ammonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a poison.", "extracted_entity": null}
{"question": "has a Christmas character ever tried to save the New Year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The New Year is not in danger.", "extracted_entity": null}
{"question": "is the Forbidden City the seat of China's government?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Forbidden City is the seat", "extracted_entity": null}
{"question": "did Woodrow Wilson have access to a telephone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "can someone eat a Dodo today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is extinct.", "extracted_entity": null}
{"question": "is the Dodo reproducing today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Dodo is extinct", "extracted_entity": null}
{"question": "can Cholera be treated successfully?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be treated successfully.", "extracted_entity": null}
{"question": "did Dale Earnheardt Sr. crash due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he died of a stroke.", "extracted_entity": null}
{"question": "would it be bad for a Member of Parliament to be photographed with Jeffrey Epstein?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. It is perfectly normal", "extracted_entity": null}
{"question": "would fans of Weird Al be likely to enjoy The Onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so. We", "extracted_entity": null}
{"question": "does Mozilla offer free educational programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Mozilla offers free educational programs", "extracted_entity": null}
{"question": "can you learn web development for free with Mozilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can learn web development for free with", "extracted_entity": null}
{"question": "would it be harder for Kaitlyn Dobrow to do calligraphy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she has autism", "extracted_entity": null}
{"question": "would a Catholic missionary meet resistance with Tibetan people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would meet resistance.", "extracted_entity": null}
{"question": "do guitarist's have finger tips that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "do finger blood tests typically hurt guitarist's less?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "do guitarist's finger tips typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, guitarists finger tips are less sensitive to", "extracted_entity": null}
{"question": "do guitarist's fingers typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think so.", "extracted_entity": null}
{"question": "if Whole Foods is too expensive for you, would Aldi be a good option?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you\u2019re not familiar with Aldi,", "extracted_entity": null}
{"question": "would you be disappointed if you went to bojangles for a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I wouldn't be disappointed.", "extracted_entity": null}
{"question": "does tourism harm the environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, tourism can harm the environment.", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the off season?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, most of them do.", "extracted_entity": null}
{"question": "do Mormons and 7th Day Adventists disagree about if Sunday is a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do Christians disagree about whether Sunday is the day of rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christians do disagree about whether Sunday is", "extracted_entity": null}
{"question": "is there disagreement among Christians regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is not.", "extracted_entity": null}
{"question": "is there argument regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is an argument regarding the status of", "extracted_entity": null}
{"question": "is David A.R. White an advocate for acknowledging the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, David A.R. White is an", "extracted_entity": null}
{"question": "is David A.R. White known for arguing for the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is known for being an actor and", "extracted_entity": null}
{"question": "did The Three Stooges get popular on Saturday Night Live?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They did.", "extracted_entity": null}
{"question": "were The Three Stooges inspired by Saturday Night Live skits?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were inspired by the Three Stoog", "extracted_entity": null}
{"question": "would Bobby Fischer be likely to know of the French Defense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He would not.", "extracted_entity": null}
{"question": "would a chocolate brownie from a marijuana dispensary be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be inappropriate for", "extracted_entity": null}
{"question": "would a chocolate brownie from a dispensary in Colorado be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you were giving it to", "extracted_entity": null}
{"question": "would it have been wise to keep alcohol away from Amy Winehouse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would have been wise.", "extracted_entity": null}
{"question": "if somebody wants a coolata is Starbucks a good place to go?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Starbucks is not a good place", "extracted_entity": null}
{"question": "did Jane Austen ever make the New York Times Bestseller list in her lifetime?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, and she never will.", "extracted_entity": null}
{"question": "could someone take Northwest Airlines to the Beijing Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can take Northwest Airlines to the", "extracted_entity": null}
{"question": "could someone take Northwest Airlines to the Rio Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they had the money to", "extracted_entity": null}
{"question": "would Benito Mussolini have success in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mussolini was a great leader and a", "extracted_entity": null}
{"question": "would the leaves in Auburn, New York be changing colors in in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's not really that late for the leaves", "extracted_entity": null}
{"question": "would the leaves in Auburn, New York be changing colors in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the leaves would be changing colors in October", "extracted_entity": null}
{"question": "would you be able to watch the leaves change in the fall in Auburn, NY?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a beautiful time of year to", "extracted_entity": null}
{"question": "do people often think that Frankenstein was a monster?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they think he was a scientist.", "extracted_entity": null}
{"question": "would Firefighters be included in a 9/11 memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were not military.", "extracted_entity": null}
{"question": "do any games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the end of civilization has slot machines", "extracted_entity": null}
{"question": "are there places with slot machines that you can get free drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can get free drinks in Las", "extracted_entity": null}
{"question": "does St. Jude's hospital help patients with Leukemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, St. Jude's hospital does", "extracted_entity": null}
{"question": "when you convince a child that they have no power, does it diminish their confidence? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It also makes them feel", "extracted_entity": null}
{"question": "are there women who get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are very few cases, but there", "extracted_entity": null}
{"question": "can a woman get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is very rare.", "extracted_entity": null}
{"question": "at a latino restaurant, would you find the dish coq au vin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, coq au vin is a french", "extracted_entity": null}
{"question": "is Call of Duty more popular than Doom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Doom is older.", "extracted_entity": null}
{"question": "is lithium considered a good treatment for the mood disorder major depressive disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes lithium is a good treatment for the", "extracted_entity": null}
{"question": "by junior high school, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. By junior high school, most people know", "extracted_entity": null}
{"question": "by their A-level exams, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They can't even add up the", "extracted_entity": null}
{"question": "by their A-levels, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they can use a calculator.", "extracted_entity": null}
{"question": "are any colors on flag of the United States shared by flags of genocide perpetrators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The red and white stripes on the", "extracted_entity": null}
{"question": "did Bonanza have greater longevity than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bonanza had 14 seasons,", "extracted_entity": null}
{"question": "did Bonanza have greater staying power than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bonanza lasted 14 seasons", "extracted_entity": null}
{"question": "was Alexander Graham Bell's telephone made by motorola?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was made by Bell himself.", "extracted_entity": null}
{"question": "are there people for whom the correct pronouns are 'he/him' who experience menstruation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "did Heracles have any full siblings that were immortal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he had three sisters, all imm", "extracted_entity": null}
{"question": "has Aretha Franklin ever collaborated with someone that later committed suicide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She has never collaborated with anyone who", "extracted_entity": null}
{"question": "are elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, CO2 is present in the atmosphere of", "extracted_entity": null}
{"question": "were several of Spartacus's companions from area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a few of his companions were from", "extracted_entity": null}
{"question": "were any of Spartacus's companions born in area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some of Spartacus's compan", "extracted_entity": null}
{"question": "were any of Spartacus's companions from the area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of Spartacus's compan", "extracted_entity": null}
{"question": "were there any famous musicians with more letter a's in their name than Isaac Newton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There were many.", "extracted_entity": null}
{"question": "were cannons built during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Bronze Age was the time period in which", "extracted_entity": null}
{"question": "were cannons wielded during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Bronze Age is the second principal period of", "extracted_entity": null}
{"question": "were cannons used during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Bronze Age was too early for", "extracted_entity": null}
{"question": "would firefighters respond to a heart attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would firefighters respond to a stabbing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Firefighters respond to medical emer", "extracted_entity": null}
{"question": "did Shane Dawson work with any creators who have Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no he did not.", "extracted_entity": null}
{"question": "are any current world leaders former KGB members?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Vladimir Putin.", "extracted_entity": null}
{"question": "will you see the doctor a lot more during pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you will have to visit the doctor more", "extracted_entity": null}
{"question": "will someone have more doctor's appointments during their pregnancy than normal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you will have more doctor's appoint", "extracted_entity": null}
{"question": "can pregnancy put strain on friendships?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It can. Pregnancy is a time", "extracted_entity": null}
{"question": "can pregnancy ruin a friendship?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is possible for pregnancy to ruin", "extracted_entity": null}
{"question": "can you eat some kinds of citrus on birth control?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you should consult your doctor.", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Fairies are materialistic and would", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of iron earrings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not alive.", "extracted_entity": null}
{"question": "was Rosalind Franklin studying in Molecular Biology?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was studying in molecular biology", "extracted_entity": null}
{"question": "would a chiropractic care center be more helpful for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would think that a chiropractic care", "extracted_entity": null}
{"question": "would a chiropractic care center be more appropriate for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A chiropractor would be better than a physical", "extracted_entity": null}
{"question": "do Doctor Who fans enjoy taking photos in pay phone booths?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "did students in 1999 have a different understanding of Pluto than those in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the students in 2020", "extracted_entity": null}
{"question": "would Kale be better to eat than Spinach for someone with scurvy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Kale is a better source of", "extracted_entity": null}
{"question": "is Kim Kardashian a supermodel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she's not a good one", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than a Zweihander?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Gladiator uses a Zweihand", "extracted_entity": null}
{"question": "eminem's daughter spells her name differently than star of 1961 Disney film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Joel Osment", "extracted_entity": null}
{"question": "eminem's daughter spells her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Joel Osment", "extracted_entity": null}
{"question": "does Eminem's daughter spell her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's the same spelling.", "extracted_entity": null}
{"question": "does Eminem's daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she spells her name Hailie", "extracted_entity": null}
{"question": "does Eminem's biological daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Joel Osment", "extracted_entity": null}
{"question": "did the early settlers of the U.S celebrate the first halloween?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the first Halloween was celebrated in", "extracted_entity": null}
{"question": "has any Doctor Who actor won a razzie award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the actors have won a Raz", "extracted_entity": null}
{"question": "would a psychiatrist be likely to recommend Prozac to someone with bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A psychiatrist would not be likely to recommend", "extracted_entity": null}
{"question": "has any Doctor Who actor won a Golden Globe award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, no Doctor Who actor has ever won a", "extracted_entity": null}
{"question": "would an SSRI be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, an SSRI would not be a first", "extracted_entity": null}
{"question": "would Zoloft be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The first line of treatment for bip", "extracted_entity": null}
{"question": "has any Duke University graduate married a Grammy winner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (His name is not important.)", "extracted_entity": null}
{"question": "do pharmacy employees need to ask for customer ID every time?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did have a classmate who", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's college?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but one of the Oscars was", "extracted_entity": null}
{"question": "does the Torah have more books than Emily Bronte wrote?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it has 5 books.", "extracted_entity": null}
{"question": "in Mexico, is Tuesday the 13th is considered more unlucky than Friday the 13th? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is the opposite. Friday the", "extracted_entity": null}
{"question": "does the Torah have more books than the Hunger Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has 5.", "extracted_entity": null}
{"question": "does the Torah have more books than the Katniss Everdeen series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s only a little bit", "extracted_entity": null}
{"question": "lil Wayne similar real name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jay-Z", "extracted_entity": null}
{"question": "lil Wayne similar name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kanye West.", "extracted_entity": null}
{"question": "lil Wayne needs over 4 times Grammy awards to equal rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "4 times Grammy awards is 40", "extracted_entity": null}
{"question": "lil Wayne needs over 4 times Grammy awards to equal with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "4 x 3 = 12", "extracted_entity": null}
{"question": "lil Wayne double Grammy awards trails rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lil Wayne, who was named best rap album for", "extracted_entity": null}
{"question": "does Lil Wayne need more Grammy awards to match rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the other rapper has a fake", "extracted_entity": null}
{"question": "do Lil Wayne's Grammy awards trail rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "do Lil Wayne's Grammy awards lag behind rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But the rapper whose real last name", "extracted_entity": null}
{"question": "did the author of \"A Doll's House\" have a mentally disabled son?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Charles Manson's body count surpassed by any Marilyn Manson band member name origin killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Charles Manson was a cult leader and", "extracted_entity": null}
{"question": "can a bumblebee suffer from spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, spider veins can occur in b", "extracted_entity": null}
{"question": "can a bumblebee get spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A bumblebee can get sp", "extracted_entity": null}
{"question": "can a bumblebee get a blood clot?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a bumblebee can get a", "extracted_entity": null}
{"question": "is the Numerologist featured on the podcast Oh No Ross and Carrie well known?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Numerologist featured on the pod", "extracted_entity": null}
{"question": "would someone opening a brewery need to get barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would need to get hops.", "extracted_entity": null}
{"question": "did the Maori people build kayaks in the 1300's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, kayaks were not invented until", "extracted_entity": null}
{"question": "spain native spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, September is the correct spelling.", "extracted_entity": null}
{"question": "did the Maori people reach New Zealand by kayak?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They arrived by canoe.", "extracted_entity": null}
{"question": "spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. The letter \"c\"", "extracted_entity": null}
{"question": "does Spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "do you need to add a consonant to September in order to get the Spanish spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In Spanish, September is pronounced the", "extracted_entity": null}
{"question": "did the Inuit fish trout from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they did", "extracted_entity": null}
{"question": "did the Inuit hunt coyotes from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they used spears and arrows from", "extracted_entity": null}
{"question": "any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Disney has never adapted any bible prophet stories", "extracted_entity": null}
{"question": "were any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Little Mermaid was adapted from the story", "extracted_entity": null}
{"question": "do Islam and Christianity have at least five prophets in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have at least five prophets in", "extracted_entity": null}
{"question": "does pi exceed square root of pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, pi is a number, square root is", "extracted_entity": null}
{"question": "5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1)The Swedish Empire was at its z", "extracted_entity": null}
{"question": "is 5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is important", "extracted_entity": null}
{"question": "is Pluto origin mythology still to be explored in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It\u2019s still in the mythology, but", "extracted_entity": null}
{"question": "pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pluto is a mythology yet to be featured", "extracted_entity": null}
{"question": "is Pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pluto is the god of the underworld in", "extracted_entity": null}
{"question": "have any plays based on graphic novels won awards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not many.", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see the second film adaptation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see both of the film adaptations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "if someone is wearing a Jack Skellington jacket, is it likely that they enjoy grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not necessarily. Jack Skellington is a character", "extracted_entity": null}
{"question": "would Nightmare Before Christmas fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably. I mean, it\u2019s the Night", "extracted_entity": null}
{"question": "would Tim Burton fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they are used to it.", "extracted_entity": null}
{"question": "would Tim Burton fans be likely to embrace grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Tim Burton fans would be likely to", "extracted_entity": null}
{"question": "were modern assembly lines inspired by the work of Henry Ford?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The assembly line was the most important development", "extracted_entity": null}
{"question": "do restaurant waiting staff in South Korea expect a tip?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is customary to tip 1", "extracted_entity": null}
{"question": "do restaurant waiting staff in South Korea expect a gratuity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it is customary to leave", "extracted_entity": null}
{"question": "does a wedding designer typically make the floral arrangements?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A wedding designer will usually have a", "extracted_entity": null}
{"question": "was John Kerry less active in the military than John McCain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, John Kerry was not less active in", "extracted_entity": null}
{"question": "is J.K. Rowling considered an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not.", "extracted_entity": null}
{"question": "does the LGBT community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she has also been accused of trans", "extracted_entity": null}
{"question": "does the transgender community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The transgender community has been very vocal about their", "extracted_entity": null}
{"question": "in teenagers with depression, are SSRI medications as safe as they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Teenagers with depression are not given SS", "extracted_entity": null}
{"question": "would a teen with depression be at higher risk for adverse effects on a medication like Zoloft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the teen may be at higher risk", "extracted_entity": null}
{"question": "have violent crowd control tactics been used against anyone in a sit-in?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, at the sit-in at the University", "extracted_entity": null}
{"question": "is tobacco a major crop in the Southern United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tobacco is a major crop in the Southern", "extracted_entity": null}
{"question": "are there turtles that are dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are some turtles that are", "extracted_entity": null}
{"question": "would someone who was good at guitar hero be good at rock band?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not necessarily. Guitar Hero is a game that", "extracted_entity": null}
{"question": "do tailors predate the sewing machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they still exist.", "extracted_entity": null}
{"question": "would Go be considerably more difficult for someone who was blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not really. Go is a game of strategy,", "extracted_entity": null}
{"question": "would Go be considerably more difficult if you were blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be.", "extracted_entity": null}
{"question": "would fauna in a tropical climate be very different from those in a temperate climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, tropical climate is hot and wet, temper", "extracted_entity": null}
{"question": "does the Pope live in an area close to Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he lives in the Vatican City", "extracted_entity": null}
{"question": "is country ham bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Country ham is a type of ham that is c", "extracted_entity": null}
{"question": "would country ham be a bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Country ham is a type of ham that is c", "extracted_entity": null}
{"question": "should someone watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, country ham is high in sodium,", "extracted_entity": null}
{"question": "should people watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, country ham is a good source of sod", "extracted_entity": null}
{"question": "does country ham taste different than city ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, country ham is cured for a longer", "extracted_entity": null}
{"question": "does Will Ferrell feature in multiple films featuring elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elf and Anchorman", "extracted_entity": null}
{"question": "has Will Ferrell been in multiple movies that feature elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Elf, Blades of Glory", "extracted_entity": null}
{"question": "can anyone with a driver's license work in transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You must have a CDL license with", "extracted_entity": null}
{"question": "can anyone with a driver's license work in bulk transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You need a Hazmat endorsement", "extracted_entity": null}
{"question": "would a comparison of a murder scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would make sense.", "extracted_entity": null}
{"question": "would a comparison of a crime scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would make sense.", "extracted_entity": null}
{"question": "at a presentation about PTSD, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you\u2019re talking about PTSD in", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising considered unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Door-to-", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising socially unacceptable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. I know, because I", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising frowned upon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It\u2019s a waste", "extracted_entity": null}
{"question": "did Victor Hugo's novel about the French Revolution ever become a Broadway show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it never did", "extracted_entity": null}
{"question": "did the Voyages of Christopher Columbus involve coming ashore on the West coast of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The West coast of America was not known", "extracted_entity": null}
{"question": "was George Washington aboard one of the ships during the voyages of Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was Jamestown the landing place for members of the Voyages of Christopher Columubus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jamestown was the landing place for", "extracted_entity": null}
{"question": "was Michael Scott still on The Office when Will Ferrell's character came onto the show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Michael Scott was still on the show when", "extracted_entity": null}
{"question": "should Peter Griffin be familiar with the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Peter Griffin should be familiar with the craft of", "extracted_entity": null}
{"question": "was the death of the CEO of Apple caused by pancreatic cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was caused by a 30", "extracted_entity": null}
{"question": "were Furbys popular during the Presidency of Bill Clinton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, very much so.", "extracted_entity": null}
{"question": "do students studying Shakespeare learn about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they also learn about comedy, history", "extracted_entity": null}
{"question": "would a Professor of a Shakespeare class have to teach the students about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would", "extracted_entity": null}
{"question": "would a Professor of a Shakespeare class have to teach about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course", "extracted_entity": null}
{"question": "did Kate Winslet and Leonardo Dicaprio star in a tragedy together?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Titanic.", "extracted_entity": null}
{"question": "is Yale University beyond 5000 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond 50", "extracted_entity": null}
{"question": "is Yale University beyond 400 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Yale University is not beyond 4", "extracted_entity": null}
{"question": "is Yale University beyond 100 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yale University is beyond 10", "extracted_entity": null}
{"question": "is Yale University beyond 30 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is 33.", "extracted_entity": null}
{"question": "is Yale University beyond 20 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond 20", "extracted_entity": null}
{"question": "is Yale University beyond a mile from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond a mile from", "extracted_entity": null}
{"question": "is Yale University beyond a quick walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond a quick walk", "extracted_entity": null}
{"question": "did lead singer of The Police ever collaborate with an Arab?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sting, lead singer of The Police, collabor", "extracted_entity": null}
{"question": "did lead singer of The Police ever collaborate with an Algerian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Shaggy ever collaborate with a Blood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Shaggy never collaborated with a", "extracted_entity": null}
{"question": "are there any spirits in the Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is no spirit in the Qur", "extracted_entity": null}
{"question": "does the average male foot fail to reach a foot (unit) on a ruler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The average male foot is 12", "extracted_entity": null}
{"question": "is Home Depot a good place to shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The best place to shop is at the", "extracted_entity": null}
{"question": "is Yale University beyond a walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The University of New Haven is located", "extracted_entity": null}
{"question": "did Reza Shah's grandfather's grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Reza Shah's great great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Reza Shah's great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a general in the army of", "extracted_entity": null}
{"question": "did Reza Shah's great great grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reza Shah's great great grand", "extracted_entity": null}
{"question": "did Reza Shah's grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He did not.", "extracted_entity": null}
{"question": "is anyone that was at the Last Supper important to Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Last Supper was a Jewish me", "extracted_entity": null}
{"question": "are chocolate brownies hypothetically fatal to Harry Potter's Fang?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "can a chocolate brownie be fatal to a member of Canidae?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a chocolate brownie can be", "extracted_entity": null}
{"question": "can a chocolate brownie be fatal to a dog?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the brownie contains chocolate", "extracted_entity": null}
{"question": "have protesters ever rallied for a governor to veto a bill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, for example the protests in Wisconsin.", "extracted_entity": null}
{"question": "is the Sun (United Kingdom) published in every United Kingdom country?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Sun (United Kingdom) is", "extracted_entity": null}
{"question": "national Transportation Safety Board has more abbreviation letters than Mulder's X-Files agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NTSB", "extracted_entity": null}
{"question": "do vendors ever claim that Quartz has healing properties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "were Republican voters pleased with the outcome of  the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not.", "extracted_entity": null}
{"question": "did any states bordering California vote for Jimmy Carter in the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hawaii, Oregon and Washington voted for", "extracted_entity": null}
{"question": "were any of the blue states in the 1980's election on the East Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "New York and Massachusetts.", "extracted_entity": null}
{"question": "were any of the blue states in the 1980's election on the West Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, California, Oregon, and Washington.", "extracted_entity": null}
{"question": "was Super Bowl 50 devoid of NFL founding teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all 32 teams were founded after", "extracted_entity": null}
{"question": "was Christopher Columbus a poor example of religious tolerance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher Columbus was a poor example of religious toler", "extracted_entity": null}
{"question": "does Burning Man effigy share similarities with The Wicker Man movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the effigy is a large wooden", "extracted_entity": null}
{"question": "is saying Melania Trump is from a third world country inaccurate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is inaccurate.", "extracted_entity": null}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same gender Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Sean Penn is not gay.", "extracted_entity": null}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same sex Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The actor that played Harvey Milk", "extracted_entity": null}
{"question": "can you reach the UNICEF headquarters from Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can get there on a bi", "extracted_entity": null}
{"question": "can you reach the UNICEF headquarters by Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. It takes about 2", "extracted_entity": null}
{"question": "has the executive director of UNICEF spent time in New England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was here in 200", "extracted_entity": null}
{"question": "has the executive director of UNICEF spent time in Massachusetts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the spring of 200", "extracted_entity": null}
{"question": "is the executive director of UNICEF familiar with a bra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She is a woman.", "extracted_entity": null}
{"question": "would a bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if she was 21 or older", "extracted_entity": null}
{"question": "did people used to believe bagel seeds were from same drug family as morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they used to believe that.", "extracted_entity": null}
{"question": "would a bartender make a cocktail for a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, I\u2019m not", "extracted_entity": null}
{"question": "would a bartender accept an order from a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, that would be wrong", "extracted_entity": null}
{"question": "does country ham taste different than spiral ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Country ham is more smokey and", "extracted_entity": null}
{"question": "would someone with celiac disease eat cookies baked with all purpose flour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. People with celiac disease cannot eat gl", "extracted_entity": null}
{"question": "would a bartender accept an order from a 2nd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a bartender is not allowed", "extracted_entity": null}
{"question": "would a bartender accept an order from a 3rd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are too young.", "extracted_entity": null}
{"question": "would a bartender accept an order from a 4th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (unless the bartender is a", "extracted_entity": null}
{"question": "would a bartender accept an order from Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he\u2019s not old enough to", "extracted_entity": null}
{"question": "would a bartender provide service to Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "would a bartender provide service to Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he is 21 or older", "extracted_entity": null}
{"question": "would a bartender make a White Russian for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would a bartender make a Mojito for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he\u2019s not old enough.", "extracted_entity": null}
{"question": "would a bartender make a cocktail for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a bartender would make a co", "extracted_entity": null}
{"question": "has name God Save the Queen been used in things other than royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's been used in a lot", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Cynthia Powell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was shot on 8 December", "extracted_entity": null}
{"question": "did John Lennon celebrate a diamond anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were married for only 11", "extracted_entity": null}
{"question": "did John Lennon celebrate a golden anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. On March 20, 2", "extracted_entity": null}
{"question": "is RoboCop director a Razzie award legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is. Paul Verhoeven has", "extracted_entity": null}
{"question": "would it be inaccurate to call Eve an orphan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she has parents", "extracted_entity": null}
{"question": "was Hulk Hogan a good guy in one organization and bad in another?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a good guy in the", "extracted_entity": null}
{"question": "can you be be denied penance for a deadly sin posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the soul can be denied penance for", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his deadly sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a Republican.", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He would not be denied penance today for his", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would be denied penance.", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he would be denied a priesthood", "extracted_entity": null}
{"question": "would Jack the Ripper be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would Jack the Ripper be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a human being.", "extracted_entity": null}
{"question": "would Jeffrey Dahmer be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not my place to judge.", "extracted_entity": null}
{"question": "when Elizabeth Glaser died was penance impossible for Jeffrey Dahmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was still alive.", "extracted_entity": null}
{"question": "penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was convicted of murder and sentenced to", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He could not possibly be forgiven.", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer in year Layne Staley died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not impossible for Jeffrey D", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer in last month of 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "was penance out of reach for Jeffrey Dahmer in December 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he had not been convicted of", "extracted_entity": null}
{"question": "have any of Kurt Cobain's bandmates had more success than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no.", "extracted_entity": null}
{"question": "will Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they don\u2019t like cold water", "extracted_entity": null}
{"question": "do Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Atlantic salmon are found only in the", "extracted_entity": null}
{"question": "do Atlantic salmon hang out in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not at all.", "extracted_entity": null}
{"question": "are Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Atlantic salmon have never been in Bu", "extracted_entity": null}
{"question": "can you spot Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Atlantic salmon is a species of salmon", "extracted_entity": null}
{"question": "do Atlantic salmon ever swim near Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "is Summer Triangle band named after grizzly bear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. The band is named", "extracted_entity": null}
{"question": "is Ginger something that a naturopath would recommend for a pregnant woman? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, ginger is a very safe and effective", "extracted_entity": null}
{"question": "is Olivia Newton John's 1980 film named after summer home of Mongols?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Xanadu.", "extracted_entity": null}
{"question": "have there been any blood eagle killings in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were in the Viking Age.", "extracted_entity": null}
{"question": "have there been any crucifixions in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jesus Christ was crucified in the", "extracted_entity": null}
{"question": "has League of Legends esports players surpassed DOTA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, League of Legends esports players have", "extracted_entity": null}
{"question": "is cancer treatment a preventative for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even close.", "extracted_entity": null}
{"question": "did Popeye eat all of Bugs Bunny's food source?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Popeye was a sailor and B", "extracted_entity": null}
{"question": "are all colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The color blue is not on the Mar", "extracted_entity": null}
{"question": "do majority of Tibetan people in their ancestral homeland believe in bodhisattvas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I don\u2019", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion be misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES. Islamophobia is the fear of", "extracted_entity": null}
{"question": "would a member of the Communist Party of the Soviet Union be likely to have read Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not. The CPSU was a b", "extracted_entity": null}
{"question": "is electrolysis possible on the human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Electrolysis is possible on", "extracted_entity": null}
{"question": "does Facebook take steps to limit fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they are not enough.", "extracted_entity": null}
{"question": "did Confederate States Army have similar fashion sense to West Point cadets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they wore grey and blue", "extracted_entity": null}
{"question": "is Fake News currently uncensored on major social media sites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is uncensored.", "extracted_entity": null}
{"question": "would someone feel safer from fake news on facebook than on the deep web?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the person. Some people might feel", "extracted_entity": null}
{"question": "is the deep web somewhere that you could encounter unchecked fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are a lot of fake news out", "extracted_entity": null}
{"question": "does Ahura Mazda live above Abaddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he lives above Abaddon.", "extracted_entity": null}
{"question": "can Boards of Canada perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can\u2019t.", "extracted_entity": null}
{"question": "can MGMT perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. but not often.", "extracted_entity": null}
{"question": "can Destiny's Child perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's not a solo,", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch harm hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by \u201c", "extracted_entity": null}
{"question": "can starch be deadly to hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "hyperglycemics should avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hyperglycemics should avoid starch.", "extracted_entity": null}
{"question": "should hyperglycemics avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, starch is a good source of car", "extracted_entity": null}
{"question": "did Alan Rickman's final film outearn his first?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the final film 'Eye in the", "extracted_entity": null}
{"question": "was Alan Rickman's final film a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died before the film was released.", "extracted_entity": null}
{"question": "did Alan Rickman outearn his debut movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the film was made in 19", "extracted_entity": null}
{"question": "did Alan Rickman ever surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it took him a long time.", "extracted_entity": null}
{"question": "did an Alan Rickman film surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He has had a couple of films that", "extracted_entity": null}
{"question": "was Alan Rickman's first movie a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a flop.", "extracted_entity": null}
{"question": "has Simo Hayha killed any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has killed many of the Red Army", "extracted_entity": null}
{"question": "did Simo Hayha kill any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he killed 402 members of", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's children followed similar profession?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, her daughter Christine Pelosi is a", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's brats followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are all either in business, or", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's kids followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nancy Pelosi's daughter, Alex", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so, but she has", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps career wise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, her daughter Christine is a politician.", "extracted_entity": null}
{"question": "was 2014 governor of New Jersey fattest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1998, 200", "extracted_entity": null}
{"question": "was 2014 governor of New Jersey heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chris Christie", "extracted_entity": null}
{"question": "did any belligerent in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even remotely.", "extracted_entity": null}
{"question": "would clown be a bad occupational fit for Ebenezer Scrooge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, because he would be too much", "extracted_entity": null}
{"question": "do jumping spiders hunt to survive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do.", "extracted_entity": null}
{"question": "are tap shoes required for breakdancing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are required for breakdancing", "extracted_entity": null}
{"question": "does a Drow reach top of a shelf before The Hobbit's hero can?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and with a spear to the back", "extracted_entity": null}
{"question": "yayoi era Japanese people didn't worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Yayoi era people did not worry about", "extracted_entity": null}
{"question": "did Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they had to worry about kami", "extracted_entity": null}
{"question": "did  Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did.", "extracted_entity": null}
{"question": "did Giovanni Battista Cybo serve his position longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, James A. Garfield served his position", "extracted_entity": null}
{"question": "was Giovanni Battista Cybo in office longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Giovanni Battista Cybo was in office", "extracted_entity": null}
{"question": "is Alexandria Ocasio-Cortez a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is a hippie.", "extracted_entity": null}
{"question": "was the first Vice President of the United States a Segregationist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. John C. Calhoun was a", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors been featured in a Val Kilmer project?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Warwick Davis was in Willow and", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Val Kilmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Val Kilmer is a human. E", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Bowie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know if any of them worked", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Jennifer Connelly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Warwick Davis who played Wicket the", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Jim Henson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Warwick Davis (Wicket the E", "extracted_entity": null}
{"question": "does Lines on the Antiquity of Microbes lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not the components we learned", "extracted_entity": null}
{"question": "does the shortest poem lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The shortest poem lacks components of haiku", "extracted_entity": null}
{"question": "does the shortest poem lack necessary component of a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It does not.", "extracted_entity": null}
{"question": "does the shortest poem lack the structure to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a haiku.", "extracted_entity": null}
{"question": "does the shortest poem written fail the criteria to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it does not have 3 lines", "extracted_entity": null}
{"question": "is a haiku longer than the shortest poem written?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a haiku is the shortest poem", "extracted_entity": null}
{"question": "are there Scientologists who know a lot about Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course. There are Scientologists", "extracted_entity": null}
{"question": "did Katy Perry and Taylor Swift appear together in a music video?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they did not", "extracted_entity": null}
{"question": "did Katy Perry create more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Katy Perry has sold 20", "extracted_entity": null}
{"question": "did Katy Perry sell more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Katy Perry sold more records than Av", "extracted_entity": null}
{"question": "are there people who carry drums at the Macy's Day Parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not playing the drums.", "extracted_entity": null}
{"question": "can coffee help with diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can help with diarrhea", "extracted_entity": null}
{"question": "would the plant Spartina Patens thrive in the Sahara?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Spartina Patens is a salt mar", "extracted_entity": null}
{"question": "during the neolithic period, did people know how to hunt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did", "extracted_entity": null}
{"question": "during the neolithic period, was hunting a valued skill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hunting was a valued skill during the", "extracted_entity": null}
{"question": "are there accessibility options for blind users who encounter CAPTCHAs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. At least, not in the mainstream", "extracted_entity": null}
{"question": "would a student in AP Chemistry have a need for scientific journals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not in AP Chemistry.", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be unusual to see frost", "extracted_entity": null}
{"question": "have there been any actors from musicals who have played Noah in a film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Russell Crowe played Noah in the", "extracted_entity": null}
{"question": "has anyone from a film about Noah been in a movie about magic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Russell Crowe was in the movie \u201c", "extracted_entity": null}
{"question": "are any of the actors from the Harry Potter film series in any films about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the actors from the Harry Pot", "extracted_entity": null}
{"question": "have any of the actors from Harry Potter participated in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The actors from Harry Potter have not", "extracted_entity": null}
{"question": "does Russell Crowe know about the story of Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He should, he played Noah in a film", "extracted_entity": null}
{"question": "has Russell Crowe ever acted in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was the first mate on the boat", "extracted_entity": null}
{"question": "did Michael Jackson own a lot of bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He did not.", "extracted_entity": null}
{"question": "did Michael Jackson invest in Bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "are goldfish found on the menu of Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, goldfish are not found on the menu", "extracted_entity": null}
{"question": "is it recommended to put goldfish in the same tank as beta fish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would not recommend putting them in the same tank", "extracted_entity": null}
{"question": "is it recommended for goldfish and beta fish to share a tank?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, goldfish and beta fish are not recommended", "extracted_entity": null}
{"question": "did Native American tribes teach Colonizers how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Native Americans taught the Colonizers how", "extracted_entity": null}
{"question": "are there types of plastic that you can't put in the recycling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are some types of plastic that", "extracted_entity": null}
{"question": "did James Roday play a private investigator on the USA network?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played Shawn Spencer on the", "extracted_entity": null}
{"question": "did James Roday play a private investigator on a TV show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played a private investigator named Sh", "extracted_entity": null}
{"question": "is Lance Armstrong known for riding a bicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is a world-class cyclist", "extracted_entity": null}
{"question": "if someone is a fan of Dale Earnheardt, would they know what the Daytona 500 was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is the first race of the season", "extracted_entity": null}
{"question": "before it sunk, did the Titanic have too few lifeboats for all of the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Titanic had only enough lif", "extracted_entity": null}
{"question": "can you go water skiing in the Adirondacks? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but if you can", "extracted_entity": null}
{"question": "in the movie The Titanic, were there too few many lifeboats for the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Titanic had 16", "extracted_entity": null}
{"question": "has Saturday Night Live been on air longer than American Idol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by 10 years.", "extracted_entity": null}
{"question": "was Ashlee Simpson lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "have any performers been caught lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most notably, Britney Spears", "extracted_entity": null}
{"question": "can a person with triskaidekophobia succeed at Sudoku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I'm", "extracted_entity": null}
{"question": "have any of the Doctor (Doctor Who) actors had famous offspring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the Doctor Who actors have had", "extracted_entity": null}
{"question": "did any of the amazons on Xena: Warrior Princess star in a Fox TV series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, one of them did.", "extracted_entity": null}
{"question": "would Library of Alexandria hypothetically need less shelf pace than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Library of Alexandria would need more sh", "extracted_entity": null}
{"question": "was Library of Alexandria commissioned by a Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was commissioned by a Pto", "extracted_entity": null}
{"question": "is it impossible for Siddhartha Gautama to become a Bodhisattva?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Bodhisattva is the", "extracted_entity": null}
{"question": "did a mystery ever happen at Tower of London?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there was a mystery of a ghost", "extracted_entity": null}
{"question": "is Tower of London relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Tower of London was used as the", "extracted_entity": null}
{"question": "would Tower of London be relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as the show's title was The", "extracted_entity": null}
{"question": "would Tower of London be relevant to Robert Stack's longest running show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Unsolved Mysteries", "extracted_entity": null}
{"question": "george Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The hornet is a very large wasp,", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a hornet.", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life zoology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I don't", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life taxonomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a hornet, all", "extracted_entity": null}
{"question": "in animal world is George Trendle's hornet a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Hornet is a fictional character. It", "extracted_entity": null}
{"question": "did Barack Obama and Osama Bin Laden go to school together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "if someone was hungry for a sandwich, would they be likely to go to taco bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would probably go to Subway or", "extracted_entity": null}
{"question": "can Aloe plants be mailed within the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, however, they can be quite expensive to", "extracted_entity": null}
{"question": "would Aloe thrive more in a dry climate than a swampy climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the soil would be drier", "extracted_entity": null}
{"question": "would Aloe grow better in a drier area than a marshy area?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "did Mark-Paul Gosselaar ever play the role of a student?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a student in Saved By", "extracted_entity": null}
{"question": "would Drew Pinsky be someone to talk to about mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a doctor and an addiction", "extracted_entity": null}
{"question": "is Drew Pinsky qualified to help people with mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Drew Pinsky is a medical doctor, a", "extracted_entity": null}
{"question": "is Dr. Drew Pinsky qualified to treat mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dr. Drew Pinsky is a physician", "extracted_entity": null}
{"question": "would Steve Martin turn down a dish of shrimp risotto? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he would, but it would be close", "extracted_entity": null}
{"question": "did any of the founding fathers get frequent migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they did not.", "extracted_entity": null}
{"question": "do any of the actors in Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the actors in Wicked get", "extracted_entity": null}
{"question": "does anyone from the cast of Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "are there any famous singers with migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are.", "extracted_entity": null}
{"question": "would a mammogram at a new doctor's office be considered preventive healthcare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be considered preventive healthcare if you", "extracted_entity": null}
{"question": "is it inappropriate to open a closed casket at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not inappropriate to", "extracted_entity": null}
{"question": "are there live performances at celebrity funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the most recent being Michael Jackson's", "extracted_entity": null}
{"question": "were there celebrities performing at the funeral of Aretha Franklin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were many celebrities who performed", "extracted_entity": null}
{"question": "was Aretha Franklin's funeral attended by celebrity guests?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the funeral was attended by many cele", "extracted_entity": null}
{"question": "would a Heath Ledger fan be interested in seeing The Dark Knight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course. Heath Ledger was", "extracted_entity": null}
{"question": "is The Dark Knight associated with any violence in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was used as a reference for the", "extracted_entity": null}
{"question": "do Northern Fur Seals encounter penguins naturally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they eat them.", "extracted_entity": null}
{"question": "would it be unusual to see a Northern Fur Seal in the south pole?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they are found in the Northern Hem", "extracted_entity": null}
{"question": "are there country artists who also integrate rapping into their work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. For example, Lil Nas X is a", "extracted_entity": null}
{"question": "did any actors from a musical play in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But which one?", "extracted_entity": null}
{"question": "did any actors from Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Colin Firth", "extracted_entity": null}
{"question": "did anyone from the film Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Meryl Streep.", "extracted_entity": null}
{"question": "did anyone from the movie Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Colin Firth was in The Living", "extracted_entity": null}
{"question": "has there been a song by Billie Eilish that has been used in a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Billie Eilish sang the theme", "extracted_entity": null}
{"question": "is it normal for prisoners to encounter situations that inspire disgust in prison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal for prisoners to encounter situations", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Russia to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Russia is a big place and there are many S", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Arkansas to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea. I don't even", "extracted_entity": null}
{"question": "do cactus plants thrive more in dry environments than wet ones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cacti thrive in dry environments", "extracted_entity": null}
{"question": "do cactus plants require less water than dafodills? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, cactus plants require less water than", "extracted_entity": null}
{"question": "are there varieties of cactus that are unsafe to touch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are several varieties of cact", "extracted_entity": null}
{"question": "have there been suicides to protest war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in every war.", "extracted_entity": null}
{"question": "would it be normal for a teacher to tell kindergartners about the golden rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Golden Rule is a good one. I would", "extracted_entity": null}
{"question": "does Halle Berry eat crustaceans often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "is Halle Berry prohibited from eating crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "is some public art illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not illegal.", "extracted_entity": null}
{"question": "is Harry Styles currently knighted in England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Harry Styles is not knighted", "extracted_entity": null}
{"question": "do Razor brand kick scooters have gears?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "is Britney Spears receiving treatment for her bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "would a prosthetic limb work as a replacement for a human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is not made from the same", "extracted_entity": null}
{"question": "was Germany already established during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "was Hugh Jackman alive during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was not.", "extracted_entity": null}
{"question": "was the movie Les Miserables filmed during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the movie Les Miserables was fil", "extracted_entity": null}
{"question": "are deep sea fishermen considered experts in oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A deep sea fisherman is not considered", "extracted_entity": null}
{"question": "would Jane Goodall be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jane Goodall is an expert in primates.", "extracted_entity": null}
{"question": "would Jane Austen be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jane Austen would not be considered an", "extracted_entity": null}
{"question": "has Iggy Pop been in a giallo film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has been in a gial", "extracted_entity": null}
{"question": "has Iggy Pop worked with a Giallo director?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "do Coen brothers take up more car space than Scott directing brothers in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Scott directing brothers in 20", "extracted_entity": null}
{"question": "are there more Coen brothers alive in 2020 than Scott directing brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "there are two Coen brothers alive in 2", "extracted_entity": null}
{"question": "did Robin Williams have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Michael Jackson suffer from a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Christoper Reeves have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had a stroke in 19", "extracted_entity": null}
{"question": "is John Cazale's 1970s partner yet to debut in a Coen brothers movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's in the new one,", "extracted_entity": null}
{"question": "is the Quran regarded as a holy text by Jewish people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jews do not regard the Quran", "extracted_entity": null}
{"question": "do practicing Presbyterians read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure, but I think that", "extracted_entity": null}
{"question": "do practicing Catholics read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That would be silly.", "extracted_entity": null}
{"question": "did Kangaroo Court originate in High Court of Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It originated in the United States.", "extracted_entity": null}
{"question": "did mongoose come from later geologic period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, rhinos came from the late geolog", "extracted_entity": null}
{"question": "nATO excludes double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, nATO does not exclude double triangle", "extracted_entity": null}
{"question": "would NATO exclude double triangle flag country to a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. NATO would not exclude a country from", "extracted_entity": null}
{"question": "is double triangle flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (Russia, China, Iran,", "extracted_entity": null}
{"question": "is double-pennon flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Saugat Malla's home delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no.", "extracted_entity": null}
{"question": "is Saugat Malla's home country delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is Saugat Malla's home country delegate barred from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "are squid considered invertebrates? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, squid are invertebrates.", "extracted_entity": null}
{"question": "are squid found in salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are there raw preparations of squid for eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are called \"calamares\"", "extracted_entity": null}
{"question": "is the white part of the citrus fruit less sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the white part of the citrus fruit", "extracted_entity": null}
{"question": "would most people discard the outside of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Most people discard the outside of citrus fruit", "extracted_entity": null}
{"question": "is citrus good for the body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, citrus is good for the body.", "extracted_entity": null}
{"question": "is citrus useful in promoting health?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, citrus is useful in promoting health", "extracted_entity": null}
{"question": "is calling creator of Gorillaz a one hit wonder ludicrous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for the reason you might think", "extracted_entity": null}
{"question": "are house cats a potential source of danger for chipmunks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is Alexander Klevan's birthplace safe from Zika virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "did same villain kill Batman and Superman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a different villain.", "extracted_entity": null}
{"question": "is Cerebrospinal fluid contents totally different than Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cerebrospinal fluid is produced", "extracted_entity": null}
{"question": "is Cerebrospinal fluid contents similar to Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the cerebrospinal fluid is", "extracted_entity": null}
{"question": "are there no shared ingredients between Cerebrospinal fluid and Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no shared ingredients between", "extracted_entity": null}
{"question": "did Billie Eilish perform a song for a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "are there paintings of Salvador Dali's on both coasts of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the Dali Museum in St.", "extracted_entity": null}
{"question": "can you see Salvador Dali's work in multiple cities of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can see his work in multiple cities", "extracted_entity": null}
{"question": "can you see Salvador Dali's work in both New York and Georgia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the High Museum in Atlanta, G", "extracted_entity": null}
{"question": "does the Bronx Zoo have an exhibit of the Dodo bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they have an exhibit of the ext", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the United States.", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so by their guards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in a very controlled manner.", "extracted_entity": null}
{"question": "did Richard III miss out on Parcheesi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think he missed out on Par", "extracted_entity": null}
{"question": "sunday name origin people enemy of Christians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sun-day (Sunday) was named", "extracted_entity": null}
{"question": "sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I do not know the answer to this question.", "extracted_entity": null}
{"question": "will Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it will not.", "extracted_entity": null}
{"question": "would Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it would be a good idea to", "extracted_entity": null}
{"question": "would Sunday name origin disturb Serena Williams's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Sunday is a day of the week", "extracted_entity": null}
{"question": "would Sunday name origin disturb Prince's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Sunday is a day of the week", "extracted_entity": null}
{"question": "would Sunday name origin disturb a Jehovah Witness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it was a female Je", "extracted_entity": null}
{"question": "would a Jehovah's Witness denounce name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would denounce the origin of Sunday", "extracted_entity": null}
{"question": "would a Jehovah's Witness reject name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. they are not allowed to use the name", "extracted_entity": null}
{"question": "would a Jehovah's Witness be upset by name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Jehovah's Witness", "extracted_entity": null}
{"question": "are all bicycles able to go off road?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some bikes are only for the road", "extracted_entity": null}
{"question": "are two names in Albany, Georgia common as country and city names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the city name is a city in", "extracted_entity": null}
{"question": "does Camp Rock feature a lot of smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (Unless you count the \u201cWe", "extracted_entity": null}
{"question": "is the Song of Roland a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no it is a poem", "extracted_entity": null}
{"question": "can the Song of Roland be considered a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Song of Roland is a saga", "extracted_entity": null}
{"question": "is KFC more global than Roy Rogers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, KFC is more global than Roy Rog", "extracted_entity": null}
{"question": "would a Dodo tower over Pauline Musters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "2000 years ago, yes.", "extracted_entity": null}
{"question": "would a Dodo be above Pauline Musters's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Dodo is taller than Pauline Must", "extracted_entity": null}
{"question": "would a Dodo be above Chandra Bahadur Dangi's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Dodo is 100", "extracted_entity": null}
{"question": "would a Dodo tower over Chandra Bahadur Dangi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Dodo would tower over Chandra", "extracted_entity": null}
{"question": "can you play a Nintendo game in your car CD player?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot play a Nintendo game in", "extracted_entity": null}
{"question": "are there any theme park attractions featuring King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are two.", "extracted_entity": null}
{"question": "is there a ride at Universal Studios Orlando about King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's called Kongfrontation", "extracted_entity": null}
{"question": "were there fifty English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were 38 English kings", "extracted_entity": null}
{"question": "were there at least fifty different English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were at least fifty different English k", "extracted_entity": null}
{"question": "did Pre-Raphaelites have a profound influence on Claude Monet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Pre-Raphaelites had", "extracted_entity": null}
{"question": "was Claude Monet influenced by Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a contemporary of the Pre-", "extracted_entity": null}
{"question": "was Claude Monet a Pre-Raphaelite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Monet was a French Impressionist", "extracted_entity": null}
{"question": "would Claude Monet value art style of the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Monet was a modernist, and", "extracted_entity": null}
{"question": "did Claude Monet have a similar art style to the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Monet was an impressionist painter.", "extracted_entity": null}
{"question": "while on the voice, did Adam Levine quit Maroon 5?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't quit. He is", "extracted_entity": null}
{"question": "would someone with a back fracture go to a chiropractic center for treatment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact chiropractors are trained", "extracted_entity": null}
{"question": "should cinnamon be eaten straight from the container?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The cinnamon should be put", "extracted_entity": null}
{"question": "does Microsoft Excel make slideshow presentations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. You can create a presentation", "extracted_entity": null}
{"question": "is In God We Trust part of the pledge of allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is on our currency.", "extracted_entity": null}
{"question": "did Ariana Grande do a duet of Give it Up with Victoria Justice?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ariana Grande has never dueted", "extracted_entity": null}
{"question": "was Ariana Grande's hair in Victorious natural?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was natural.", "extracted_entity": null}
{"question": "did Ariana Grande get popular on show created by Disney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes she did", "extracted_entity": null}
{"question": "did Ariana Grande get popular on a Disney Channel Original Series?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was on Victorious.", "extracted_entity": null}
{"question": "while on Broadway, did Ariana Grande audition for The Spongebob Musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "is Chris Hemsworth qualified to perform surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "in WW2, were there people who did surgery on the battlefield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were called \"battlefield sur", "extracted_entity": null}
{"question": "can a Sphynx cat make wool?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Sphynx cats do not", "extracted_entity": null}
{"question": "in New York, is snowboarding a popular summer activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Snowboarding is a", "extracted_entity": null}
{"question": "has an appendix been successfully transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only once.", "extracted_entity": null}
{"question": "do most nuclear families in America have Bengal Cats?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what a Bengal cat", "extracted_entity": null}
{"question": "can lacto fermentation preserve cucumbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, cucumbers are very high in", "extracted_entity": null}
{"question": "can cucumbers be preserved for longer storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cucumbers can be preserved for", "extracted_entity": null}
{"question": "do you pass through Wyoming following Route 66?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the road goes through the state of Wy", "extracted_entity": null}
{"question": "is building design the responsibility of a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Building design is the responsibility of a construction", "extracted_entity": null}
{"question": "are turtles vulnerable to Raccoons? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, turtles are vulnerable to racc", "extracted_entity": null}
{"question": "could you see turtles at the Boston Aquarium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are too big.", "extracted_entity": null}
{"question": "did Ada Lovelace know the python coding language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was born in the 19", "extracted_entity": null}
{"question": "would a person with arachnophobia fear spiders? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be afraid of spiders.", "extracted_entity": null}
{"question": "were the family of Donald Shea angry with Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were not angry with Charles Manson", "extracted_entity": null}
{"question": "would a mastectomy be a treatment for colorectal cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Mastectomy is the removal of", "extracted_entity": null}
{"question": "do most gynecologists have to see the firsthand health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because most gynecologists see their", "extracted_entity": null}
{"question": "would a home garden be growing cucumbers during the winter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not be growing cucum", "extracted_entity": null}
{"question": "can you order pancakes at Dennys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can order pancakes at Den", "extracted_entity": null}
{"question": "are Zebra common sights in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "does Greek goddess Venus was named after have a Roman equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Roman equivalent of the Greek goddess", "extracted_entity": null}
{"question": "are bench trials performed before a jury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A bench trial is a trial conducted before a", "extracted_entity": null}
{"question": "would the Temple of Kom Ombo feature Egyptian hieroglyphs? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would feature Egyptian hieroglyph", "extracted_entity": null}
{"question": "as of 2020 does John Key have more PM's succeed than precede him?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has 3 PMs that have", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah's and Abraham combined?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Methuselah lived to be", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah's lifetime over 5 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Methuselah outlived", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Methuselah died 7", "extracted_entity": null}
{"question": "did Charlemagne's father fight in the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not even alive at the time", "extracted_entity": null}
{"question": "did Charlemagne's father win the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "has J.K. Rowling written mysteries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but she is a mystery.", "extracted_entity": null}
{"question": "do any of Powerpuff Girls share name with character in Princess Bride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Buttercup", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of reptile?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A turtle.", "extracted_entity": null}
{"question": "would adherents to Ahimsa be appalled at Paul the Apostle's cause of death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, yes.", "extracted_entity": null}
{"question": "was Florence Nightingale's death a better example of Ahimsa than Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Paul the Apostle was a violent", "extracted_entity": null}
{"question": "would people prefer Florence Nightingale's cause of death to Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd rather Paul's cause of death", "extracted_entity": null}
{"question": "was Paul the Apostle's death more violent than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Paul was killed by the Romans.", "extracted_entity": null}
{"question": "was Paul the Apostle's death crueler than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was beheaded.", "extracted_entity": null}
{"question": "was Paul the Apostle's death more eventful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Paul's was more eventful.", "extracted_entity": null}
{"question": "was Paul the Apostle's death more painful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Apostle Paul was beheaded, and", "extracted_entity": null}
{"question": "is number of US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's a lucky number in", "extracted_entity": null}
{"question": "does the band Marilyn Manson get their entire name from Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the band Marilyn Manson gets", "extracted_entity": null}
{"question": "did Pre-Colonial America have Ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they had a lot of other sp", "extracted_entity": null}
{"question": "are vanity plates important to any of Stephen King's children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are any of Stephen King's children familiar with vanity plates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I'm sure they are.", "extracted_entity": null}
{"question": "have any of Stephen King's offspring embraced their father's genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "have any of Stephen King's offspring followed in his footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Joe Hill, Stephen King's son, is", "extracted_entity": null}
{"question": "did members of singer Prince's Christian sect celebrate Easter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they celebrated Passover.", "extracted_entity": null}
{"question": "do more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but I don\u2019t think that\u2019", "extracted_entity": null}
{"question": "will Gremlins tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the 2015 re", "extracted_entity": null}
{"question": "is Gremlins set to tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a single film, but", "extracted_entity": null}
{"question": "are Gremlins sequels less prevalent than Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Gremlins sequels are less", "extracted_entity": null}
{"question": "are there more Matrix sequels than Gremlins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'd", "extracted_entity": null}
{"question": "is it possible for a mortal to escape the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he has a friend to help him", "extracted_entity": null}
{"question": "would Alexander Graham Bell have supported Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was a Jew.", "extracted_entity": null}
{"question": "can Halle Berry eat crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is allergic to crust", "extracted_entity": null}
{"question": "can letters in first line of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The letters on the first row of the", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of Henry VIII male king heirs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were only 5 male heirs", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Sofia Vergara has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The letter B is the 2nd", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons of Bruce Dern?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Adam Sandler has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Adam Sandler has 8 sons.", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Bruce Willis has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 26 letters in the", "extracted_entity": null}
{"question": "does Richard Dawkins donate to Science Partners?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He does not.", "extracted_entity": null}
{"question": "is Love Song from November 11, 2000 an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is not a lullaby", "extracted_entity": null}
{"question": "would Richard Dawkins donate to the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would. He would support the work", "extracted_entity": null}
{"question": "would Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "is Richard Dawkins joining the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has kindly agreed to let us", "extracted_entity": null}
{"question": "would Holy Wood's Love Song be an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is an appropriate lullaby.", "extracted_entity": null}
{"question": "is Holy Wood's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The song is about a", "extracted_entity": null}
{"question": "is Manson's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it has a very sinister undert", "extracted_entity": null}
{"question": "will Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I hope so", "extracted_entity": null}
{"question": "is Narcissus Narcosis an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would say so.", "extracted_entity": null}
{"question": "is Narcissus Narcosis an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a very effective lullaby", "extracted_entity": null}
{"question": "is Target Audience an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is an effective lullaby.", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a lullaby to put", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is.", "extracted_entity": null}
{"question": "did Christopher Columbus receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he did discover America.", "extracted_entity": null}
{"question": "can a brain be transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the brain is a complex organ and", "extracted_entity": null}
{"question": "do internet forums require natural gas to operate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Internet is a global network of computers", "extracted_entity": null}
{"question": "are wind turbines instrumental in making internet forums available to everyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But they do help to power the servers", "extracted_entity": null}
{"question": "was the tenth Amendment written on a mobile phone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was written on a computer.", "extracted_entity": null}
{"question": "was the tenth Amendment written on a typewriter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was written by a quill pen", "extracted_entity": null}
{"question": "are Atlantic Cod a source of income for fishermen in California?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Atlantic Cod are a species of fish", "extracted_entity": null}
{"question": "did John Lewis work alongside Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did John Lewis march with Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was one of the \"Big Six", "extracted_entity": null}
{"question": "does Bill Clinton have the ability to post in internet forums?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is too busy to post in internet", "extracted_entity": null}
{"question": "would Goofy hypothetically enjoy Milk Bone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019d say yes, but I don\u2019", "extracted_entity": null}
{"question": "is wheat important to make unleavened bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, wheat is important to make unle", "extracted_entity": null}
{"question": "is wheat essential for bread making?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, bread can be made from a number of", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ludacris' rap career started in", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he attended a prom before he started his", "extracted_entity": null}
{"question": "did Ludacris' rap career begin before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he began his rap career before he attended", "extracted_entity": null}
{"question": "did Ludacris' rap career begin before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he attended prom.", "extracted_entity": null}
{"question": "was Clark Gable handy with a wrench when he was young?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a mechanic in the army", "extracted_entity": null}
{"question": "was Clark Gable familiar with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was only a tool to him", "extracted_entity": null}
{"question": "was Clark Gable handy with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a boy scout.", "extracted_entity": null}
{"question": "is week old chlorine water safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the concentration of the chlorine", "extracted_entity": null}
{"question": "would Teri Hatcher have most pay stubs from Desperate Housewives in her career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "8 years.", "extracted_entity": null}
{"question": "was Desperate Housewives Teri Hatcher's greatest marathon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she was in Lois & Clark", "extracted_entity": null}
{"question": "was Desperate Housewives Teri Hatcher's longest tenure as actress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was in Lois & Clark:", "extracted_entity": null}
{"question": "did Desperate Housewives provide Teri Hatcher her longest acting paycheck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was paid $1 million per episode", "extracted_entity": null}
{"question": "are there more games after FInal Fantasy VI than before in its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Final Fantasy VI was the sixth installment in", "extracted_entity": null}
{"question": "is silicon better for making wedding rings than bromine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because silicon is less reactive than", "extracted_entity": null}
{"question": "can a comatose person produce art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not necessarily art", "extracted_entity": null}
{"question": "do any adherents to Christianity in China have different saints than Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many of the saints are not recognized", "extracted_entity": null}
{"question": "it it illegal to pump your own gas in Morris County New Jersey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not illegal to pump your", "extracted_entity": null}
{"question": "has September shifted to later month from the oldest calendar it appeared on?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it shifted from September to October.", "extracted_entity": null}
{"question": "are Sram's kin found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Sram are a faction found", "extracted_entity": null}
{"question": "is Sram's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is Balthor the Defiled's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Balthor is a member of the", "extracted_entity": null}
{"question": "is Balthor the Defiled's race represented in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the undead", "extracted_entity": null}
{"question": "is Balthor the Defiled's race found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is yes.", "extracted_entity": null}
{"question": "was surveyor of Norman, Oklahoma a viking?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Norman was a viking.", "extracted_entity": null}
{"question": "do Snow White dwarves fill out entire The Hobbit dwarves roster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are 13 dwarves", "extracted_entity": null}
{"question": "are there enough Snow White dwarves to play all dwarves in The Hobbit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "does Eli Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he's the new press secretary.", "extracted_entity": null}
{"question": "does Peyton Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he is the president of the United", "extracted_entity": null}
{"question": "is the Hague interested in geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "does the Hague care about geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s a purely legal document.", "extracted_entity": null}
{"question": "will Wednesday origin religion followers be denied Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they are not in communion with", "extracted_entity": null}
{"question": "is Wednesday traditionally a pagan day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "can Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, cognition can be impaired by", "extracted_entity": null}
{"question": "cab Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Impaired Cognition is not a result", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Impaired cognition is not a result of", "extracted_entity": null}
{"question": "is impaired Cognition affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is affected by hereditary factors", "extracted_entity": null}
{"question": "is Cognition impairment affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cognition impairment is affected by her", "extracted_entity": null}
{"question": "is Cognition impairment influenced by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cognitive impairment can be influenced by", "extracted_entity": null}
{"question": "does citrus grow in the Arctic circle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, citrus does grow in the Arctic", "extracted_entity": null}
{"question": "can you watch Rick and Morty at the bottom of Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can watch Rick and Morty at", "extracted_entity": null}
{"question": "is Mark Cuban visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is a citizen of Northern", "extracted_entity": null}
{"question": "can I talk to Vishnu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not to get your work done.", "extracted_entity": null}
{"question": "is President Trump xenophobic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think he is.", "extracted_entity": null}
{"question": "do any countries that practice capitalism still have homelessness problem?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as the USA", "extracted_entity": null}
{"question": "is homelessness still a problem in countries that practice capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because capitalism is a system that is", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book ever been in same project with Eva Green?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I think not.", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book endured through other media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The character of Dorian Gray has been", "extracted_entity": null}
{"question": "can Michael Jordan lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Michael Jordan cannot lease property in Northern", "extracted_entity": null}
{"question": "can Mark Cuban lease land in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it has to be a 9", "extracted_entity": null}
{"question": "does human with no gear die capturing giant squid in natural habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not if they are in good shape and", "extracted_entity": null}
{"question": "is Western Honey bee important to US brand Nice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the bee is part of the", "extracted_entity": null}
{"question": "are crustaceans consumed raw by people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "in New York, is snowboarding a popular July activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not.", "extracted_entity": null}
{"question": "is Freya a composite of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. She is a composite of Athena and", "extracted_entity": null}
{"question": "are there many similarities between Athena and Freya?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are many similarities between Athena", "extracted_entity": null}
{"question": "did Ivan the Terrible have harshest nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He did not.", "extracted_entity": null}
{"question": "did Ivan the Terrible have the worst nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the worst nickname was Ivan the Ter", "extracted_entity": null}
{"question": "do placozoa have learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Placozoa are a group of organisms", "extracted_entity": null}
{"question": "can an accordion player benefit from eye surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eye surgery is not something that is recommended", "extracted_entity": null}
{"question": "did Alexander the Great conquer part of Land of Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can a minotaur hypothetically break a tibia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is a mythical creature.", "extracted_entity": null}
{"question": "does Alex Rodriguez have fewer championship memories than Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn\u2019t.", "extracted_entity": null}
{"question": "does Michael Jordan have more championship memories than Alex Rodriguez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by a lot", "extracted_entity": null}
{"question": "is John Kerry familiar with cross checking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "does table tennis make use of prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Table tennis is a sport. Prime numbers", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA recommended dailyrecommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "7 zucchini's would satisfy", "extracted_entity": null}
{"question": "can Mark Cuban lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is a US citizen or", "extracted_entity": null}
{"question": "is Pan Satan-like in appearance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has horns and a tail.", "extracted_entity": null}
{"question": "did The King of Rock'n Roll's snack with bananas come from plants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it came from a banana tree", "extracted_entity": null}
{"question": "did The King of Rock'n Roll love a snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did!", "extracted_entity": null}
{"question": "was one of the fruits in apple banana bread in The King of Rock'n Roll's favorite snacks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "did the The King of Rock'n Roll love one of the fruits in apple banana bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The King of Rock'n Roll loved banana", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles table tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are all female.", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles table tennis game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are dead.", "extracted_entity": null}
{"question": "would it be impossible for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not impossible.", "extracted_entity": null}
{"question": "does Jabberwocky have the length to be a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jabberwocky has 18", "extracted_entity": null}
{"question": "has one EGOT category eluded Justin Timberlake for nominations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He has been nominated for 12 Grammy", "extracted_entity": null}
{"question": "did Benjamin Butler attend a college that has the first name of a UFC welterweight fighter in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The school is actually named after the", "extracted_entity": null}
{"question": "can a Nissan product beat a Ferrari in a race? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Nissan product cannot beat a", "extracted_entity": null}
{"question": "can a Nissan win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is impossible to race against a", "extracted_entity": null}
{"question": "does Nissan have any products that can win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Nissan GT-R can", "extracted_entity": null}
{"question": "does Nissan have any products that can win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Nissan GT-R", "extracted_entity": null}
{"question": "did Nissan build any products that win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nissan built a Nissan", "extracted_entity": null}
{"question": "was the Land of Israel under the control of a muslim empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Ottoman Empire controlled the Land of", "extracted_entity": null}
{"question": "did a Turkish University of North Carolina at Chapel Hill professor win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did win the 19", "extracted_entity": null}
{"question": "did a University of North Carolina at Chapel Hill professor from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She won a UNC-Chap", "extracted_entity": null}
{"question": "did a University of North Carolina at Chapel Hill biochemist from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Nobel Prize was awarded to a University", "extracted_entity": null}
{"question": "did a 6-time NBA champion graduate from the University of North Carolina at Chapel Hill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael Jordan.", "extracted_entity": null}
{"question": "did University of North Carolina at Chapel Hill give scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he got a full basketball scholarship", "extracted_entity": null}
{"question": "did University of North Carolina at Chapel Hill offer scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the university offered a scholarship to Michael", "extracted_entity": null}
{"question": "was Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Morris County was named after the governor of", "extracted_entity": null}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Morris County was named after Chief Justice Richard", "extracted_entity": null}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Morris County was named after the Morris", "extracted_entity": null}
{"question": "is there a wide range between smallest and largest asteroid size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a wide range between the smallest", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after being fired from the band?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Kirk Hammett replaced him", "extracted_entity": null}
{"question": "did founder of Skype win a race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to the Allied victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because they used it to spy on the", "extracted_entity": null}
{"question": "does soy milk increase chances of heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no studies to indicate that so", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in restaurant in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but she would have to be trained in", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can.", "extracted_entity": null}
{"question": "would a model be a good choice to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a model is a good choice to star", "extracted_entity": null}
{"question": "can you redeem more Bing search reward points than Google's equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Bing search reward points are only valid", "extracted_entity": null}
{"question": "can Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can attend the Paralymp", "extracted_entity": null}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh Blue is a comedian, not an", "extracted_entity": null}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh Blue can play Paralympics Games", "extracted_entity": null}
{"question": "could Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Josh Blue is not eligible to", "extracted_entity": null}
{"question": "could Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could attend Paralympics", "extracted_entity": null}
{"question": "could Josh Blue compete in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has a physical disability. Par", "extracted_entity": null}
{"question": "could Mohammed Ali attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Paralympics Games are for people", "extracted_entity": null}
{"question": "could Stephen Hawking play chess? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I think he", "extracted_entity": null}
{"question": "could Stephen Hawking play computer games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "could Aretha Franklin vote for a president when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Franklin was born on March 25", "extracted_entity": null}
{"question": "could Aretha Franklin vote in an election when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she was a woman.", "extracted_entity": null}
{"question": "can you hide the state of Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can hide it in the G", "extracted_entity": null}
{"question": "can you hide Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can hide it in the Irish", "extracted_entity": null}
{"question": "does soy milk cause heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Soy milk is a good source of protein and", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A Jain would argue that there is no hell", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jains do not believe in a god or gods", "extracted_entity": null}
{"question": "wold a Jain believe in the Buddhist idea of karma?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jains believe in the idea of kar", "extracted_entity": null}
{"question": "are Northern European characters portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. In the play, Mac", "extracted_entity": null}
{"question": "are Northern European characters depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "did Macbeth depict Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Macbeth was based on a true", "extracted_entity": null}
{"question": "did Macbeth feature Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Macbeth was written by a Northern", "extracted_entity": null}
{"question": "did a German-Irish actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did an oscar-winning actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he played Macduff.", "extracted_entity": null}
{"question": "did an actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the actors played themselves.", "extracted_entity": null}
{"question": "did an Oscar-winning actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "were any of the characters in Macbeth played by an Oscar-winning actor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The only Oscar-winning actor in", "extracted_entity": null}
{"question": "was Napoleonic Wars longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Napoleonic Wars lasted from", "extracted_entity": null}
{"question": "did Napoleonic Wars last longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Napoleonic Wars lasted longer than the", "extracted_entity": null}
{"question": "do we need arithmetic to launch rockets into space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we don\u2019t.", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Communist Party of China was founded in", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are.", "extracted_entity": null}
{"question": "do members of the communist party of China come from Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "can Uighurs become members of the communist party of China ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can not.", "extracted_entity": null}
{"question": "are mongolians members of the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "do days get shorter in Seattle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as in the rest", "extracted_entity": null}
{"question": "do days get shorter in Seattle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they get longer.", "extracted_entity": null}
{"question": "is September followed by colder months in the Northern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, September is followed by October, November,", "extracted_entity": null}
{"question": "can I eat Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the best time to eat salmon", "extracted_entity": null}
{"question": "can I hunt Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The season is closed.", "extracted_entity": null}
{"question": "can I hunt Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t. It\u2019s", "extracted_entity": null}
{"question": "can I catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can catch salmon in California in", "extracted_entity": null}
{"question": "can you catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You can catch salmon in California in", "extracted_entity": null}
{"question": "can you fish Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can fish Salmon in September.", "extracted_entity": null}
{"question": "do people speak Germanic languages in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they speak Czech.", "extracted_entity": null}
{"question": "is homeostasis associated with thermoregulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, thermoregulation is associated with home", "extracted_entity": null}
{"question": "can one Ancient Greek letter represent every season of Survivor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you know the Greek alphabet", "extracted_entity": null}
{"question": "do traditional calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "has small children's painting style been profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've sold a few paintings, but", "extracted_entity": null}
{"question": "can small children's painting style be profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a matter of time and effort", "extracted_entity": null}
{"question": "can a judo expert defeat a kata expert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the kata expert is not a", "extracted_entity": null}
{"question": "can a judo expert defeat someone that only does kata?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course. Kata is a set of techniques", "extracted_entity": null}
{"question": "is Jenny McCarthy in same industry as her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Both are actresses.", "extracted_entity": null}
{"question": "do all the animals hedgehogs eat have spinal cords?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, hedgehogs eat mice,", "extracted_entity": null}
{"question": "does number of Hamlet adaptations exceed Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the number of Hamlet adaptations exceed", "extracted_entity": null}
{"question": "is Hamlet's number of movie adaptations similar to Romeo and Juliet's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Romeo and Juliet has had", "extracted_entity": null}
{"question": "is Hamlet one of Shakespeare's most movie adapted plays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. it has been adapted to", "extracted_entity": null}
{"question": "did Queen Margot keep Moliere's severed head?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and she kept it for years.", "extracted_entity": null}
{"question": "would a Lord Protector hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A Tsar is a monarch. A", "extracted_entity": null}
{"question": "could a Tsar hypothetically boss a Lord Protector around?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If the Tsar was a Lord Protector,", "extracted_entity": null}
{"question": "could a Tsar hypothetically tell a Lord Protector what to do?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a Tsar is the supreme r", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a Lord Protector?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the Tsar would be an al", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a Protector of the Realm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A Protector of the Realm would have the", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a US Vice President?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only because the Tsar would be", "extracted_entity": null}
{"question": "can SARS-COV-2 infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "has Leninist ideology influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they want to change the government and", "extracted_entity": null}
{"question": "can United States Secretary of State slap United Kingdom representative without repercussions?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the United States Secretary of State is", "extracted_entity": null}
{"question": "were Whirling Dervishes in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Whirling Dervishes are", "extracted_entity": null}
{"question": "is Reiki stored in a pill bottle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Reiki is not stored in a pill", "extracted_entity": null}
{"question": "does Reiki require refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Reiki does not require refills.", "extracted_entity": null}
{"question": "does Reiki require frequent refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki is a natural, self-", "extracted_entity": null}
{"question": "does Reiki require a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki does not require a prescription", "extracted_entity": null}
{"question": "can viruses infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "have politics been influenced by Marxist ideology in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "are most Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is Tokyo Tower close to city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tokyo Tower is a television tower in the Shiba", "extracted_entity": null}
{"question": "is Tokyo Tower in same country as famed city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Tokyo Tower is in the country of Japan", "extracted_entity": null}
{"question": "can someone have a long life after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is made of shoes and", "extracted_entity": null}
{"question": "is shoe soup relatively harmless?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is not a", "extracted_entity": null}
{"question": "can a person survive eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are a troll.", "extracted_entity": null}
{"question": "can a person survive after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not possible for a person to", "extracted_entity": null}
{"question": "are all jokes harmless to health?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Some jokes can be harmful to", "extracted_entity": null}
{"question": "could someone in Boston watch the same episode of The Tonight Show in two days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the same episode would not be shown", "extracted_entity": null}
{"question": "could someone in Houston see the opening of The Tonight Show before the day change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Tonight Show was a New York", "extracted_entity": null}
{"question": "could someone in Houston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the time zone difference.", "extracted_entity": null}
{"question": "could someone in Boston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they had cable", "extracted_entity": null}
{"question": "is Beauty and the Beast (1991 film) another Disney pilfered plot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The original fairy tale by Jeanne", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically play a full game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. His eldest child, Eric, is", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically play a game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are siblings.", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in half a day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Centurylink max internet plan is not", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in a quarter day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if Centurylink offers a", "extracted_entity": null}
{"question": "is LG Electronics located in a city with an official bird that has a blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if you answered yes, you are correct.", "extracted_entity": null}
{"question": "did Holy Land belong to Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and to Noah's people after", "extracted_entity": null}
{"question": "can Holy Land be claimed by Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Holy Land is the birthplace of Adamu'", "extracted_entity": null}
{"question": "did Adamu's people have claim to Holy Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Adamu's people had claim to", "extracted_entity": null}
{"question": "did Adamu's people have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is claimed that they did, but", "extracted_entity": null}
{"question": "did Assyrians have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Assyrians were not in the", "extracted_entity": null}
{"question": "did another group have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Hebrews, who were the ancestors", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land other than three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Jews.", "extracted_entity": null}
{"question": "would US states in 1900 be enough to divide up ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there would be enough states to divide up", "extracted_entity": null}
{"question": "is Armageddon exclusive to the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The concept of Armageddon is found", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The land was uninhabited and", "extracted_entity": null}
{"question": "were there worse vote differences than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were worse vote differences than 1", "extracted_entity": null}
{"question": "were there worse defeats than Carter's 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and I'll give you a hint", "extracted_entity": null}
{"question": "will sports announcing be possible with Bell's palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it will be possible.", "extracted_entity": null}
{"question": "can Bell's palsy sufferers still have productive show business careers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you can still smile, you can", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in three minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. I\u2019ve done it", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in half an hour?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is possible to finish reading The", "extracted_entity": null}
{"question": "was Florence ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Florence was never a theocracy.", "extracted_entity": null}
{"question": "was Flornce ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was a short lived theocracy", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell name of Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can United States Army Rangers complete a Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Mozambique Drill is a technique of", "extracted_entity": null}
{"question": "can United States Army Rangers perform a successful Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they will be shot in the back", "extracted_entity": null}
{"question": "can Kelly Clarkson's singing break glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It can't.", "extracted_entity": null}
{"question": "did multiple weekdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were multiple weekdays during the Apol", "extracted_entity": null}
{"question": "did two Wednesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1971.", "extracted_entity": null}
{"question": "did two Tuesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, one Tuesday passed at the beginning", "extracted_entity": null}
{"question": "did two Mondays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Apollo 15 mission last", "extracted_entity": null}
{"question": "did Maya Angelou win a Triple Crown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "are felidae represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no felidae represented on the", "extracted_entity": null}
{"question": "does the Atlantic reach 500 battalions worth of people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. 500 battalions is", "extracted_entity": null}
{"question": "can Cheb Mami possibly win a Pulitzer Prize in History?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He\u2019s an Arab, and he", "extracted_entity": null}
{"question": "did Martin Luther believe in God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is Martin Luther celebrated by many people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Martin Luther is celebrated by many people.", "extracted_entity": null}
{"question": "was Martin Luther capable of logical thinking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Luther was a well-educated", "extracted_entity": null}
{"question": "was Martin Luther susceptible to infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was susceptible to infection", "extracted_entity": null}
{"question": "was Martin Luther susceptible to death by infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was. He was a man.", "extracted_entity": null}
{"question": "do people still fear the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but that is changing.", "extracted_entity": null}
{"question": "can a swastika cause emotional pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a symbol of the nazis", "extracted_entity": null}
{"question": "can a swastika cause pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A swastika is a symbol that is used", "extracted_entity": null}
{"question": "is a nerd a common character in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "did the Great Depression break up families?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Depression did break up families", "extracted_entity": null}
{"question": "is dopamine snorted up the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is injected.", "extracted_entity": null}
{"question": "is dopamine snorted through the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dopamine is not snorted through", "extracted_entity": null}
{"question": "can you buy Reddit at Barnes & Noble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a website.", "extracted_entity": null}
{"question": "could an elephant seal reach the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Lake Superior is not very deep", "extracted_entity": null}
{"question": "could an elephant seal dive to the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could.", "extracted_entity": null}
{"question": "could an elephant seal dive to the bottom of Lake Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. An elephant seal is a", "extracted_entity": null}
{"question": "could a nymph tick pass through a ISO 838 hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a nymph tick could pass", "extracted_entity": null}
{"question": "did Wilbur Scoville know about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Wilbur Scoville care about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a pharmacist", "extracted_entity": null}
{"question": "did George Washington cross the Potomac in a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he crossed the Delaware in a row", "extracted_entity": null}
{"question": "did Edward of Westminster ever spend time in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was at the siege of Gl", "extracted_entity": null}
{"question": "was Edward of Westminster ever in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Edward was at Gloucester in", "extracted_entity": null}
{"question": "is Antarctica ideal for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Groundhog Day", "extracted_entity": null}
{"question": "did it get dark in Tennessee during the Solar eclipse of August 21, 2017?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it got dark in Tennessee during the Sol", "extracted_entity": null}
{"question": "are polar bears indigenous to the Golan Heights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are a sub-species of the", "extracted_entity": null}
{"question": "is it easier for a human to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the water in the Atlantic Ocean is", "extracted_entity": null}
{"question": "is it easier to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because the Atlantic Ocean is warmer and the", "extracted_entity": null}
{"question": "is it easier to float in the Dead Sea than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Dead Sea is saltier than the", "extracted_entity": null}
{"question": "is Canada south of the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is north of the Arctic Ocean", "extracted_entity": null}
{"question": "would John the Baptist be alone at a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would be the life of the party", "extracted_entity": null}
{"question": "could Jamie Lee Curtis buy a package of Marlboro in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the California legislature passed a law", "extracted_entity": null}
{"question": "could Jamie Lee Curtis buy a bottle of chardonnay in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she could, because she is over", "extracted_entity": null}
{"question": "could Jamie Lee Curtis purchase alcohol in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she is 21 or older", "extracted_entity": null}
{"question": "could 2020 Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Roman fleet was larger, better equ", "extracted_entity": null}
{"question": "could 2020 Brazilian Navy hypothetically win Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Brazilian Navy is much more", "extracted_entity": null}
{"question": "would a plate of crustaceans be appropriate for Diwali?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are a Hindu, you", "extracted_entity": null}
{"question": "would an Olmec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is the same style of building", "extracted_entity": null}
{"question": "would an Incan recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they were never there.", "extracted_entity": null}
{"question": "would an Aztec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "would an Aztec warrior recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not recognize it.", "extracted_entity": null}
{"question": "would Mickey Mouse be camouflaged by the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he's an American.", "extracted_entity": null}
{"question": "can you fish Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, salmon fishing is open through September", "extracted_entity": null}
{"question": "have the Dukes of Beaufort ever been to Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they\u2019ve never been to Glouc", "extracted_entity": null}
{"question": "do people interested in numismatics care about British royals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do people interested in numismatics care about British royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, especially the Royal Mint, which is", "extracted_entity": null}
{"question": "do people interested in numismatics care about British kings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do people interested in numismatics care about Edward III?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they do, and this is why:", "extracted_entity": null}
{"question": "did Pink Floyd sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the song \u201cThe Wall\u201d", "extracted_entity": null}
{"question": "did Roger Waters sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he sang about the Wall Street in New", "extracted_entity": null}
{"question": "is Bing useless for anonymous browsing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bing is a search engine developed by Microsoft.", "extracted_entity": null}
{"question": "is a lacrosse ball blocked by carapace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the lacrosse ball is blocked by", "extracted_entity": null}
{"question": "are there bridges people would rather fall from than Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I can\u2019t think of any, but I", "extracted_entity": null}
{"question": "can someone survive a fall from Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they survive the impact", "extracted_entity": null}
{"question": "is a fall from Golden Gate bridge likely fatal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is likely fatal", "extracted_entity": null}
{"question": "did Popeye eat an unhealthy amount of spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can you catch Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can catch salmon in September.", "extracted_entity": null}
{"question": "has Leninism affected politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has.", "extracted_entity": null}
{"question": "did Pink Floyd play a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did. The song was called \"", "extracted_entity": null}
{"question": "is Return of the Jedi's Han Solo bad hypothetical United States Air Force candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a smuggler,", "extracted_entity": null}
{"question": "did Stone Cold Steve Austin outshine Four Horsemen's tag team accomplishments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Stone Cold Steve Austin better tag teamer than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd say the Horsemen, but", "extracted_entity": null}
{"question": "does AC/DC's success trump Lil Wayne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. AC/DC's success trumps", "extracted_entity": null}
{"question": "do large mammals exhibit human jealousy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not quite the same as", "extracted_entity": null}
{"question": "is human jealousy exhibited in larger mammals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, jealousy is exhibited in larger", "extracted_entity": null}
{"question": "does the goat zodiac hypothetically make good entertainers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are very creative, energet", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 18th century judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The 18th century judges", "extracted_entity": null}
{"question": "is Hilo Hawaii an ideal location for snowboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's in Hawaii", "extracted_entity": null}
{"question": "could Tata Hexa get the Spice Girls to a nearby venue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Tata Hexa could get", "extracted_entity": null}
{"question": "would Fortuner be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure, but I\u2019d", "extracted_entity": null}
{"question": "would Tata Hexa be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Spice Girls are not going to India", "extracted_entity": null}
{"question": "would Tata Hexy be more roomy for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Hexa is not a h", "extracted_entity": null}
{"question": "can Julia Roberts hang out with BTS?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but she can hang", "extracted_entity": null}
{"question": "can Julia Roberts ride Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if she wants to.", "extracted_entity": null}
{"question": "can Julia Roberts enjoy Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she\u2019s dead.", "extracted_entity": null}
{"question": " Is The Invisible Man its author's most adapted book to film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, H.G. Wells'", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Pirate's Flight at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's a pirate.", "extracted_entity": null}
{"question": "is Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is The Island of Dr Moreau", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "is H.G. Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's The Time Machine.", "extracted_entity": null}
{"question": "can Justin Timberlake ride the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is a douchebag", "extracted_entity": null}
{"question": "is H.G. Wells' most prolific book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The most prolific book to film", "extracted_entity": null}
{"question": "is The Invisible Man H.G. Wells most adapted film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Invisible Man is H.G", "extracted_entity": null}
{"question": "academy Awards most prolific amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "academy Awards most prolific amusement park", "extracted_entity": null}
{"question": "academy Awards most celebrated amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "academy Awards most celebrated amusement park guy", "extracted_entity": null}
{"question": "was Walt Disney most celebrated amusement park creator by Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "would Shaun King disagree with Immanuel Kant's views?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shaun King, a social justice activist", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant disagree about race?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would disagree about race.", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant not see eye to eye about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shaun King and Immanuel Kant would not", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant disagree about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "is disco still influencing music in the 2000s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the 2000s", "extracted_entity": null}
{"question": "is Bornean Orangutan higher on World Wildlife Fund priority list than Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Leopard cat is higher than Borne", "extracted_entity": null}
{"question": "does World Wildlife Fund prioritize Bornean Orangutan over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bornean Orangutan is", "extracted_entity": null}
{"question": "is Bornian Orangutan prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Leopard cat is a priority species,", "extracted_entity": null}
{"question": "is Black Rhino prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. WWF does not prioritize one", "extracted_entity": null}
{"question": "has Stone Cold Steve Austin had more Intercontinental success than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he\u2019s had more success than", "extracted_entity": null}
{"question": "is genre that dominated late 1970's music still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think it is.", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still relevant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course, I think it's still relevant", "extracted_entity": null}
{"question": "did one of the Beatles have similar taste in women to Eric Clapton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. George Harrison.", "extracted_entity": null}
{"question": "are mollusks important to the history of the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, mollusks are important to the", "extracted_entity": null}
{"question": "is Bill Gates more productive than the country of Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bill Gates is more productive than", "extracted_entity": null}
{"question": "is Bill Gates more productive than Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bill Gates has more money than Domin", "extracted_entity": null}
{"question": "did the Bible borrow from ancient myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Bible borrowed from ancient myth,", "extracted_entity": null}
{"question": "did ancient myths inspire the Bible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Bible was inspired by God, not", "extracted_entity": null}
{"question": "could a single bitcoin ever buy a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could.", "extracted_entity": null}
{"question": "is black pepper often combined with a substance that contains chloride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, black pepper is often combined with a", "extracted_entity": null}
{"question": "is it best to avoid chocolate with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There is no one-size-fits-", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be one of Bacchus's favorite holidays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bacchus was the god of wine,", "extracted_entity": null}
{"question": "has Drew Carey surpassed Doug Davidson at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Drew Carey is the host of", "extracted_entity": null}
{"question": "has Drew Carey surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as host of The Price Is Right.", "extracted_entity": null}
{"question": "is music genre that dominated late 70's still alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as much as it was in", "extracted_entity": null}
{"question": "is Drew Carey one of the longest Price is Right hosts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Drew Carey is one of the", "extracted_entity": null}
{"question": "is Drew Carey second longest Price is Right Host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he's second longest to Bob B", "extracted_entity": null}
{"question": "does it snow in the arctic circle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "has Leninism influenced politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has influenced politics in Bohemia.", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is still evolving today", "extracted_entity": null}
{"question": "has Drew Carey's surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Drew Carey is the current host", "extracted_entity": null}
{"question": "are more tamarinds grown in Honduras than Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only because Alaska is in the", "extracted_entity": null}
{"question": "would a tamarind tree have difficulty growing in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the tree would be a tropical tree and", "extracted_entity": null}
{"question": "would a tamarind tree grow poorly in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "would a tamarind tree grow poorly in Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is not a tropical plant. It", "extracted_entity": null}
{"question": "is shrimp essential to Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are many cuisines in Eth", "extracted_entity": null}
{"question": "does Drew Carey's Price is Right tenure surpass Dennis James's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, James hosted the show for 3 years", "extracted_entity": null}
{"question": "are more tamarinds grown in Honduras than Canada?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, tamarinds are grown in Hond", "extracted_entity": null}
{"question": "would someone in a sauna be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is A. Hypothermia is", "extracted_entity": null}
{"question": "can a student from Beder understand the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "can a student from Beder read the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he has a good knowledge of Alban", "extracted_entity": null}
{"question": "can all Suikoden characters find dates among Final Fantasy VI characters?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you will need to find a Su", "extracted_entity": null}
{"question": "would a tenth of McCain's electoral votes make John Kerry President in 2004?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kerry won 251 elect", "extracted_entity": null}
{"question": "could Amazon defeat Iceland in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes, because Amazon has more money", "extracted_entity": null}
{"question": "did Jerry Seinfeld's mouth help him earn a lot of cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he earned a lot of cash.", "extracted_entity": null}
{"question": "did Jerry Seinfeld's mouth help him earn a substantial amount of money?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "did Jerry Seinfeld earn a lot of money using his mouth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he earned a lot of money using his", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Vishnu?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn't.", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Brahma?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a very close friend of Bra", "extracted_entity": null}
{"question": "would Alan Alda fit in on the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he'd fit in on any cast.", "extracted_entity": null}
{"question": "does Secretariat have a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has 4 championships", "extracted_entity": null}
{"question": "does Secretariat hold a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he holds the record for Formula One champion", "extracted_entity": null}
{"question": "will a living thing survive the entirety of Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it is a microbe.", "extracted_entity": null}
{"question": "is the Greek alphabet barred for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Greek alphabet is not barred for use in", "extracted_entity": null}
{"question": "is the Greek alphabet prohibited for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a very rare sight to", "extracted_entity": null}
{"question": "is the Greek alphabet prohibited for use in Korean pop music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Greek alphabet is prohibited for use", "extracted_entity": null}
{"question": "can Tony Blair be imprisoned in the Tower of London?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is found guilty of War Cr", "extracted_entity": null}
{"question": "do travelers to Casablanca have a hard time finding a BLT?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "does Jamie Oliver care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he does care about flavor.", "extracted_entity": null}
{"question": "does Gordon Ramsey know about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he does not know about solub", "extracted_entity": null}
{"question": "does Gordon Ramsey care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "can people in San Antonio cheer on the Memphis Grizzlies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "does Moon Jae-in own multiple species of pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he owns a dog and a cat", "extracted_entity": null}
{"question": "does Moon Jae-in support animal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "does Moon Jae-in own any pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has two cats and a dog", "extracted_entity": null}
{"question": "do humans pose a threat to Bengal foxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do. The Bengal fox", "extracted_entity": null}
{"question": "do Bengal foxes take care of their children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "did a manta ray murder Steve Irwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a sting ray.", "extracted_entity": null}
{"question": "does celery contain any essential nutrients?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Celery is a good source of potassium", "extracted_entity": null}
{"question": "has celery been used for purposes other than food?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, celery has been used as a medicine", "extracted_entity": null}
{"question": "did one person cause the price of celery to spike at any time in the past?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you look at the data.", "extracted_entity": null}
{"question": "was Bill Clinton President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "can computers play Poker better than humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they are better than humans.", "extracted_entity": null}
{"question": "is it possible to make a living only playing Poker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But it's not easy.", "extracted_entity": null}
{"question": "could James Brown's ex-wives play a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They couldn't even play a singles", "extracted_entity": null}
{"question": "could James Brown's ex-wives have played a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because there was only one James Brown.", "extracted_entity": null}
{"question": "could James Brown's ex-wives have held a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (James Brown had 4 wives", "extracted_entity": null}
{"question": "did slaves play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "can Evander Holyfield consume pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a Muslim.", "extracted_entity": null}
{"question": "can an artisan sell items to someone in another country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the person has the funds to pay", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau require subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but it's", "extracted_entity": null}
{"question": "did Malcolm X play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he played a role in the history of", "extracted_entity": null}
{"question": "could one Amazon stock ever buy twenty four year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible.", "extracted_entity": null}
{"question": "can French Toast cause Salmonella poisoning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the eggs are not cooked properly", "extracted_entity": null}
{"question": "were there any pandemics during Moon Jae-in's presidency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Coronavirus Pandemic began", "extracted_entity": null}
{"question": "is the paleo diet healthy for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The paleo diet is not healthy for", "extracted_entity": null}
{"question": "does soy milk increase heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There is no evidence that soy milk increases the", "extracted_entity": null}
{"question": "are Northern Europeans depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Norwegians are depicted as villa", "extracted_entity": null}
{"question": "are Northern Europeans portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are", "extracted_entity": null}
{"question": "can a one armed drummer be elite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as he has one arm he", "extracted_entity": null}
{"question": "has Leninism influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it has.", "extracted_entity": null}
{"question": "is ABBA's 1970's music genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's music.", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still evolving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. In fact, it is", "extracted_entity": null}
{"question": "would Alan Alda fit in on Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's a great actor", "extracted_entity": null}
{"question": "does Moon Jae-in own both cats and dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "was Franjo Tudjman President of Croatia when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes (1995)", "extracted_entity": null}
{"question": "is the paleo diet beneficial for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the paleo diet is beneficial", "extracted_entity": null}
{"question": "was Al Gore Vice President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a Senator from Tennessee.", "extracted_entity": null}
{"question": "is Noah's Ark more imposing than Batillus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (The Batillus is 1", "extracted_entity": null}
{"question": "does a horse fly neigh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, horses can fly", "extracted_entity": null}
{"question": "could Brooke Shields handle University of Pennsylvania curriculum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She's a smart girl.", "extracted_entity": null}
{"question": "would University of Pennsylvania admit actress Brooke Shields?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would be admitted.", "extracted_entity": null}
{"question": "did actress Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she was a good student in high", "extracted_entity": null}
{"question": "did Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was a great student and had a", "extracted_entity": null}
{"question": "were many young females attracted to Justin Timberlake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a teen heartthrob", "extracted_entity": null}
{"question": "did Justin Timberlake have many young female fans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he had many young female fans", "extracted_entity": null}
{"question": "would a physician be unlikely to prescribe Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no.", "extracted_entity": null}
{"question": "can Viper Room crowd squeeze into National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "can National Diet building accommodate Viper Room crowd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it is a large place.", "extracted_entity": null}
{"question": "would John Muir be unlikely to have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would not be.", "extracted_entity": null}
{"question": "would Alan Alda be comfortable on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he'd be too comfortable.", "extracted_entity": null}
{"question": "would Alan Alda fit in on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He'd be the only one who'd", "extracted_entity": null}
{"question": "would Alan Alda fit in on the set of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Absolutely. He's a great actor", "extracted_entity": null}
{"question": "would Alan Alda fit in with the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He would play a character who is a", "extracted_entity": null}
{"question": "would John Muir dislike being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he liked being outside", "extracted_entity": null}
{"question": "would John Muir hate being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not hate being inside all day", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen Romeo and Juliet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Romeo and Juliet was written", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen the plays of Shakespeare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she was a queen and she could", "extracted_entity": null}
{"question": "could someone have visited the Library of Alexandria and the pyramids on the same vacation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so, but I could", "extracted_entity": null}
{"question": "would Gal Gadot be hired as Victoria Secret model before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Gal Gadot has a more", "extracted_entity": null}
{"question": "do middle school students typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Pythagoras developed a theorem", "extracted_entity": null}
{"question": "does the bengal fox have hair covering it's body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Bengal fox has hair covering", "extracted_entity": null}
{"question": "would a librarian use a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would. I would use a computer", "extracted_entity": null}
{"question": "can Linus Torvald's family ride in a Maruti Suzuki Baleno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can't.", "extracted_entity": null}
{"question": "can a Maruti Suzuki Baleno transport Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can Royal Albert Hall hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can only hold 5,2", "extracted_entity": null}
{"question": "can Madison Square Garden hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can hold every McDonald's", "extracted_entity": null}
{"question": "could Titanic hypothetically hold Spanish-American War combat casualties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the Spanish-American War was over", "extracted_entity": null}
{"question": "could Spanish-American War combat casualties hypothetically fit on Titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Spanish-American War was fought in", "extracted_entity": null}
{"question": "did Ptolemy use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he used the Julian calendar.", "extracted_entity": null}
{"question": "do camels live in Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Africa and Asia", "extracted_entity": null}
{"question": "will Victoria's Secret hire Gal Gadot before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they won't.", "extracted_entity": null}
{"question": "do 10-12 year olds typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "does the little penguin share space with Kiwis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the little penguin shares space with", "extracted_entity": null}
{"question": "does horseradish have hooves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn't.", "extracted_entity": null}
{"question": "is paper a product of logging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Paper is made from trees.", "extracted_entity": null}
{"question": "would Boticelli's mythological subjects fill a gallery before Salvador Dali's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Boticelli's subjects were", "extracted_entity": null}
{"question": "would ten gallons of seawater hold a six year old in the air on a seesaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the seesaw is in the water", "extracted_entity": null}
{"question": "are Genghis Khan's descendants more prolific than those of Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "was an event in the Balkans considered a contributing factor to World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was the assassination of the Arch", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field an NHL team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the crew of Apollo 13", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field a full ice hockey team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Apollo 13 had 5", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field a full basketball team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they had a 12th", "extracted_entity": null}
{"question": "was Fraktur likely to have been used in the publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019ve never", "extracted_entity": null}
{"question": "was Fraktur likely to have been used in publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I don\u2019", "extracted_entity": null}
{"question": "does the spinal cord play a role in skateboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the spinal cord plays a role in", "extracted_entity": null}
{"question": "does the visual cortex play a role in regulating metabolic function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the visual cortex plays a role in", "extracted_entity": null}
{"question": "does the visual cortex play a role in regulating lung function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The visual cortex plays a role in", "extracted_entity": null}
{"question": "can people see the code behind OpenOffice.org?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to sign a non-", "extracted_entity": null}
{"question": "does a Jetta require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it does need a parking space", "extracted_entity": null}
{"question": "does a Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It has a sensor on the back that", "extracted_entity": null}
{"question": "does a Toyota Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It has four wheels and it can", "extracted_entity": null}
{"question": "could the children of Greek hero Jason hypothetically field a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would have to field 1", "extracted_entity": null}
{"question": "is the artist of the album Slay Belles a drag queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she is a queen", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind The Brig?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was a fan of the play", "extracted_entity": null}
{"question": "did John Lennon collaborate with the man who made the film Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Beethoven ever compose an EDM song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Beethoven never composed an EDM", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film starring Michael Palin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Swallow played a role in the film \"", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film starring John Cleese?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He collaborated with the filmmaker behind", "extracted_entity": null}
{"question": "was John Searle born in a state that is bordered by Wyoming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "does the Illuminati leave messages in modern society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all the time.", "extracted_entity": null}
{"question": "would King Henry V hypothetically demolish Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the battle of Agincourt was a", "extracted_entity": null}
{"question": "would King Henry V hypothetically prevail against Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because King Henry V had better weaponry", "extracted_entity": null}
{"question": "would King Henry V hypothetically defeat Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. King Henry V was a great warrior", "extracted_entity": null}
{"question": "would Constantinople repel a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The city was not well defended and", "extracted_entity": null}
{"question": "would Constantinople survive a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but I think so", "extracted_entity": null}
{"question": "would Constantinople withstand a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Charlemagne had a bigger army than", "extracted_entity": null}
{"question": "in the year 1000, did the Eastern Orthodox Church and the Byzantine Empire use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin apply his finisher to the largest mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "is kaffir lime the man ingredient in key krueng?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Key krueng is a type of", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam two White-tailed deer simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES, and he can make them submit.", "extracted_entity": null}
{"question": "can someone from Denmark go to Sweden without their passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you need to have a passport to", "extracted_entity": null}
{"question": "can a male mule deer be body slammed by Stone Cold Steve Austin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not if he's wearing", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam a male mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's just a matter of finding", "extracted_entity": null}
{"question": "are most of the products labelled Fair Trade exported from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. Most are imported from", "extracted_entity": null}
{"question": "was Harry Houdini's wife bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "was Harry Houdini bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he exposed them as frauds", "extracted_entity": null}
{"question": "did Harry Houdini disprove the claims of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Houdini was a magician.", "extracted_entity": null}
{"question": "is Drew Carey in a sports Hall of Fame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Drew Carey is not in a", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rand has valid eye exam certification in every state", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rand has valid eye exam certification in", "extracted_entity": null}
{"question": "is Rand Paul prohibited from conducting eye exams?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not prohibited from conducting", "extracted_entity": null}
{"question": "can a Toyota Hilux transport a human heart from NYC to Raleigh NC?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would take 23 hours", "extracted_entity": null}
{"question": "can a Toyota Hilux get to Havana from NYC before sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not without a time machine.", "extracted_entity": null}
{"question": "can a Toyota Hilux get to Havana from NYC in sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have a big enough trailer", "extracted_entity": null}
{"question": "is one of the ingredients of photosynthesis found in emeralds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the ingredients of photosynthesis", "extracted_entity": null}
{"question": "can you pour diethyl ether?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, diethyl ether is a liquid", "extracted_entity": null}
{"question": "did Larry King fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was born in 193", "extracted_entity": null}
{"question": "have any WWE champions outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Kane's record still stands", "extracted_entity": null}
{"question": "has a Neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Clarence Thomas is on the Supreme", "extracted_entity": null}
{"question": "are infants excluded from joining the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "can a worm suffer a myocardial infarction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can every Prime Minister of the United Kingdom since 1970 fill all the US Supreme Court chairs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Margaret Thatcher would be left out", "extracted_entity": null}
{"question": "can every Prime Ministers of the United Kingdom since 1970 fill all the US Supreme Court positions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only Margaret Thatcher can do that", "extracted_entity": null}
{"question": "will Justin Bieber replace Mike Pence in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I think that Mike", "extracted_entity": null}
{"question": "will Justin Bieber be Trump's running mate in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will be too busy trying to get", "extracted_entity": null}
{"question": "can Justin Bieber represent New York in Congress in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is 25 years old,", "extracted_entity": null}
{"question": "will Justin Bieber become US President in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he runs.", "extracted_entity": null}
{"question": "will Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he will be the first President to", "extracted_entity": null}
{"question": "can Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He is a Canadian.", "extracted_entity": null}
{"question": "were there highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they were born in the 19", "extracted_entity": null}
{"question": "did cars travel on highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did members of the Silent Generation use highways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as much as the Baby Bo", "extracted_entity": null}
{"question": "can a rabbi perform any Christian rituals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a rabbi cannot perform any Christian rit", "extracted_entity": null}
{"question": "can baptism be done by a rabbi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only a priest can do baptism.", "extracted_entity": null}
{"question": "can a Rabbi perform a Christian baptism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Rabbi is not allowed to perform", "extracted_entity": null}
{"question": "can a Rabbi administer any of the Christian Sacraments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A Rabbi is a Jew and Jews", "extracted_entity": null}
{"question": "were there typewriters in the Empire of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were.", "extracted_entity": null}
{"question": "did people listen to The Blues during World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, people listened to the blues during World", "extracted_entity": null}
{"question": "can a person from New Brunswick understand the language of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not speaking the same language", "extracted_entity": null}
{"question": "can a person from New Brunswick understand the speech of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not understand each other.", "extracted_entity": null}
{"question": "can a person from Madagascar order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. New Brunswick is a biling", "extracted_entity": null}
{"question": "can a person from Burundi order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Burundi is one of the", "extracted_entity": null}
{"question": "do lizards breed in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "can lizards survive temperatures in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they would have to be in a", "extracted_entity": null}
{"question": "is the Swastika a religious symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The swastika is an ancient symbol that is", "extracted_entity": null}
{"question": "did Japanese people win the War of the Bucket?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Americans were the ones who started", "extracted_entity": null}
{"question": "can you transport nine pounds of weed in a bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not easy.", "extracted_entity": null}
{"question": "did the husband of Elizabeth Hamilton work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the husband of Elizabeth Hamilton was working with", "extracted_entity": null}
{"question": "did the husband of Alexander Hamilton's granddaughter work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the husband of Alexander Hamilton's grand", "extracted_entity": null}
{"question": "did Elizabeth Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Elizabeth Hunter's husband did not work", "extracted_entity": null}
{"question": "did Elizabeth Margaret Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he worked with a captain.", "extracted_entity": null}
{"question": "do Adobe Suite and Life Is Strange video game share similar coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, both games are coded in C++", "extracted_entity": null}
{"question": "did Sugar Ray Robinson ever experience pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did the creator of Saturday Night Live ever work with Jack Paar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a writer for The Tonight", "extracted_entity": null}
{"question": "did the creator of Saturday Night Live ever work with Johnny Carson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lorne Michaels was a writer", "extracted_entity": null}
{"question": "does the author of Rebecca use Cornwall in another novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the author of Rebecca, D", "extracted_entity": null}
{"question": "could a participant in the American Civil War read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in English.", "extracted_entity": null}
{"question": "could the winner of a Nobel Peace Prize have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it was Shakespeare.", "extracted_entity": null}
{"question": "could the author of The Bell Jar have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He could have read any book he wanted", "extracted_entity": null}
{"question": "could Saint Augustine have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have.", "extracted_entity": null}
{"question": "can a human safely consume Cannabis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the human body is equipped with the", "extracted_entity": null}
{"question": "did Metallica ever release a rap song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they did release a rap metal song", "extracted_entity": null}
{"question": "does the name C-SPAN refer to a form of telecommunications?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a cable-only network", "extracted_entity": null}
{"question": "could someone watch C-Span on cable in 1980?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I watched it.", "extracted_entity": null}
{"question": "was C-Span on the air in 1987?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was founded in 197", "extracted_entity": null}
{"question": "did Christopher Hitchens ever appear on C-Span?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was on C-Span a few", "extracted_entity": null}
{"question": "did Brian Lamb ever host a television program on C-SPAN?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has not.", "extracted_entity": null}
{"question": "is Battle Hymn of the Republic considered christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a patriotic song.", "extracted_entity": null}
{"question": "did Fanny Crosby write Christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she wrote over 8,00", "extracted_entity": null}
{"question": "is Christian music played in Churches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is played in churches.", "extracted_entity": null}
{"question": "did the founders of Encyclop\u00e6dia Britannica enjoy views of the Firth of Forth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The first volume of the Encyclop\u00e6dia Britannica", "extracted_entity": null}
{"question": "was the country featured in All My Life one of the participants in the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The country was not one of the participants", "extracted_entity": null}
{"question": "is an economist able to determine why the price of a burrito changes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Economists can only determine how the price", "extracted_entity": null}
{"question": "is an economist able to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are able to measure the effects", "extracted_entity": null}
{"question": "is an economist qualified to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was Manny Pacquiao a boxing champion when Obama was first elected US president?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "did Martin Luther know Latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was a professor of Theology in", "extracted_entity": null}
{"question": "will Jay-Z reach the age of 60 before Kendrick Lamar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but if he doesn", "extracted_entity": null}
{"question": "did Jay-Z know Kanye West in 2012? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jay-Z and Kanye West have a", "extracted_entity": null}
{"question": "did Jay-Z know Kanye West in 2011? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Kanye West and Jay-Z", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Prussian is a dialect of German", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they both speak German.", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with someone from Austria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, German was the same in both places.", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever play in a park designed by a poet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was designed by a landscape architect.", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever play as a child in Leasowes Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Robert Plant, the lead singer of Led", "extracted_entity": null}
{"question": "did Abraham Lincoln help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Abraham Lincoln responsible for founding the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did have a hand in found", "extracted_entity": null}
{"question": "was the current U.S. speaker of the house alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nancy Pelosi was born in 1", "extracted_entity": null}
{"question": "does Gavin Rossdale have a large enough net worth to purchase the most expensive Thoroughbred horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Gavin Rossdale has a large", "extracted_entity": null}
{"question": "would nickel boil if placed in the core of Neptune?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, nickel is a metal, and it", "extracted_entity": null}
{"question": "is there a state that touches several of the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michigan, Minnesota, Wisconsin, and Illinois.", "extracted_entity": null}
{"question": "are any of the Great Lakes entirely in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lake Michigan", "extracted_entity": null}
{"question": "do the Great Lakes have at least 15% of the worlds freshwater fish supply?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Great Lakes have at least", "extracted_entity": null}
{"question": "are there any fresh water sand dunes in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are all salt water.", "extracted_entity": null}
{"question": "has anyone drowned in the Great Lakes in 2019?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at least 12 people have d", "extracted_entity": null}
{"question": "has anyone ever drowned in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It has happened.", "extracted_entity": null}
{"question": "do the Great Lakes have bacteria in the water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Great Lakes have bacteria", "extracted_entity": null}
{"question": "do people ice fish on the Great Lakes in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do the Great Lakes freeze in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the winter, the Great Lakes", "extracted_entity": null}
{"question": "do the Great Lakes have more endangered species than other parts of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Great Lakes have fewer endanger", "extracted_entity": null}
{"question": "do the Great Lakes have any endangered species?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Lake Sturgeon is considered end", "extracted_entity": null}
{"question": "is John the Baptist still important to people today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because he is the one who baptized Jesus", "extracted_entity": null}
{"question": "is John the Baptist still worshiped to this day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, John the Baptist was a prophet", "extracted_entity": null}
{"question": "was The Hobbit written before The Lord of the Rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "is David Cameron a Tory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He\u2019s the leader of the Conserv", "extracted_entity": null}
{"question": "is Dragon Hall located in Norfolk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the historic centre of Norwich", "extracted_entity": null}
{"question": "is Norfolk predominantly rural?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is an urban area.", "extracted_entity": null}
{"question": "is Norfolk on the eastern coast of England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "are Rogue DnD characters suited for playing in cities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and here\u2019s why.", "extracted_entity": null}
{"question": "are DnD Ranger characters suited for play in wilderness areas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "could George Washington have recorded a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he could not.", "extracted_entity": null}
{"question": "do Hindus believe Krishna exists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hindus believe Krishna exists", "extracted_entity": null}
{"question": "is Lord Hill's column in Shropshire the tallest of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "are there several  towns located in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are several towns located in Shro", "extracted_entity": null}
{"question": "were Charles Darwin and A.E. Housman born in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they did live there for some time", "extracted_entity": null}
{"question": "is there a david bowie song with the word year or years in the title?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a David Bowie song with", "extracted_entity": null}
{"question": "did the Hopi Indians use a symbol that was similar to the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Hopi Indians used a symbol that", "extracted_entity": null}
