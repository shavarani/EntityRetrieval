{"question": "was Pi an acceptable number of children in 1980s China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "People's_Republic_of_China"}
{"question": "does highest US Court have enough seats for every Prime Minister of the United Kingdom since 1952?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;Prime_Minister_of_the_United_Kingdom"}
{"question": "does Alec Baldwin have more children than Clint Eastwood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alec_Baldwin;Clint_Eastwood"}
{"question": "does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "is a cory catfish likely to eat another living fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "are selfies more dangerous than plague in modern times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Selfie"}
{"question": "are amoebas safe from breast cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Marvel_Comics"}
{"question": "could a hamster experience two leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hamster"}
{"question": "are there some countries where waiting staff need no tip?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Shinto practitioners keep to a kosher diet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Shinto;Kashrut"}
{"question": "is the Yellow Pages the fastest way to find a phone number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Yellow_pages"}
{"question": "could Intel products be purchased at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Intel;McDonald's"}
{"question": "do you have to pass through circle of lust to find Saladin in Dante's Inferno?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saladin;Dante_Alighieri;Inferno_(Dante)"}
{"question": "does Woody Allen await the Second Coming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Woody_Allen;Second_Coming"}
{"question": "do you need a large room if you want to get into 3D printing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "3D_printing"}
{"question": "was Emperor Commodus paid tribute in Pound sterling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Commodus;Pound_sterling"}
{"question": "can French Defence initial move defend against four move checkmate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "Paleolithic_diet;Saint_Patrick's_Day"}
{"question": "do Armenians tend to dislike System of a Down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "System_of_a_Down"}
{"question": "did Ivan the Terrible's father and grandfather have nicer nicknames?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ivan_the_Terrible"}
{"question": "did the confederate states speak Old English before the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Old_English;American_Civil_War"}
{"question": "do Republicans reject all forms of welfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Republican_Party_(United_States)"}
{"question": "are brown rock fish found in the waters surrounding Saint Kitts and Nevis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saint_Kitts;Saint_Kitts_and_Nevis"}
{"question": "is a fever cured by listening to a cowbell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cowbell"}
{"question": "do all parts of the aloe vera plant taste good?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Aloe_vera"}
{"question": "after viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mona_Lisa"}
{"question": "did Mike Tyson do something very different than McGruff's slogan to Evander Holyfield in 1997?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ke", "extracted_entity": "Mike_Tyson;Evander_Holyfield"}
{"question": "can you hide a basketball in a sand cat's ear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Basketball"}
{"question": "could largest asteroid crush a whole city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do Chinese Americans face discrimination at a Federal level in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chinese_Americans;United_States"}
{"question": "is Kobe's famous animal product used in a BLT?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kobe;BLT"}
{"question": "is narcissism's origin a rare place to get modern words from?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Narcissism"}
{"question": "is it unusual to eat spaghetti without a fork?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you drown in a Swan Lake performance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Swan_Lake"}
{"question": "are Christmas trees typically deciduous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can a traffic collision make someone a millionaire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Traffic_collision"}
{"question": "is winter solstice in Northern Hemisphere closer to July than in Southern Hemisphere? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Northern_Hemisphere;Southern_Hemisphere"}
{"question": "can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Ludacris"}
{"question": "have Douglas fir been used to fight wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are common carp sensitive to their environments?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would John the Baptist be invited to a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_the_Baptist"}
{"question": "are there bones in an anchovy pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "did Jerry Seinfeld have reason to cheer in 1986?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jerry_Seinfeld"}
{"question": "is a platypus immune from cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Platypus;Cholera"}
{"question": "is Europa linked to Viennese waltzes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Vienna"}
{"question": "is the tibia required for floor exercises?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tibia"}
{"question": "was The Great Gatsby inspired by the novel 1984?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Great_Gatsby"}
{"question": "does a lapidary work with items that are studied by geologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Geologist"}
{"question": "does Santa Claus hypothetically give Joffrey Baratheon presents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "False", "extracted_entity": "Santa_Claus;Joffrey_Baratheon"}
{"question": "would a Jehovah's witness approve of Alice's Adventures in Wonderland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jehovah's_Witnesses;Alice's_Adventures_in_Wonderland"}
{"question": "can you chew argon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "E.T._the_Extra-Terrestrial;Urban_legend"}
{"question": "does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Coast_to_Coast_AM;Rush_Limbaugh"}
{"question": "is the tongue part of a creature's head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the largest city in New Mexico also known as Yoot\u00f3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_Mexico"}
{"question": "is most coffee produced South of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Amy Winehouse always perform live perfectly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amy_Winehouse"}
{"question": "does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lupita_Nyong'o;Barack_Obama"}
{"question": "do American wheelchair users know what the ADA is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "can brewing occur in a prison environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "will Ronda Rousey hypothetically defeat X-Men's Colossus in a fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ronda_Rousey;X-Men;Colossus_(comics)"}
{"question": "can a believer in agnosticism become pope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Agnosticism"}
{"question": "can French Toast hypothetically kill a Lannister?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Moon Jae-in earn the Abitur as a teenager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Moon_Jae-in;Abitur"}
{"question": "would a kindergarten teacher make a lesson of the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "bably", "extracted_entity": "New_Testament"}
{"question": "are you likely to hear Rammstein playing in smooth jazz clubs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Rammstein;Smooth_jazz"}
{"question": "did Electronic Arts profit from Metroid sales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Electronic_Arts"}
{"question": "is Earth Day celebrated in summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Earth_Day"}
{"question": "is viscosity unimportant in making jello shots?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is waltz less injurious than slam dance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Waltz"}
{"question": "are LinkedIn and LeafedIn related companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "LinkedIn"}
{"question": "can a snow leopard swim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snow_leopard"}
{"question": "can citrus grow in Ulaanbaatar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ulan_Bator"}
{"question": "will a Holstein cow and the Liberty Bell balance out a giant scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Holstein_Friesian_cattle;Liberty_Bell"}
{"question": "is the bull shark more bull than shark?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is World of Warcraft heavier than a loaf of bread?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "World_of_Warcraft"}
{"question": "did Thomas Greenhill's parents violate the concept of monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Monogamy"}
{"question": "would John Muir not likely have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "John_Muir"}
{"question": "did the original lead guitarist of Metallica fail after parting from the band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dave", "extracted_entity": "Metallica"}
{"question": "has a baby ever had a moustache?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shaggy_(musician)"}
{"question": "do solo pianists require a conductor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Kim Kardashian a guru?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kim_Kardashian"}
{"question": "were karaoke and the turtle power tiller patented in the same country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a caracal be defeated by Javier Sotomayor in a high jump competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Caracal;Javier_Sotomayor"}
{"question": "do Jehovah's Witnesses celebrate day before New Year's Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jehovah's_Witnesses"}
{"question": "do moths that live on sloths have family dinners?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they", "extracted_entity": null}
{"question": "can a greyhound walk on two legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Moon Jae-in born outside of Khanbaliq?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Moon_Jae-in;Khanbaliq"}
{"question": "were paparazzi involved in the death of a member of the royal family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can native wolverines be found in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Miami"}
{"question": "did Al Unser Jr. win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Al_Unser,_Jr.;Space_Race"}
{"question": "do the Ubuntu people speak Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ubuntu_(philosophy)"}
{"question": "does Rupert Murdoch's alma mater have more history than the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rupert_Murdoch;United_States"}
{"question": "are there Pink music videos that are triggering for eating disorder patients?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pink", "extracted_entity": "Pink_(singer)"}
{"question": "were gladiators associated with the Coloseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Colosseum"}
{"question": "could Larry King's marriages be counted on two feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Larry_King"}
{"question": "do mountain goats inhabit the summit of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mountain_goat"}
{"question": "will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "AC/DC;Boeing_B-52_Stratofortress;Lil_Wayne"}
{"question": "is Autumn a good time to collect bear pelts in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "did Wednesday have something to do with Thor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Thor"}
{"question": "did Dale Jr hug his dad after their last Daytona 500 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Daytona_500"}
{"question": "can an ostrich fit into the nest of a swallow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Swallow"}
{"question": "would an Olympic athlete be tired out after running a mile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Summer_Olympic_Games"}
{"question": "would the current president of Ohio University hypothetically wear a jockstrap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ohio_University"}
{"question": "would ramen be bad for someone with heart failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "was Christina Aguilera born in the forgotten borough?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christina_Aguilera"}
{"question": "will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Donald_Duck"}
{"question": "were the first missionaries required to attend mass on Sundays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is an ocelot a good present for a kindergartener?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ocelot"}
{"question": "did villain that killed Superman murder Robin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Superman;Robin_(comics)"}
{"question": "would a Germaphobia be able to participate in Judo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Judo"}
{"question": "does  Lionel Richie believe in holistic medicine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lionel_Richie;Alternative_medicine"}
{"question": "is the Fibonacci number sequence longer than every number discovered in Pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was The Canterbury Tales written before One Thousand and One Nights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "The_Canterbury_Tales"}
{"question": "did John Kerry run in the 2010 United Kingdom general election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Kerry;United_Kingdom_general_election,_2010"}
{"question": "does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cuisine_of_Hawaii;Contiguous_United_States"}
{"question": "did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Democratic_Party_(United_States);President_of_the_United_States;United_States"}
{"question": "did original Nintendo have games in same format as Playstation 3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nintendo;PlayStation_3"}
{"question": "would you be more likely to die of hypothermia in New York than Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "New_York;Florida"}
{"question": "was Moliere Queen Margot's ill fated lover?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can parachuting amateurs ignore hurricane force winds bulletins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a ten-pin bowling pin be a deadly weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jennifer_Lawrence;Martin_Scorsese"}
{"question": "would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Anorexia_nervosa"}
{"question": "is cycling a high-risk activity for pelvis fractures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Dale Earnhardt Jr. be considered a newbie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dale_Earnhardt_Jr."}
{"question": "does Final Fantasy VI require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did number of Imams Reza Shah believed in exceed number of Jesus's disciples?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reza_Shah;Jesus"}
{"question": "did Terry Pratchett write about quantum mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Terry_Pratchett"}
{"question": "is it hard for Rahul Dravid to order food at a restaurant in Aurangabad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Rahul_Dravid;Aurangabad,_Maharashtra"}
{"question": "is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "President_of_India;IPhone_11"}
{"question": "are swastikas used in the most common religion in India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "India"}
{"question": "will electric car struggle to finish Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Daytona_500"}
{"question": "does walking across Amazonas put a person's life at risk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Amazonas_(Brazilian_state)"}
{"question": "is Olivia Newton-John hyphenated celebrity name with most letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Olivia_Newton-John"}
{"question": "is Godzilla's image likely grounds for a lawsuit in 2050?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Godzilla"}
{"question": "does ontology require a scalpel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "will silicon wedding rings outsell bromine wedding rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "is Cape Town south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cape_Town;Equator"}
{"question": "would the number 666 appear in a church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can a single honey bee sting multiple humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would someone pay for a coffee in NYC with Euros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "New_York_City"}
{"question": "could a student at the University of Houston see a caracal on campus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "University_of_Houston"}
{"question": "do sun bears stay active during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sun_bear"}
{"question": "is Christmas celebrated during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christmas"}
{"question": "could the endowment of Johns Hopkins University pay off the MBTA debt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Johns_Hopkins_University;Massachusetts_Bay_Transportation_Authority"}
{"question": "was song of Roland protagonist friendly with group that had sagas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Roland_Corporation"}
{"question": "did Dr. Seuss make himself famous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dr._Seuss"}
{"question": "can a goat be used for one of the ingredients in French toast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "French_toast"}
{"question": "can you measure a Caracal with a protractor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Caracal"}
{"question": "for bone growth, is kale more beneficial than spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kale;Spinach"}
{"question": "would only warm weather attire be a good idea on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mercury_(planet)"}
{"question": "do Do It Yourself channels online always show realistic projects?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Do_it_yourself"}
{"question": "could Bart Simpson have owned comics with The Joker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bart_Simpson;Joker_(comics)"}
{"question": "was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harry_S._Truman;United_States_Constitution"}
{"question": "would a baby gray whale fit in a tractor-trailer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gray_whale"}
{"question": "would Emmanuel Macron celebrate Cinco de Mayo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Emmanuel_Macron;Cinco_de_Mayo"}
{"question": "does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Metallica band members cutting their hair hurt their sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Metallica"}
{"question": "could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Moulin_Rouge;Spain;Spanish\u2013American_War"}
{"question": "would a veteran of the Phillippine-American War come home craving SPAM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Spam_(food)"}
{"question": "were Walkman's used in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Walkman;Kingdom_of_Hungary"}
{"question": "will Communion be denied to Wednesday name origin followers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would someone with a nosebleed benefit from Coca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Coca-Cola"}
{"question": "did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Led_Zeppelin;Ernie_Chan"}
{"question": "can Darth Vader hypothetically outdunk Bill Walton without using The Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Darth_Vader;Bill_Walton;The_Force"}
{"question": "does Oprah Winfrey have a degree from an Ivy League university?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Oprah_Winfrey;Ivy_League"}
{"question": "can you watch Rick and Morty in Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mariana_Trench"}
{"question": "did Elizabeth II frequently visit Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elizabeth_II;Queen_Victoria"}
{"question": "can a sesame seed grow in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the Watergate scandal help the Republican party?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Watergate_scandal;Republican_Party_(United_States)"}
{"question": "would Bandy be likely to become popular in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bandy;Texas"}
{"question": "do inanimate objects come alive in Beauty and the Beast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Home Depot sell item in late September zodiac sign symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Home_Depot"}
{"question": "does monster name in West African Folklore that witches send into villages set Scrabble record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Scrabble"}
{"question": "can an American black bear swallow a sun bear whole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "American_black_bear;Sun_bear"}
{"question": "can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Reconstruction_Era"}
{"question": "would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snowdon;Tenzing_Norgay"}
{"question": "can preventive healthcare reduce STI transmission?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would 1996 leap year baby technically be 1 year old in 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "CERN;High_Speed_1"}
{"question": "would a Superbowl Football Game be crowded on the Gettysburg Battlefield?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Super_Bowl;Gettysburg_Battlefield"}
{"question": "can you purchase a dish with injera at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Injera;Taco_Bell"}
{"question": "has Drew Carey outshined Doug Davidson's tenure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Drew_Carey"}
{"question": "could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bitcoin;Volkswagen_Jetta"}
{"question": "is eggplant deadly to most atopic individuals? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Cyndi Lauper use milk substitute in her rice pudding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cyndi_Lauper"}
{"question": "can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "QWERTY"}
{"question": "would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "B\u00f6rek"}
{"question": "are potatoes native to the European continent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Europe"}
{"question": "was Iggy Pop named after his father?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iggy_Pop"}
{"question": "can a 2019 Toyota Hilux hypothetically support weight of thirty Big John Studd clones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Toyota_Hilux;Big_John_Studd"}
{"question": "did Tom Bosley enjoy video games on the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tom_Bosley;Video_game;PlayStation_4"}
{"question": "do German Shepherds worry about the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Germany"}
{"question": "do you find glutamic acid in a severed finger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Glutamic_acid"}
{"question": "can black swan's formation type help spell longest word in Dictionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alexander_Graham_Bell;Nazi_Germany"}
{"question": "could Stephen King join the NASA Astronaut Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Stephen_King;NASA_Astronaut_Corps"}
{"question": "does the art from Family Guy look a lot like the art in American Dad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Family_Guy;American_Dad!"}
{"question": "did DARPA influence Albert Einstein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "AR", "extracted_entity": "DARPA;Albert_Einstein"}
{"question": "is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cheb_Mami;Pulitzer_Prize"}
{"question": "did Moon Jae-in's residence exist when the World Trade Center was completed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Moon_Jae-in;World_Trade_Center_(1973\u20132001)"}
{"question": "would a kaffir lime be a good ingredient for making a candle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "is Glenn Beck known for his mild temper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Glenn_Beck"}
{"question": "was the 1980 presidential election won by a member of the Grand Old Party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Nancy Pelosi have hypothetically been on same side as Gerald Ford?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nancy_Pelosi;Gerald_Ford"}
{"question": "was the MLB World Series held in Newcastle, New South Wales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "World_Series;Newcastle,_New_South_Wales;New_South_Wales"}
{"question": "will a sloth explode if it's not upside down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could Rhode Island sink into the Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Rhode_Island;Bohai_Sea"}
{"question": "would it be hard to get toilet paper if there were no loggers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are banana trees used by judges for maintaining order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Banana"}
{"question": "would four shoes be insufficient for a set of octuplets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "four", "extracted_entity": null}
{"question": "did Alan Rickman have an improperly functioning organ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Rickman"}
{"question": "was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Jeff_Goldblum"}
{"question": "does the United States Navy create radioactive waste?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States_Navy"}
{"question": "is the United States Capitol located near the White House?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_Capitol;White_House"}
{"question": "is winter associated with hot temperatures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Sony definitively win the video game war against Sega?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Sony;Sega"}
{"question": "could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Edward_Snowden"}
{"question": "can you find a snow leopard in the Yucatan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snow_leopard;Yucat\u00e1n"}
{"question": "could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eddie_Murphy"}
{"question": "is it common to see frost during some college commencements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Walt Disney ever interviewed by Anderson Cooper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Walt_Disney;Anderson_Cooper"}
{"question": "does a dentist treat Bluetooth problems?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Bluetooth"}
{"question": "did Richard III's father have greater longevity than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_III_of_England"}
{"question": "does Ariana Grande's signature style combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ariana_Grande"}
{"question": "did the color green help Theodor Geisel become famous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dr._Seuss"}
{"question": "does the United States Secretary of State answer the phones for the White House?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States_Secretary_of_State;White_House"}
{"question": "would a honey badger's dentures be different from a wolverine's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Wolverine"}
{"question": "are thetan levels found in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Thetan;New_Testament"}
{"question": "would Bobby Jindal's high school mascot eat kibble?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bobby_Jindal"}
{"question": "are multiple Christmas Carol's named after Saints?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do placozoa get learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Placozoa"}
{"question": "would Jon Brower Minnoch break a chair before Voyager 2 launch mass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Voyager_2"}
{"question": "would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Monday_Night_Football;WWE_Raw"}
{"question": "are any of the destinations of Japan Airlines former Axis Powers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "an", "extracted_entity": "Japan_Airlines;Axis_powers"}
{"question": "is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Eid_al-Fitr"}
{"question": "could an infant solve a sudoku puzzle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sudoku"}
{"question": "did Gauss have a normal brain structure?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Carl_Friedrich_Gauss"}
{"question": "did Benito Mussolini wear bigger shoes than Haf\u00fe\u00f3r Bj\u00f6rnsson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Benito_Mussolini;Haf\u00fe\u00f3r_J\u00fal\u00edus_Bj\u00f6rnsson"}
{"question": "did U2 play a concert at the Polo Grounds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "U2;Polo_Grounds"}
{"question": "does Ludacris perform classical music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ludacris;Classical_music"}
{"question": "can olive oil kill rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Julia Roberts lose the prolific acting contest in her family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Julia_Roberts"}
{"question": "does Super Mario mainly focus on a man in green?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Super_Mario"}
{"question": "will Elijah Cummings cast a vote in the 2020 presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Elijah_Cummings;2020_United_States_presidential_election"}
{"question": "can an adult human skull hypothetically pass through the birth canal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are any mollusks on Chinese New Year calendar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chinese_New_Year"}
{"question": "could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "2008_Summer_Olympics"}
{"question": "if he were poor, would Christopher Reeve have lived?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Reeve"}
{"question": "is it impossible to tell if someone is having a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Super Mario require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Super_Mario"}
{"question": "does the actress who played Elizabeth II speak fluent Arabic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Elizabeth_II;Arabic_language"}
{"question": "were the Spice Girls inspired by Little Mix?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spice_Girls;Little_Mix"}
{"question": "was Augustus his real name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the brain located in the torso?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Cosmic Girls play League of Legends alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Channel_Awesome;League_of_Legends"}
{"question": "has a tumulus been discovered on Mars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mars"}
{"question": "would a Bengal cat be afraid of catching a fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bengal_cat"}
{"question": "is anyone at the Last Supper celebrated in Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Last_Supper;Islam"}
{"question": "did John Lennon listen to Compact discs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "John_Lennon;Compact_disc"}
{"question": "did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Queen_Elizabeth_The_Queen_Mother"}
{"question": "was Nikola Tesla's home country involved in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nikola_Tesla;American_Civil_War"}
{"question": "were there eight humans on Noah's Ark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Noah's_Ark"}
{"question": "are people banned from entering the Forbidden City?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Forbidden_City"}
{"question": "did Kurt Cobain's music genre survive after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Kurt_Cobain"}
{"question": "was Lorenzo de Medici's patronage of Da Vinci exclusive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lorenzo_de'_Medici"}
{"question": "when the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a nickel fit inside a koala pouch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Koala"}
{"question": "did the Football War last at least a month?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Masaharu Morimoto rely on glutamic acid?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Masaharu_Morimoto;Glutamic_acid"}
{"question": "can you put bitcoin in your pocket?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bitcoin"}
{"question": "would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "United_States_Air_Force;Dunkin'_Donuts"}
{"question": "at midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ot", "extracted_entity": "Times_Square;New_Year's_Eve"}
{"question": "did Irish mythology inspire Washington Irving?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Irish_mythology;Washington_Irving"}
{"question": "can a microwave melt a Toyota Prius battery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Toyota_Prius"}
{"question": "does Elizabeth II reign over the Balearic Islands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Elizabeth_II;Balearic_Islands"}
{"question": "can Michael Jordan become a professional cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Michael_Jordan;United_States"}
{"question": "will speed reader devour The Great Gatsby before the Raven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Great_Gatsby;The_Raven"}
{"question": "would the 10th doctor enjoy a dish of stuffed pears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "during the time immediately after 9/11, was don't ask don't tell still in place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was King Kong (2005 film) solvent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "King_Kong_(2005_film)"}
{"question": "would the Who concert in international space station be audible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Who"}
{"question": "would an ancient visitor to Persia probably consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Iran"}
{"question": "do you need different colored pens for sudoku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can the Supreme Court of Canada fight a Lucha trios match?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Supreme_Court_of_Canada"}
{"question": "could Eric Clapton's children play a regulation game of basketball among themselves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eric_Clapton"}
{"question": "is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Batman_(1989_film);New_York_City;Kansas_City,_Missouri"}
{"question": "did Saddam Hussein witness the inauguration of Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saddam_Hussein;Donald_Trump"}
{"question": "is the Flying Spaghetti Monster part of an ancient pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gladiator_(2000_film);Soulcalibur;Faust"}
{"question": "would a slingshot be improperly classified as artillery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is it normal to blow out candles during a funeral?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": null}
{"question": "could Darth Vader hypothetically catch the Coronavirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Darth_Vader"}
{"question": "did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Portugal;House_of_Tudor"}
{"question": "snowboarding is a rarity in Hilo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hilo,_Hawaii"}
{"question": "was John Gall from same city as Stanford University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Glenn;Stanford_University"}
{"question": "is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bob_Marley"}
{"question": "is San Diego County the home of a Shamu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "San_Diego_County,_California"}
{"question": "was Elmo an original muppet character on Sesame Street?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elmo"}
{"question": "would a Fakir be surprised if they saw a comma in their religious book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "in the world of Harry Potter, would a snake and skull tattoo be good luck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harry_Potter"}
{"question": "did England win any Olympic gold medals in 1800?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "England_national_football_team"}
{"question": "is video surveillance of a room possible without an obvious camera or new item?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do pirates care about vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Vitamin_C"}
{"question": "will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Conan_the_Barbarian"}
{"question": "were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jackson_Pollock;Pledge_of_Allegiance_(United_States)"}
{"question": "could a fan of the Botany Swarm vote for John Key?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key"}
{"question": "would Jackie Chan have trouble communicating with a deaf person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jackie_Chan"}
{"question": "was Oscar Wilde's treatment under the law be considered fair in the US now?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Oscar_Wilde;United_States"}
{"question": "can a lemon aggravate dyspepsia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Indigestion"}
{"question": "can musicians become knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Uranium"}
{"question": "did Disney's second film rip off a prophet story?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Alan Alda old enough to have fought in the Vietnam War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Alda;Vietnam_War"}
{"question": "is Samsung accountable to shareholders?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Samsung"}
{"question": "are chinchillas cold-blooded?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Chinchilla"}
{"question": "did Johann Sebastian Bach ever win a Grammy Award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Johann_Sebastian_Bach;Grammy_Award"}
{"question": "did Dr. Seuss live a tragedy free life?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dr", "extracted_entity": "Dr._Seuss"}
{"question": "would a Nike shoebox be too small to fit a swan in?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nike,_Inc."}
{"question": "were Beauty and the Beast adaptations devoid of Kurt Sutter collaborators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kurt_Sutter"}
{"question": "was Florence a Theocracy during Italian Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Florence;Theocracy;Italian_Renaissance"}
{"question": "did Sojourner Truth use the elevator at the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sojourner_Truth;Washington_Monument"}
{"question": "is it safe to eat hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Lionel Richie ever have dinner with Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lionel_Richie;Abraham_Lincoln"}
{"question": "would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "JPEG;Leonardo_da_Vinci;Vitruvian_Man"}
{"question": "are fresh garlic cloves as easy to eat as roasted garlic cloves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Osama bin Laden likely abstain from alcohol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Osama_bin_Laden"}
{"question": "do Youtube viewers get unsolicited audiobook advice often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "YouTube"}
{"question": "is 2018 Ashland, Oregon population inadequate to be a hypothetical military division?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ashland,_Oregon"}
{"question": "does Carmen Electra own a junk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Carmen_Electra"}
{"question": "could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Maroon_5;Colosseum"}
{"question": "would Glen Beck and Stephen Colbert be likely to tour together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Glenn_Beck;Stephen_Colbert"}
{"question": "could a young Wizard of Oz Scarecrow have gotten Cerebral palsy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cerebral_palsy"}
{"question": "can the Toyota Hilux tip the scales against Mr. Ed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Toyota_Hilux"}
{"question": "is it normally unnecessary to wear a coat in Hollywood in July?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hollywood"}
{"question": "could the Atlantic readers fill 500 battalions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can oysters be preserved without refrigeration? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a hippie hypothetically be bummed out by Augustus's Pax Romana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hippie;Augustus;Bosporan_Kingdom;Pax_Romana"}
{"question": "can professional boxers expect to have low dental bills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tokyo_Tower"}
{"question": "would  bald eagle deliver an urgent message before B-52?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Boeing_B-52_Stratofortress"}
{"question": "is it difficult to conduct astrophotography in the summer in Sweden?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sweden"}
{"question": "was a woman Prime Minister directly before or after Stanley Baldwin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rams", "extracted_entity": "Stanley_Baldwin"}
{"question": "do people in middle school usually get breast exams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Black Lives Matter connected with capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Black_Lives_Matter;Capsaicin"}
{"question": "does Capricorn astrology symbol have all of the parts of a chimera?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Capricorn_(astrology)"}
{"question": "would Lord Voldemort hypothetically be an effective fighter after Final Fantasy silence is cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "demort", "extracted_entity": "Lord_Voldemort;Final_Fantasy"}
{"question": "does Sam Harris worship Shiva?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sam_Harris;Shiva"}
{"question": "do the James Bond and Doctor Who series have a similarity in format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "James_Bond;Doctor_Who"}
{"question": "can jackfruit be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jackfruit"}
{"question": "does Disney own a major comic book publisher?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "did William Shaespeare read the Daily Mirror?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Daily_Mirror"}
{"question": "did the 40th president of the United States forward lolcats to his friends?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "did Clark Gable marry more women once than Richard Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "lark", "extracted_entity": "Clark_Gable;Richard_Burton"}
{"question": "would an expensive tailor use adhesive to create a shorter hem on slacks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would the yearly precipitation on Snowdon submerge an upright bowling pin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snowdon"}
{"question": "can a honey bee sting a human more than once?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did U.S. soldiers listen to Justin Bieber's Believe album during the Battle of Baghdad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;Justin_Bieber;Believe_(Cher_album);Battle_of_Baghdad_(2003)"}
{"question": "can sunlight travel to the deepest part of the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Black_Sea"}
{"question": "is Christopher Walken close to achieving EGOT status?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher", "extracted_entity": "Christopher_Walken"}
{"question": "are the brooms from curling good for using on house floors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They", "extracted_entity": null}
{"question": "would a dog easily notice ammonia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ammonia"}
{"question": "are vinegar pickled cucumbers rich in lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Lactobacillus"}
{"question": "is a construction worker required to build a portfolio?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is it hard to get a BLT in Casablanca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Casablanca"}
{"question": "would half muggle wizards fear Lord Voldemort?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lord_Voldemort"}
{"question": "would ISIS agree with Al-Farabi's religious sect?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Islamic_State_of_Iraq_and_the_Levant"}
{"question": "is Casio's founding year a composite number?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "would it be unusual to use paypal for drug deals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "PayPal"}
{"question": "is growing seedless cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Spartina Patens thrive in the Sahara Desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Sahara"}
{"question": "does taking ukemi halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Uke_(martial_arts)"}
{"question": "are there five different single-digit Fibonacci numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "can a snow leopard eat twice its own body weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snow_leopard"}
{"question": "are ground bell peppers the main ingredient of black pepper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Black_pepper"}
{"question": "would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Olive_oil"}
{"question": "would a monkey outlive a human being on average?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Harry Potter book a flight on Asiana Airlines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Harry_Potter_(character);Asiana_Airlines"}
{"question": "is latitude required to determine the coordinates of an area?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did polio medicine save the life of polio vaccine creator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are Donkeys part of Christmas celebrations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "if you have a serious injury in Bangladesh, would you probably dial a Fibonacci number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bangladesh;Fibonacci_number"}
{"question": "hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Atomic", "extracted_entity": "Spice_Girls"}
{"question": "is Great Pyramid of Giza the last wonder of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Great_Pyramid_of_Giza"}
{"question": "do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Coen_brothers;Guinea-Bissau"}
{"question": "will Elijah Cummings vote for Joe Biden in the next presidential elections?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elijah_Cummings;Joe_Biden"}
{"question": "did Joan Crawford guest star on  JAG (TV series)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Joan_Crawford;JAG_(TV_series)"}
{"question": "can an African Elephant get pregnant twice in a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "African_elephant"}
{"question": "would a sofer be a bad job for a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Veganism"}
{"question": "is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mercedes-Benz;United_States"}
{"question": "was Jackson Pollock straight edge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jackson_Pollock"}
{"question": "do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Goldstone_Deep_Space_Communications_Complex"}
{"question": "would a fungal life-form be threatened by a pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do urban legends always have to occur in cities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Freemasonry been represented on the Moon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Freemasonry;Moon"}
{"question": "is Hamlet more common on IMDB than Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hamlet;IMDb"}
{"question": "coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Stone_Cold_Steve_Austin;Audi_TT"}
{"question": "was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Robin", "extracted_entity": "Mrs._Doubtfire"}
{"question": "can people die from brake failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "if it socially acceptable to wear an icon depicting crucifixion? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could a monarch butterfly rule a kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Monarch_butterfly"}
{"question": "can vitamin C rich fruits be bad for health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Vitamin_C"}
{"question": "did breakdancing grow in popularity during WW2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "World_War_II"}
{"question": "would it be unusual to see frost in September in Texas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Texas"}
{"question": "are Aldi's foods discounted due to being out of date?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aldi"}
{"question": "would a recruit for the United States Marine Corps be turned away for self harm?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States_Marine_Corps"}
{"question": "can you order an Alfa Romeo at Starbucks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Alfa_Romeo;Starbucks"}
{"question": "will Chick Fil A be open on Halloween 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hick", "extracted_entity": "Chick-fil-A;Halloween"}
{"question": "do drag kings take testosterone to look masculine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Drag_king"}
{"question": "are pennies commonly used in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Canada"}
{"question": "could the Pope be on an episode of Pimp My Ride?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pope;Pimp_My_Ride"}
{"question": "can E6000 cure before a hoverboard finishes the Daytona 500? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Daytona_500"}
{"question": "does Nigella Lawson care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nigella_Lawson"}
{"question": "would Cuba Libre consumption help with insomnia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Cuba"}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jack_Black;Bear_McCreary"}
{"question": "would someone on Venus be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Jean Valjean imprisoned due to hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jean_Valjean"}
{"question": "is Europa (moon) name origin related to Amunet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Adobe Suite have video game engine coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tokyo;Metropolitan_Museum_of_Art"}
{"question": "would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "United_States_Military_Academy;Multiple_sclerosis"}
{"question": "is strep throat harmless to singer Rita Ora after her 2020 tonsilitis surgery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rita", "extracted_entity": "Rita_Ora"}
{"question": "would an Alfa Romeo vehicle fit inside a barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alfa_Romeo"}
{"question": "gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If", "extracted_entity": "Rincewind"}
{"question": "could two newborn American Black Bear cubs fit on a king size bed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "American_black_bear"}
{"question": "are coopers required in the beverage industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Superhero fiction invented in the digital format?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Antarctica have a lot of problems relating to homelessness?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antarctica"}
{"question": "does an individual oceanographer study many sciences?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Supreme_Court_of_Canada"}
{"question": "did Julio Gonzalez like acetylene?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Julio_Gonz\u00e1lez_(sculptor)"}
{"question": "is the letter D influenced by the shape of ancient doors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a parody of the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Bible"}
{"question": "was the death of Heath Ledger caused by his work on The Dark Knight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Heath_Ledger;Dark_Knight"}
{"question": "did Donatello use a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donatello;Smartphone"}
{"question": "did Rahul Dravid ever kick a field goal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Rahul_Dravid"}
{"question": "do you often hear Marco Polo's name shouted near water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Marco_Polo"}
{"question": "are people more likely than normal to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Burning_Man"}
{"question": "is an espresso likely to assuage fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Immanuel_Kant;Black_Lives_Matter"}
{"question": "did a gladiator kill his opponent with a shotgun?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Vellore_Fort;Grizzly_bear"}
{"question": "could B be mistaken for an Arabic numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Arabic_language"}
{"question": "can COVID-19 spread to maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "COVID-19"}
{"question": "is 500GB USB device enough to save 10 hours of Netflix shows a day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Netflix"}
{"question": "do hornets provide meaningful data for oceanographers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Oceanography"}
{"question": "would Michael Phelps be good at pearl hunting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Phelps"}
{"question": "at Christmastime, do some films remind us that groundhog day is approaching?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Christmas"}
{"question": "was Great Recession the period of severest unemployment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Great_Recession"}
{"question": "could Katharine Hepburn have ridden the AirTrain JFK?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Katharine_Hepburn;AirTrain_JFK"}
{"question": "was Walt Disney able to email his illustrations to people living far away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Walt_Disney"}
{"question": "can a person who knows only English read Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "English_language;Kanji"}
{"question": "would three commas be sufficient for displaying US 2018 GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Brian Warner be a good singer for a soul music band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Brian_Wilson;Soul_music"}
{"question": "could someone listen to the entire U2 debut studio album during an episode of Peppa Pig?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "U2;Peppa_Pig"}
{"question": "did Hanuman ever experience an orgasm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hanuman"}
{"question": "would you find a tibia beside parsley on a holiday plate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tibia;Parsley"}
{"question": "does someone from Japan need a passport to go to a Nordic country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan;Nordic_countries"}
{"question": "would a black widow woman have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if", "extracted_entity": null}
{"question": "does a person using tonsure have hair at the top of their scalp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Terence Tao outperform Eminem in a math competition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Terence_Tao;Eminem"}
{"question": "do any video games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is number of different US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;President_of_the_United_States;Hong_Kong"}
{"question": "can lettuce result in spontaneous abortion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "has mummification in the Andes been prevented by rainfall?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Andes"}
{"question": "are some Do It Yourself projects potentially lethal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Some", "extracted_entity": "Do_it_yourself"}
{"question": "was King Kong (2005 film) the lead actress's biggest box office role?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "King_Kong_(2005_film)"}
{"question": "would a bodybuilder choose maize over chicken breast for dinner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Maize"}
{"question": "karachi was a part of Alexander the Great's success?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Alexander", "extracted_entity": "Alexander_the_Great"}
{"question": "do guitarists need both hands to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Kenny G hold the qualifications to be a tax collector?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Kenny_G"}
{"question": "in baseball, is a \"Homer\" named after the poet Homer who wrote the Odyssey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Baseball;Homer;Odyssey"}
{"question": "is a felony jury enough people for a Bunco game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is letter C crucial to spelling the two most common words in English language?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "C;English_language"}
{"question": "do most middle class families have butlers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Middle_class"}
{"question": "is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Isaac_Newton;Great_Expectations"}
{"question": "did Mickey Mouse appear in a cartoon with Bugs Bunny in 1930?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mickey_Mouse;Bugs_Bunny"}
{"question": "did Jackson 5 members exceed number in The Osmonds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Jackson_5;The_Osmonds"}
{"question": "can you get a ride on Amtrak to the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Amtrak;Underworld"}
{"question": "are all limbs required for jujutsu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jujutsu"}
{"question": "would The Dark Knight be appropriate for a preschool class?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dark_Knight"}
{"question": "did Jesus know anyone who spoke Estonian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jesus;Estonian_language"}
{"question": "does 2015 have more unlucky Friday's than usual?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Tony Bennett's middle name shared by a former UFC champion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tony_Bennett;Ultimate_Fighting_Championship"}
{"question": "do you need both hot and cold water to peel a tomato?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of Reptile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Newt_Gingrich;Reptile"}
{"question": "do spider wasps have eight legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Spider_wasp"}
{"question": "does the New York Public Library sell Alpo products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_York_Public_Library"}
{"question": "can petroleum jelly be used as fuel in a car?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Petroleum_jelly"}
{"question": "can Aerosmith legally drive in the carpool lane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aerosmith"}
{"question": "are Durian fruits an olfactory delight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they", "extracted_entity": null}
{"question": "could a wandering albatross fly from Bucharest to New York City without a rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bucharest;New_York_City"}
{"question": "do some people soak in olive oil and water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do depressed people travel to the Golden Gate Bridge often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Golden_Gate_Bridge"}
{"question": "can members of the Green Party of England and Wales vote in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Green_Party_of_England_and_Wales;United_States"}
{"question": "is Poseidon similar to the god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Poseidon;Vulcan_(mythology)"}
{"question": "is Glycol something United Airlines would buy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_Airlines"}
{"question": "does a person suffering from Thalassophobia enjoy oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Oceanography"}
{"question": "can a Bengal cat survive eating only pancakes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bengal"}
{"question": "does it seem like the Gorillaz is composed of more members than they have?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gorillaz"}
{"question": "could Rich and Morty be triggered for children of alcoholics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was the Japanese street aesthetic once illuminated by noble gasses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan"}
{"question": "in isopropyl alcohol, is the solubility of salt low?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "n", "extracted_entity": "Isopropyl_alcohol"}
{"question": "could you watch all of JAG in six months?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "JAG_(TV_series)"}
{"question": "do silicone suits make judo difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Judo"}
{"question": "will The Exorcist stimulate limbic system?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Exorcist_(film);Limbic_system"}
{"question": "was P. G. Wodehouse's favorite book The Hunger Games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "P._G._Wodehouse;The_Hunger_Games"}
{"question": "would J.K Rowling's top sellers be on a fantasy shelf?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "J._K._Rowling"}
{"question": "are some types of pancakes named after coins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Paprika be made without a dehydrator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Paprika"}
{"question": "in Doctor Who, did the war doctor get more screen time than his successor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Doctor_Who"}
{"question": "was Mesopotamia part of what is now China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mesopotamia;People's_Republic_of_China"}
{"question": "could Arnold Schwarzenegger hypothetically defeat Haf\u00fe\u00f3r Bj\u00f6rnsson in a powerlifting competition if both are at their peak strength?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Arnold_Schwarzenegger;Haf\u00fe\u00f3r_J\u00fal\u00edus_Bj\u00f6rnsson"}
{"question": "was the British car, the Mini, the first car manufactured?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "United_Kingdom;Mini"}
{"question": "will you see peach blossoms and Andromeda at the same time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Andromeda_(mythology)"}
{"question": "can The Hobbit be read in its entirety in four minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Hobbit"}
{"question": "is number of stars in Milky Way at least ten times earth's population?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Milky_Way"}
{"question": "could Christopher Walken enlist in the United States Marine Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Christopher_Walken;United_States_Marine_Corps"}
{"question": "is white light the absence of color?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Jabberwocky be considered a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Jabberwocky;Sonnet"}
{"question": "would Eye surgery on a fly be in vain?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "should you ask a neighbor for candy on New Year's Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "only", "extracted_entity": null}
{"question": "are all types of pottery safe to cook in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "QWERTY"}
{"question": "does the United States of America touch the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;Indian_Ocean"}
{"question": "would Donald Duck be allowed into most grocery stores?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Donald_Duck"}
{"question": "is it best to avoid kola nuts with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA daily recommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States_Department_of_Agriculture"}
{"question": "was the Treaty of Versailles settled over blueberry scones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Treaty_of_Versailles"}
{"question": "will the producer of Easy Rider become an octogenarian in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Easy_Rider"}
{"question": "does Osama bin Laden put a wafer on his tongue every Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Osama_bin_Laden"}
{"question": "can you taste Law & Order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are queen bees unnecessary for growing apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does a bumblebee have to worry about spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bumblebee"}
{"question": "can the Department of Defense perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "United_States_Department_of_Defense"}
{"question": "can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Mount_Emei"}
{"question": "was Land of Israel in possession of an Islamic empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Land_of_Israel;Islam"}
{"question": "do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Africa;Portuguese_Colonial_War"}
{"question": "could the Toyota Stadium sit a tenth of the population of Gotheburg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gothenburg"}
{"question": "does the word swastika have meaning in sanskrit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Swastika;Sanskrit"}
{"question": "were there greater landslides than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States_presidential_election,_1980"}
{"question": "can you hunt Iberian wolves in the Southern United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iberian_wolf;Southern_United_States"}
{"question": "is hanging a viable execution method on a ship at sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Hanging"}
{"question": "is pig meat considered inedible within the cuisine of Hawaii?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cuisine_of_Hawaii"}
{"question": "are the events of Star Trek: The Next Generation in the history of the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Star_Trek:_The_Next_Generation"}
{"question": "if you're pregnant, might you be recommended ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does The Jungle Book contain racist subtext?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Jungle_Book_(1967_film)"}
{"question": "does the Prime Minister of the United Kingdom have poor job security?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Prime_Minister_of_the_United_Kingdom"}
{"question": "would Gomer Pyle salute a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gomer_Pyle"}
{"question": "when en route from China to France, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "People's_Republic_of_China;France"}
{"question": "could you brew beer from start to finish in the month of September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Freya a combination of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Athena;Aphrodite"}
{"question": "in most Mennonite homes, would children know of The Powerpuff Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mennonite;The_Powerpuff_Girls"}
{"question": "would a sophist use an \u00e9p\u00e9e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paul_the_Apostle"}
{"question": "do sand cats avoid eating all of the prey of eels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sand_cat;Eel"}
{"question": "was Muhammed a member of the Uniting Church in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Uniting_Church_in_Australia"}
{"question": "would the historic Hattori Hanz\u014d admire Naruto?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Hattori_Hanz\u014d;Naruto"}
{"question": "should you wrap a gift for a mother of a stillborn in stork wrapping paper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "does crucifixion violate US eighth amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "would a TMNT coloring book have pizza in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "of", "extracted_entity": null}
{"question": "could Sugar Ray Robinson box if he stole in Iran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sugar_Ray_Robinson;Iran"}
{"question": "would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "E.T._the_Extra-Terrestrial"}
{"question": "can a banana get a virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the use of the word Gypsy by non-Romani people considered okay?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are implants from an ORIF surgery affected by the magnetic field of the Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Magnetic_field"}
{"question": "was Subway involved in a pedophilia scandal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Subway_(restaurant)"}
{"question": "did Jack Dempsey have most title fight wins in either of his weight classes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jack_Dempsey"}
{"question": "did Bill Nye vote for Franklin Delano Roosevelt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Nye;Franklin_D._Roosevelt"}
{"question": "is grief always obvious when it is being experienced?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "did either side score a touchdown during the Football War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is there a Yeti associated with Disney theme parks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Yeti;The_Walt_Disney_Company"}
{"question": "did Zorro carve his name into items regularly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Zorro"}
{"question": "did Subway have a sex offender as a spokesperson?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Subway_(restaurant)"}
{"question": "could Christopher Nolan's movies finance Cyprus's entire GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Christopher_Nolan;Cyprus"}
{"question": "is Steve Carell's character on The Office portrayed as one with tremendous leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Steve_Carell;The_Office_(U.S._TV_series)"}
{"question": "could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dominican_Order"}
{"question": "can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does the texture of leaves remain the same independent of their coloring changing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Torah;Bunkie,_Louisiana;Louisiana"}
{"question": "would Doctor Strange like the Pittsburgh Steelers logo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Doctor", "extracted_entity": "Doctor_Strange;Pittsburgh_Pirates"}
{"question": "would Iris (mythology) and Hermes hypothetically struggle at a UPS job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iris", "extracted_entity": "Iris_(mythology);Hermes;United_Parcel_Service"}
{"question": "is pi in excess of square root of 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "pi", "extracted_entity": null}
{"question": "would it be uncommon for a high schooler to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Columbus", "extracted_entity": "Voyages_of_Christopher_Columbus;Statue_of_Liberty"}
{"question": "does the human stomach destroy a bee if ingested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is blonde hair green eyed Sara Paxton considered a Latino?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sara_Paxton;Hispanic_and_Latino_Americans"}
{"question": "are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Photosynthesis;Atmosphere_of_Mars"}
{"question": "can you see live harbor seals in Washington DC?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harbor_seal;Washington,_D.C."}
{"question": "was story of Jesus inspired by Egyptian myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jesus;Egypt"}
{"question": "has CNES planted a French flag on the lunar surface?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Earth's_magnetic_field"}
{"question": "can Iowa be hidden in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Iowa;English_Channel"}
{"question": "can you house a giant squid at Soldier Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Soldier_Field"}
{"question": "was hippie culture encouraged by the government in the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Soviet_Union"}
{"question": "would Mickey Mouse blend in with the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mickey_Mouse"}
{"question": "does penicillin cure a learning disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is it dark is Basel during the day in Los Angeles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Basel;Los_Angeles"}
{"question": "is honey associated with queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Speaker_of_the_United_States_House_of_Representatives;Attack_on_Pearl_Harbor"}
{"question": "does a giant green lady stand in New York Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_York_Harbor"}
{"question": "could chives be mistaken for grass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Chives"}
{"question": "could a two-year old win a Scrabble tournament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Scrabble"}
{"question": "is Y2K relevant to the plot of The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Godfather"}
{"question": "is Sirius part of a constellation of an animal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sirius"}
{"question": "would alligator best saltwater crocodile in hypothetical Lake Urmia battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Alligator;Saltwater_crocodile;Lake_Urmia"}
{"question": "would a Common warthog starve in a greenhouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is purchasing food for a Lolcat unnecessary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lolcat"}
{"question": "do some psychotherapy patients have no mental illness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Psychotherapy"}
{"question": "did Harry Houdini appear on Chris Angel Mindfreak?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Harry_Houdini"}
{"question": "could  jockey win Triple Crown between Eid al-Fitr endpoints?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Triple_Crown_of_Thoroughbred_Racing;Eid_al-Fitr"}
{"question": "would a vegetarian be able to eat something at Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chick-fil-A"}
{"question": "does the Pixar film Brave feature Scottish people?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pixar;Brave_(2012_film);Scottish_people"}
{"question": "do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do most college students own a fax machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He", "extracted_entity": "Paul_Bunyan"}
{"question": "could a delicious recipe be made with The Onion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Onion"}
{"question": "could a markhor give birth three times in a single year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Markhor"}
{"question": "would a model be likely to frequently enjoy the menu at Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does the history of Europe include the age of dinosaurs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Europe"}
{"question": "was John George Bice's birthplace near Cornwall?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cornwall"}
{"question": "do white blood cells outnumber red blood cells in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "White_blood_cell;Red_blood_cell"}
{"question": "was Edward II crucial to England's victory at Battle of Falkirk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Edward_II_of_England;England;Battle_of_Falkirk"}
{"question": "did Holy Land belong to Adamu's tribe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Holy_Land"}
{"question": "does Ahura Mazda have a rivalry with Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ahura_Mazda;Zeus"}
{"question": "is an Eastern chipmunk likely to die before seeing two leap years?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "would the owners of the company Peter Griffin works for need barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Peter_Griffin"}
{"question": "is the CIA part of the Department of Defense?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Central_Intelligence_Agency;United_States_Department_of_Defense"}
{"question": "was Harry Potter and the Philosopher's Stone popular during the great depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Harry_Potter"}
{"question": "is a slime mold safe from cerebral palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cerebral_palsy"}
{"question": "is British Airways the air force of the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "British_Airways;United_Kingdom"}
{"question": "has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alfred_Nobel"}
{"question": "are aggressive bumblebees suicidal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "is Edward Snowden in hiding from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Edward_Snowden;United_States"}
{"question": "can the Great Depression be treated with Prozac?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Great_Depression;Fluoxetine"}
{"question": "can photography be considered abstract art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Abstract_art"}
{"question": "could a bee hummingbird balance a scale with a single pea on it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Disney associated with Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Los_Angeles_County,_California"}
{"question": "did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "University_of_Pittsburgh"}
{"question": "is a doctorate required to teach at a SUNY School?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does the swastika have positive uses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Nintendo's link ever see an astronomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Elon Musk's hairline changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elon_Musk"}
{"question": "do American teams in National Hockey League outnumber Canadian teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "United_States;National_Hockey_League;Canada"}
{"question": "did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Teri_Hatcher;Desperate_Housewives;Superman"}
{"question": "were all the materials to make a cannon known during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Cookie Monster's diet Paleo friendly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would it be impossible to seat every Chief Justice of the United States on a Boeing 737?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Chief_Justice_of_the_United_States;Boeing_737"}
{"question": "could Toyota stadium house people suffering homelessness in Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Toyota;Michigan"}
{"question": "can a martyr saint have been excommunicated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bob_Marley"}
{"question": "does a Disney princess on Broadway have red hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Broadway_theatre"}
{"question": "could Steven Spielberg send emails as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Steven_Spielberg"}
{"question": "does Felix Potvin have a position on a dodgeball team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "F\u00e9lix_Potvin"}
{"question": "would someone with leukophobia enjoy looking at the Flag of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Flag_of_the_United_States"}
{"question": "would 1943-S penny be good for making silverware?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "did mongoose come from later period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Rhinoceros"}
{"question": "would it be possible to fit a football field in Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "Alcatraz_Island"}
{"question": "is Issac Newton often associated with a red fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Isaac_Newton"}
{"question": "would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan"}
{"question": "can you buy spinal cord at Home Depot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Home_Depot"}
{"question": "are some chiropractic manipulations dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Chiropractic"}
{"question": "were some people afraid of New Years Day coming in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_Year's_Day"}
{"question": "did travelers sing sea shanties on the Oregon Trail?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oregon_Trail"}
{"question": "do members of the Supreme Court of the United States have longer terms than most senators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Supreme_Court_of_the_United_States"}
{"question": "paleography hypothetically helps to understand Cthulhu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Cthulhu"}
{"question": "is surfing popular in Des Moines, Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Des_Moines,_Iowa"}
{"question": "is Statue of Unity hypothetically more level with Statue of Liberty than Lighthouse of Alexandria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Statue_of_Liberty;Lighthouse_of_Alexandria"}
{"question": "is CEO of Nissan an internationally wanted fugitive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan"}
{"question": "has type of political association Pompey had with Caesar influenced reality TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Pompey;Julius_Caesar"}
{"question": "would Rime of the Ancient Mariner make a good sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Rime_of_the_Ancient_Mariner;Sonnet"}
{"question": "do Snow White dwarves best The Hobbit dwarves in battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Snow", "extracted_entity": "Snow_White;The_Hobbit"}
{"question": "can the history of art be learned by an amoeba?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "New_York_City;Raleigh,_North_Carolina"}
{"question": "walt Disney dominated his amusement park peers at Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He", "extracted_entity": "Walt_Disney;Academy_Award"}
{"question": "would human race go extinct without chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Chlorophyll"}
{"question": "is the average bulk carrier ideal for transporting bromine at room temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bromine"}
{"question": "can depression be mistaken for laziness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Easy_Rider;Dean_Cain;Princeton_University"}
{"question": "did pirates who had scurvy need more Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Scurvy;Vitamin_C"}
{"question": "could the leader of Heaven's Gate save images in JPEG format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Heaven's_Gate_(religious_group);JPEG"}
{"question": "does the FDA require sell by dates using Roman Numerals?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Roman_numerals"}
{"question": "would Kylee Jenner ask for no cream in her coffee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kylie_Jenner"}
{"question": "does Nicole Kidman know any Scientologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nicole_Kidman"}
{"question": "is Lionel Richie related to Sheila E?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lionel_Richie"}
{"question": "was a nuclear bomb used in the Napoleonic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Napoleonic_Wars"}
{"question": "is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eiffel_Tower"}
{"question": "can you buy a fair trade laptop?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ou", "extracted_entity": "Fair_trade"}
{"question": "was Noah concerned with buoyancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Noah"}
{"question": "is a watchmaker likely to be able to fix an Apple Watch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Apple_Watch"}
{"question": "are sables related to wolverines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are some Brazilian Navy ships built in Britian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Brazilian_Navy"}
{"question": "could Carl Friedrich Gauss speak to someone 100 miles away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Carl_Friedrich_Gauss"}
{"question": "in Hey Arnold, did any characters stay on a porch all the time?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "has Johns Hopkins University always treated subjects ethically?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Johns_Hopkins_University"}
{"question": "is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jackson_Pollock"}
{"question": "do black-tailed jackrabbits fear the European wildcat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Black-tailed_jackrabbit;European_wildcat"}
{"question": "has Kelly Clarkson outsold season 4 American Idol winner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Kelly_Clarkson;American_Idol"}
{"question": "is Hermes equivalent to the Roman god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Hermes;Roman_mythology;Vulcan_(mythology)"}
{"question": "can an elite runner circle the Pyrenees in one day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Pyrenees"}
{"question": "is the Golden eagle considered a scavenger bird?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "QWERTY"}
{"question": "are grapes essential to winemaking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would the Cookie Monster decline an offer of free Keebler products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Keebler_Company"}
{"question": "did Jeremy Irons master sweep picking as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jeremy_Irons"}
{"question": "for Hostas to look their best, do they need lots of chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hlor", "extracted_entity": "Chlorophyll"}
{"question": "has spinach been a source of power in a comic movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Spinach"}
{"question": "can you see the Statue of Freedom from the Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Statue_of_Freedom;Statue_of_Liberty"}
{"question": "does Ludacris have Greek heritage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ludacris;Greece"}
{"question": "will bumblebees derail the United States presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "United_States"}
{"question": "does Rahul Dravid belong to the family Gryllidae?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rahul_Dravid"}
{"question": "is Metallica protective over their music?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Metal", "extracted_entity": "Metallica"}
{"question": "does Dean Cain have less days to birthday than Will Ferrell every 4th of July?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Dean_Cain;Will_Ferrell"}
{"question": "is Lord Voldemort associated with a staff member of Durmstrang?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lord_Voldemort"}
{"question": "are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bern;Switzerland;Genghis_Khan"}
{"question": "is starving Hamas agent eating pig bad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hamas"}
{"question": "would Ringo Starr avoid the pot roast at a restaurant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ringo_Starr"}
{"question": "was ship that recovered Apollo 13 named after a World War II battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "USS", "extracted_entity": "Apollo_13;World_War_II"}
{"question": "are both founders of Ben & Jerry's still involved in the company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is a krabby patty similar to a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cheeseburger"}
{"question": "did Beethoven enjoy listening to EDM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ludwig_van_Beethoven;Electronic_dance_music"}
{"question": "is it true that gay male couples cannot naturally reproduce?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could an American confuse breakfast in British cuisine for dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;British_cuisine"}
{"question": "can Kit & Kaboodle hypothetically help someone past the Underworld gates?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are two cans of Campbell's Soup a day good for hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Campbell_Soup_Company"}
{"question": "are there any official American knights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "is the Foreign and Commonwealth Office a European political agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Foreign_and_Commonwealth_Office;Europe"}
{"question": "do gorillas fight with panda bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Gorilla;Giant_panda"}
{"question": "would lumberjacks get full after eating three dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Dosa_(food)"}
{"question": "is being 5 year Capital One Venture member more cost effective than being in Church of Satan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Church_of_Satan"}
{"question": "tata Hexa can accomodate every Spice Girl?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could common warthog be useful for scrimshaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Common_warthog"}
{"question": "is breast cancer associated with a ribbon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Nuno_Gomes;Red_Sea"}
{"question": "is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was a Tiny House ceiling out of Osama bin Laden's reach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ma", "extracted_entity": "Tiny_house_movement;Osama_bin_Laden"}
{"question": "can you conduct surveillance from a teddy bear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does table tennis use prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was the Carnation Revolution the deadliest revolution in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Europe"}
{"question": "is the Muslim world hostile to Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Muslim_world;Israel"}
{"question": "is the saltwater crocodile less endangered than the European otter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saltwater_crocodile;Eurasian_otter"}
{"question": "can someone from New England profit by growing coffee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "New_England"}
{"question": "could a dichromat probably easily distinguish chlorine gas from neon gas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": "Chlorine"}
{"question": "nATO doesn't recognize double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "true", "extracted_entity": "NATO"}
{"question": "did Julius Caesar read books on Pharmacology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Julius_Caesar"}
{"question": "does water have viscosity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Los_Angeles_Memorial_Sports_Arena;Coachella_Valley_Music_and_Arts_Festival"}
{"question": "would you spend less on your food at Aldi than at Whole Foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Aldi;Whole_Foods_Market"}
{"question": "are Scottish people Albidosi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Scotland"}
{"question": "are System of a Down opposed to globalization?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "System_of_a_Down"}
{"question": "was Alexander the Great baptized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alexander_the_Great"}
{"question": "did Demi Lovato's ancestors help turn maize into popcorn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Demi_Lovato"}
{"question": "would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Goldfish"}
{"question": "is Pearl Harbor the mythical home of a shark goddess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Pearl_Harbor"}
{"question": "would Elon Musk be more likely to know about astrology than physics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elon_Musk"}
{"question": "can paresthesia be caused by a white pigment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Charlie Bucket be a hotel manager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can children be soldiers in the US Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Army"}
{"question": "is Jack Black's height enough to satisfy Coronavirus distancing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jack_Black"}
{"question": "are peaches best eaten when firm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Peach"}
{"question": "was Charles Manson's body unwanted?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Charles_Manson"}
{"question": "was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kurt_Cobain;Greenfield,_Greater_Manchester"}
{"question": "can a human eat an entire 12-lb roast turkey in an hour? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a house full of aloe vera hypothetically be ideal for Unsinkable Sam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aloe_vera"}
{"question": "would dual-energy X-ray absorptiometry be useful if performed on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Crab"}
{"question": "would Harvey Milk have approved of Obama?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Harvey_Milk"}
{"question": "would a jumping spider need over half a dozen contact lenses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are seasons of Survivor surpassed by number of Ancient Greek letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Survivor_(franchise);Ancient_Greek"}
{"question": "do citizens of Cheshire sing La Marseillaise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cheshire;La_Marseillaise"}
{"question": "are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "in teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "SS", "extracted_entity": null}
{"question": "could eating Chinook salmon help Ryan Reynolds?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Chinook_salmon;Ryan_Reynolds"}
{"question": "would kaffir lime be good in a White Russian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kaffir_lime;White_movement"}
{"question": "is entire Common Era minuscule to lifespan of some trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Common_Era"}
{"question": "did Charlemagne have a bar mitzvah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Charlemagne;Bar_and_Bat_Mitzvah"}
{"question": "did Monty Python write the Who's on First sketch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Monty_Python;The_Who"}
{"question": "in geometry terms, is the Royal Observatory in Greenwich similar to a yield sign?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Royal_Observatory,_Greenwich;Greenwich"}
{"question": "did Sugar Ray Robinson win a fight against Canelo Alvarez?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sugar_Ray_Robinson;Canelo_\u00c1lvarez"}
{"question": "can an asteroid be linked with virginity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Asteroid"}
{"question": "would a Yeti be likely to have prehensile limbs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Yeti"}
{"question": "is groundhog day used as a global season indicator? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "did mercenaries fight for England in the Glorious Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kingdom_of_England;Glorious_Revolution"}
{"question": "was the sable depicted in Marvel comics anthropomorphic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Marvel_Comics"}
{"question": "are twinkies considered artisan made products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "w", "extracted_entity": "Twinkie"}
{"question": "were mollusks an ingredient in the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Christianity better for global warming than Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Christianity;Satanism"}
{"question": "was Lord Voldemort taught by Professor Dumbledore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lord_Voldemort;Albus_Dumbledore"}
{"question": "would costumes with robes and pointy hats be helpful for Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Macbeth_(character)"}
{"question": "are the knights in the Medieval Times show not authentic knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Medieval_Times"}
{"question": "could William Franklyn-Miller win a 2020 Nascar Cup Series race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Al-Farabi ever meet Mohammed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Al-Farabi;Mohammed_bin_Rashid_Al_Maktoum"}
{"question": "did the Pearl Harbor attack occur during autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "have the Israelis played the Hammerstein Ballroom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Israel;Hammerstein_Ballroom"}
{"question": "if you're reducing salt intake, are olives a healthy snack?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do pediatricians perform abortions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Pediatrics"}
{"question": "could casualties from deadliest war rival France's population?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ce", "extracted_entity": "France"}
{"question": "does Lorem ipsum backwards fail to demonstrate alliteration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "rem", "extracted_entity": "Alliteration"}
{"question": "can you see Stonehenge from a window in Dusseldorf?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Stonehenge;D\u00fcsseldorf"}
{"question": "can Reiki be stored in a bottle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Reiki"}
{"question": "would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Toyota_Supra;NASCAR"}
{"question": "would Tom Cruise ever insult L. Ron Hubbard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tom_Cruise;L._Ron_Hubbard"}
{"question": "does Lemon enhance the flavor of milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Harry_Potter_(character);The_Matrix;Warner_Bros."}
{"question": "is the United States the largest exporter of Fair Trade products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States"}
{"question": "is Snickers helpful for weight loss?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snickers"}
{"question": "could the Jackson 5 play a full game of rugby with each other?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Jackson_5"}
{"question": "did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the founders of the biggest city in Orange County, California speak Italian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Orange_County,_California;California;Italian_language"}
{"question": "is an ammonia fighting cleaner good for pet owners?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could you buy Hershey's Kisses in red foil with farthings after 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Hershey_Company"}
{"question": "could morphine cure HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pyrenees"}
{"question": "would the chef at La Grenouille find salsa to be a strange request?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Michael_Crichton;Harvard_University"}
{"question": "would toast for a vegan have margarine instead of butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Veganism;Margarine"}
{"question": "did any cultures associate celery with death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Celery"}
{"question": "are emus related to elks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Emu"}
{"question": "is the Forbidden City host to a wooden rollercoaster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Forbidden_City"}
{"question": "are slime lilies in a different scientific family than asparagus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Asparagus"}
{"question": "in order to work in district management, does one need a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you buy Reddit at Walmart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Reddit;Walmart"}
{"question": "if you have black hair and want red hair, do you need bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is a curling iron necessary in curling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Curling"}
{"question": "was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alice_in_Wonderland_(1951_film)"}
{"question": "was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saint_Vincent_and_the_Grenadines;Italy"}
{"question": "did Lamarck and Darwin agree about the origin of species diversity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jean-Baptiste_Lamarck"}
{"question": "if you were at an Apple store, would most of the computers be running Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Apple_Inc.;Ubuntu_(operating_system)"}
{"question": "can you make an MP3 from the Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Golden_Gate_Bridge"}
{"question": "are there any chives hypothetically good for battling vampires?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chives"}
{"question": "could a Diwali celebration feature a crustacean?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Diwali"}
{"question": "do astronomers write horoscopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the name of a mythical creature also the name of a Small Solar System body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Small_Solar_System_body"}
{"question": "did Medieval English lords engage in fair trade with peasants?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "England_in_the_Middle_Ages;Fair_trade"}
{"question": "are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gulf_of_Mexico"}
{"question": "do anatomical and symbolic hearts look remarkably different?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are you more likely to find bipolar disorder in a crowd than diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bipolar_disorder;Diabetes_mellitus"}
{"question": "does having lip piercings lead to more expensive dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are most mall Santa Claus actors white?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the Taco Bell kitchen contain cinnamon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Taco_Bell"}
{"question": "could white rice go rancid before sesame seeds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "can you listen to the entire Itunes song catalog in one year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "ITunes"}
{"question": "do mollymawks live where albatrosses cannot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Natalie_Portman;Snickers"}
{"question": "are Mayors safe from harm from the federal government?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is it normal to see a red panda in Shanghai outside of a zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Shanghai"}
{"question": "would a human following a hyena diet be unwelcome at a vegan festival?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Hyena"}
{"question": "can DRL Racer X drone get across Brooklyn Bridge in 18 seconds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "German_Aerospace_Center;Brooklyn_Bridge"}
{"question": "would Temujin hypothetically be jealous of Charlemagne's conquests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Charlemagne"}
{"question": "would a bodybuilder enjoy wearing a cast for several weeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "can you carry a Chrysler in a laptop bag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Chrysler"}
{"question": "is Home Depot a one stop shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Home_Depot"}
{"question": "is the Holy Land important to Eastern religions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Holy_Land;Eastern_religions"}
{"question": "is immersion in virtual reality testable on cnidarians before humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Virtual_reality"}
{"question": "did Jack Dempsey ever witness Conor McGregor's fights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jack_Dempsey;Conor_McGregor"}
{"question": "are black and white prison uniforms made to resemble a zebra?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "New_Year's_Eve"}
{"question": "did Malcolm X avoid eating ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Malcolm", "extracted_entity": "Malcolm_X"}
{"question": "would a snakebite hypothetically be a threat to T-1000?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is there a Harry Potter character named after Florence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harry_Potter;Florence"}
{"question": "could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Azerbaijan"}
{"question": "was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ivan_the_Terrible"}
{"question": "can Poland Spring make money in the Sahara?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Poland_Spring;Sahara"}
{"question": "do people with mood disorders need permanent institutionalization?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "will Dustin Hoffman likely vote for Trump in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dustin_Hoffman"}
{"question": "can your psychologist say hello to you while you are out at the supermarket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Johann Sebastian Bach influence heavy metal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Johann_Sebastian_Bach;Heavy_metal_music"}
{"question": "are flag of Gabon colors found in rainbow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gabon"}
{"question": "does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "C-SPAN"}
{"question": "can a grey seal swim in the same water as the subject of Moby Dick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Grey_seal;Moby-Dick"}
{"question": "did Modern Family win a Slammy award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Modern_Family"}
{"question": "would Mount Wycheproof be a breeze for Edmund Hillary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Edmund_Hillary"}
{"question": "does Canada have a relationship with a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Canada"}
{"question": "would a compact disc melt in magma?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Compact_disc;Magma"}
{"question": "is Sea World hazardous to leopard seal's health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sea_World_(Australia);Leopard_seal"}
{"question": "was being a mail carrier considered one of the most dangerous jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were Depeche Mode heavily influenced by blues music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Depeche_Mode;Blues"}
{"question": "do people with swallowing disorders need high viscosity drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can whole genome sequencing be used for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "COVID-19"}
{"question": "is Anakin Skywalker from Star Wars associated with the color black?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Darth_Vader;Star_Wars;Black"}
{"question": "do you need to schedule separate preventive healthcare and sickness visits? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was the original Metroid groundbreaking for its polygons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are any of the words that CAPTCHA stands for palindromes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "CAPTCHA;Palindrome"}
{"question": "is the Jurassic era a tourist destination?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jurassic"}
{"question": "can a dolphin keep a diary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dolphin"}
{"question": "is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Xenophobia;Saladin"}
{"question": "does Carl Linnaeus share the same final resting place as Michael Jackson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Carl_Linnaeus;Michael_Jackson"}
{"question": "could Eddie Murphy dial 911 in a car as a young child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Eddie_Murphy"}
{"question": "do flying fish have good eyesight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Flying_fish"}
{"question": "does Justin Bieber vote in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Justin_Bieber"}
{"question": "did Ice make people rich?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is J.D. Salinger's most successful work influential to killers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "J._D._Salinger"}
{"question": "is it impossible for pigs to use pig latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can the theory of cultural hegemony explain global warming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cultural_hegemony;Global_warming"}
{"question": "is March named after Jupiter's son in Roman mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jupiter_(mythology);Roman_mythology"}
{"question": "can furniture be made of hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hair"}
{"question": "is SnapCap an example of a retail store?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Rand Paul frequently swim in Lake Michigan during his undergraduate years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Rand_Paul;Lake_Michigan"}
{"question": "would Hannah Nixon be proud of Richard Nixon following the Watergate scandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hannah_Montana;Richard_Nixon;Watergate_scandal"}
{"question": "would Tony Stark be considered a polymath?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Iron_Man;Polymath"}
{"question": "is Supreme Court of the United States analogous to High Courts of Justice of Spain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Supreme_Court_of_the_United_States;Spain"}
{"question": "does Amtrak run from NYC directly to the Moai location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Amtrak;New_York_City"}
{"question": "does Ronda Rousey avoid BBQ restaraunts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ronda_Rousey"}
{"question": "can someone in Uberlandia work for Mitsubishi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mitsubishi_Motors"}
{"question": "do oak trees have leaves during winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Argon near Neon on the periodic table of elements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Neon"}
{"question": "does the Constitution of the Philippines copy text from the British constitution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Constitution_of_the_Philippines;United_Kingdom"}
{"question": "did Ada Lovelace die tragically young for her era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ada_Lovelace"}
{"question": "is dyslexia the most common intellectual disability in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "is the referee at a soccer match highly visible against the field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Family Guy take place on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Family_Guy;West_Coast_of_the_United_States"}
{"question": "could someone in the Canary Islands fish for largemouth bass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Canary_Islands;Largemouth_bass"}
{"question": "did the Presidency of Bill Clinton conclude with his impeachment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Presidency_of_Bill_Clinton;Bill_Clinton;Impeachment_of_Bill_Clinton"}
{"question": "did Marco Polo travel with Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Marco_Polo;Christopher_Columbus"}
{"question": "could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Scrubs_(TV_series)"}
{"question": "will a 2 Euro coin float across the Red Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Red_Sea"}
{"question": "have any murderers outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kane_(wrestler);Royal_Rumble"}
{"question": "can the US branch of government that has power over the military also have the power to veto?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "are Brussels sprout particularly good for adrenal fatigue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ls", "extracted_entity": "Brussels"}
{"question": "can oysters be used in guitar manufacturing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "are parodies of the President of the United States illegal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Par", "extracted_entity": "President_of_the_United_States;United_States"}
{"question": "can lobster breathe in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Phobos part of the Andromeda galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Phobos_(moon);Andromeda_Galaxy"}
{"question": "were all of Heracles's children present for his funeral pyre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Heracles"}
{"question": "can Africanized bees be considered multicultural?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Multiculturalism"}
{"question": "is nickel dominant material in US 2020 nickels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nickel;United_States"}
{"question": "did Leonardo da Vinci lack contemporary peers in his home city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Leonardo_da_Vinci"}
{"question": "was there fear leading up to the year 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can you buy furniture and meatballs in the same store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "during the Cuban revolution, did the US experience a population boom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States"}
{"question": "could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Usain_Bolt"}
{"question": "can a copy of The Daily Mirror sustain a campfire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Daily_Mirror"}
{"question": "would Bugs Bunny harm an olive tree in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bugs_Bunny"}
{"question": "did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Robert_Downey_Jr."}
{"question": "can actress Danica McKellar skip astronaut education requirements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dan", "extracted_entity": "Danica_McKellar"}
{"question": "can you transport a coin along a sea of mercury?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are the founders of Skype from Asia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Skype;Asia"}
{"question": "was the tenth Amendment to the Constitution written using Pitman shorthand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you save every HD episode of Game of Thrones on Samsung Galaxy A10e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Game_of_Thrones"}
{"question": "does Super Mario protagonist hypothetically not need continuing education classes in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Super_Mario;Illinois"}
{"question": "would a body builder prefer an elk burger over a beef burger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Elk"}
{"question": "will NY Stock Exchange closing bell be heard in Universal Music Group's headquarters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "New_York_Stock_Exchange;Universal_Music_Group"}
{"question": "could Saint Peter watch television?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saint_Peter"}
{"question": "are all United States Aldi locations owned by the same company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "United_States;Aldi"}
{"question": "was the first Vice President of the United States an Ottoman descendant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vice_President_of_the_United_States"}
{"question": "would it be safe to have a jackfruit thrown at your head?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jackfruit"}
{"question": "can a woman on average have a baby before wheat seed blooms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Donald Trump come up with the idea for the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Donald_Trump;New_York_Harbor"}
{"question": "did Saint Augustine use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Augustine_of_Hippo;Julian_calendar"}
{"question": "do Koalas prefer Eucalyptus over meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ko", "extracted_entity": "Eucalyptus"}
{"question": "are goats found on abyssal plains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are gorillas closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Star Wars fans say \"beam me up\" often?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Star_Wars"}
{"question": "does USA fail separation of church and state in multiple ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "could Bob Marley's children hypothetically win tug of war against Kublai Khan's children?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bob_Marley;Kublai_Khan"}
{"question": "does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Alice_in_Wonderland_(1951_film)"}
{"question": "is dessert eaten before breakfast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was Krishna skilled at using the bow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Krishna"}
{"question": "was Aristotle a member of the House of Lords?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Aristotle", "extracted_entity": "Aristotle;House_of_Lords"}
{"question": "would Ibn Saud tolerate salsa music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Ibn_Saud;Salsa_music"}
{"question": "would a responsible bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Millie_Bobby_Brown"}
{"question": "is Bactrian Camel most impressive animal when it comes to number of humps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bactrian_camel"}
{"question": "do giraffes require special facilities at zoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch kill hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Hyperglycemia"}
{"question": "can a snake wear a snowshoe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a false pope become a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "D\u00fcsseldorf"}
{"question": "can dementia be cured with a cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dementia"}
{"question": "can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Justin_Timberlake;Six_Flags"}
{"question": "would Dave Chappelle pray over a Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Dave_Chappelle;Qur'an"}
{"question": "is a jellyfish safe from atherosclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jellyfish;Atherosclerosis"}
{"question": "is the Riksdag a political entity in Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Riksdag;Scandinavia"}
{"question": "did Linnaeus edit Darwin's draft of Origin of Species?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Carl_Linnaeus"}
{"question": "is Hermione Granger eligible for the Order of the British Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hermione_Granger;Order_of_the_British_Empire"}
{"question": "was Saudi Aramco started due to an assassination?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saudi_Aramco"}
{"question": "should a Celiac sufferer avoid spaghetti?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Coeliac_disease"}
{"question": "does Olympia Washington share name with Hephaestus's workshop location?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Olympia,_Washington;Washington_(U.S._state);Hephaestus"}
{"question": "could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Time_(magazine)"}
{"question": "does Jason have anything in common with Dr. Disrespect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they", "extracted_entity": null}
{"question": "is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan;Sweden"}
{"question": "are ropes required to operate a frigate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Brad Peyton need to know about seismology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Seismology"}
{"question": "is Ludacris in same music genre as 2000's Binaural?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ludacris"}
{"question": "would LeBron James hypothetically glance upwards at Yuri Gagarin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "LeBron_James;Yuri_Gagarin"}
{"question": "do you need a farmer to make a circuit board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Gandhi watch the television show Bonanza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mahatma_Gandhi;Bonanza"}
{"question": "is Shakespeare famous because of the infinitive form?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "William_Shakespeare;Infinitive"}
{"question": "would the crew of Apollo 15 have difficulty riding a unicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They", "extracted_entity": "Apollo_15"}
{"question": "do Squidward Tentacles and Alan Greenspan have different musical passions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Squidward_Tentacles;Alan_Greenspan"}
{"question": "can a minor replicate the double-slit experiment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can an anchovy born in 2020 survive 25th US census?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do you have to put on glasses to read a QR code?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Seroquel be the first treatment recommended by a doctor to someone with depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do women often need new shoes during their pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Persephone be a good consultant to a landscape architect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Persephone;Landscape_architect"}
{"question": "is University of Pittsburgh easier to enter than FBI?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "University", "extracted_entity": "University_of_Pittsburgh"}
{"question": "will the small intenstine break down a cotton ball?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": null}
{"question": "do the Eskimos sunbathe frequently?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eskimo"}
{"question": "was the French Revolution televised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "French_Revolution"}
{"question": "is the most expensive color in the world Blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Rosalind_Franklin"}
{"question": "can you substitute the pins in a bowling alley lane with Dustin Hoffman's Oscars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dustin_Hoffman;Academy_Award"}
{"question": "did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hundred_Years'_War"}
{"question": "is the Sea of Japan landlocked within Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sea_of_Japan;Japan"}
{"question": "is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "will Lhamo Thondup be considered by Catholic Church to be a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Catholic_Church"}
{"question": "would someone in Boston not receive the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Boston;Toronto_Star"}
{"question": "is Christmas always celebrated on a Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christmas"}
{"question": "are pancakes typically prepared in a pot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Ringo Starr been in a relatively large number of bands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Ringo_Starr"}
{"question": "did Richard III know his grandson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_III_of_England"}
{"question": "can Kane challenge Joe Biden in this year's primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Joe", "extracted_entity": "Joe_Biden"}
{"question": "did Stone Cold Steve Austin wrestle in three different centuries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stone_Cold_Steve_Austin"}
{"question": "do embalmed bodies feel different at funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are leaves from coca good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Coca"}
{"question": "did the Beatles write any music in the Disco genre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Beatles"}
{"question": "is Nine Inch Nails a good guest for students in earliest grade to take Iowa tests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Nine_Inch_Nails;Iowa"}
{"question": "did Harvey Milk ever run for governor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harvey_Milk"}
{"question": "is a bengal fox likely to see the Superbowl?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Super_Bowl"}
{"question": "could all Tahiti hotels hypothetically accommodate US D-Day troops?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tahiti;Normandy_landings"}
{"question": "was Amazon involved in the lunar landing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon.com"}
{"question": "could the Spice Girls compete against \u017dRK Kumanovo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Spice_Girls"}
{"question": "are saltwater crocodiles related to alligators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hypothermia;Zoot_suit;Triton_(moon)"}
{"question": "has every astronaut survived their space journey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can any person with a driver's license work in transport of aviation fuel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Driver's_license;Aviation_fuel"}
{"question": "could a snowy owl survive in the Sonoran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sonoran_Desert"}
{"question": "can you buy chlorine at a dollar store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the voice of the Genie from Disney's Aladdin still alive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Robin", "extracted_entity": "The_Walt_Disney_Company;Aladdin_(1992_Disney_film)"}
{"question": "was the amount of spinach Popeye ate unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Popeye"}
{"question": "would an explosion at a gunpowder storage facility result in a supersonic shock wave?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Marco_Rubio;Hersheypark"}
{"question": "can a person be diagnosed with a Great Depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Great_Depression"}
{"question": "could Godzilla have been killed by the Tohoku earthquake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Godzilla"}
{"question": "is retail a job anybody can be suited for?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does cell biology teach about the life cycle of Al Qaeda?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Al-Qaeda"}
{"question": "did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pablo_Escobar;Robert_Moses"}
{"question": "has Burger King  contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Burger_King"}
{"question": "would a snake have reasons to fear a honey badger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Honey_badger"}
{"question": "do most people only memorize slightly over half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "ZIP_Code"}
{"question": "does Happy Gilmore Productions CEO own a Torah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Torah"}
{"question": "does Mercury make for good Slip N Slide material?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mercury_(element)"}
{"question": "is RoboCop director from same country as Gaite Jansen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "RoboCop"}
{"question": "was the Parc des Princes fully operational during June of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Parc_des_Princes"}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do worshipers of Shiva make a pilgrimage to the Holy Land?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Shiva;Holy_Land"}
{"question": "was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Woodrow_Wilson"}
{"question": "was it typical to see Johnny Cash on stage in a rainbow-colored outfit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Johnny_Cash"}
{"question": "is slitting your wrists an unreliable suicide method?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hawaii"}
{"question": "is Christopher Nolan indebted to Bob Kane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Christopher_Nolan;Bob_Kane"}
{"question": "did Kim Il-sung network on LinkedIn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kim_Il-sung;LinkedIn"}
{"question": "could all People's Volunteer Army hypothetically be transported on Symphony of the Seas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": null}
{"question": "is Bern a poor choice for a xenophobic Swiss citizen to live?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Bern;Switzerland"}
{"question": "would a vegan prefer a natural bongo drum over a synthetic one?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Veganism"}
{"question": "is an inappropriate lullaby Love Song from November 11, 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Orange_County,_California;California"}
{"question": "are kayaks used at the summit of Mount Everest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Mount_Everest"}
{"question": "would someone on a keto diet be able to eat Dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "was Raphael's paintings influenced by the country of Guam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Raphael;Guam"}
{"question": "is it normal to find parsley in multiple sections of the grocery store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Parsley"}
{"question": "was the subject of Parsifal taken from British folklore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Parsifal;United_Kingdom"}
{"question": "do drummers need spare strings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is there a jukebox musical about a sweet transvestite from Transexual, Transylvania?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jukebox_musical;Transylvania"}
{"question": "did Malcolm X use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Malcolm_X;Unicode"}
{"question": "would the fastest tortoise win a race against a Chicago \"L\"?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chicago"}
{"question": "does Hammurabi's Code violate Christians Golden Rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christian;Golden_Rule"}
{"question": "does the Roman god Vulcan have a Greek equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Roman_mythology;Vulcan_(mythology);Greek_language"}
{"question": "can the Dalai Lama fit in a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "14th_Dalai_Lama"}
{"question": "is watermelon safe for people with a tricarboxylic acid allergy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is a Chinchilla breed of felis catus a type of rodent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Chinchilla;Rodent"}
{"question": "can printing books in kanji instead of the Roman alphabet save trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kanji;Latin_alphabet"}
{"question": "is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Audi_R8"}
{"question": "would a Rockette look odd with a moustache? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "The_Rockettes"}
{"question": "would Hodor hypothetically be a good math mathematician?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Hades and Osiris hypothetically compete for real estate in the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Hades;Osiris"}
{"question": "would a German Shepherd be welcome in an airport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "German_Shepherd_Dog"}
{"question": "does a person need a college degree to become a bartender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is capsaicin associated with cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Capsaicin"}
{"question": "would Arnold Schwarzenegger be unable to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Arnold_Schwarzenegger;President_of_the_United_States;United_States"}
{"question": "when Hugh Jackman was a teacher, would he have taught The Great Gatsby?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Hugh_Jackman;The_Great_Gatsby"}
{"question": "is Fiat Chrysler associated with Japanese cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Fiat_Chrysler_Automobiles;Japan"}
{"question": "can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Immersion_baptism;Jeff_Buckley"}
{"question": "would many meals heavy in brussels sprouts benefit someone on Coumadin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Brussels", "extracted_entity": null}
{"question": "is it safe to share silverware with an HIV positive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "do movies always show nerds as the losers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are eagles and young bears both used as labels for skills-training youth groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could ancient Tony Bennett have a baby in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tony_Bennett"}
{"question": "can a lifeboat rescue people in the Hooke Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bohol_Sea"}
{"question": "would a stool be useful for a Lusotitan to reach the top of an almond tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Robert_Stack;Tower_of_London"}
{"question": "would somebody leave reiki with bruises?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is there a popular Broadway character who is a missionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "Broadway_theatre"}
{"question": "would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "are months based on the solar cycle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do many fans of J.K Rowling know who Alan Rickman is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "J._K._Rowling;Alan_Rickman"}
{"question": "could someone in a coma experience fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could Johnny Carson's children fill out a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Johnny_Carson"}
{"question": "would downloading Mario 64 on an emulator be legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mario_Kart_64"}
{"question": "are Tom and Jerry featured in a ride at Disneyland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_and_Jerry;Disneyland"}
{"question": "can a blind person tell time by Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Big_Ben"}
{"question": "are sea turtles enjoying life during quarantine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Don_King_(boxing_promoter);Charles_Manson"}
{"question": "does Paulo Coelho's wife make a living through speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Paulo_Coelho"}
{"question": "is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "is Final Fantasy VI closer to beginning than end of its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Final_Fantasy_VI"}
{"question": "were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Apollo_13"}
{"question": "is chaff produced by hydropower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chaff"}
{"question": "is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mark_Cuban;Northern_Mariana_Islands"}
{"question": "is Garfield known for hating italian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "James_A._Garfield;Italian_cuisine"}
{"question": "is basil safe from Hypervitaminosis D?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Basil", "extracted_entity": "Vitamin_D"}
{"question": "does a kangaroo incubate its offspring?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did J. Edgar Hoover take his calls in Langley, Virginia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "J._Edgar_Hoover;Langley,_Virginia"}
{"question": "is Elijah part of a Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can a cheetah generate enough force to topple Big Show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Big_Show"}
{"question": "could Buzz Aldrin have owned a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Buzz_Aldrin"}
{"question": "was Mercedes-Benz associated with the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Mercedes-Benz;Nazi_Party"}
{"question": "did Johnny Carson win enough Emmy's to fill a carton if Emmy's were eggs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Johnny_Carson;Emmy_Award"}
{"question": "can bottlenose dolphins hypothetically outbreed human women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bottlenose_dolphin"}
{"question": "if a baby was born on Halloween would they be a Scorpio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Halloween;Scorpius"}
{"question": "could the Great Wall of China connect the Dodgers to the White Sox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Great_Wall_of_China;Los_Angeles_Dodgers;Chicago_White_Sox"}
{"question": "would a honey badger fit inside an oven?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Heracles famous labors exceed a baker's dozen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Heracles"}
{"question": "does Guam have a state capital?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are Disney's seven dwarves the original ones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Hague"}
{"question": "is Jesse W. Moore a potential recipient of a Snoopy-themed award from NASA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "NASA"}
{"question": "has Gorillaz creator been in more bands than Bernard Sumner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gorillaz;Bernard_Sumner"}
{"question": "is it safe to eat kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is zoology unconcerned with strigoi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the phone Alexander Graham Bell use have call waiting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alexander_Graham_Bell"}
{"question": "were paparazzi directly responsible for the death of Amy Winehouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Amy_Winehouse"}
{"question": "on August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Tonight_Show_Starring_Jimmy_Fallon;Eastern_Time_Zone"}
{"question": "are more watermelons grown in Brazil than Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Brazil;Antarctica"}
{"question": "does store bought milk have cream at the top?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Port_of_Baltimore"}
{"question": "did eggs need to be kept cold in the middle ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Middle_Ages"}
{"question": "would Topa Inca Yupanqui have encountered the western honey bee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Western_honey_bee"}
{"question": "is \"A Tale of Two Cities\" a popular science novel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "A_Tale_of_Two_Cities"}
{"question": "was Amy Winehouse a fan of Star Wars: Rogue One?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Amy_Winehouse"}
{"question": "can shooting bald eagle get a person more prison time than Michael Vick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Vick"}
{"question": "could you drive a Rowe 550 to the 2008 Summer Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "2008_Summer_Olympics"}
{"question": "can you go water skiing on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a Wolverine and a Lynx be hard to tell apart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wolverine_(character);Lynx_(constellation)"}
{"question": "could Plato have agreed with the beliefs of Jainism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Plato;Jainism"}
{"question": "do bald eagles nest on Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could James Brown's ex-wives hold a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "James", "extracted_entity": "James_Brown"}
{"question": "should Peter Griffin be an expert at the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Peter_Griffin"}
{"question": "did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Charles_Manson"}
{"question": "is there any absolute way to prevent abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Saturn named after king of gods in Greek mythology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Greek_mythology"}
{"question": "were any of despised Pope Alexander VI's descendants canonized?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Pope_Alexander_VI"}
{"question": "would it be difficult to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Venus"}
{"question": "can chemicals in onion help create a thermonuclear bomb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are anchovies associated with Italian food?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Italian_cuisine"}
{"question": "did Operation Barbarossa or Barbarossa's last expedition succeed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Operation_Barbarossa"}
{"question": "has numerology helped shape hotel layouts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Numerology"}
{"question": "did The Who have to cancel tours due to World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Who;World_War_II"}
{"question": "can an art dealer buy Boeing 737-800 with a Da Vinci painting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Boeing_737_Next_Generation;Leonardo_da_Vinci"}
{"question": "would a crocodile survive longer in Great Salt Lake than alligator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Crocodile;Great_Salt_Lake;Alligator"}
{"question": "would a clouded leopard encounter an awake pangolin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does acupuncture cause pain in many people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "are twins always born during the same year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could a nymph tick pass through a standard hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would WWF be angrier if you killed koala instead of black swan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "WW", "extracted_entity": "World_Wide_Fund_for_Nature;Koala"}
{"question": "does New Year's Day always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "New_Year's_Day"}
{"question": "is a pottery kiln inappropriate for use with glass blowing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do people of the Iyer caste eat meat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iyer"}
{"question": "do carpenters understand geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Geometry"}
{"question": "did the crew of Apollo 15 take pictures of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Apollo_15"}
{"question": "are human footprints absent from Mount Sharp?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Tom Cruise explain mental auditing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tom_Cruise"}
{"question": "is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "People's_Republic_of_China;Chevrolet_Cruze;Milan"}
{"question": "would drinking a glass of lemonade provide Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does a game engine have a fuel injector?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his father for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "J._D._Salinger"}
{"question": "were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Greeks"}
{"question": "is the Joker in a healthy romantic relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Joker_(character)"}
{"question": "would an adherent of Zoroastrianism consult the Quran for religious guidance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Zoroastrianism;Qur'an"}
{"question": "would a diet of ice eventually kill a person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can binary numbers and standard alphabet satisfy criteria for a strong password?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Phobos (moon) name origin similar to Roman god Pavor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Phobos_(moon);Ancient_Rome;Pauravas"}
{"question": "would members of Blue Lives Matter support every element of Grand Theft Auto III?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Blue_Lives_Matter;Grand_Theft_Auto_III"}
{"question": "will twenty pea pods contents cover entire chess board?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was the Joker an enemy of the Avengers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Avengers_(comics)"}
{"question": "could Christopher Nolan borrow pants from Danny Devito?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Nolan;Danny_DeVito"}
{"question": "could you watch Naruto and Puzzle Place on the same channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Naruto"}
{"question": "is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "LG_Electronics"}
{"question": "was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Andes;Colossus_of_Rhodes"}
{"question": "does Fraktur have a sordid history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Fraktur"}
{"question": "would a hedgehog avoid animals without a spinal cord?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can you worship Ahura Mazda at a mosque?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ahura_Mazda"}
{"question": "would a moose hypothetically be too much for a minotaur to devour whole?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Minotaur"}
{"question": "is Gandalf hypothetically a formidable foe for Charmed's Barbas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Gandalf;Charmed"}
{"question": "do suburbs encourage the use of cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Al Capone have read works from the Harlem Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Al_Capone;Harlem_Renaissance"}
{"question": "can pancreas removal cause bankruptcy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is shrimp prevalent in Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Ethiopian_cuisine"}
{"question": "does Marco Rubio have a close relationship with Allah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Marco_Rubio;Allah"}
{"question": "does The Doctor keep his ship in his childhood home?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Doctor_(Doctor_Who)"}
{"question": "did Archduke Franz Ferdinand of Austria participate in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Archduke_Franz_Ferdinand_of_Austria;Austria;Pacific_War"}
{"question": "is a beard is moss that grows on a human?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is there radiation where Nikola Tesla once worked?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nikola_Tesla"}
{"question": "would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mary,_mother_of_Jesus;Artemis;Greece"}
{"question": "could one Amazon share ever buy twenty year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amazon.com;Netflix"}
{"question": "would you have to wear a coat when on Phobos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Phobos_(moon)"}
{"question": "is Cambodia too expensive for 2020 richest billionaire to own?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cambodia"}
{"question": "did Emma Stone pursue a higher education?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Emma_Stone"}
{"question": "was San Antonio the site of a major battle in the 19th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "San_Antonio"}
{"question": "can the United Nations Framework Convention on Climate Change be held at the Javits Center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_Nations;United_Nations_Framework_Convention_on_Climate_Change;Jacob_K._Javits_Convention_Center"}
{"question": "can music be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could an ocelot outrun a kindergartner? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "was Elizabeth II the Queen during the Persian Gulf War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elizabeth_II;Gulf_War"}
{"question": "would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Venus", "extracted_entity": "Horoscope"}
{"question": "did Woodrow Wilson consider Blacks to be equal members of society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Woodrow_Wilson"}
{"question": "if your electric stove has a glass top, should you use cast iron skillets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can soup be eaten with the hands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Cleopatra have ethnicity closer to Egyptians than Greeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Cleopatra;Egyptians;Greeks"}
{"question": "does the judo rank system reach the triple digits?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was the son of Tsar Nicholas a daredevil?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Nicholas_II_of_Russia"}
{"question": "did Bill Gates achieve Latin honors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Gates"}
{"question": "has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Greenspan"}
{"question": "if Goofy were a pet, would he need heartworm prevention?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Goofy"}
{"question": "did Richard Wagner support the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Richard_Wagner;Nazism"}
{"question": "was historical Dracula from a town in Bucharest?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bucharest"}
{"question": "is electricity necessary to balance an account in Microsoft Excel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Microsoft_Excel"}
{"question": "did Pink Floyd have a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pink_Floyd;French_Riviera"}
{"question": "has the creator of Futurama lived in multiple centuries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Futurama"}
{"question": "is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "stern", "extracted_entity": "James_Weldon_Johnson"}
{"question": "did Elizabeth I of England own any viscose fiber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elizabeth_I_of_England"}
{"question": "could a cat ride Panzer VIII Maus tank missile from Barcelona to Madrid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Panzer_VIII_Maus;Barcelona;Madrid"}
{"question": "did Jack Dempsey fight the current WBC heavyweight champion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jack_Dempsey;World_Boxing_Council"}
{"question": "would a cauliflower farmer prosper at a latitude of 75\u00b0 N?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "would a student in eleventh grade be unable to run for president of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "President_of_the_United_States;United_States"}
{"question": "was Amy Winehouse familiar with Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amy_Winehouse;Brexit"}
{"question": "can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eurovision_Song_Contest"}
{"question": "are Scottish people descended from Mary, Queen of Scots part French?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Scottish_people;Mary,_Queen_of_Scots;France"}
{"question": "are quadrupeds represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chinese_calendar"}
{"question": "is the Louvre in billionaire George Soros's price range?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Louvre;George_Soros"}
{"question": "would a duke hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can a jet plane be made without society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paralympic_Games"}
{"question": "could Ryan Crouser throw a bengal fox with ease?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ryan_Crouser;Bengal_fox"}
{"question": "would a physician be unlikely to recommend Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reiki"}
{"question": "can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Michael_Bloomberg;Micronesia"}
{"question": "can Burundi's communicate with citizens of New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Burundi;New_Brunswick"}
{"question": "is Morocco an ideal location for water skiing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Morocco;Water_skiing"}
{"question": "did Al Capone carry a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Al_Capone"}
{"question": "was Mark Twain a struggling inventor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mark_Twain"}
{"question": "is B's place in alphabet same as Prince Harry's birth order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Prince_Harry"}
{"question": "would a vegan eat a traditional Paella dish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Veganism;Paella"}
{"question": "is cow methane safer for environment than cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Darth Vader monogamous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Darth_Vader"}
{"question": "were there footprints on the moon in 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Pantheon in Paris have a unique name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Panth\u00e9on;Paris"}
{"question": "can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Arvanites;Albanian_Declaration_of_Independence"}
{"question": "would a Beaver's teeth rival that of a Smilodon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "North_American_beaver;Smilodon"}
{"question": "can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Auburn,_New_York;New_York;Tropicana_Field"}
{"question": "do calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Calico"}
{"question": "are all cucumbers the same texture?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Library of Alexandria need less shelf space than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Library", "extracted_entity": "Library_of_Alexandria;Library_of_Congress"}
{"question": "would a spider wasp be more effective than a bullet ant to stop a criminal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Spider_wasp;Paraponera_clavata"}
{"question": "would a triples tandem bike support Apollo 15 crew?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Apollo_15"}
{"question": "if one of your feet is in a leg cast, should the other be in a sandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Klingons appear in the movie The Last Jedi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can you find Depala's race in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dep", "extracted_entity": "World_of_Warcraft"}
{"question": "would a Dodo hypothetically tower over Ma Petite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can the Very Large Telescope observe the largest mountain on Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Very_Large_Telescope"}
{"question": "was Morris County named after a chief justice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Morris_County,_New_Jersey"}
{"question": "is the Gujarati script the same category of script as Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kanji"}
{"question": "did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Napoleonic_Wars"}
{"question": "would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Antarctic_Peninsula"}
{"question": "if Martin Luther did one theses a day would he run out in half a year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Martin_Luther"}
{"question": "are honey badgers and hyenas anatomically dissimilar? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hyena"}
{"question": "did Al Pacino act in a movie during World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Al_Pacino;World_War_II"}
{"question": "could you watch a new Seinfeld episode every day for a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Seinfeld"}
{"question": "did Barack Obama participate in the Reformation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Barack_Obama;Protestant_Reformation"}
{"question": "did President William Howard Taft read DC Comics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "William_Howard_Taft;DC_Comics"}
{"question": "can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Stone_Cold_Steve_Austin;Mule_deer"}
{"question": "are there Americans still enlisted in the Confederate States Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States;Confederate_States_Army"}
{"question": "are right wing Amreicans opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Marxism"}
{"question": "can a sea turtle play tennis using a tennis racket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sea_turtle"}
{"question": "in a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "American_woodcock"}
{"question": "is shoe soup innocuous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Korea_under_Japanese_rule;Iron_Chef"}
{"question": "did Holy Saturday 2019 have special significance to pot smokers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Holy_Saturday"}
{"question": "do you need lactobacillus to make pickles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lactobacillus"}
{"question": "would a Gray Whale fit easily in an above ground pool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Gray_whale"}
{"question": "will a celibate cleric likely suffer a stoning in Somalia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Somalia"}
{"question": "could Little Women have been a book read by veterans of the civil war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Little_Women"}
{"question": "is MIX a word and a roman numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Downton Abbey finale viewership defeat every Kazakhstan citizen in tug of war?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Downton_Abbey;Kazakhstan"}
{"question": "was dynamite used during Middle Ages warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Middle_Ages"}
{"question": "is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_Vick;People_for_the_Ethical_Treatment_of_Animals"}
{"question": "can Kate Gosselin's household fill out a Bandy team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bandy"}
{"question": "is the tree species that the name Leipzig refers to an evergeen tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Leipzig"}
{"question": "will Gremlins sequels tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gremlins;The_Matrix"}
{"question": "is Snow White an example of good consent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Snow_White"}
{"question": "is MF Doom a Fantastic Four villain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "MF_Doom;Fantastic_Four"}
{"question": "would most grand masters know what the French Defense is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing style of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Othello"}
{"question": "would an environmentalist advocate for preventing domestic canine reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Environmentalist"}
{"question": "could a monolingual American read Lenovo's native name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;Lenovo"}
{"question": "can 1980 United States presidential election result be considered a photo finish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_presidential_election,_1980;Photo_finish"}
{"question": "did Cynthia Powell celebrate a silver anniversary with John Lennon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cynthia_Lennon;John_Lennon"}
{"question": "is it possible for biologist Rachel Carson to have flown to the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rachel_Carson"}
{"question": "did Confederate States Army influence West Point fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Confederate_States_Army;United_States_Military_Academy"}
{"question": "would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Will_Ferrell"}
{"question": "is the Mona Lisa in the same museum as the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mona_Lisa;Venus_de_Milo"}
{"question": "does the land in close proximity to beaver dams suffer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Jackson Pollock trained by Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jackson_Pollock;Leonardo_da_Vinci"}
{"question": "did Francois Mitterrand ever meet Barak Obama while they both held the position of President?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Fran\u00e7ois_Mitterrand;Barack_Obama"}
{"question": "would Carrie Poppy be likely to trust a psychic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would stargazers prefer binoculars over a telescope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is it possible to binge entire EastEnders series without water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "EastEnders"}
{"question": "was Mother Theresa a follower of atheism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mother_Teresa;Atheism"}
{"question": "is the BBC World Service hosted in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "BBC_World_Service;Europe"}
{"question": "did Melania Trump have same profession as Olga Kurylenko?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Melania_Trump;Olga_Kurylenko"}
{"question": "was Robert Downey Jr. a good role model as a young man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Robert_Downey_Jr."}
{"question": "should a finished website have lorem ipsum paragraphs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Jacques Du\u00e8ze have been friends with Richard Dawkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Richard_Dawkins"}
{"question": "would Gordon Ramsey use uranium as a seasoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Uranium"}
{"question": "does Robert De Niro use a microscope at work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Robert_De_Niro"}
{"question": "is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rand_Paul;John_Kerry"}
{"question": "are pirate lieutenants like navy lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "American_Ballet_Theatre;Swan_Lake"}
{"question": "does Homer Simpson need two hands worth of fingers to count to 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Homer_Simpson"}
{"question": "can you see the moon in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wembley_Arena"}
{"question": "is the Asian black bear multicolored?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Asian_black_bear"}
{"question": "can United States Secretary of State do crimes in U.K. without being arrested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "United_States_Secretary_of_State;United_Kingdom"}
{"question": "did Alice's Adventures in Wonderland inspire Macbeth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alice's_Adventures_in_Wonderland;Macbeth"}
{"question": "could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Quartz"}
{"question": "was England directly involved in the Arab-Israeli conflict?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "England"}
{"question": "can an adult male stand on top Donatello's bronze David and touch the Sistine Chapel ceiling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donatello;Sistine_Chapel"}
{"question": "was Dioskourides a lapidary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a CEO typically clean the toilets in a company's building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you purchase General Motors products at a movie theater?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "General_Motors"}
{"question": "did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Xena:_Warrior_Princess"}
{"question": "could all of the people who pass through 30th Street Station every day fit in Dorton Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "30th_Street_Station"}
{"question": "can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Aerosmith;Mitsubishi_Outlander"}
{"question": "could all the unemployed people due to 1933 Great Depression fit in Tiger Stadium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Great_Depression;Tiger_Stadium_(Detroit)"}
{"question": "would a duck ever need a Caesarean section?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Caesarean_section"}
{"question": "is nickel a better payout than mercury if given a dollar per atomic number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nickel;Mercury_(element)"}
{"question": "does Pikachu like Charles Darwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pikachu;Charles_Darwin"}
{"question": "can Simon Cowell vote for the next Supreme Court judge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Simon_Cowell"}
{"question": "is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is US route 1 dominated by historically red states?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a cell fit inside of a shoebox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Alice's_Adventures_in_Wonderland"}
{"question": "can Tame Impala's studio band play a proper game of Hot Potato?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tame_Impala"}
{"question": "is the title of Shirley Bassey's 1971 diamond song a true statement?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Shirley_Bassey"}
{"question": "would P. G. Wodehouse be taught in second grade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "P._G._Wodehouse"}
{"question": "can Curiosity take samples of rocks from Lacus Temporis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Curiosity_(rover)"}
{"question": "is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Wheel_of_Fortune_(U.S._game_show)"}
{"question": "would Phineas and Ferb enjoy winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saturday_Night_Live;Jenny_McCarthy"}
{"question": "can children become lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could you drive from New England to a Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "New_England;Sainsbury's"}
{"question": "do seven McDonald's hamburgers exceed USDA recommended fat allowance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "McDonald's;United_States_Department_of_Agriculture"}
{"question": "was Mozart accused of stealing from Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Wolfgang_Amadeus_Mozart;Richard_Wagner"}
{"question": "will Justin Bieber take over Mike Pence's position in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Justin_Bieber;Mike_Pence"}
{"question": "did any citizen of San Antonio vote for Boris Johnson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "San_Antonio;Boris_Johnson"}
{"question": "does being good at guitar hero make you a good guitarist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can Roman numerals fill the normal number of Sudoku box options?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Roman_numerals;Sudoku"}
{"question": "is a Cassowary safer pet than a crane?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was proofreading Edgar Allan Poe works lucrative?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Edgar_Allan_Poe"}
{"question": "was Hillary Clinton's deputy chief of staff in 2009 baptised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hillary_Rodham_Clinton"}
{"question": "can dessert be made with vegetables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "absolutely", "extracted_entity": null}
{"question": "would Othello be Shakespeare's play to buy Scheherazade most time with king?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Othello;William_Shakespeare;Scheherazade"}
{"question": "did Jon Brower Minnoch suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does ancient Olympics crown fail to hide tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can someone with dermatitis be a hand model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "dermat", "extracted_entity": "Dermatitis"}
{"question": "is it bad to have lactic acid in your body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lactic_acid"}
{"question": "are cucumbers often found in desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Cucumber"}
{"question": "are blue lips normal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can someone with celiac disease have potato vodka?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Coeliac_disease;Vodka"}
{"question": "are deaf people left out of enjoying music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Pope_John_Paul_I"}
{"question": "would Cardi B. benefit from soy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cardi_B"}
{"question": "are the names of The Powerpuff Girls alliterative? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Powerpuff_Girls"}
{"question": "did the leader of Heaven's Gate consider himself a prophet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Heaven's_Gate_(film)"}
{"question": "would King Leonidas have succeeded with an army the size of Mozart's compositions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Leonidas_I;Wolfgang_Amadeus_Mozart"}
{"question": "do Elementary School students typically need graphing calculators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Family of Barack Obama ride comfortably in 2020 Jaguar F Type?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Barack_Obama;Jaguar_F-Type"}
{"question": "could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Apollo;Charles_de_Batz-Castelmore_d'Artagnan"}
{"question": "does the Eighth Amendment to the United States Constitution protect freedom of speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eighth_Amendment_to_the_United_States_Constitution"}
{"question": "do most fans follow Katy Perry for gospel music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Katy_Perry;Gospel_music"}
{"question": "does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jerry_Seinfeld"}
{"question": "are rainbows devoid of color made by mixing yin and yang colors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Christopher Columbus sail representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Christopher_Columbus"}
{"question": "can an Asian black bear use chopsticks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Coen_brothers;Brothers_Grimm"}
{"question": "would a Durian be dangerous if it fell on your head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can nitric acid break the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Louvre"}
{"question": "does autopilot rely on fossil fuels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "were muskets used in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Pacific_War"}
{"question": "do tourists prefer Tuvalu to Niue?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "u", "extracted_entity": "Tuvalu;Niue"}
{"question": "could you windsurf in Puerto Rico during Hurricane Maria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Puerto_Rico;Tropical_Storm_Maria"}
{"question": "did Northwest Airlines' longevity surpass Betty White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Betty", "extracted_entity": "Northwest_Airlines;Betty_White"}
{"question": "is myocardial infarction a brain problem?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Newspeak considered very straightforward?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Newspeak"}
{"question": "was Pope Alexander VI's origin country least represented in papal history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Spain", "extracted_entity": "Pope_Alexander_VI"}
{"question": "would you find olives at a heladeria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do people celebrate Earth Day with a ceremonial tire fire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Earth_Day"}
{"question": "did France win the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "France;French_Revolution"}
{"question": "does menthol make cigarettes less addictive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Anthony_Quinn;J._D._Salinger"}
{"question": "could Reza Shah be related to Queen Elizabeth I?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Reza_Shah;Elizabeth_I_of_England"}
{"question": "is it possible that June got its name from mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is it possible to get killed walking to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Very_Large_Telescope"}
{"question": "is Canon Inc. a Kabushiki gaisha?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Canon_Inc.;Kabushiki_gaisha"}
{"question": "can a diamond float on water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does horseradish have a fetlock?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Fetlock"}
{"question": "can you hide a pet macaque under your desk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Paralympic_Games"}
{"question": "would fans of Jonathan Larson be unaware of HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Jonathan_Larson;HIV"}
{"question": "would Carolina Reaper decrease sales if added to all US salsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al", "extracted_entity": "Al_Pacino;Margaret_Qualley"}
{"question": "was the Eiffel tower used as a symbol of the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "French_Revolution"}
{"question": "does James Watson believe that Africans are inferior to Europeans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "James_Watson;Africa;Europe"}
{"question": "is The Joy of Painting TV show still producing new episodes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the skull formed as one whole bone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was  Godfrey of Bouillon an Islamaphobe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Godfrey_of_Bouillon;Islamophobia"}
{"question": "are lengths measured in metres in the UK?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do quadragenarian's have little memory capacity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Jackie Kennedy wear Dolce & Gabbana to her husband's inauguration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jacqueline_Kennedy_Onassis;Dolce_&_Gabbana"}
{"question": "did Larry King sign the Magna Carta?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Larry_King;Magna_Carta"}
{"question": "will a Euro sink in water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Euro"}
{"question": "would Iggy Pop travel with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Iggy_Pop;Justin_Bieber"}
{"question": "is Drew Carey important to the history of wrestling?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drew_Carey"}
{"question": "did Jay-Z ever collaborate with Louis Armstrong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jay_Z;Louis_Armstrong"}
{"question": "is CAS number 8009-03-8 harmful for a rash?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would a model be appropriate to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Michael J Fox qualify for the Army Rangers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Michael_J._Fox;United_States_Army_Rangers"}
{"question": "do salmon mate in the Caspian Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Caspian_Sea"}
{"question": "could Chuck Norris ride a horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chuck_Norris"}
{"question": "can Ford F-350 tow entire Yale University student body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Yale_University"}
{"question": "are any of J.K. Rowling's books in the genre of And Then There Were None?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would an actuary be confused about what prime numbers are?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Actuary;Prime_number"}
{"question": "was 1941 Operation Barbarossa related to The Crusades?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Operation_Barbarossa;Crusades"}
{"question": "can ham make a cut seal up quicker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can Chinese mountain cat survive in the orbit? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "People's_Republic_of_China"}
{"question": "are there special traffic laws associated with funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did George Washington drive a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "George_Washington;Lexus"}
{"question": "is metal a type of folk music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Folk_music"}
{"question": "did Neanderthals use arithmetic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Neanderthal"}
{"question": "would East India Company prefer China's modern trade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "East_India_Company;People's_Republic_of_China"}
{"question": "could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Samoa;JPMorgan_Chase"}
{"question": "would World War II have been the same without Alan Turing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "World_War_II;Alan_Turing"}
{"question": "would General Zod prefer an iPhone over a Samsung Galaxy S4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "IPhone;Samsung_Galaxy_S4"}
{"question": "can 200 men end to end cover Great Pyramid of Giza's base?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Great_Pyramid_of_Giza"}
{"question": "does meat from cows fed only grass taste more like wild game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alan_Turing;Abraham_Lincoln"}
{"question": "do people associate greyhounds with the movie 'Homeward Bound'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would it be unusual to find a yellow perch in the Red Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Red_Sea"}
{"question": "can every digit in Pi be memorized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Norman Oklahoma named after a viking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Norman,_Oklahoma;Oklahoma"}
{"question": "did Alfred Hitchcock include internet slang in his films?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "lfred", "extracted_entity": "Alfred_Hitchcock"}
{"question": "did Supernatural break 2001 CW debuting shows seasons record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Supernatural_(U.S._TV_series);The_CW"}
{"question": "would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Marvel", "extracted_entity": "Marvel_Comics;The_Doctor_(Doctor_Who);Doctor_Who;TARDIS"}
{"question": "would Jean Harris's victim have avoided lentils?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would an art dealer prize a print of a Van Goh? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is it comfortable to wear sandals outside Esperanza Base?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does welding with acetylene simulate the temperature of a star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Acetylene"}
{"question": "is pickled cucumber ever red?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Days", "extracted_entity": "Days_of_Our_Lives"}
{"question": "do any Islamic dominated countries have a Starbucks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Starbucks"}
{"question": "was Achilles a direct descendent of Gaia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Achilles", "extracted_entity": "Vila_Nova_de_Gaia"}
{"question": "was Noah associated with a dove?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Noah"}
{"question": "will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Taco_Bell;Roy_Rogers"}
{"question": "are Saturn's famous rings solid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saturn"}
{"question": "would menu at Chinese Starbucks be familiar to an American?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "United_States"}
{"question": "would Lee Sedol understand the complexities of the Sicilian Defence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Lee_Sedol"}
{"question": "would a northern fur seal pass a driving test?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was Hundred Years' War a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hundred_Years'_War"}
{"question": "did Nine Inch Nails inspire Aretha Franklin's sound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nine_Inch_Nails;Aretha_Franklin"}
{"question": "does giant panda have colors that differ from yin yang?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Giant_panda;Yin_and_yang"}
{"question": "would vegans consider chickpeas for a tuna substitute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Chickpea"}
{"question": "do the directors of The Matrix advocate for transgender rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Matrix"}
{"question": "is Dungeons and Dragons a game well suited for solo play?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would you buy bananas for tostones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Banana"}
{"question": "are birds important to badminton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "if you're running focal fossa, are you using linux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is jalapeno heat outclassed by Bhut jolokia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jalape\u00f1o"}
{"question": "would a goblin shark eat at Crossroads Kitchen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "is Lines on the Antiquity of Microbes briefer than any haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Haiku"}
{"question": "could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Eiffel_Tower;Arctic_Ocean"}
{"question": "would Kurt Cobain have benefited from Project Semicolon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kurt_Cobain"}
{"question": "did Douglas Adams use email as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Douglas_Adams"}
{"question": "could Goofy have counted nine planets in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Javier Sotomayor jump over the head of the average giraffe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Javier_Sotomayor"}
{"question": "can you avoid internet trolls on reddit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You", "extracted_entity": "Reddit"}
{"question": "is Ganymede in the Milky Way galaxy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ganymede_(moon);Milky_Way"}
{"question": "does Billy Graham support agnosticism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Billy_Graham"}
{"question": "is euphoria associated with drug addiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Euphoria"}
{"question": "can the Moscow Kremlin fit inside Disney Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Moscow_Kremlin;Disneyland"}
{"question": "did the Cherokee people send a delegation to oppose allotment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cherokee"}
{"question": "would an Orthodox Presbyterian object to 1700s judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Robert Wadlow tower over a German Shepherd?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "German_Shepherd_Dog"}
{"question": "is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a lullaby be enough to wake Hellen Keller up?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Helen_Keller"}
{"question": "would an average American Public University be welcoming to Ku Klux Klan members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "American_Public_University_System;Ku_Klux_Klan"}
{"question": "do the Egyptian pyramids look the same from outside as they did when new?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Egypt"}
{"question": "is popular science used to peer review papers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Popular_science;Scholarly_peer_review"}
{"question": "would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Messiah_(Handel)"}
{"question": "could Snoopy transmit rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did the Wall Street Crash of 1929 hurt the stocks of robotics companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Wall_Street_Crash_of_1929"}
{"question": "can I find my home with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the QWERTY keyboard layout meant to be slow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "QWERTY"}
{"question": "is dysphoria around one's pelvis treatable without surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "does Southwest Airlines use bulk carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Southwest_Airlines"}
{"question": "is helium the cause of the Hindenburg explosion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "were French people involved in the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "France;American_Civil_War"}
{"question": "could Edward Snowden join MENSA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Edward_Snowden"}
{"question": "did the 23rd amendment give Puerto Ricans the right to vote for president?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Elle Fanning play an essential part in ending apartheid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elle_Fanning"}
{"question": "could Jamie Brewer have attended the United States Naval Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "United_States_Naval_Academy"}
{"question": "if someone is lactose intolerant, do they have to avoid cream?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "They", "extracted_entity": null}
{"question": "can numerologists become members of Royal Society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Royal_Society"}
{"question": "could pickled cucumbers from 1,000 years ago be good still?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the most recent Democrat President in the US known for his painting practice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Obama", "extracted_entity": "Democratic_Party_(United_States);United_States"}
{"question": "do black swan cygnets typically know both of their genetic parents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Edward II of England have been born without Vikings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Edward_II_of_England;Vikings"}
{"question": "do workers at Nissan's headquarters eat with chopsticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nissan"}
{"question": "does Hanuman have some of the same duties as Athena?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hanuman;Athena"}
{"question": "does title of Van Morrison's most played song apply to a minority of women worldwide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Van_Morrison"}
{"question": "did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Austria;Seven_Years'_War;Indianapolis_Motor_Speedway"}
{"question": "can a rabbi save the soul of a Christian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christian"}
{"question": "would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dante_Alighieri"}
{"question": "can a student from Smithtown's Cleary School understand the speech of a French person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Smithtown,_New_York;France"}
{"question": "is the Berlin University of the Arts a Baroque period relic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Berlin_University_of_the_Arts;Baroque"}
{"question": "are paratroopers good at mountain rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "should wool be hand washed only?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "do people put creatures from the Black Sea on their pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Black_Sea"}
{"question": "is Pan a symbol of virtue and virginity in women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pan_(god)"}
{"question": "is Nine Inch Nails's lead singer associated with David Lynch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Trent", "extracted_entity": "Nine_Inch_Nails;David_Lynch"}
{"question": "if you bottle your own milk, would there be cream on top of it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Constitution of the United States paper offend PETA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_Constitution;People_for_the_Ethical_Treatment_of_Animals"}
{"question": "was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "King_Kong;Eiffel_Tower"}
{"question": "is Albany, Georgia the most populous US Albany?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Albany,_Georgia;United_States"}
{"question": "can Curiosity (rover) kill a cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are the Great Lakes part of an international border?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Great_Lakes"}
{"question": "did Karl Marx influence the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Karl_Marx;People's_Republic_of_China"}
{"question": "does American Independence Day occur during autumn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Independence_Day_(United_States)"}
{"question": "has the Holy Grail been featured in at least five films?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Holy_Grail"}
{"question": "can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Bathyal_zone"}
{"question": "are hippos dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hip", "extracted_entity": null}
{"question": "would it be impossible to get to Burning Man on the Mayflower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "t", "extracted_entity": "Burning_Man;Mayflower"}
{"question": "is the US Secretary of State similar to an administrative secretary of an office?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Secretary_of_State"}
{"question": "would Stephen King fans be likely to own an image of a clown?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stephen_King"}
{"question": "is snoring a sign of good breathing while sleeping?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sn", "extracted_entity": "Snoring"}
{"question": "did Mozart ever buy anything from Dolce & Gabbana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wolfgang_Amadeus_Mozart;Dolce_&_Gabbana"}
{"question": "did any Golden Globe winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Golden_Globe_Award;John_Kerry"}
{"question": "did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gracie_family"}
{"question": "have any members of the 2020 British royal family allegedly committed a felony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "British_Royal_Family"}
{"question": "can first letter row of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "QWERTY"}
{"question": "does March begin on the same day of the week as February during leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Leap_year"}
{"question": "would a Drow tower over The Hobbit's hero?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Drow_(Dungeons_&_Dragons);The_Hobbit"}
{"question": "are Big Ben's bells currently rung on their normal schedule at the Palace of Westminster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Big_Ben;Palace_of_Westminster"}
{"question": "is it dangerous to consume chlorine when mixed with sodium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Chlorine"}
{"question": "was Al-Farabi a student of the Great Sheikh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Al-Farabi"}
{"question": "can telescopes hear noise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Roman Gallic Wars army struggle to build the pyramids faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gallic_Wars"}
{"question": "can a software engineer work during a power outage?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Software_engineer;Power_outage"}
{"question": "would Goofy hypothetically enjoy Nylabone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Goofy"}
{"question": "was a USB flash drive used in The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "USB_flash_drive;The_Godfather"}
{"question": "can surgery prevent an existential crisis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "is an internet connection essential for someone using Chrome OS?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chrome_OS"}
{"question": "does butter industry survive cow extinction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is a Coca plant farm likely to be found in Yakutsk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Coca-Cola;Yakutsk"}
{"question": "does Siri know geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Siri"}
{"question": "in star rating systems, is 5 stars considered good?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "has Billy Joel sold out Astana Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Billy_Joel"}
{"question": "did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Solomon;Islam;Kings_of_Judah"}
{"question": "can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "CenturyLink"}
{"question": "was Surfing popular when pogs came out?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Surf", "extracted_entity": "Surfing"}
{"question": "does Hades have a loose grip on the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hades"}
{"question": "was Dorothea Wendling from same place Porsche originated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Porsche"}
{"question": "are there mental disorders you can hide?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "there", "extracted_entity": null}
{"question": "can a Muslim eat a McRib sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Islam"}
{"question": "did Julia Roberts practice blast beats as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Julia_Roberts"}
{"question": "do Muslims have a different idea of Seraphim than Christians?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Islam"}
{"question": "would nickel boil in the outer core of the earth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did the Paramount leader produce Titanic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "RMS_Titanic"}
{"question": "has Aretha Franklin ever collaborated with a suicidal person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aretha_Franklin"}
{"question": "would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Harry_Potter"}
{"question": "is capturing giant squid in natural habitat impossible with no gear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not", "extracted_entity": null}
{"question": "would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dante_Alighieri;Rupert_Murdoch"}
{"question": "did Andy Warhol influence Art Deco style?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Andy_Warhol;Art_Deco"}
{"question": "are any minor league baseball teams named after felines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do Apollo and Baldur share similar interests?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Apollo;Baldr"}
{"question": "would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Saddam_Hussein;Saladin;Idris_I_of_Morocco"}
{"question": "is August a winter month for part of the world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Naruto escape the Temple of Doom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is the Louvre's pyramid known for being unbreakable? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Louvre"}
{"question": "is breakdancing safe for people with tendonitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Christopher Columbus go to Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Christopher_Columbus;Antarctica"}
{"question": "was Dr. Seuss a liar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO", "extracted_entity": "Dr._Seuss"}
{"question": "is 1936 Summer Olympics venue too small for a Superbowl crowd?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "1936_Summer_Olympics;Super_Bowl"}
{"question": "does bull shark bite hurt worse than crocodile bite?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bull_shark"}
{"question": "could a white belt defeat Jon Jones in a Brazilian jiu-jitsu match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jon_Jones;Brazilian_jiu-jitsu"}
{"question": "would Communist Party of the Soviet Union hypothetically support Trickle Down Economics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Communist_Party_of_the_Soviet_Union"}
{"question": "lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lil", "extracted_entity": "Lil_Wayne;Grammy_Award"}
{"question": "is it unusual to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Happy_hardcore"}
{"question": "could a cow produce Harvey Milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Harvey_Milk"}
{"question": "do people still see Henry Ford's last name often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Henry_Ford"}
{"question": "would a thesis paper be unusual to assign to kindergartners? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "did George W. Bush grow up speaking Cantonese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "George_W._Bush;Cantonese"}
{"question": "was Eve involved in an incestuous relationship?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eve"}
{"question": "can the Communist Party of the Soviet Union get a perfect all kill?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Communist_Party_of_the_Soviet_Union"}
{"question": "was Ariana Grande inspired by Imogen Heap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ariana_Grande;Imogen_Heap"}
{"question": "will Futurama surpass the number of episodes of The Simpsons by the end of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Futurama;The_Simpsons"}
{"question": "would Bonanza marathon end before WWE Heat marathon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bonanza;WWE"}
{"question": "would a greyhound be able to outrun a greyhound bus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Tony Bennett have more children than he had wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tony_Bennett"}
{"question": "is it safe to use Ammonia with Clorox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "Ammonia;Clorox"}
{"question": "is Bill Gates the wealthiest of the Baby Boomers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Gates;Baby_boomers"}
{"question": "did Switzerland support the United States in the Spanish\u2013American War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Switzerland;United_States;Spanish\u2013American_War"}
{"question": "is Rosemary outclassed as plant found in most song titles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Rosemary"}
{"question": "did Methuselah live at least 800 years as long as Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Methuselah;Sarah_Brady"}
{"question": "did any of Maya Angelou's children follow in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Maya_Angelou"}
{"question": "is Alistair Darling in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alistair_Darling;Scottish_independence"}
{"question": "would it be difficult for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Mount_Emei"}
{"question": "could an NBA game be completed within the span of the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "National_Basketball_Association;Six-Day_War"}
{"question": "would Arnold Schwarzenegger have a hard time picking up a red fox in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Arnold_Schwarzenegger"}
{"question": "did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Albanian_civil_war_of_1997"}
{"question": "do all cancer patients get disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was the father of social security system serving in the white house during the Panic of 1907?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Panic_of_1907"}
{"question": "would the host of The Colbert Report be likely to vote for Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Colbert_Report"}
{"question": "would Felicity Huffman vote for Mike DeWine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Felicity_Huffman;Mike_DeWine"}
{"question": "are Naruhito's ancestors the focus of Romance of the Three Kingdoms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Naruhito;Romance_of_the_Three_Kingdoms"}
{"question": "would Recep Tayyip Erdo\u011fan be unfamiliar with b\u00f6rek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Recep_Tayyip_Erdo\u011fan"}
{"question": "did Richard Wagner compose the theme songs for two television series?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Wagner"}
{"question": "are the colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Marlboro_Township,_New_Jersey;France"}
{"question": "would Matt Damon be afraid of parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Matt_Damon"}
{"question": "does Long John Silver's serve sea otter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Long_John_Silver's"}
{"question": "does United Airlines have a perfect operation record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_Airlines"}
{"question": "is it wise to feed a Snickers bar to a poodle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Snickers"}
{"question": "can someone sell their time through the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Toronto_Star"}
{"question": "does Zelda Williams have any cousins on her father's side?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Zelda_Williams"}
{"question": "could Casio's first invention be worn around the ankle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eric_Clapton;United_States"}
{"question": "is the Federal Reserve a quick walk from Space Needle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Federal_Reserve_System;Space_Needle"}
{"question": "did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dr._Seuss"}
{"question": "were the Ten commandments the part of the bible that Jewish people do not believe in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ten_Commandments;Jews"}
{"question": "would Modafinil be effective in completing a suicide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Modafinil"}
{"question": "did US President during Spanish-American War suffer similar demise to Abraham Lincoln?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "United_States;President_of_the_United_States;Spanish\u2013American_War;Abraham_Lincoln"}
{"question": "does Post Malone have a fear of needles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Post", "extracted_entity": "Post_Malone"}
{"question": "hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "African_elephant;Hulk"}
{"question": "can a New Yorker get their eyes checked by Rand Paul legally?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "New_York_City;Rand_Paul"}
{"question": "would Hapshetsut be considered a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Hatshepsut"}
{"question": "does Sockington enjoy onions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the Gunpowder plot eliminate Mary, Queen of Scots bloodline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mary,_Queen_of_Scots"}
{"question": "did goddess Friday is named after despise felines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Friday_(Robinson_Crusoe)"}
{"question": "would a blooming onion be possible with a shallot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shallot"}
{"question": "is there a popular Disney character made from living ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Alcatraz_Island;Siberia"}
{"question": "would an Evander Holyfield 2020 boxing return set age record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Evander_Holyfield"}
{"question": "can methane be seen by the naked eye?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are all students guaranteed lunch at school in the US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "did Maroon 5 go on tour with Nirvana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Maroon_5;Nirvana_(band)"}
{"question": "are pancakes a bad snack for cats?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a modern central processing unit circuit chip fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Central_processing_unit"}
{"question": "would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "demort", "extracted_entity": "Lord_Voldemort;Hogwarts"}
{"question": "is silicon important in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "California"}
{"question": "did Rumi spend his time in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Rumi"}
{"question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "is ABBA's 1970's genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "e", "extracted_entity": "ABBA"}
{"question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": null}
{"question": "did P. G. Wodehouse like the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "P._G._Wodehouse"}
{"question": "could amoebas have played a part in the Black Death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Black_Death"}
{"question": "did anyone in the 1912 election take a majority of the popular vote?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does parsley sink in milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would someone go to San Francisco for a nature escape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "San_Francisco"}
{"question": "did the iPhone usher in the scientific revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "IPhone"}
{"question": "were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Four_Horsemen_of_the_Apocalypse"}
{"question": "do people with DID have a good memory?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a carrot receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Organ_transplantation"}
{"question": "does Darth Vader's character resemble Severus Snape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Darth_Vader;Severus_Snape"}
{"question": "can spiders help eggplant farmers control parasites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was John Lennon known to be a good friend to Sasha Obama?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Lennon;Family_of_Barack_Obama"}
{"question": "did the population of the Warsaw Ghetto record secret police on cell phones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Warsaw_Ghetto"}
{"question": "do children send their Christmas letters to the South Pole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do all crustaceans live in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Crustacean"}
{"question": "does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "James_Webb_Space_Telescope;Krypton_(comics)"}
{"question": "does Robert Downey Jr's Marvel Cinematic Universe character survive the Infinity War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Robert_Downey_Jr.;Marvel_Cinematic_Universe;Production_of_Avengers:_Infinity_War_and_the_untitled_Avengers_sequel"}
{"question": "did Secretariat win a Formula One championship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Secretariat_(horse);Formula_One"}
{"question": "can Billie Eilish afford a Porsche?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She", "extracted_entity": "Billie_Eilish;Porsche"}
{"question": "is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gulf_of_Mexico"}
{"question": "do most religious people in Quebec refer to the Quran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Quebec;Qur'an"}
{"question": "do members of NFL teams receive infantry training?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "National_Football_League"}
{"question": "is it normal for people to sing when the YMCA is mentioned?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "YMCA"}
{"question": "does 50 Cent get along with Jeffrey Atkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "50_Cent"}
{"question": "are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Reddit;Pledge_of_Allegiance_(United_States)"}
{"question": "did H.G. Wells' \"War of the Worlds\" include cosmic rays?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "H._G._Wells;The_War_of_the_Worlds;Cosmic_ray"}
{"question": "do skeletons have hair?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does conservatism repulse Blaire White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Swiss Guard defeat the Marines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Swiss_Guard;United_States_Marine_Corps"}
{"question": "can a firewall protect against a short circuit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Firewall_(computing);Short_circuit"}
{"question": "can Hulk's alter ego explain atomic events?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hulk"}
{"question": "does US brand Nice depend on Western honey bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;OGC_Nice;Western_honey_bee"}
{"question": "would Benito Mussolini hypothetically play well in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Benito_Mussolini;National_Basketball_Association"}
{"question": "would the author of Little Women have remembered the ratification of the 13th Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Little_Women;Thirteenth_Amendment_to_the_United_States_Constitution"}
{"question": "do bodies movie during hanging?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Hurricane Harvey catch a Peregrine falcon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hurricane_Harvey;Peregrine_falcon"}
{"question": "is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Aloe"}
{"question": "could ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "ABBA"}
{"question": "can you swim to Miami from New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Miami;New_York_City"}
{"question": "could the children of Greek hero Jason hypothetically fill a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Greece;Jason"}
{"question": "could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Brazilian_Navy;Battle_of_Actium"}
{"question": "can you find Bugs Bunny at Space Mountain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bugs_Bunny"}
{"question": "did a Polish poet write sonnets about Islamic religion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Poland;Islam"}
{"question": "is Hanuman associated with a Norse god?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hanuman"}
{"question": "does Iphone have more iterations than Samsung Galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Samsung_Galaxy"}
{"question": "can the city of Miami fit inside Uppsala?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Miami;Uppsala"}
{"question": "was King Arthur at the beheading of Anne Boleyn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "King_Arthur;Anne_Boleyn"}
{"question": "can you transport a primate in a backpack?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Christopher Columbus break the fifth commandment in Christianity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christopher_Columbus;Christianity"}
{"question": "do most high school head coaches make as much as the Head Coach at NCSU?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "has Justin Timberlake ever written a song about Britney Spears?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Justin_Timberlake;Britney_Spears"}
{"question": "could Eddie Hall hypothetically deadlift the world's largest cheeseburger?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Eddie_Hall"}
{"question": "did Johann Sebastian Bach leave his first wife for his second wife?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Johann_Sebastian_Bach"}
{"question": "is the Mona Lisa based on a real person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does ABBA have similar gender configuration to The Mamas & The Papas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "ABBA;The_Mamas_&_the_Papas"}
{"question": "is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Mozambique Drill an easy shot for United States Army Ranger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States_Army_Rangers"}
{"question": "have rhinoceroses been killed to improve human sex lives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Rhinoceros"}
{"question": "is the Easter Bunny popular in September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Easter_Bunny"}
{"question": "were veterans of the War in Vietnam (1945\u201346) given free education by the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Vietnam_War;Vietnam;Soviet_Union"}
{"question": "can a Kia Rio fit inside the Oval Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Kia_Rio;Oval_Office"}
{"question": "are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christianity_in_China;Catholic_Church"}
{"question": "would someone buying crickets be likely to own pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "is Bugs Bunny known for carrying a root vegetable around with him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bugs_Bunny;List_of_root_vegetables"}
{"question": "would moon cakes be easy to find in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chinatown,_Manhattan"}
{"question": "does the Red Sea have biblical significance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Red_Sea;Bible"}
{"question": "could a newborn look over the top of a fully grown horseradish plant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "will a rock float in the atmosphere of Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Atmosphere_of_Earth"}
{"question": "would three newborn kittens fit on a standard Amtrak coach seat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amtrak"}
{"question": "would a loudspeaker be useful for most Gallaudet students?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Bucharest located south of Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Egypt"}
{"question": "do Bing (search engine) searches earn the searcher more than competitors do?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bing", "extracted_entity": "Bing"}
{"question": "was Alaska part of the Northern Army during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alaska;American_Civil_War"}
{"question": "will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "do all shooting sports involve bullets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Shooting_sport"}
{"question": "is Edgar Allan Poe obscure in the world of horror fiction?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Edgar_Allan_Poe;Horror_fiction"}
{"question": "would Achilles dominate Legolas in a hypothetical fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "eg", "extracted_entity": "Achilles;Legolas"}
{"question": "would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Martin_Luther;Protestant_Reformation"}
{"question": "can a prime number be represented by the number of days in a week?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Planned Parenthood tell your University that you have Herpes simplex virus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Planned", "extracted_entity": "Planned_Parenthood;Herpes_simplex_virus"}
{"question": "can the Palace of Westminster tell time in the dark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Palace_of_Westminster"}
{"question": "can Arnold Schwarzenegger deadlift an adult Black rhinoceros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Arnold_Schwarzenegger;Black_rhinoceros"}
{"question": "did Jesus go to school to study railroad engineering?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jesus"}
{"question": "could a white cockatoo have lived through the entire Thirty Years' War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Thirty_Years'_War"}
{"question": "would most school children in New York be wearing jackets on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "New_York"}
{"question": "do children's bicycles often have extra wheels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are all Wednesdays in a year enough to read Bible 15 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Bible"}
{"question": "could Bernie Sanders visit the Metropolitan Museum of Art twenty times for under two hundred dollars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Bernie_Sanders;Metropolitan_Museum_of_Art"}
{"question": "has Nikola Tesla's name inspired multiple brands?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Nikola_Tesla"}
{"question": "is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Noah's_Ark;Golden_Age_of_Piracy"}
{"question": "is the Matrix a standalone movie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Matrix"}
{"question": "could JPMorgan Chase give every American $10?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "JPMorgan_Chase;United_States"}
{"question": "will the Stanford Linear Accelerator fit on the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "SLAC_National_Accelerator_Laboratory;Golden_Gate_Bridge"}
{"question": "have Jamie Lee Curtis been the subject of fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jamie_Lee_Curtis"}
{"question": "would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Statue_of_Liberty;Bohai_Sea"}
{"question": "does New York Harbor sit on a craton without volcanic activity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_York_Harbor"}
{"question": "was United Airlines blameless in worst crash in history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_Airlines"}
{"question": "did Polar Bears roam around in Ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ancient_Greece"}
{"question": "did Queen Elizabeth I read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Elizabeth_I_of_England;Jean-Paul_Sartre"}
{"question": "is there historic graffiti on Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Alcatraz_Island"}
{"question": "does Adam Sandler skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Adam_Sandler;Easter"}
{"question": "would a sesame seed be mistaken for a wood frog egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sesame"}
{"question": "is Capricorn the hypothetical zodiac sign of Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Capricorn_(astrology);Satanism"}
{"question": "is Nicole Kidman ideal choice to play Psylocke based on height and weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nicole_Kidman;Psylocke"}
{"question": "is a paraplegic suitable for conducting an orchestra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would you hire someone with dyscalculia to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Dyscalculia"}
{"question": "are there enough people in the Balkans to match the population of Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Balkans;Japan"}
{"question": "does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pan_(god)"}
{"question": "can the Persian Gulf fit in New Jersey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Persian_Gulf;New_Jersey"}
{"question": "did the swallow play a role in a famous film about King Arthur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Swallow;King_Arthur"}
{"question": "could Durian cause someone's stomach to feel unwell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "do people take laxatives because they enjoy diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would someone on antidepressants need to be cautious of some citrus fruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": null}
{"question": "was Daniel thrown into the lion's den in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "iel", "extracted_entity": "Daniel_(biblical_figure);New_Testament"}
{"question": "can a Toyota Supra make a vlog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Toyota_Supra;Video_blog"}
{"question": "did brother of Goofy creator's employer commit an excommunicable offense?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Goofy"}
{"question": "did any of religions in which Himalayas are sacred originate in 19th century?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Himalayas"}
{"question": "could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "New_York_Public_Library;Six_Flags_Great_Escape_and_Hurricane_Harbor"}
{"question": "would an oil painter avoid reds from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is coal needed to practice parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can food be cooked in the cosmic microwave background?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Central Park Zoo located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Central_Park_Zoo"}
{"question": "is Michael an unpopular name in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States"}
{"question": "is it common for women to have moustaches?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is dopamine snorted nasally by drug users?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dopamine"}
{"question": "can I ski in Steamboat Springs, Colorado in August?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Steamboat_Springs,_Colorado;Colorado"}
{"question": "could the surface of Europa fry an egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Europa_(moon)"}
{"question": "can too many oranges cause diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would someone in CHE101 require a Maya Angelou book?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Maya_Angelou"}
{"question": "can I build a house on an asteroid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you get Raclette in YMCA headquarters city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Kathleen_Raine;YMCA"}
{"question": "is a fairy more prevalent in world myths than a valkyrie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Fairy;Valkyrie"}
{"question": "does Nicole Kidman despise Roman Josi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Nicole_Kidman"}
{"question": "are looks the easiest way to tell rosemary from lavender? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "t", "extracted_entity": "Rosemary;Lavandula"}
{"question": "are all twins the same gender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Sainsbury's buy Tesco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tesco"}
{"question": "is the Greek alphabet as common as Sumerian cuneiform?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Greek_alphabet;Sumerian_language"}
{"question": "would Jesus understand the Easter Bunny?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jesus;Easter_Bunny"}
{"question": "would Avengers Comics be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Avengers", "extracted_entity": "Avengers_(comics);DC_Comics"}
{"question": "would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States_Air_Force;Return_of_the_Jedi;Han_Solo"}
{"question": "were any members of Canidae in Aesop's Fables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Canidae;Aesop's_Fables"}
{"question": "does Snoopy look like Chance from Homeward Bound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Snoopy;Homeward_Bound_(Simon_&_Garfunkel_song)"}
{"question": "did Martin Luther believe in Satan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Martin_Luther;Satan"}
{"question": "could Barron Trump have lived through the Mexican Revolution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mexican_Revolution"}
{"question": "is the current Chief Justice of the United States forbidden from buying alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chief_Justice_of_the_United_States"}
{"question": "would Columbus have discovered Durian trees during his 1492 expedition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Columbus"}
{"question": "could a camel fit in a dog house?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Immanuel Kant ever meet the 14th president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Immanuel_Kant;United_States"}
{"question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Krispy_Kreme"}
{"question": "will a person survive a fever of NY's highest recorded temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was latest Republican governor of New Jersey as of 2020 heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chris", "extracted_entity": "Republican_Party_(United_States);New_Jersey"}
{"question": "did Dale Jr.'s father crash his car due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Conan_O'Brien"}
{"question": "should you bring your own bags to Aldi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You", "extracted_entity": "Aldi"}
{"question": "is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rurouni_Kenshin"}
{"question": "could you make the kitchen 'holy trinity' without celery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Celery"}
{"question": "can a snake swallow an M60 Patton?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "M60_Patton"}
{"question": "would an American feel lost due to language barriers at Disneyland Paris?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States;Disneyland_Paris"}
{"question": "did Christopher Columbus condone multiple deadly sins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Christopher_Columbus"}
{"question": "would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_Dawkins;Last_rites"}
{"question": "should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would an owl monkey enjoy a strawberry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": null}
{"question": "do beeswax alternatives to cling wrap use plsatic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are Brian Cranston and Saoirse Ronan's combined Emmy Awards a prime number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bryan_Cranston;Saoirse_Ronan;Emmy_Award"}
{"question": "does Soylent use Pea for their source of protein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does the United States Department of Education oversee services benefiting undocumented migrants? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "United_States_Department_of_Education"}
{"question": "should cactus soil always be damp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was disc jockey Jay Thomas enemies with Clayton Moore?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jay_Thomas;Clayton_Moore"}
{"question": "are there people who are men who experience menstruation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a retail associate envy the retailer's CEO's pay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is a person with St. Vitus's Dance likely to win a ballet competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a pacifist be opposed to hunting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Jane Austen suffer from middle child syndrome?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jane_Austen"}
{"question": "do hyenas appear in a Broadway musical?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Broadway_theatre"}
{"question": "is calling ABBA the Swedish Beatles a preposterous claim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "ABBA"}
{"question": "can an emu chase a bogan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Emu;Bogan"}
{"question": "would the Ku Klux Klan welcome Opal Tometi into their group?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Ku_Klux_Klan"}
{"question": "was the Mentalist filmed in black and white?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Mentalist"}
{"question": "did Mike Tyson train to use the gogoplata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Mike_Tyson"}
{"question": "does the JPEG acronym stand for a joint committee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "JPEG"}
{"question": "could a Jujutsu expert hypothetically defeat a Janissary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Jujutsu;Janissaries"}
{"question": "could boolean algebra be described as binary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "lean", "extracted_entity": "Boolean_algebra;Boolean_algebra_(structure)"}
{"question": "would Iceland lose to Amazon in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Iceland;Amazon.com"}
{"question": "was Florence Nightingale's death more painful than Saint Peter's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Florence_Nightingale;Saint_Peter"}
{"question": "are a dozen pickles good for easing hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "can Larry King's ex-wives form a water polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Larry_King"}
{"question": "did Hamlet's author use email?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hamlet"}
{"question": "would Garfield like canid food?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": null}
{"question": "did Helen Keller ever read a novel by J. K. Rowling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Helen_Keller;J._K._Rowling"}
{"question": "was the AK-47 used in the Seven Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "AK-47;Seven_Years'_War"}
{"question": "would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Emma_Roberts;Nancy_Drew"}
{"question": "are there multiple Star Wars TV shows produced by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Walt_Disney_Company"}
{"question": "did origin dynasty of Go precede Jia Sidao?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Jia_Sidao"}
{"question": "do Australians ride Kangaroos to work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Australian_people;Kangaroo"}
{"question": "will every resident of Los Angeles County, California go to Jehovah Witnesses's heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Los_Angeles_County,_California;California;Jehovah's_Witnesses"}
{"question": "are the Vietnamese people a great untapped resource for NBA players?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Vietnamese_people;National_Basketball_Association"}
{"question": "did the Berlin Wall prevent any athletes from competing in the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Berlin_Wall;1936_Summer_Olympics"}
{"question": "is Route 66 generally unknown to Americans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "U.S._Route_66"}
{"question": "would a Bulgarian priest eat a four-course meal on Christmas Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bulgaria;Christmas_Eve"}
{"question": "would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Woodrow_Wilson"}
{"question": "is Shiva's divine dance an ancient physical fitness pose?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Shiva"}
{"question": "is Pig Latin related to real Latin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Pig_Latin;Latin"}
{"question": "does the Dalai Lama believe in the divine barzakh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "14th_Dalai_Lama"}
{"question": "will the torso be safe from blows to the largest and smallest bones in body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Spiderman fight against Falcon in the MCU?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would Alexander Hamilton have known about koalas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alexander_Hamilton;Koala"}
{"question": "are tumors in the lymph nodes ignorable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did the Qwerty keyboard layout predate computers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "QWERTY"}
{"question": "can crane slamdunk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is a railroad engineer needed during NASCAR events?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Railway_engineering;NASCAR"}
{"question": "would Kelly Clarkson's voice shake glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Kelly_Clarkson"}
{"question": "does an organ donor need to be dead to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Disney", "extracted_entity": "The_Walt_Disney_Company;Rudyard_Kipling;The_Jungle_Book_(1967_film);The_Jungle_Book"}
{"question": "is Fiat Chrysler gaining a new overall corporate identity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Fiat_Chrysler_Automobiles"}
{"question": "is ID required to get all medications from all pharmacies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Benjamin", "extracted_entity": "Benjamin_Franklin;Black_Lives_Matter"}
{"question": "would it be common to find a penguin in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Miami"}
{"question": "is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mickey_Mouse"}
{"question": "during the pandemic, is door to door advertising considered inconsiderate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would early Eastern Canadian Natives language have use of the letter B?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "should someone prone to jealousy be in a polyamorous relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "can eating grapefruit kill besides allergies or choking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Grapefruit"}
{"question": "do Shivambu practitioners believe ammonia is unhealthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does a Starbucks passion tea have ginger in it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Starbucks"}
{"question": "was The Jackson 5 bigger family band than The Isley Brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "The_Jackson_5;The_Isley_Brothers"}
{"question": "does Mercury help detect coronavirus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mercury_(mythology)"}
{"question": "would someone typically confuse a sweet potato with a pineapple?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sweet_potato;Pineapple"}
{"question": "would a rabbi worship martyrs Ranavalona I killed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Ranavalona_I"}
{"question": "are there tearjerkers about United Airlines flights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_Airlines"}
{"question": "would James Cotton's instrument be too strident for a smooth jazz band?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "James_Cotton"}
{"question": "is Oculudentavis more dangerous than Allosaurus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Allosaurus"}
{"question": "would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wembley_Stadium"}
{"question": "is Cholera alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does Magnus Carlsen enjoy KFC?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Magnus_Carlsen;KFC"}
{"question": "should oysters be avoided by people with ADHD?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Attention_deficit_hyperactivity_disorder"}
{"question": "is it expected that Charla Nash would be anxious near a gorilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can amoebas get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Snoop Dogg advocate a straight edge lifestyle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Snoop_Dogg"}
{"question": "is menthol associated with Christmas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Christmas"}
{"question": "would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Christopher_Hitchens"}
{"question": "did Fran\u00e7ois Mitterrand serve under Napoleon Bonapart in the French army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Fran\u00e7ois_Mitterrand;Napoleon_I;France"}
{"question": "do ants outperform apes on language ability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Cantonese spoken in Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Cantonese;Japan"}
{"question": "would a birdwatcher pursue their hobby at a Philadelphia Eagles game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Philadelphia_Eagles"}
{"question": "do shrimp taste best when cooked for a long time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sh", "extracted_entity": "Shrimp"}
{"question": "did Columbus obtain his funding from the rulers of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christopher_Columbus;Portuguese_Empire"}
{"question": "can a minotaur hypothetically injure a tibia playing football?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Minotaur;Tibia;Association_football"}
{"question": "did the Royal Air Force fight in the Boxer Rebellion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Royal_Air_Force;Boxer_Rebellion"}
{"question": "are monks forbidden from engaging in warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Metallica use Soulseek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Metallica"}
{"question": "would the operating system of a Samsung Galaxy 1 sound edible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can the President of Mexico vote in New Mexico primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "President_of_Mexico;New_Mexico"}
{"question": "could a firewall be destroyed by a hammer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is radioactive waste a plot device for many shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "are raw carrots better for maximizing vitamin A intake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are all characters in Legend of Robin Hood fictional?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can Amtrak's Acela Express break the sound barrier?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amtrak;Acela_Express"}
{"question": "can Vice President of the United States kill with impunity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Vice_President_of_the_United_States"}
{"question": "is Brooklyn known for its bread products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Brooklyn"}
{"question": "can children be hurt by jalapeno peppers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would a cattle farmer be useful to a drum maker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Pedubastis I know Japanese people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan"}
{"question": "can Cyril Ramaphosa become Secretary General of NATO?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cyril_Ramaphosa;Secretary_General_of_NATO"}
{"question": "could an elephant easily defeat a male macaque?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would the trees in Auburn, New York be changing colors in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "bably", "extracted_entity": "Auburn,_New_York"}
{"question": "would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably", "extracted_entity": "Stanley_Cup_Finals;Rock_in_Rio"}
{"question": "are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Led_Zeppelin"}
{"question": "can atheism surpass Christianity in American black communities by 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Atheism;Christianity"}
{"question": "did James Watson's partner in studying the double helix outlive him? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "James_Watson"}
{"question": "does the central processing unit usually have a dedicated fan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Central_processing_unit"}
{"question": "was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Snoop_Dogg"}
{"question": "are sesame seeds glued onto hamburger buns?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "were the Great Pyramids built by a theocratic government?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Giza_pyramid_complex;Theocracy"}
{"question": "would it be wise to bring a robusto into Central Park Zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Central_Park_Zoo"}
{"question": "do restaurants associate meatballs with the wrong country of origin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can eating your weight in celery prevent diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Mario use mushrooms to run faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mario"}
{"question": "are goldfish more difficult to care for than isopods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Goldfish;Isopoda"}
{"question": "is the rise of agriculture attributed to rivers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "History_of_agriculture"}
{"question": "were number of states in Ancient Greece underwhelming compared to US states in 1900?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ancient_Greece;United_States"}
{"question": "did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eastern_Orthodox_Church;Byzantine_Empire"}
{"question": "has Cesar Millan ever tamed a short-eared dog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cesar_Millan"}
{"question": "can a chess board be converted to a Shogi board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could R. Kelly write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "R._Kelly"}
{"question": "could the moon fit inside the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Black_Sea"}
{"question": "can paratroopers be used in a vacuum?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can I hold Bing in a basket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nepalese_Civil_War;India"}
{"question": "is clementine pith highly sought after?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Clementine"}
{"question": "would a Rabbi celebrate Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Christmas"}
{"question": "are psychiatric patients welcome to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Psych", "extracted_entity": "United_States_Air_Force"}
{"question": "did Ivan the Terrible use the Byzantine calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ivan_the_Terrible"}
{"question": "can you get a fever from consuming meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "can Viper Room concert hypothetically be held at National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "The_Viper_Room"}
{"question": "would you be likely to see storks at a baby shower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would Methuselah hypothetically hold a record in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Common_Era"}
{"question": "as of 2020 have more women succeeded John Key than preceded him?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "John_Key"}
{"question": "does Princess Peach's dress resemble a peach fruit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Princess_Peach"}
{"question": "is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Steve_Martin"}
{"question": "do mail carriers need multiple uniforms?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Easy Rider make a profit at the theater when it was released?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Easy_Rider"}
{"question": "is most store bought rice pudding made with brown rice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would the chef at Carmine's restaurant panic if there was no basil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are Chipotle Cinnamon Pork Chops appropriate for a Seder?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chipotle_Mexican_Grill;Passover_Seder"}
{"question": "could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bengal_cat;Javier_Sotomayor"}
{"question": "does a sea otter eat spiders?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is unanimously elected president's birthday a break for mail carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Rusev have to worry about human overpopulation in his homeland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Rusev_(wrestler);Human_overpopulation"}
{"question": "does Buddy The Elf know anyone who works in publishing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Liberty Bell still in its original location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Liberty_Bell"}
{"question": "does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Columbia_University"}
{"question": "could a hundred thousand lolcats fit on a first generation iPhone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "IPhone"}
{"question": "are there multiple Disney Zorro?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Zorro"}
{"question": "would students at Marist have to petition to get a rowing team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Marist_College"}
{"question": "does frost mean that it will be a snowy day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Boris", "extracted_entity": "Boris_Yeltsin;2008_Summer_Olympics"}
{"question": "can a quarter fit inside of a human kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Alfred Nobel write a banned book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Alfred_Nobel"}
{"question": "could Palm Beach be held in the palm of your hand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Palm_Beach,_Florida"}
{"question": "would Jason Voorhees hypothetically fail at being a martyr?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jason_Voorhees"}
{"question": "do more Cauliflower grow in Arizona than California?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Arizona;California"}
{"question": "is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Steven_Spielberg;War_Horse_(play)"}
{"question": "does a Trek 9000 require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can a Goblin shark hypothetically ride a bike if it had limbs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "could John Key issue an executive order in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "John_Key;United_States"}
{"question": "does Evander Holyfield eat pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Evander_Holyfield"}
{"question": "are the headquarters of All Nippon Airways near a beach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "All_Nippon_Airways"}
{"question": "will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Chick-fil-A"}
{"question": "do hamsters provide food for any animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ham", "extracted_entity": "Hamster"}
{"question": "can horseradish be eaten in a religious context?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Horseradish"}
{"question": "would Taylor Swift refer to Snoopy as oppa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Taylor_Swift;Snoopy"}
{"question": "should children be kept from \"special brownies\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can monkeys use QWERTY keyboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "QWERTY"}
{"question": "are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Johnny_Cash;France"}
{"question": "did the Nazis use the Hammer and sickle flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are any letters directly before and after H missing from Roman numerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Roman_numerals"}
{"question": "do storks need golden toads to survive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did compact discs make computer gaming more popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Compact_disc;PC_game"}
{"question": "would a teacher still have their job if they called a black student an ape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Bill Gates help to develop the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bill_Gates;PlayStation_4"}
{"question": "would a Catholic priest commend someone's pride?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Catholic_Church"}
{"question": "did the Social Democratic Party of Germany help Frederick II become King of Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Social_Democratic_Party_of_Germany;Frederick_the_Great;Prussia"}
{"question": "is Eighth Amendment to the United States Constitution popular in court?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eighth_Amendment_to_the_United_States_Constitution"}
{"question": "could Aretha Franklin vote for a president when her second child was born?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aretha_Franklin"}
{"question": "can you use the T-Mobile tuesdays app if you aren't a T-Mobile customer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "T", "extracted_entity": "T-Mobile_US"}
{"question": "are classic nintendo games for emulator legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can a Sphynx cat be used for wool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sphynx_cat"}
{"question": "do people remember Lucille Ball's winemaking as successful?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Lucille_Ball"}
{"question": "would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Atlantic_salmon;David_Duchovny"}
{"question": "could Brooke Shields succeed at University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Brooke", "extracted_entity": "Brooke_Shields;University_of_Pennsylvania"}
{"question": "does Neville Longbottom have more courage as a child than as an adult?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Neville", "extracted_entity": "Neville_Southall"}
{"question": "do Windows or Android smartphones run newer versions of Linux?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "id", "extracted_entity": "Microsoft_Windows;Android_(operating_system);Linux"}
{"question": "can the largest crustacean stretch out completely on a king-sized mattress?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Crustacean"}
{"question": "did Bruiser Brody wrestle on WWE Raw?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bruiser_Brody;WWE_Raw"}
{"question": "is Atlantic cod found in a vegemite sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Atlantic_cod"}
{"question": "is an astronomer interested in drosophila?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "do people who smoke Djarum's like cloves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Bruce Lee absent from the 1964 University of Washington graduation ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bruce_Lee;University_of_Washington"}
{"question": "does chlorine inhibit photosynthesis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Amtrak operate four wheel vehicles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amtrak"}
{"question": "will parma ham be ready for New Year's if the pig is slaughtered in December?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "New_Year"}
{"question": "have jokes killed more people than rats in history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Islamophobia;Cyprus"}
{"question": "are there multiple American government holidays during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "United_States"}
{"question": "is Romeo and Juliet an unusual title to teach high schoolers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does the density of helium cause voices to sound deeper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did Snoop Dogg refuse to make music with rival gang members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Snoop_Dogg"}
{"question": "are Leopard cats in less dire straits than Bornean Orangutan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Leopard;Bornean_orangutan"}
{"question": "did the Wehrmacht affect the outcome of the War to End All Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Wehrmacht"}
{"question": "could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oprah_Winfrey;Bugatti"}
{"question": "has Ivan the Terrible flown to Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ivan_the_Terrible;Europe"}
{"question": "would a silicon shortage be bad for Intel's sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Intel"}
{"question": "would a Frigatebird in Ontario be a strange sight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Frigatebird;Ontario"}
{"question": "can cancer cause excess adrenaline production?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Epinephrine"}
{"question": "can you cure hepatitis with a tonsillectomy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "if someone loves buffalo wings do they enjoy capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Capsaicin"}
{"question": "are tampons a good 24 hour solution for mentruation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the Royal Air Force ensign on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Royal_Air_Force"}
{"question": "would a customer be happy if their grocery store meat tasted like game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is the cuisine of Hawaii suitable for a vegan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cuisine_of_Hawaii"}
{"question": "is it safe to wear sandals in snow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Very Large Telescope the most productive telescope in the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Very_Large_Telescope"}
{"question": "is All Purpose Flour safe for someone who has celiac disease?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Coeliac_disease"}
{"question": "while viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Scary_Movie"}
{"question": "should spaghetti be slick when cooked?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Sweet Potatoes prevent other plants from growing in their place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was Richard III ruler of Adelaide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Richard_III_of_England"}
{"question": "are Sable's a good choice of Mustelidae to weigh down a scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sable;Mustelidae"}
{"question": "was Snoop Dogg's debut studio album released on the weekend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Snoop_Dogg"}
{"question": "do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Krishna similar to Holy Spirit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Krishna;Holy_Spirit"}
{"question": "is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Hobbit;United_States_Constitution"}
{"question": "does Santa Claus work during summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Santa_Claus"}
{"question": "would the average American family find Adam Sandler's home to be too small?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;Adam_Sandler"}
{"question": "could the first European visitor to Guam been friends with Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Europe;Guam;Queen_Victoria"}
{"question": "could someone have arrived at Wrestlemania X in a Toyota Prius?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "WrestleMania_X;Toyota_Prius"}
{"question": "would Republic of Korea Navy dominate Eritrea navy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Republic_of_Korea_Navy;Eritrea"}
{"question": "could a Gladiator's weapon crush a diamond?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Second_Amendment_to_the_United_States_Constitution"}
{"question": "can Lamborghini's fastest model win a race against a Porsche 911?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lamborghini;Porsche_911"}
{"question": "does a mongoose have natural camouflage for desert?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "if someone is a vegan, would they eat honey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Veganism"}
{"question": "does Jack Sparrow know any sea shantys?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Jack_Sparrow"}
{"question": "is Thanksgiving sometimes considered a day of mourning?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "were there fifty English kings throughout the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kingdom_of_England;Middle_Ages"}
{"question": "can rowing competitions take place indoors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Sartre write a play about Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Jean-Paul_Sartre"}
{"question": "was Martin Luther same sect as Martin Luther King Jr.?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Martin_Luther;Martin_Luther_King_Jr."}
{"question": "could Amazon afford The Mona Lisa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Amazon.com;Mona_Lisa"}
{"question": "can you find a railroad engineer on TNT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Franz Ferdinand have survived with armadillo armor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Archduke_Franz_Ferdinand_of_Austria"}
{"question": "would keelhauling be a fair punishment under the Eighth Amendment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eighth_Amendment_to_the_United_States_Constitution"}
{"question": "is Bern located east of Paris?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Bern;Paris"}
{"question": "can Herpes simplex virus spread on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Herpes_simplex_virus;Venus"}
{"question": "is Mixed martial arts totally original from Roman Colosseum games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mixed_martial_arts;Colosseum"}
{"question": "are those incapable of reproduction incapable of parenthood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Amy Winehouse's death have been prevented with Narcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Amy_Winehouse"}
{"question": "was Gandalf present at the death of Eomer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gandalf;\u00c9omer"}
{"question": "does coding rely on Boolean algebra characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Boolean_algebra"}
{"question": "will Queen Elizabeth be buried in the Pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elizabeth_II"}
{"question": "can fish get Tonsillitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tonsillitis"}
{"question": "would Sophist's have hypothetically made good lawyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Sophist"}
{"question": "would a pescatarian be unable to eat anchovy pizza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Pescetarianism;Anchovy"}
{"question": "could a chipmunk fit 100 chocolate chips in his mouth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does The Hague border multiple bodies of water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "The_Hague"}
{"question": "is Rick and Morty considered an anime?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could a llama birth twice during War in Vietnam (1945-46)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Vietnam_War"}
{"question": "do some religions look forward to armageddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to a war victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eiffel_Tower"}
{"question": "was the Euro used in Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Euro;Prussia"}
{"question": "would someone with back pain enjoy picking strawberries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is Olympia, Washington part of \"Ish river country\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Olympia,_Washington;Washington_(U.S._state)"}
{"question": "was Rumi's work serialized in a magazine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Rumi"}
{"question": "was Kane (wrestler) banned from WCW  headquarters city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kane_(wrestler);World_Championship_Wrestling"}
{"question": "can you only see hippopotamus in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Africa"}
{"question": "would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can giant pandas sell out a Metallica show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Metallica"}
{"question": "would Janet Jackson avoid a dish with ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Janet_Jackson"}
{"question": "was Charlemagne's father instrumental in outcome of the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Charlemagne;Battle_of_Tours"}
{"question": "did Doctor Strange creators also make Batman?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Doctor_Strange;Batman"}
{"question": "was ethanol beneficial to Jack Kerouac's health?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Jack_Kerouac"}
{"question": "do some home remedies result in your skin color turning blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is eleventh grade required to get a driver's licence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are moose used for work near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do onions have a form that resembles the inside of a tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does handedness determine how you use American Sign Language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "American_Sign_Language"}
{"question": "did Harry Houdini's wife make psychics look foolish?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Harry_Houdini"}
{"question": "did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Evander_Holyfield"}
{"question": "did King James I despise fairy beings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "James_VI_and_I"}
{"question": "could the main character of \"Alice's Adventures in Wonderland\" join a Masonic Lodge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Alice's_Adventures_in_Wonderland;Masonic_lodge"}
{"question": "does a Generation Y member satisfy NYPD police officer age requirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Millennials;New_York_City_Police_Department"}
{"question": "is polyamory allowed in the Catholic Church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Catholic_Church"}
{"question": "would an anxious person benefit from receiving courage from the Wizard of Oz?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Wizard_of_Oz_(character)"}
{"question": "do Christians anticipate an existence in Sheol after death?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sheol"}
{"question": "did Christina Aguilera turn her chair around for Kelly Clarkson on The Voice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Christina_Aguilera;Kelly_Clarkson;The_Voice_(U.S._TV_series)"}
{"question": "do Jews believe in any New Testament angels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Jews;New_Testament"}
{"question": "has cannabis been a big influence in rap music genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hip_hop_music"}
{"question": "does Bombyx mori have a monopoly over silk production?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "will Chuck Norris be a nonagenarian by time next leap year after 2020 happens?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chuck_Norris"}
{"question": "does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "1976_Summer_Olympics"}
{"question": "are most books written as a Haiku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Haiku"}
{"question": "did the writer of Christmas carol fast during Ramadan? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ramadan"}
{"question": "do you need to worry about Zika virus in Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antarctica"}
{"question": "did a Mediterranean Sea creature kill Steve Irwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mediterranean_Sea;Steve_Irwin"}
{"question": "does Linus Torvalds make money off of DirectX?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lin", "extracted_entity": "Linus_Torvalds;DirectX"}
{"question": "could someone theoretically use an armadillo as a shield?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could a Hwasong-15 missile hypothetically reach Voyager 2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Voyager_2"}
{"question": "were weather phenomena avoided when naming minor league baseball teams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Hades appear in a Disney Channel musical movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Hades;Disney_Channel"}
{"question": "would it be typical for a Rede Globo anchor to say Konnichiwa to the viewers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Rede_Globo"}
{"question": "is the foot part of the metric system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "is week old chlorine water safe to drink?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Britney Spears' breakdown attributed to bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Britney_Spears;Bipolar_disorder"}
{"question": "could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Elizabeth_I_of_England;Dido"}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bob_Marley"}
{"question": "could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "The_Atlantic"}
{"question": "were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "at a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ariana_Grande"}
{"question": "in American society, will a bachelor's degree often include a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a", "extracted_entity": "United_States"}
{"question": "is Guitar Hero Beatles inappropriate for a US third grader?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "The_Beatles;United_States"}
{"question": "can a computer be programmed entirely in Boolean algebra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Boolean_algebra"}
{"question": "would Eminem perform well at the International Mathematical Olympiad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eminem;International_Mathematical_Olympiad"}
{"question": "is the span in C-SPAN named after Alan Greenspan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "C-SPAN;Alan_Greenspan"}
{"question": "did Van Gogh suffer from a mental disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Vincent_van_Gogh"}
{"question": "does a person need to be a parent to become a grandparent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did either Kublai Khan or his grandfather practice monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kublai_Khan"}
{"question": "do frogs feel disgust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Oscar_Wilde;Eva_Green"}
{"question": "did Native American tribes teach Spaniards how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Indigenous_peoples_of_the_Americas;Spain"}
{"question": "were there under 150,000 American troops in Vietnam in 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "United_States;Vietnam"}
{"question": "do guitarist's have fingers that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could someone mistake the smell of your brussels sprouts for a fart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Golden_Gate_Bridge"}
{"question": "was Donald Trump the target of Islamophobia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Donald_Trump;Islamophobia"}
{"question": "are there winged statuettes in the home of the creator of Law & Order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "could a silverfish reach the top of the Empire State Building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Silverfish;Empire_State_Building"}
{"question": "did Eddie Murphy's father see his first stand up show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Eddie_Murphy"}
{"question": "is watching  Star Wars necessary to know who Darth Vader is?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Star_Wars;Darth_Vader"}
{"question": "did Eric Clapton have similar taste in women to one of the Beatles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Eric_Clapton;The_Beatles"}
{"question": "is Linus Torvalds' wife unable to physically defend herself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Linus_Torvalds"}
{"question": "did Clark Gable appear in any movies scored by John Williams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Clark_Gable;John_Williams"}
{"question": "can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Goofy;Bugs_Bunny;United_States;Flag_of_the_United_States"}
{"question": "does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "can Clouded leopards chase down many Pronghorn antelopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Clouded_leopard;Pronghorn"}
{"question": "has a neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Supreme_Court_of_the_United_States"}
{"question": "will Tokyo Tower be repainted only once during President Trump's first term?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Tokyo_Tower;Donald_Trump"}
{"question": "would George Fox support stoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "George_Fox;Stoning"}
{"question": "could Oscar Wilde have operated a motor vehicle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oscar_Wilde"}
{"question": "will Oasis cruise boat traverse the Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lincoln_Tunnel"}
{"question": "is there a full Neptunian orbit between the first two burials of women in the Panth\u00e9on?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Panth\u00e9on"}
{"question": "would an eleventh-grader be eligible for Medicare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Medicare_(United_States)"}
{"question": "is 3D printing able to make adenovirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "3D_printing"}
{"question": "does rock star Keith Richards play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Keith_Richards"}
{"question": "will someone die without white blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can the Powerpuff Girls form a complete tag team wrestling match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Antarctica a good location for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Antarctica;Groundhog_Day"}
{"question": "do more anchovy live in colder temperature waters than warmer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could a giant squid fit aboard the deck of the titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would a Deacon be likely to be a fan of the podcast 'God Awful Movies'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Deacon"}
{"question": "could ten gallons of seawater crush a six year old?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "would the high school class of 2010 have lived through the Presidency of Richard Nixon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Presidency_of_Richard_Nixon"}
{"question": "has the Subway restaurant franchise had any connections with child abusers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Subway_(restaurant)"}
{"question": "is the kayak a traditional boat in New Zealand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kayak;New_Zealand"}
{"question": "do manta rays live in water above the safe temperature for cold food storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Scooby Doo fit in a kangaroo pouch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Scooby-Doo_(character)"}
{"question": "would you take a photo of a Saltwater crocodile in Memphis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Saltwater_crocodile;Memphis,_Tennessee"}
{"question": "can you write a whole Haiku in a single tweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Haiku"}
{"question": "wIll Noah's Ark hypothetically sail through flooded Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Noah's_Ark;Lincoln_Tunnel"}
{"question": "would a pear sink in water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Northern fur seal make good pet for six year old?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Millard Fillmore help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Millard_Fillmore;University_of_Pittsburgh"}
{"question": "was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Creative_Commons;The_Birth_of_Venus"}
{"question": "was The Little Prince's titular character allergic to flowers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Little_Prince"}
{"question": "are you likely to find a crucifix in Karachi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Karachi"}
{"question": "would a hypothetical Yeti be towered over by Andre the Giant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Yeti;Andr\u00e9_the_Giant"}
{"question": "are any animals in Chinese calendar Chordata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chinese_calendar"}
{"question": "does Andrew Johnson's presidential number exceed Elagabalus's Emperor number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Andrew_Johnson;Elagabalus"}
{"question": "would Firefighters be included in a September 11th memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did J. P. Morgan have healthy lungs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "J._P._Morgan"}
{"question": "was the original James Bond actor born near the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "James_Bond;Washington_Monument"}
{"question": "does Ukrainian Greek Catholic Church recognize Alexander Nevsky as a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Ukrainian_Greek_Catholic_Church;Alexander_Nevsky"}
{"question": "could an ocelot subsist on a single bee hummingbird per day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ocelot"}
{"question": "could Lil Wayne legally operate a vehicle on his own at the beginning of his career?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lil_Wayne"}
{"question": "is double duty an incorrect phrase for host of Dancing With The Stars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Lil Wayne's children ride in a Chevrolet Corvette ZR1 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Lil_Wayne;Chevrolet_Corvette_(C6)"}
{"question": "is the best tasting part of the papaya in the center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did Japanese serfdom have higher status than English counterpart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Japan;England"}
{"question": "was the Louisiana Purchase made with bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Louisiana_Purchase;Bitcoin"}
{"question": "would a Pict be confused by Old English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Picts;Old_English"}
{"question": "would a week be enough time to watch every episode of Ugly Betty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ugly_Betty"}
{"question": "did any country in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Portuguese_Colonial_War"}
{"question": "could a dandelion suffer from hepatitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Hepatitis"}
{"question": "would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "RMS_Titanic;Gulf_of_Finland"}
{"question": "would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "while on a liquid diet, are there some types of soup you cannot eat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "While", "extracted_entity": null}
{"question": "is art prioritized in the US education system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "United_States"}
{"question": "is it okay to lie after taking an oath in a court of law?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "would someone in Mumbai refer to Solanum melongena as an eggplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mumbai;Eggplant"}
{"question": "is Menthol associated with Thanksgiving?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Thanksgiving_(United_States)"}
{"question": "can a strawberry get worms similar to dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the Illuminati card game still popular?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dolce_&_Gabbana;Amish"}
{"question": "if your skin was turning the color of a zombie, could it be because of nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nickel_(United_States_coin)"}
{"question": "can the Swiss Guard fill the Virginia General Assembly chairs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Swiss_Guard;Virginia_General_Assembly"}
{"question": "if you add water to rice pudding is it horchata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Horchata"}
{"question": "would an ethics professor teach a class on Cezanne?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Paul_C\u00e9zanne"}
{"question": "is the tibia necessary to win the Stanley Cup?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tibia;Stanley_Cup"}
{"question": "could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is Miami a city on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Miami;Western_United_States;West_Coast_of_the_United_States"}
{"question": "would a broadcast from Spirit make the news in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "would food made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": null}
{"question": "was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Lil_Jon;Billboard_(magazine);The_Lox"}
{"question": "could George Washington's own speeches have been recorded live to a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "George_Washington"}
{"question": "would a geographer use biochemistry in their work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Biochemistry"}
{"question": "does Disney have an ice princess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is there a warthog on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Adam_Sandler"}
{"question": "could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is Disneyland Paris the largest Disney resort?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Disneyland_Paris;The_Walt_Disney_Company"}
{"question": "was the Donatello crucifix identified in 2020 life size?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donatello"}
{"question": "would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "CT_scan"}
{"question": "would the top of Mount Fuji stick out of the Sea of Japan? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Mount_Fuji;Sea_of_Japan"}
{"question": "could the Powerpuff Girls hypothetically attend the Camden Military Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "if you were on a diet, would you have to skip lunch at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "McDonald's"}
{"question": "is Dustin Hoffman one of the B'nei Yisrael?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dustin_Hoffman"}
{"question": "does actress Leila George lack the height to be a model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is average number of peas in a pod enough commas for a billion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a psychic who admits to hot reading be trustworthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is greed the most prevalent of the Seven Deadly Sins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Seven_deadly_sins"}
{"question": "would a student of the class of 2017 have amnesia about 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "September_11_attacks"}
{"question": "does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Dragon_Ball"}
{"question": "would Nancy Pelosi publicly denounce abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Nancy_Pelosi"}
{"question": "did land owners elect their rulers in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Kingdom_of_Hungary"}
{"question": "does Biochemistry study gluons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Biochemistry;Gluon"}
{"question": "are Christmas trees dissimilar to deciduous trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "did the Space Race use relay batons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Space_Race"}
{"question": "can you buy Casio products at Petco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Casio;Petco"}
{"question": "is the language used in Saint Vincent and the Grenadines rooted in English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Saint_Vincent_and_the_Grenadines;English_language"}
{"question": "is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Boeing_737;Wonder_Woman_(2017_film)"}
{"question": "will the Albany in Georgia reach a hundred thousand occupants before the one in New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Albany,_Georgia;Georgia_(country);New_York_City"}
{"question": "do the anchors on Rede Globo speak Chinese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Rede_Globo;Chinese_language"}
{"question": "is shrimp scampi definitely free of plastic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is a pound sterling valuable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Pound_sterling"}
{"question": "would a dog respond to bell before Grey seal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Grey_seal"}
{"question": "would a Monoamine Oxidase candy bar cheer up a depressed friend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": null}
{"question": "could the members of The Police perform lawful arrests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "The_Police"}
{"question": "are more people today related to Genghis Khan than Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Genghis_Khan;Julius_Caesar"}
