{"question": "was Pi an acceptable number of children in 1980s China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Demographics_of_China\";Pi;\"Pi_(film)\";\"Zero_population_growth\""}
{"question": "does highest US Court have enough seats for every Prime Minister of the United Kingdom since 1952?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cyril_Asquith,_Baron_Asquith_of_Bishopstone\";\"Inner_Temple\";\"History_of_the_Supreme_Court_of_the_United_States\";\"Supreme_Court_of_the_United_States\""}
{"question": "does Alec Baldwin have more children than Clint Eastwood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Alec_Baldwin\";\"Alec_Baldwin\";\"Alec_Baldwin\";\"Alec_Baldwin\""}
{"question": "does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Achilles'_heel\";\"Ankle_jerk_reflex\";\"Ankle_jerk_reflex\";\"Achilles'_heel\""}
{"question": "is a cory catfish likely to eat another living fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Corydoras;\"Corydoras_julii\";\"Corydoras_julii\";Corydoras"}
{"question": "are selfies more dangerous than plague in modern times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Selfie;Selfie;\"Portrayal_of_ISIL_in_American_media\";Selfie"}
{"question": "are amoebas safe from breast cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Egg_donation\";\"Molar_pregnancy\";\"Anti-Mu\u0308llerian_hormone\";\"Neural_tube_defect\""}
{"question": "is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Captain_Citrus\";\"Captain_Citrus\";\"Yellowjacket_(Charlton_Comics)\";\"Marvel_Noir\""}
{"question": "could a hamster experience two leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Ham_(chimpanzee)\";\"Leap_second\";Half-birthday;\"Circannual_cycle\""}
{"question": "are there some countries where waiting staff need no tip?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Waiting_staff\";Gratuity;\"Waiting_room\";Hitchhiking"}
{"question": "do Shinto practitioners keep to a kosher diet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Religious_views_on_euthanasia\";\"Freedom_of_religion_in_Japan\";\"Freedom_of_religion_in_Japan\";\"Shindo\u0304_jinen-ryu\u0304\""}
{"question": "is the Yellow Pages the fastest way to find a phone number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Find_me/follow_me\";\"Find_me/follow_me\";\"Long_number\";\"Telephone_numbers_in_Brazil\""}
{"question": "could Intel products be purchased at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"McDonald's\";IBeacon;\"McDonald's\";\"McDonald's\""}
{"question": "do you have to pass through circle of lust to find Saladin in Dante's Inferno?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Purgatorio;\"Dante's_Inferno_(video_game)\";\"Dante's_Inferno_(video_game)\";\"Dante's_Inferno_(video_game)\""}
{"question": "does Woody Allen await the Second Coming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Second_Coming\";\"Second_Coming\";\"Second_Coming\";\"Second_Coming\""}
{"question": "do you need a large room if you want to get into 3D printing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"3D_printing_processes\";Stereolithography;Stereolithography;\"3D_printing\""}
{"question": "was Emperor Commodus paid tribute in Pound sterling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Commodus;Decury;\"Double_sestertius\";Commodus"}
{"question": "can French Defence initial move defend against four move checkmate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"First-move_advantage_in_chess\";\"Checkmate_pattern\";\"Deschapelles_coup\";\"French_Defence\""}
{"question": "is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Green_River_(soft_drink)\";Green;\"Non-alcoholic_fatty_liver_disease\";\"Nobody_Likes_Onions\""}
{"question": "do Armenians tend to dislike System of a Down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "System", "extracted_entity": "\"System_of_a_Down\";\"Racism_in_Russia\";\"System_of_a_Down\";\"System_of_a_Down_discography\""}
{"question": "did Ivan the Terrible's father and grandfather have nicer nicknames?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ivan_the_Terrible\";\"Tsardom_of_Russia\";\"Ivan_the_Terrible\";\"Ivan_the_Terrible_in_Russian_folklore\""}
{"question": "did the confederate states speak Old English before the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Older_Southern_American_English\";\"Older_Southern_American_English\";\"History_of_the_Poles_in_the_United_States\";\"Powhatan_language\""}
{"question": "do Republicans reject all forms of welfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Republicans", "extracted_entity": "\"Republican_Party_(United_States)\";\"Republican_Party_(United_States)\";\"Republican_Party_(United_States)\";\"Republican_Party_(United_States)\""}
{"question": "are brown rock fish found in the waters surrounding Saint Kitts and Nevis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Brown_rockfish\";\"Brown_rockfish\";\"Brown_rockfish\";\"Cape_Cod\""}
{"question": "is a fever cured by listening to a cowbell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Gertrude_Lawrence\";\"Nellie_the_Elephant\";\"The_Mosquito\";\"Childhood_immunizations_in_the_United_States\""}
{"question": "do all parts of the aloe vera plant taste good?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Aloe_vera\";\"Aloe_vera\";Aloe;\"Aloe_vera\""}
{"question": "after viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"You_Enjoy_Myself\";\"Mount_St._Peter_Church\";\"Whoa_Nellie_Deli\";\"Church_of_Bethphage\""}
{"question": "did Mike Tyson do something very different than McGruff's slogan to Evander Holyfield in 1997?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Michael_Ramirez\";\"Sound_bite\";\"Blaster_(computer_worm)\";\"Personalized_marketing\""}
{"question": "can you hide a basketball in a sand cat's ear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Sand_cat\";Rattlesnake;\"The_Grinch_Grinches_the_Cat_in_the_Hat\";\"Psychedelic_frogfish\""}
{"question": "could largest asteroid crush a whole city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Asteroid_(film)\";\"Crush,_Crumble_and_Chomp!\";\"Asteroid_laser_ablation\";\"Crush,_Crumble_and_Chomp!\""}
{"question": "do Chinese Americans face discrimination at a Federal level in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Stereotypes_of_East_Asians_in_the_United_States\";\"Stereotypes_of_East_Asians_in_the_United_States\";\"Chinese_for_Affirmative_Action\";\"Racism_in_the_United_States\""}
{"question": "is Kobe's famous animal product used in a BLT?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kobe_beef\";Kobe;\"Kobe_beef\";Yakult"}
{"question": "is narcissism's origin a rare place to get modern words from?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Greek_to_me\";Memetics;Naupactia;Hypercorrection"}
{"question": "is it unusual to eat spaghetti without a fork?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sp", "extracted_entity": "Vermicelli;Vermicelli;\"Spaghetti_with_meatballs\";\"Spaghetti_with_meatballs\""}
{"question": "can you drown in a Swan Lake performance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Swan_Lake_(1895)\";\"Sing_If_You_Can\";\"Band_in_a_Bubble\";\"Swan_Lake\""}
{"question": "are Christmas trees typically deciduous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Christmas_tree\";\"Christmas_tree\";\"Christmas_tree_cultivation\";\"Christmas_tree\""}
{"question": "can a traffic collision make someone a millionaire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Charles_Ingram\";\"Who_Wants_to_Be_a_Millionaire?_(UK_game_show)\";\"Who_Wants_to_Be_a_Millionaire?\";\"Charles_Ingram\""}
{"question": "is winter solstice in Northern Hemisphere closer to July than in Southern Hemisphere? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Winter_solstice\";\"Christmas_in_July\";\"Winter_solstice\";\"Summer_solstice\""}
{"question": "can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Visitors_(opera)\";\"Othello_Castle\";\"Medici:_Masters_of_Florence\";Superfantozzi"}
{"question": "have Douglas fir been used to fight wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"New_Zealand_Wars\";\"Pinus_brutia\";\"Arauco_War\";\"Korean_fighting_fan\""}
{"question": "are common carp sensitive to their environments?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Carp_fishing\";\"Common_carp\";\"Coldwater_fish\";Carp"}
{"question": "would John the Baptist be invited to a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spamalot;Excarnation;\"Fe\u0301lix_Granda\";Rapture"}
{"question": "are there bones in an anchovy pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ossobuco;\"I_Survived_BTK\";\"Vanitas:_Flesh_Dress_for_an_Albino_Anorectic\";\"I_Survived_BTK\""}
{"question": "did Jerry Seinfeld have reason to cheer in 1986?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Run_it_up_the_flagpole\";\"Good_Times\";\"The_Fly_(1986_film)\";\"The_Red_Buttons_Show\""}
{"question": "is a platypus immune from cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chickenpox;\"Varicella_vaccine\";Vaccination;\"Complement_system\""}
{"question": "is Europa linked to Viennese waltzes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Viennese_waltz\";\"Viennese_waltz\";\"Viennese_waltz\";\"Medieval_dance\""}
{"question": "is the tibia required for floor exercises?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tibia_Clausa\";\"Stiletto_heel\";Tibia;\"Floor_(gymnastics)\""}
{"question": "was The Great Gatsby inspired by the novel 1984?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Great_Gatsby\";\"The_Great_Gatsby_(2013_film)\";\"The_Great_Gatsby\";\"The_Great_Gatsby\""}
{"question": "does a lapidary work with items that are studied by geologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Petrography;Petrography;\"Remote_sensing_(geology)\";\"Ceramic_petrography\""}
{"question": "does Santa Claus hypothetically give Joffrey Baratheon presents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Father_Christmas\";\"Christmas_Who?\";\"A_Visit_from_St._Nicholas\";\"Christmas_elf\""}
{"question": "would a Jehovah's witness approve of Alice's Adventures in Wonderland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Jehovah's_Witnesses\";\"Jehovah's_Witnesses\";\"Alice_in_Wonderland_and_Through_the_Looking-Glass_(2001_Adrian_Mitchell_stage_adaptation)\";\"Jehovah's_Witnesses\""}
{"question": "can you chew argon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Sodium_hyaluronate\";Argon;\"The_Eye_of_Argon\";Argon"}
{"question": "is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Polybius_(urban_legend)\";\"Polybius_(urban_legend)\";\"Atari_video_game_burial\";\"Polybius_(urban_legend)\""}
{"question": "does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Streamlink;\"Coast_to_Coast_AM\";\"Coast_to_Coast_AM\";\"Coast_to_Coast_AM\""}
{"question": "is the tongue part of a creature's head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Fish_head\";Tongue;\"Papilla_(fish_anatomy)\";Head"}
{"question": "is the largest city in New Mexico also known as Yoot\u00f3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Stack\";\"Ahwatukee,_Phoenix\";\"Yoot_Tower\";\"Gilbert,_Arizona\""}
{"question": "is most coffee produced South of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "South", "extracted_entity": "\"Coffee_production_in_Ivory_Coast\";Coffee;\"Coffee_production_in_Ivory_Coast\";Coffee"}
{"question": "did Amy Winehouse always perform live perfectly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Live_It_Up_(Jennifer_Lopez_song)\";\"Alive_Tour\";\"The_Glamorous_Life\";\"Liza's_at_The_Palace....\""}
{"question": "does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Family_of_Barack_Obama\";\"Nyang'oma_Kogelo\";\"Family_of_Barack_Obama\";\"Barack_Obama_citizenship_conspiracy_theories\""}
{"question": "do American wheelchair users know what the ADA is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Americans_with_Disabilities_Act_of_1990\";\"ADA_Signs\";\"Americans_with_Disabilities_Act_of_1990\";\"Suffrage_for_Americans_with_Disabilities\""}
{"question": "can brewing occur in a prison environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fermentation_lock\";Brewing;\"Fermentation_in_winemaking\";Brewing"}
{"question": "will Ronda Rousey hypothetically defeat X-Men's Colossus in a fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "R", "extracted_entity": "\"Ronda_Rousey\";\"Ronda_Rousey\";\"Ronda_Rousey\";\"Ronda_Rousey\""}
{"question": "can a believer in agnosticism become pope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Antipope;\"Papal_conclave\";Conclavism;\"Nolo_episcopari\""}
{"question": "can French Toast hypothetically kill a Lannister?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Scotch_woodcock\";Ergot;\"French_toast\";\"Sake_bomb\""}
{"question": "did Moon Jae-in earn the Abitur as a teenager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Moon_Jae-in\";\"Moon_Jae-in\";\"Song_Yoo-geun\";\"Moon_Jae-in\""}
{"question": "would a kindergarten teacher make a lesson of the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Catechism;Catechesis;Catechesis;\"Magdalena_Heymair\""}
{"question": "are you likely to hear Rammstein playing in smooth jazz clubs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bill_Evans\";\"You_Can_Feel_Me\";\"Robert_Fripp\";\"Life_Flight_(album)\""}
{"question": "did Electronic Arts profit from Metroid sales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Digital_rights_management\";\"Electronic_Arts\";\"Digital_rights_management\";Cilvaringz"}
{"question": "is Earth Day celebrated in summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Earth_Day\";\"Earth_Day\";\"Earth_Day\";\"Earth_Day\""}
{"question": "is viscosity unimportant in making jello shots?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stencil_printing\";Laparoscopy;Appendectomy;\"Loa_loa_filariasis\""}
{"question": "is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Exploding_whale\";\"Exploding_whale\";\"Exploding_whale\";\"Shotgun_shell\""}
{"question": "is waltz less injurious than slam dance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cross-step_waltz\";\"Artistic_roller_skating\";\"Cross-step_waltz\";\"Vivat_Russia!\""}
{"question": "are LinkedIn and LeafedIn related companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Leaf_Group\";\"Mike_Linksvayer\";\"LinkedIn_Learning\";\"Leaf_Group\""}
{"question": "can a snow leopard swim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Winter_swimming\";\"Common_basilisk\";\"Ram_Barkai\";\"Una_National_Park\""}
{"question": "can citrus grow in Ulaanbaatar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Ulaanbaatar;Ulaanbaatar;\"New_Ulaanbaatar_International_Airport\";\"Ulaanbaatar_Metro\""}
{"question": "will a Holstein cow and the Liberty Bell balance out a giant scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Salem_Sue\";\"Elsie_the_Cow\";\"Salem_Sue\";\"Don't_Have_a_Cow_(That's_So_Raven)\""}
{"question": "is the bull shark more bull than shark?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Bull_shark\";\"Bull_shark\";\"Bull_shark\";\"Bull_shark\""}
{"question": "is World of Warcraft heavier than a loaf of bread?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Wulfgar_(Forgotten_Realms)\";\"Batman_(unit)\";\"Avoirdupois_system\";Warwolf"}
{"question": "did Thomas Greenhill's parents violate the concept of monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Thomas_Cranmer\";\"Sir_Thomas_Green\";\"Thomas_M._Green_Sr.\";\"Thomas_Jefferson_Hogg\""}
{"question": "would John Muir not likely have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"James_Tyler_Kent\";\"Pope_John_Paul_II\";\"Duncan_McDuffie\";\"Frank_and_John_Craighead\""}
{"question": "did the original lead guitarist of Metallica fail after parting from the band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Craig_Gruber\";\"Ralf_Hu\u0308tter\";\"Cliff_Burton\";Echobrain"}
{"question": "has a baby ever had a moustache?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"La_Moustache\";\"La_Moustache\";\"Mark_Spitz\";Moustache"}
{"question": "would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Beerfest;\"Karl_Stefan\";\"Gru\u0308nkohlessen\";\"Shredd_and_Ragan\""}
{"question": "do solo pianists require a conductor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Conducting;\"Conductorless_orchestra\";\"Baton_(conducting)\";\"Reduction_(music)\""}
{"question": "is Kim Kardashian a guru?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kim", "extracted_entity": "\"Kino_MacGregor\";\"Kino_MacGregor\";\"The_Guru_(2002_film)\";\"Cameron_Alborzian\""}
{"question": "were karaoke and the turtle power tiller patented in the same country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Karaoke;\"AC_power_plugs_and_sockets:_British_and_related_types\";\"Turtle_Power!\";\"AC_power_plugs_and_sockets\""}
{"question": "would a caracal be defeated by Javier Sotomayor in a high jump competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Javier_Sotomayor\";\"Javier_Sotomayor\";\"Spanish-style_bullfighting\";\"Javier_Sotomayor\""}
{"question": "do Jehovah's Witnesses celebrate day before New Year's Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Jehovah's_Witnesses_practices\";\"Jehovah's_Witnesses_practices\";\"Jehovah's_Witnesses\";Holiday"}
{"question": "do moths that live on sloths have family dinners?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Sloth_moth\";\"Arthropods_associated_with_sloths\";\"Sloth_moth\";\"Mauritian_tomb_bat\""}
{"question": "can a greyhound walk on two legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Greyhound;\"Animal_locomotion\";BigDog;Bipedalism"}
{"question": "was Moon Jae-in born outside of Khanbaliq?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Moon", "extracted_entity": "\"Khorloogiin_Choibalsan\";Babur;\"Kwangmyo\u0306ngso\u0306ng-1\";\"Ulugh_Beg\""}
{"question": "were paparazzi involved in the death of a member of the royal family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Isabella_de'_Medici\";\"Orsini_affair\";\"Alexander_I_of_Serbia\";\"The_Matarese_Circle\""}
{"question": "can native wolverines be found in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Miami_blue\";\"Oleta_River_State_Park\";\"Arch_Creek,_Florida\";\"Sabal_miamiensis\""}
{"question": "did Al Unser Jr. win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Scott_Carpenter\";\"John_Glenn\";\"Space_Race\";\"Wernher_von_Braun\""}
{"question": "do the Ubuntu people speak Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ubuntu_philosophy\";\"Ubuntu_philosophy\";\"Ubuntu_philosophy\";\"Pajuba\u0301\""}
{"question": "does Rupert Murdoch's alma mater have more history than the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"University_of_British_Columbia\";\"University_of_British_Columbia\";\"University_of_British_Columbia\";\"University_of_British_Columbia\""}
{"question": "are there Pink music videos that are triggering for eating disorder patients?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"Turn_Down_for_What\";Oculolinctus;\"Hot_flash\";\"Hot_flash\""}
{"question": "were gladiators associated with the Coloseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gladiator;Gladiator;Gladiator;Bustuarius"}
{"question": "could Larry King's marriages be counted on two feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Larry_King_(tennis)\";\"Two_Mothers_for_Zachary\";\"One_in_a_Million_(Ne-Yo_song)\";\"Larry_King\""}
{"question": "do mountain goats inhabit the summit of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Goat_Rocks\";\"Mountain_goat\";\"Goat_Rocks_Wilderness\";\"Mount_Meager_massif\""}
{"question": "will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Lil_Wayne\";\"Lil_Wayne\";\"Lil_Wayne\";\"Lil_Wayne_albums_discography\""}
{"question": "is Autumn a good time to collect bear pelts in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bear_hunting\";Antler;\"Polar_bear\";\"Groundhog_Day\""}
{"question": "did Wednesday have something to do with Thor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Wednesday;\"A_Wednesday!\";\"A_Wednesday!\";Wednesday"}
{"question": "did Dale Jr hug his dad after their last Daytona 500 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Dale_Houston\";\"Ken_Squier\";\"Lou_Brissie\";\"Angry_Grandpa\""}
{"question": "can an ostrich fit into the nest of a swallow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Cave_swallow\";\"Small_tortoiseshell\";\"Lord_Howe_gerygone\";\"Common_ostrich\""}
{"question": "would an Olympic athlete be tired out after running a mile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Long-distance_running\";\"Alberto_Salazar\";\"Long-distance_running\";\"Claude_Bracey\""}
{"question": "would the current president of Ohio University hypothetically wear a jockstrap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ohio_State_University_attack\";\"Donald_Trump_Access_Hollywood_tape\";\"David_S._Pumpkins\";\"Jocko_Homo\""}
{"question": "would ramen be bad for someone with heart failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ramen", "extracted_entity": "Hypernatremia;\"Bernard_Tomlinson\";\"Heart_arrhythmia\";\"Physicians_Committee_for_Responsible_Medicine\""}
{"question": "was Christina Aguilera born in the forgotten borough?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Giovanna_Negretti\";\"Pene\u0301lope_Cruz\";\"Grace_Aguilar\";\"Carla_Carli_Mazzucato\""}
{"question": "will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Donald", "extracted_entity": "\"DuckTales_the_Movie:_Treasure_of_the_Lost_Lamp\";\"Donald_Duck_universe\";\"Donald's_Ostrich\";\"Donald_Duck_universe\""}
{"question": "were the first missionaries required to attend mass on Sundays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_early_Christianity\";\"Susenyos_I\";\"Fast_Sunday\";\"Early_Christianity\""}
{"question": "is an ocelot a good present for a kindergartener?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"El_cant_dels_ocells\";Ocelot;Peek-A-Poo;\"El_Jardi\u0301_dels_Ocells\""}
{"question": "did villain that killed Superman murder Robin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Superman/Batman;\"Batman_vs._Robin\";\"Robin_(character)\";\"Batman_vs._Robin\""}
{"question": "would a Germaphobia be able to participate in Judo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Hifumi_Abe\";\"Kenshiro_Abbe\";\"Rena_Kanokogi\";\"Hiroshi_Nagao\""}
{"question": "does  Lionel Richie believe in holistic medicine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Andrew_Weil\";\"Lionel_Blue\";\"Andrew_Weil\";\"Do_You_Believe_in_Magic?_(book)\""}
{"question": "is the Fibonacci number sequence longer than every number discovered in Pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Fibonacci_number\";\"Generalizations_of_Fibonacci_numbers\";\"Fibonacci_numbers_in_popular_culture\";\"Integer_sequence\""}
{"question": "was The Canterbury Tales written before One Thousand and One Nights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Canterbury_Tales\";\"The_Canterbury_Tales\";\"The_Canterbury_Tales\";\"Order_of_The_Canterbury_Tales\""}
{"question": "did John Kerry run in the 2010 United Kingdom general election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"John_Bercow\";\"2010_Labour_Party_(UK)_leadership_election\";\"David_Davis_by-election_campaign,_2008\";\"2008_Haltemprice_and_Howden_by-election\""}
{"question": "does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"American_cuisine\";\"Terry_Shintani\";\"Customs_and_etiquette_in_Hawaii\";\"Terry_Shintani\""}
{"question": "did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Radio_in_1920s_elections\";\"Radio_in_1920s_elections\";\"Campaign_button\";\"United_States_presidential_debates\""}
{"question": "did original Nintendo have games in same format as Playstation 3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Nintendo_Selects\";\"History_of_Nintendo\";\"Video_gaming_in_Japan\";Nintendo"}
{"question": "would you be more likely to die of hypothermia in New York than Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"2018_New_York_City_helicopter_crash\";\"Maybe_You've_Been_Brainwashed_Too\";\"2018_New_York_City_helicopter_crash\";\"2018_New_York_City_helicopter_crash\""}
{"question": "was Moliere Queen Margot's ill fated lover?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "M", "extracted_entity": "\"Sarah_Bernhardt\";Lillet;\"Rome\u0301o_et_Juliette\";\"Charles_IX_of_France\""}
{"question": "can parachuting amateurs ignore hurricane force winds bulletins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Amateur_radio_emergency_communications\";\"Scud_running\";\"Flash_flood_watch\";\"Hazards_of_outdoor_recreation\""}
{"question": "can a ten-pin bowling pin be a deadly weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Ten-pin_bowling\";\"Ten-pin_bowling\";\"Ten-pin_bowling\";\"Bowling_pin\""}
{"question": "is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Jennifer_Lawrence\";\"Florence_of_Arabia\";\"Lawrence_Durrell\";\"Laura_Albert\""}
{"question": "would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Anorexia_nervosa\";\"Anorexia_nervosa\";\"Anorexia_nervosa\";\"Bulimia_nervosa\""}
{"question": "is cycling a high-risk activity for pelvis fractures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cyclosportive;Cycling;\"Mechanical_doping\";Cycling"}
{"question": "would Dale Earnhardt Jr. be considered a newbie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Dale_Earnhardt_Jr.\";\"Dale_Earnhardt\";\"Dale_Earnhardt_Jr.\";\"Rick_Pe\u0301we\u0301\""}
{"question": "does Final Fantasy VI require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Final_Fantasy_VI\";\"Ultima_VII:_The_Black_Gate\";\"Final_Fantasy_IV\";\"No_load_power\""}
{"question": "did number of Imams Reza Shah believed in exceed number of Jesus's disciples?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Persecution_of_Zoroastrians\";Manichaeism;\"Martyrdom_in_Iran\";Zoroastrianism"}
{"question": "did Terry Pratchett write about quantum mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Terry_Pratchett\";Rincewind;\"Terry_Pratchett\";\"Terry_Pratchett\""}
{"question": "is it hard for Rahul Dravid to order food at a restaurant in Aurangabad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tandoori_chicken\";\"Culture_of_Dhaka\";\"Dhanmondi_Thana\";\"Islam_in_England\""}
{"question": "is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Frugal_innovation\";\"2012_Indian_presidential_election\";\"Freedom_251\";\"2014_Indian_general_election\""}
{"question": "are swastikas used in the most common religion in India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hindu", "extracted_entity": "Swastika;\"Hindu_iconography\";\"Christianity_in_India\";Sauwastika"}
{"question": "will electric car struggle to finish Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"National_Electric_Drag_Racing_Association\";\"Electric_drag_racing\";\"Electric_drag_racing\";\"Electric_drag_racing\""}
{"question": "does walking across Amazonas put a person's life at risk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Walk_of_the_Amazon_Heroes\";\"Ciclovi\u0301a\";\"Trans-Amazonian_Highway\";\"Walking_the_Amazon\""}
{"question": "is Olivia Newton-John hyphenated celebrity name with most letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Olivia_Newton-John_videography\";\"Physical_(Olivia_Newton-John_song)\";\"Olivia_Newton-John\";\"Olivia_Newton-John\""}
{"question": "is Godzilla's image likely grounds for a lawsuit in 2050?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Godzilla", "extracted_entity": "Godzilla;Godzilla;\"Godzilla_in_popular_culture\";\"Godzilla_in_popular_culture\""}
{"question": "does ontology require a scalpel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Notalgia_paresthetica\";Radiology;\"Lithotomy_position\";\"Chest_radiograph\""}
{"question": "will silicon wedding rings outsell bromine wedding rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "brom", "extracted_entity": "\"Engagement_ring\";Moissanite;\"Rings_(2017_film)\";\"Engagement_ring\""}
{"question": "is Cape Town south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Port_Elizabeth\";\"Cullinan,_Gauteng\";\"Journeys_In_Africa\";\"Cape_Town\""}
{"question": "would the number 666 appear in a church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"666_(number)\";Semantron;\"Temple_of_Set\";Pyx"}
{"question": "can a single honey bee sting multiple humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bee_sting\";\"Vespula_vulgaris\";\"Bombus_ternarius\";\"Bee_sting\""}
{"question": "would someone pay for a coffee in NYC with Euros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Caffe\u0300_sospeso\";\"Caffe\u0300_sospeso\";\"Coffee_vending_machine\";\"Caffe\u0300_sospeso\""}
{"question": "could a student at the University of Houston see a caracal on campus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Chicago_Varnish_Company_Building\";\"Mount_Holyoke_College\";\"Purple_Cow\";\"Abert's_towhee\""}
{"question": "do sun bears stay active during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Polar_bear\";\"Big_Bear_Lake\";Bear;\"Polar_bear\""}
{"question": "is Christmas celebrated during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christmas", "extracted_entity": "\"Christmas_traditions\";\"Christmas_and_holiday_season\";\"Twelve_Days_of_Christmas\";\"Christmas_traditions\""}
{"question": "could the endowment of Johns Hopkins University pay off the MBTA debt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hodson_Trust_Scholarship\";\"Harvard_University\";\"Harvard_University_endowment\";\"Boston_University\""}
{"question": "was song of Roland protagonist friendly with group that had sagas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Song_of_Roland\";\"Looking_for_Group\";\"Song_Quest\";\"The_Song_of_Roland\""}
{"question": "did Dr. Seuss make himself famous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dr", "extracted_entity": "\"Dr._Seuss\";\"Edison's_Conquest_of_Mars\";\"Why_Man_Creates\";\"Dr._Seuss\""}
{"question": "can a goat be used for one of the ingredients in French toast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Goat_meat\";\"Goat_meat\";\"Goat_farming\";Goat"}
{"question": "can you measure a Caracal with a protractor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Vinegar_test\";\"Rapid_strep_test\";\"Gravimetric_analysis\";\"Cold_pressor_test\""}
{"question": "for bone growth, is kale more beneficial than spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Kale;Karuka;Kabocha;Muskmelon"}
{"question": "would only warm weather attire be a good idea on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Diogenes_and_Alexander\";\"Mercury_in_fiction\";Phocion;\"Mercury_in_fiction\""}
{"question": "do Do It Yourself channels online always show realistic projects?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Video_self-modeling\";Scientastic!;\"Do_It_(TV_series)\";\"Do_It_(TV_series)\""}
{"question": "could Bart Simpson have owned comics with The Joker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Alice_Cooper\";\"The_Day_the_Violence_Died\";\"The_Simpsons:_Bart_&_the_Beanstalk\";\"Three_Men_and_a_Comic_Book\""}
{"question": "was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Twenty-second_Amendment_to_the_United_States_Constitution\";\"Twenty-second_Amendment_to_the_United_States_Constitution\";\"Twenty-second_Amendment_to_the_United_States_Constitution\";\"Twenty-second_Amendment_to_the_United_States_Constitution\""}
{"question": "would a baby gray whale fit in a tractor-trailer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Weddell_seal\";\"Gray_whale\";\"Springer_(killer_whale)\";\"Humpback_whale\""}
{"question": "would Emmanuel Macron celebrate Cinco de Mayo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cinco_de_Mayo\";\"Cinco_de_Mayo\";\"Cinco_de_Mayo\";\"Cinco_de_Mayo\""}
{"question": "does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Transit_of_Venus_(album)\";\"Solar_eclipse_of_July_10,_1972\";\"November_2012_lunar_eclipse\";\"Doomsday_rule\""}
{"question": "did Metallica band members cutting their hair hurt their sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Cut_Your_Hair\";\"Can't_Stop_the_Music\";\"Fair_use\";\"Hedwig_and_the_Angry_Inch_(musical)\""}
{"question": "could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Trocadero,_San_Francisco\";\"La_Caban\u0303a\";\"Pentagon_Barracks\";\"Roses,_Girona\""}
{"question": "would a veteran of the Phillippine-American War come home craving SPAM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Palmetto_Leaves\";\"Thank_You_for_Smoking\";\"Hacksaw_Ridge\";\"Hacksaw_Ridge\""}
{"question": "were Walkman's used in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Goose_step\";\"Postage_stamps_and_postal_history_of_Hungary\";\"Recorder_(musical_instrument)\";\"Kingdom_of_Hungary\""}
{"question": "will Communion be denied to Wednesday name origin followers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"North\u2013South_divide_(England)\";\"Holy_Wednesday\";\"Nolo_episcopari\";\"Ash_Wednesday\""}
{"question": "would someone with a nosebleed benefit from Coca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Self-licensing;\"Chewing_tobacco\";Cocaine;\"Prenatal_cocaine_exposure\""}
{"question": "did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Bill_Eyden\";\"Paul_Gonsalves\";\"Frank_Butler_(musician)\";\"The_Winkies\""}
{"question": "can Darth Vader hypothetically outdunk Bill Walton without using The Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Norman_Osborn\";\"Vanquish_(video_game)\";\"Zaheer_(The_Legend_of_Korra)\";\"Sea_Dart\""}
{"question": "does Oprah Winfrey have a degree from an Ivy League university?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Iris_Mack\";\"Oprah_Winfrey\";\"Tyra_Banks\";\"Grace_Adler\""}
{"question": "can you watch Rick and Morty in Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Justin_Roiland\";\"Marianas_Trench_(band)\";\"Marianas_Trench_(band)\";\"Marianas_Trench_(band)\""}
{"question": "did Elizabeth II frequently visit Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ueen", "extracted_entity": "\"Queen_Elizabeth_II_Wildlands_Provincial_Park\";\"Queen_Elizabeth_II_Wildlands_Provincial_Park\";\"Queen_Elizabeth_Provincial_Park\";\"Queen_Elizabeth_Provincial_Park\""}
{"question": "can a sesame seed grow in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sesame;Sesame;Sesame;Sesame"}
{"question": "did the Watergate scandal help the Republican party?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Watergate_scandal\";\"Watergate_scandal\";\"Republican_Party_(United_States)\";\"Watergate_scandal\""}
{"question": "would Bandy be likely to become popular in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Texas_country_music\";\"Cowboy_polo\";\"If_You're_Gonna_Play_in_Texas_(You_Gotta_Have_a_Fiddle_in_the_Band)\";\"Texas_country_music\""}
{"question": "do inanimate objects come alive in Beauty and the Beast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Helen_Chadwick\";\"Human_uses_of_living_things\";\"Insects_in_mythology\";\"Alien_(creature_in_Alien_franchise)\""}
{"question": "does Home Depot sell item in late September zodiac sign symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Generic_brand\";\"Tobacco_packaging_warning_messages\";\"Paranoid_(Ty_Dolla_Sign_song)\";Playtex"}
{"question": "does monster name in West African Folklore that witches send into villages set Scrabble record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Witch-hunt;\"Dartmoor_kistvaens\";Witch-hunt;Witch-hunt"}
{"question": "can an American black bear swallow a sun bear whole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bear_attack\";\"The_Biggest_Bear\";\"Big_Bear_Lake\";Bear"}
{"question": "can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mavic_(UAV)\";\"Mavic_(UAV)\";\"Mavic_(UAV)\";\"Mavic_(UAV)\""}
{"question": "would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tenzing_Peak\";\"Tenzing_Peak\";\"Tashi_Tenzing\";\"Jamling_Tenzing_Norgay\""}
{"question": "can preventive healthcare reduce STI transmission?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Preventive_healthcare\";\"Safe_sex\";\"HIV_vaccine\";\"Preventive_healthcare\""}
{"question": "would 1996 leap year baby technically be 1 year old in 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Child_Citizenship_Act_of_2000\";\"Pregnancy_over_age_50\";2000;2000"}
{"question": "would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"High_Speed_1\";\"High_Speed_1\";\"High_Speed_2\";\"High_Speed_2\""}
{"question": "would a Superbowl Football Game be crowded on the Gettysburg Battlefield?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Battle_of_Gettysburg\";\"Veterans_Stadium\";\"Memorial_Stadium_(Baltimore)\";\"Battle_of_Gettysburg\""}
{"question": "can you purchase a dish with injera at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Enchirito;Enchirito;\"In-N-Out_Burger\";Delicatessen"}
{"question": "has Drew Carey outshined Doug Davidson's tenure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Doug_Davidson\";\"The_Drew_Carey_Show\";\"The_Drew_Carey_Show\";\"Drew_Carey\""}
{"question": "could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"If_I_Had_$1000000\";\"The_First_$20_Million_Is_Always_the_Hardest\";\"If_I_Had_$1000000\";\"If_I_Had_$1000000\""}
{"question": "is eggplant deadly to most atopic individuals? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chickenpox;Chickenpox;Chickenpox;Chickenpox"}
{"question": "would Cyndi Lauper use milk substitute in her rice pudding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Candle_salad\";\"Pig_blood_curd\";\"Lactose_intolerance\";Maypo"}
{"question": "can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Zucchini;QWERTY;\"Hebrew_keyboard\";\"Zanchi_(cymbals)\""}
{"question": "would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"S\u0327u\u0308kru\u0308_Saracog\u0306lu\";\"Cela\u0302l_Bayar\";\"Murder_of_Sevag_Bal\u0131kc\u0327\u0131\";\"Hasan_Cemal\""}
{"question": "are potatoes native to the European continent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Potato;\"Potato_priest\";Potato;\"Artemisia_maritima\""}
{"question": "was Iggy Pop named after his father?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Iggy_Pop\";\"Iggy_Pop\";\"Iggy_Pop\";\"P._G._Wodehouse\""}
{"question": "can a 2019 Toyota Hilux hypothetically support weight of thirty Big John Studd clones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Union_Pacific_3985\";\"Andre_the_Giant_Has_a_Posse\";\"Future_Vertical_Lift\";\"Modern_Medium_Weight_Tank\""}
{"question": "did Tom Bosley enjoy video games on the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tom_Wisdom\";\"Tom_Blofeld\";\"Tom_Fordyce\";\"Kenneth_Clarke\""}
{"question": "do German Shepherds worry about the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Abitur;Abitur;\"Amish_religious_practices\";Abitur"}
{"question": "do you find glutamic acid in a severed finger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Cicuta_maculata\";\"Pathology_of_multiple_sclerosis\";Gout;Gout"}
{"question": "can black swan's formation type help spell longest word in Dictionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "L-vocalization;\"Diaeresis_(diacritic)\";\"Phonological_history_of_Scots\";\"Chumashan_languages\""}
{"question": "would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"History_of_eugenics\";\"Alexander_Graham_Bell\";\"George_Sweigert\";\"Elisha_Gray_and_Alexander_Bell_telephone_controversy\""}
{"question": "could Stephen King join the NASA Astronaut Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Astronaut;\"Stephen_Manley\";\"The_Right_Stuff_(film)\";Astronaut-politician"}
{"question": "does the art from Family Guy look a lot like the art in American Dad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Mom_and_Pop_Art\";\"Mom_and_Pop_Art\";\"Mom_and_Pop_Art\";\"Mom_and_Pop_Art\""}
{"question": "did DARPA influence Albert Einstein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DARPA;\"Charles_M._Herzfeld\";DARPA;DARPA"}
{"question": "is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Tori_Amos\";\"Tori_Amos\";\"Liu_Sola\";\"Mame_(musical)\""}
{"question": "did Moon Jae-in's residence exist when the World Trade Center was completed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Yeouido;\"Trump_World_Tower\";\"World_Trade_Center_(1973\u20132001)\";\"James_V._Forrestal_Building\""}
{"question": "would a kaffir lime be a good ingredient for making a candle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kerosene_lamp\";\"Kerosene_lamp\";\"Kerosene_lamp\";\"Kaffir_lime\""}
{"question": "is Glenn Beck known for his mild temper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bob_Lassiter\";\"Glenn_Beck\";\"Glenn_Beck\";\"Steven_Wright\""}
{"question": "was the 1980 presidential election won by a member of the Grand Old Party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"1980_Tanzanian_general_election\";\"1980_Democratic_National_Convention\";\"1979_Kenyan_general_election\";\"1979_Malian_general_election\""}
{"question": "would Nancy Pelosi have hypothetically been on same side as Gerald Ford?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Ford_Show\";\"Ruth_Clusen\";\"John_F._Kennedy_assassination_conspiracy_theories\";\"Nancy_Hanks_Lincoln_heritage\""}
{"question": "was the MLB World Series held in Newcastle, New South Wales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Newcastle_International_Sports_Centre\";\"New_South_Wales_Major_League\";\"2000\u201301_International_Baseball_League_of_Australia_season\";\"Australian_Schools_Championship_(baseball)\""}
{"question": "will a sloth explode if it's not upside down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"\u02bbOumuamua\";\"\u02bbOumuamua\";\"Rupture_disc\";\"Orbital_blowout_fracture\""}
{"question": "could Rhode Island sink into the Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Hurricane_Ioke\";\"Spiral_Island\";\"Deep_sea\";\"Spiral_Island\""}
{"question": "would it be hard to get toilet paper if there were no loggers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Flush_toilet\";\"Wave_Hill_walk-off\";\"Narada_Falls_Comfort_Station\";Sluice"}
{"question": "are banana trees used by judges for maintaining order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"White-headed_capuchin\";\"Ra\u0304hui\";Jujube;\"Banana_pipistrelle\""}
{"question": "would four shoes be insufficient for a set of octuplets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Seven-league_boots\";\"Octet_rule\";\"Montreal_Snow_Shoe_Club\";\"The_Shoes_(Seinfeld)\""}
{"question": "did Alan Rickman have an improperly functioning organ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Rupert_E._Billingham\";\"Rupert_E._Billingham\";\"Alan_Rickman\";\"Jim_Dempster\""}
{"question": "was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Fly_(1986_film)\";\"The_Fly_(1986_film)\";\"The_Fly_(1986_film)\";\"The_Fly_(1986_film)\""}
{"question": "does the United States Navy create radioactive waste?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Smoking_in_the_United_States_military\";\"U.S._nuclear_weapons_in_Japan\";\"Smoking_in_the_United_States_military\";\"United_States_Navy_Nuclear_Propulsion\""}
{"question": "is the United States Capitol located near the White House?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"United_States_Capitol\";\"United_States_Capitol\";\"United_States_Capitol\";\"President's_Park\""}
{"question": "is winter associated with hot temperatures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Winter;\"Indian_summer\";\"Indian_summer\";\"Cold_urticaria\""}
{"question": "did Sony definitively win the video game war against Sega?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Strickland_v._Sony\";\"Sega_v._Accolade\";\"Sega_v._Accolade\";\"Sega_Saturn\""}
{"question": "could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Hotel_President_Wilson\";\"Hotel_President_Wilson\";\"Garry_Davis\";\"Headquarters_of_the_United_Nations\""}
{"question": "can you find a snow leopard in the Yucatan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Yucatan_squirrel\";\"Yucatan_yellow_bat\";\"Yucatan_yellow_bat\";\"Yucatan_wren\""}
{"question": "could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Imagine_That_(film)\";\"Three_Little_Pigskins\";\"Cisero_Murphy\";\"Little_Giants\""}
{"question": "is it common to see frost during some college commencements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Frost_Amphitheater\";\"Frost_Amphitheater\";\"Robert_Frost\";\"Hotel_Baxter\""}
{"question": "was Walt Disney ever interviewed by Anderson Cooper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"E._G._Marshall\";\"E._G._Marshall\";\"Richard_Anderson\";\"Bill_Anderson_(producer)\""}
{"question": "does a dentist treat Bluetooth problems?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Toothache;\"Electric_toothbrush\";Toothache;\"What_Is_a_Man_Without_a_Moustache?\""}
{"question": "did Richard III's father have greater longevity than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Robert_de_Crull\";\"Richard_II_of_England\";\"Edward_IV_of_England\";\"Exhumation_and_reburial_of_Richard_III_of_England\""}
{"question": "does Ariana Grande's signature style combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"History_of_fashion_design\";\"Angela_DeMontigny\";\"Kiko_Mizuhara\";\"Andre\u0301_Courre\u0300ges\""}
{"question": "did the color green help Theodor Geisel become famous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Green;\"Humphry_Davy\";\"Scheele's_Green\";\"Alfred_Jarry\""}
{"question": "does the United States Secretary of State answer the phones for the White House?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"United_States_Department_of_State\";\"United_States_Secretary_of_State\";\"United_States_Secretary_of_State\";\"United_States_Secretary_of_State\""}
{"question": "would a honey badger's dentures be different from a wolverine's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Honey_badger\";\"European_badger\";\"Honey_badger\";\"European_badger\""}
{"question": "are thetan levels found in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Thetan;Thetan;\"Operating_Thetan\";\"Operating_Thetan\""}
{"question": "would Bobby Jindal's high school mascot eat kibble?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dole_Food_Company\";\"Jumanji:_Welcome_to_the_Jungle\";\"Cleveland_Brown_Jr.\";\"Beavis_and_Butt-Head_in_Virtual_Stupidity\""}
{"question": "are multiple Christmas Carol's named after Saints?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Christmas_traditions\";\"Christmas_traditions\";\"Christmas_traditions\";\"Epiphany_(holiday)\""}
{"question": "do placozoa get learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Placozoa;Ploidy;Ploidy;Placozoa"}
{"question": "would Jon Brower Minnoch break a chair before Voyager 2 launch mass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cannikin;\"Roger_Boisjoly\";\"Richard_Feynman\";\"Thomas_G._W._Settle\""}
{"question": "would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"WWE_Raw\";\"Monday_Night_Football\";\"Monday_Night_Football\";\"Monday_Night_Football\""}
{"question": "are any of the destinations of Japan Airlines former Axis Powers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Nippon_(aircraft)\";\"Japan_Airlines\";\"Japan_Airlines\";\"Axis_powers\""}
{"question": "is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Eid_al-Fitr\";\"Holiday_stamp\";Post-9/11;\"Eid_al-Fitr\""}
{"question": "could an infant solve a sudoku puzzle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Autism;\"Brain_Age:_Train_Your_Brain_in_Minutes_a_Day!\";\"Little_Professor\";Joybubbles"}
{"question": "did Gauss have a normal brain structure?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Glia;\"Paul_Loye\";\"Auguste_Forel\";\"History_of_Parkinson's_disease\""}
{"question": "did Benito Mussolini wear bigger shoes than Haf\u00fe\u00f3r Bj\u00f6rnsson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Al_Tomaini\";\"Plus_fours\";\"Holiday_for_Shoestrings\";\"1960s_in_Western_fashion\""}
{"question": "did U2 play a concert at the Polo Grounds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"U2360\u00b0_at_the_Rose_Bowl\";\"Ticket_to_Ride\";\"U2360\u00b0_at_the_Rose_Bowl\";\"The_Secret_Policeman's_Ball\""}
{"question": "does Ludacris perform classical music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Lunic;\"Neoclassical_metal\";\"John_Alldis\";\"George_Enescu_Festival\""}
{"question": "can olive oil kill rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ricinus;Mucositis;\"Olivella_alba\";\"Pine_oil\""}
{"question": "does Julia Roberts lose the prolific acting contest in her family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Julia_Roberts\";\"Julia_Roberts\";\"Julia_Roberts\";\"Julia_Roberts_filmography\""}
{"question": "does Super Mario mainly focus on a man in green?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Luigi;Mario;\"Green_Green_(anime)\";\"Green_anarchism\""}
{"question": "will Elijah Cummings cast a vote in the 2020 presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Faithless_electors_in_the_2016_United_States_presidential_election\";\"Pierre_Nkurunziza\";\"2020_United_States_gubernatorial_elections\";\"2020_United_States_presidential_election\""}
{"question": "can an adult human skull hypothetically pass through the birth canal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Implantation_(human_embryo)\";\"Implantation_(human_embryo)\";\"Starchild_skull\";\"Implantation_(human_embryo)\""}
{"question": "are any mollusks on Chinese New Year calendar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Shen_(clam-monster)\";Pearl;\"Shen_(clam-monster)\";\"Razdolnaya_River\""}
{"question": "could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"LGBT_athletes_in_the_Olympic_and_Paralympic_Games\";\"Israel_at_the_2008_Summer_Olympics\";\"United_States_at_the_2016_Summer_Olympics\";\"United_States_at_the_2008_Summer_Olympics\""}
{"question": "if he were poor, would Christopher Reeve have lived?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if", "extracted_entity": "\"Christopher_Reeve\";\"Saint_Christopher\";\"Arthur_Honeyman\";\"20th_Century_Man\""}
{"question": "is it impossible to tell if someone is having a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Stroke;Stroke;Stroke;Stroke"}
{"question": "does Super Mario require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Super_Mario_Run\";\"Mario_Party_(video_game)\";\"Super_Mario_World\";\"Mario_Party_2\""}
{"question": "does the actress who played Elizabeth II speak fluent Arabic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Prunella_Scales\";\"Elizabeth_(film)\";\"Amr_Waked\";\"Rachel_Shelley\""}
{"question": "were the Spice Girls inspired by Little Mix?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Spice_Girls\";\"Spice_Girls\";\"Spice_Girls\";\"Spice_Girls\""}
{"question": "was Augustus his real name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "August", "extracted_entity": "Augustus;\"Augustus_(title)\";Augustus;\"Augustus_(title)\""}
{"question": "is the brain located in the torso?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Brainstem;Neurocranium;Head;Neurocranium"}
{"question": "could Cosmic Girls play League of Legends alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Frag_Dolls\";\"League_of_Legends\";\"Cosmic_Girls\";\"Cosmic_Girls\""}
{"question": "has a tumulus been discovered on Mars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Lacus_Mortis\";\"Kubrick_Mons\";\"Volcanology_of_Mars\";\"Hottah_(Mars)\""}
{"question": "would a Bengal cat be afraid of catching a fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bengal_cat\";\"Tiger_catshark\";\"Bengal_tiger\";\"Kuhli_loach\""}
{"question": "is anyone at the Last Supper celebrated in Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Last_Supper\";\"Abraham_in_Islam\";\"Passover_sacrifice\";\"Abraham_in_Islam\""}
{"question": "did John Lennon listen to Compact discs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"John_Lennon\";\"John_Lennon/Plastic_Ono_Band\";\"Yoko_Ono/Plastic_Ono_Band\";\"Sentimental_Journey_(Ringo_Starr_album)\""}
{"question": "did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"Elizabeth_I_of_England\";\"Lady_Grace_Mysteries\";\"Coronation_of_King_George_VI_and_Queen_Elizabeth\";\"House_of_York\""}
{"question": "was Nikola Tesla's home country involved in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Diplomacy_of_the_American_Civil_War\";\"American_Expeditionary_Force,_North_Russia\";\"Tesla's_Letters\";\"Nikola_Tesla\""}
{"question": "were there eight humans on Noah's Ark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Noah's_Ark\";\"Noah's_Ark\";\"Noah's_Ark\";\"Noah's_Ark\""}
{"question": "are people banned from entering the Forbidden City?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Forbidden_City\";\"History_of_the_Forbidden_City\";\"Forbidden_City\";\"Forbidden_City\""}
{"question": "did Kurt Cobain's music genre survive after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kurt_Cobain\";Post-grunge;\"Kurt_Cobain\";\"Suicide_of_Kurt_Cobain\""}
{"question": "was Lorenzo de Medici's patronage of Da Vinci exclusive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Lorenzo_de'_Medici\";\"House_of_Medici\";\"Michelangelo_and_the_Medici\";\"Italian_Renaissance_painting\""}
{"question": "when the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Christmas_tree\";\"National_Christmas_Tree_(United_States)\";\"A_Christmas_Record\";\"\u00a1Happy_Birthday_Guadalupe!\""}
{"question": "would a nickel fit inside a koala pouch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"If_I_Had_$1000000\";\"If_I_Had_$1000000\";\"Platinum_Koala\";\"Maui_Trade_Dollars\""}
{"question": "did the Football War last at least a month?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Football_War\";\"Football_War\";\"Combat_football\";\"Football_War\""}
{"question": "does Masaharu Morimoto rely on glutamic acid?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Kikunae_Ikeda\";\"Ju\u0304rokucha\";\"Yonekura_Shigetsugu\";\"Rokusaburo_Michiba\""}
{"question": "can you put bitcoin in your pocket?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bitcoin;Bitcoin;Bitcoin;Bitcoin"}
{"question": "would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dunkin'_Donuts\";\"Army_and_Air_Force_Exchange_Service\";\"Leonard_Matlovich\";\"Dunkin'_Donuts\""}
{"question": "at midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Poisoned_candy_myths\";\"Do_you_know_where_your_children_are?\";\"It's_Always_Sunny_in_Philadelphia\";\"Cult_film\""}
{"question": "did Irish mythology inspire Washington Irving?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_King_of_Ireland's_Son\";\"Culture_of_Belfast\";\"National_epic\";\"National_epic\""}
{"question": "can a microwave melt a Toyota Prius battery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Microwave_oven\";\"Ninja_rocks\";\"Microwave_oven\";\"Microwave_oven\""}
{"question": "does Elizabeth II reign over the Balearic Islands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"States_headed_by_Elizabeth_II\";\"Monarchy_of_Saint_Kitts_and_Nevis\";\"Constitution_of_Saint_Kitts_and_Nevis\";\"Monarchies_in_the_Americas\""}
{"question": "can Michael Jordan become a professional cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Michael_Jordan\";\"Michael_Jordan\";\"Tony_Gemignani\";\"Michael_Jordan\""}
{"question": "will speed reader devour The Great Gatsby before the Raven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Conqueror_Worm\";\"Freefall_(novel)\";\"Gormenghast_(novel)\";\"At_the_Boar's_Head\""}
{"question": "would the 10th doctor enjoy a dish of stuffed pears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hibiscus_(restaurant)\";\"Prezzo_(restaurant)\";\"Waldorf_Salad_(Fawlty_Towers)\";\"Dead_Parrot_sketch\""}
{"question": "during the time immediately after 9/11, was don't ask don't tell still in place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Don't_ask,_don't_tell\";\"Don't_ask,_don't_tell\";\"Don't_ask,_don't_tell\";\"Don't_ask,_don't_tell\""}
{"question": "was King Kong (2005 film) solvent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "King", "extracted_entity": "\"King_Kong_(2005_film)\";\"King_Kong_(2005_film)\";\"King_Kong_(2005_film)\";\"King_Kong_(2005_film)\""}
{"question": "would the Who concert in international space station be audible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Isles_of_Wonder_(album)\";\"Electric_Light_Orchestra\";\"Calling_Festival\";\"The_Wall_Live_(2010\u201313)\""}
{"question": "would an ancient visitor to Persia probably consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Crocus;\"Silk_Road\";\"Smuggling_of_silkworm_eggs_into_the_Byzantine_Empire\";\"Byzantine_silk\""}
{"question": "do you need different colored pens for sudoku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Hepburn_romanization\";\"Hatsune_Miku:_Project_DIVA\";\"Japanese_script_reform\";\"Killer_sudoku\""}
{"question": "can the Supreme Court of Canada fight a Lucha trios match?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Supreme_Court_of_Canada\";\"Supreme_Court_of_Canada\";\"Supreme_Court_of_Canada\";\"Women_in_law\""}
{"question": "could Eric Clapton's children play a regulation game of basketball among themselves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Wayne_Gretzky\";\"Robin_Friday\";\"Oteil_Burbridge\";\"Erroll_Garner\""}
{"question": "is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Gotham_City\";\"Fox_Theatre_(Detroit)\";\"New_York_Transit_Museum\";\"Chrysler_Imperial_Parade_Phaeton\""}
{"question": "did Saddam Hussein witness the inauguration of Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"February_2003_Saddam_Hussein_interview\";\"February_2003_Saddam_Hussein_interview\";\"First_inauguration_of_Barack_Obama\";\"Assassination_threats_against_Barack_Obama\""}
{"question": "is the Flying Spaghetti Monster part of an ancient pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Flying_Spaghetti_Monster\";\"Flying_Spaghetti_Monster\";\"Flying_Spaghetti_Monster\";\"Flying_Spaghetti_Monster\""}
{"question": "did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Gladiator;\"Gladiator_Begins\";\"Scissor_(gladiator)\";Gladiator"}
{"question": "would a slingshot be improperly classified as artillery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "Shooting;Artillery;Catapult;\"History_of_cannon\""}
{"question": "is it normal to blow out candles during a funeral?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ceremonial_use_of_lights\";\"Ceremonial_use_of_lights\";\"Shabbat_candles\";\"Torch-lighting_Ceremony_(Israel)\""}
{"question": "could Darth Vader hypothetically catch the Coronavirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Darth_Vader\";Coronavirus;\"Turkey_coronavirus\";\"Darth_Vader\""}
{"question": "did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Portuguese_Empire\";\"Sephardi_Jews\";\"Afonso_VI_of_Portugal\";\"Portuguese_Inquisition\""}
{"question": "snowboarding is a rarity in Hilo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Tahoma,_California\";\"Hilo,_Hawaii\";\"Sandy_Beach_(Oahu)\";Molokai"}
{"question": "was John Gall from same city as Stanford University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"John_Gall_(baseball)\";\"John_Stockton\";\"John_of_Paris\";\"John_Gall_(baseball)\""}
{"question": "is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"George_Nidever\";\"Health_and_appearance_of_Michael_Jackson\";\"Bob_Marley\";Barbicide"}
{"question": "is San Diego County the home of a Shamu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "San", "extracted_entity": "\"Saguaro_Correctional_Center\";\"Havasupai_Indian_Reservation\";\"Havasupai_Indian_Reservation\";\"Supai,_Arizona\""}
{"question": "was Elmo an original muppet character on Sesame Street?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "El", "extracted_entity": "\"Kevin_Clash\";Elmo;\"Sesame_Street_video_games\";\"Kevin_Clash\""}
{"question": "would a Fakir be surprised if they saw a comma in their religious book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Falnama;Aqeeq;Misbaha;Falnama"}
{"question": "in the world of Harry Potter, would a snake and skull tattoo be good luck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Black_cat_bone\";\"Magical_creatures_in_Harry_Potter\";\"Gerald_Gardner_(Wiccan)\";Luck"}
{"question": "did England win any Olympic gold medals in 1800?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Great_Britain_at_the_1900_Summer_Olympics\";\"Great_Britain_at_the_1900_Summer_Olympics\";\"Great_Britain_at_the_1900_Summer_Olympics\";\"Great_Britain_at_the_1900_Summer_Olympics\""}
{"question": "is video surveillance of a room possible without an obvious camera or new item?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hidden_camera\";\"Hidden_camera\";\"Hidden_camera\";\"Safe_room\""}
{"question": "do pirates care about vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Pirates!_In_an_Adventure_with_Scientists!\";\"Pirate_Islands\";\"The_Pirates!_In_an_Adventure_with_Scientists!\";\"Vitamin_C\""}
{"question": "will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Conan", "extracted_entity": "\"Conan_the_Barbarian\";\"Conan_the_Barbarian_(1982_film)\";\"Conan_the_Barbarian\";\"Conan_the_Barbarian\""}
{"question": "were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jackson", "extracted_entity": "\"Jackson_Pollock\";\"Jackson_Pollock_(supercentenarian)\";\"Musa_McKim\";\"Julian_Bond:_Reflections_from_the_Frontlines_of_the_Civil_Rights_Movement\""}
{"question": "could a fan of the Botany Swarm vote for John Key?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Donkey_vote\";\"The_Keys_to_the_White_House\";\"Vecepia_Towery\";\"For_the_Win\""}
{"question": "would Jackie Chan have trouble communicating with a deaf person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sheena_Iyengar\";\"Komi_Can't_Communicate\";\"Jameela_Jamil\";\"The_Mary_Whitehouse_Experience\""}
{"question": "was Oscar Wilde's treatment under the law be considered fair in the US now?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "In", "extracted_entity": "\"Predicted_effects_of_the_FairTax\";\"Fair_use\";\"FAIR_Education_Act\";\"Fair_Sentencing_Act\""}
{"question": "can a lemon aggravate dyspepsia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Lifitegrast;Diazoxide;Eflornithine;\"Sjo\u0308gren_syndrome\""}
{"question": "can musicians become knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Knights_(orchestra)\";Knight;Knight;Knight"}
{"question": "is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Paraquat;\"Depleted_uranium\";\"Shriek_If_You_Know_What_I_Did_Last_Friday_the_Thirteenth\";Pagophagia"}
{"question": "did Disney's second film rip off a prophet story?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Prophecy\";\"The_Bible_in_film\";\"The_Prophecy_(film_series)\";\"The_Celestine_Prophecy_(film)\""}
{"question": "is Alan Alda old enough to have fought in the Vietnam War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "is", "extracted_entity": "\"Don_Holleder\";\"Alan_Alda\";\"William_Albracht\";\"Jack_L._Tilley\""}
{"question": "is Samsung accountable to shareholders?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chaebol;Chaebol;Chaebol;\"Fair_Trade_Commission_(South_Korea)\""}
{"question": "are chinchillas cold-blooded?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "h", "extracted_entity": "Chinchillidae;Chinchilla;Chinchilla;\"Short-tailed_chinchilla\""}
{"question": "did Johann Sebastian Bach ever win a Grammy Award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Johann", "extracted_entity": "\"Johann_Sebastian_Bach\";\"Johann_Sebastian_Bach\";\"Johann_Sebastian_Bach_(painter)\";\"Johann_Christian_Bach\""}
{"question": "did Dr. Seuss live a tragedy free life?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dr._Seuss\";\"Dr._Seuss\";\"Nevermore:_The_Final_Maximum_Ride_Adventure\";\"Golden_Age_of_Freethought\""}
{"question": "would a Nike shoebox be too small to fit a swan in?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Swan_dress\";\"Can't_Be_Tamed_(song)\";\"Condor_Shoestring\";\"Brian_Jungen\""}
{"question": "were Beauty and the Beast adaptations devoid of Kurt Sutter collaborators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Beauty_and_the_Beast_(2017_film)\";\"Beast_(Beauty_and_the_Beast)\";\"Beauty_and_the_Beast_(1991_film)\";\"Beauty_and_the_Beast_(1991_film)\""}
{"question": "was Florence a Theocracy during Italian Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Republic_of_Florence\";\"Duchy_of_Florence\";\"Republic_of_Florence\";\"Republic_of_Florence\""}
{"question": "did Sojourner Truth use the elevator at the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Elevator;\"Eiffel_Tower\";\"Elevator_operator\";Bubbleator"}
{"question": "is it safe to eat hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hair;\"Hair_care\";\"Ear_hair\";\"Long_hair\""}
{"question": "did Lionel Richie ever have dinner with Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Omni_Parker_House\";\"Orson_Welles_Cinema\";\"Waldorf_pudding\";\"The_Rye\""}
{"question": "would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Motion_JPEG\";JPEG;JPEG;\"Motion_JPEG\""}
{"question": "are fresh garlic cloves as easy to eat as roasted garlic cloves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fresh", "extracted_entity": "Garlic;Chimichurri;\"Indian_relish\";\"Sofrito_(stew)\""}
{"question": "did Osama bin Laden likely abstain from alcohol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Beliefs_and_ideology_of_Osama_bin_Laden\";\"Muhammad_Zia-ul-Haq\";Fatwa;\"Muhammad_Zia-ul-Haq's_Islamization\""}
{"question": "do Youtube viewers get unsolicited audiobook advice often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hypersociability;Clickwrap;\"Personalized_audio_messaging\";Audiobook"}
{"question": "is 2018 Ashland, Oregon population inadequate to be a hypothetical military division?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"186th_Infantry_Regiment_(United_States)\";\"Ashland,_Oregon\";\"Livermore,_California\";\"United_States_Army_Volunteer_Reserve_Association\""}
{"question": "does Carmen Electra own a junk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Carmen_Get_It!\";\"Carmen_Sandiego_(character)\";\"Barn_find\";\"Theft:_A_Love_Story\""}
{"question": "could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Circus_Maximus\";\"Marquee_Club\";\"Adonai\u0308s\";Delirious?"}
{"question": "would Glen Beck and Stephen Colbert be likely to tour together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stephen_Colbert\";\"Stephen_Colbert\";\"Stephen_Colbert_(character)\";\"A_Tribe_Called_Quest\""}
{"question": "could a young Wizard of Oz Scarecrow have gotten Cerebral palsy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Scarecrow_(Oz)\";\"The_Wizard_of_Oz_(1939_film)\";\"Tom_Greenwade\";\"James_Richard_Cocke\""}
{"question": "can the Toyota Hilux tip the scales against Mr. Ed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Ed-touchables_/_Nagged_to_Ed\";\"Ed,_Edd_n_Eddy:_The_Mis-Edventures\";\"The_Ed-touchables_/_Nagged_to_Ed\";\"Tommy_Carcetti\""}
{"question": "is it normally unnecessary to wear a coat in Hollywood in July?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Basil_Rathbone\";\"Hollywood_Steps_Out\";\"Hollywood_Steps_Out\";\"The_Man_in_the_Gray_Flannel_Suit\""}
{"question": "could the Atlantic readers fill 500 battalions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Atlantic_Charter\";\"The_Atlantic\";\"The_Atlantic\";5,000,000"}
{"question": "can oysters be preserved without refrigeration? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Oyster;Oyster;Oyster;\"Oyster_farming\""}
{"question": "would a hippie hypothetically be bummed out by Augustus's Pax Romana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Obelix;\"Pax_Romana\";\"Gaius_Mucius_Scaevola\";\"Glossary_of_ancient_Roman_religion\""}
{"question": "can professional boxers expect to have low dental bills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dentures;\"Raymond_Tooth\";Dentures;\"Niche_insurance\""}
{"question": "did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stephen_Sauvestre\";\"Takamasa_Yoshizaka\";\"Makoto_Sei_Watanabe\";\"Stephen_Sauvestre\""}
{"question": "would  bald eagle deliver an urgent message before B-52?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Air_Force_One\";\"John_Lerew\";\"A_Gathering_of_Eagles\";\"Air_Force_One\""}
{"question": "is it difficult to conduct astrophotography in the summer in Sweden?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Landsort;\"Westerlund_telescope\";\"Schmidt-Va\u0308isa\u0308la\u0308_camera\";\"O\u0308rska\u0308r\""}
{"question": "was a woman Prime Minister directly before or after Stanley Baldwin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Baldwin", "extracted_entity": "\"Lucy_Baldwin\";\"The_Prime_Minister_(film)\";\"Edwards_Pierrepont\";\"Records_of_Prime_Ministers_of_the_United_Kingdom\""}
{"question": "do people in middle school usually get breast exams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Well-woman_examination\";\"Well-woman_examination\";\"Well-woman_examination\";\"Breast_cancer_screening\""}
{"question": "is Black Lives Matter connected with capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Inocybe_maculata\";Capsaicin;Capsaicin;\"Molluscum_contagiosum\""}
{"question": "does Capricorn astrology symbol have all of the parts of a chimera?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Capricorn_(astrology)\";\"Delta_Capricorni\";\"Iota_Capricorni\";\"Negative_sign_(astrology)\""}
{"question": "would Lord Voldemort hypothetically be an effective fighter after Final Fantasy silence is cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Flight_of_Dragons\";\"Excalibur_(film)\";\"The_Gamers:_Dorkness_Rising\";Mordred"}
{"question": "does Sam Harris worship Shiva?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sam", "extracted_entity": "\"Sam_Harris\";\"Sam_Harris\";\"Samstag_aus_Licht\";\"Daniel_Waterman\""}
{"question": "do the James Bond and Doctor Who series have a similarity in format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Doctor_Who_and_the_Pescatons\";Whoniverse;\"The_Doctor_(Doctor_Who)\";\"Doctor_Who_missing_episodes\""}
{"question": "can jackfruit be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jackfruit;Jackfruit;Jackfruit;\"Jack_jumper_ant\""}
{"question": "does Disney own a major comic book publisher?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Marvel_Comics\";\"Marvel_Comics\";\"Marvel_Comics\";\"Disney_Comics\""}
{"question": "did William Shaespeare read the Daily Mirror?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "William", "extracted_entity": "\"Hale_White\";\"True_Sun_(London_newspaper)\";\"Thomas_Chenery\";\"William_Heath_(artist)\""}
{"question": "did the 40th president of the United States forward lolcats to his friends?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Farewell_Dossier\";\"Executive_Order_13769\";\"United_Nations_General_Assembly_Resolution_3379\";\"Presidential_M&M's\""}
{"question": "did Clark Gable marry more women once than Richard Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Clark", "extracted_entity": "\"Clark_Gable\";\"Too_Many_Husbands\";\"Clark_Gable\";\"Irving_Thalberg\""}
{"question": "would an expensive tailor use adhesive to create a shorter hem on slacks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Catsuit;Hotpants;\"1930\u20131945_in_Western_fashion\";\"Dolphin_shorts\""}
{"question": "would the yearly precipitation on Snowdon submerge an upright bowling pin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Bridger_Bowl_Ski_Area\";\"Appley_Bridge\";Fallingwater;\"Ice_jacking\""}
{"question": "can a honey bee sting a human more than once?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Bee_sting\";\"Vespula_vulgaris\";\"Bee_sting\";Hornet"}
{"question": "did U.S. soldiers listen to Justin Bieber's Believe album during the Battle of Baghdad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Believe_(Cher_song)\";\"Believe_(The_Bravery_song)\";\"Believe_Tour\";\"Kill_to_Believe\""}
{"question": "can sunlight travel to the deepest part of the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Blackfin_poacher\";\"Black_Sea\";\"Black_Sea\";\"Black_Sea_deluge_hypothesis\""}
{"question": "is Christopher Walken close to achieving EGOT status?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jeffrey_Marsh\";\"Slater_Bradley\";\"Id,_ego_and_super-ego\";\"Men_in_feminism\""}
{"question": "are the brooms from curling good for using on house floors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Curling;Curling;Curling;\"Men_with_Brooms\""}
{"question": "would a dog easily notice ammonia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Rico_(dog)\";\"Barry_(dog)\";\"Rico_(dog)\";\"Dog_training\""}
{"question": "are vinegar pickled cucumbers rich in lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lactarius;Lactarius;\"Lactarius_rufus\";\"Lactobacillus_pontis\""}
{"question": "is a construction worker required to build a portfolio?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Laborer;Laborer;\"Construction_worker\";Construction"}
{"question": "is it hard to get a BLT in Casablanca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"Casablanca_(1955_TV_series)\";\"Casablanca_(film)\";Bousbir;\"Roman_de_Fauvel\""}
{"question": "would half muggle wizards fear Lord Voldemort?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Lord_Voldemort\";\"Lord_Voldemort\";\"Lord_Voldemort\";\"Lord_Voldemort\""}
{"question": "would ISIS agree with Al-Farabi's religious sect?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al", "extracted_entity": "\"Islamic_State_of_Iraq_and_the_Levant\";\"Ideology_of_the_Islamic_State_of_Iraq_and_the_Levant\";\"Ideology_of_the_Islamic_State_of_Iraq_and_the_Levant\";Wahhabism"}
{"question": "is Casio's founding year a composite number?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_the_Italian_Republic\";\"Neri_per_Caso\";\"Neri_per_Caso\";\"I_Corvi\""}
{"question": "would it be unusual to use paypal for drug deals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"Drug_coupon\";\"Drug_fraud\";\"Advance-fee_scam\";\"Carding_(fraud)\""}
{"question": "is growing seedless cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Banana_Xanthomonas_wilt\";\"Banana_Xanthomonas_wilt\";\"Alternaria_dauci\";\"South_Park\""}
{"question": "can Spartina Patens thrive in the Sahara Desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Spartina_patens\";\"Spartina_patens\";\"Physcomitrella_patens\";\"Subularia_monticola\""}
{"question": "does taking ukemi halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Maai;\"Aikido_concepts\";CHAdeMO;Kyeshi"}
{"question": "are there five different single-digit Fibonacci numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Fibonacci_numbers_in_popular_culture\";\"Fibonacci_number\";\"Fibonacci_number\";\"Fibonacci_polynomials\""}
{"question": "can a snow leopard eat twice its own body weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Spotted_hyena\";\"Bear_attack\";\"Olympic_marmot\";\"Snow_leopard\""}
{"question": "are ground bell peppers the main ingredient of black pepper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Chili_pepper\";Paprika;Paprika;\"Bell_pepper\""}
{"question": "would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Achille_Lauro_hijacking\";\"Oysters_Rockefeller\";\"Il_Cuore_nel_Pozzo\";\"Acqua_pazza_(food)\""}
{"question": "would a monkey outlive a human being on average?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Inequity_aversion_in_animals\";\"Inequity_aversion_in_animals\";\"Inequity_aversion_in_animals\";\"Altruism_(biology)\""}
{"question": "can Harry Potter book a flight on Asiana Airlines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "NokScoot;\"Soekarno\u2013Hatta_International_Airport\";\"Airline_meal\";\"Hainan_Airlines\""}
{"question": "is latitude required to determine the coordinates of an area?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Spherical_coordinate_system\";\"Geographic_coordinate_system\";Latitude;Latitude"}
{"question": "did polio medicine save the life of polio vaccine creator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Polio_vaccine\";\"Polio_vaccine\";\"Polio_vaccine\";\"Hilary_Koprowski\""}
{"question": "are Donkeys part of Christmas celebrations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Donkey;Donkey;\"Dominick_the_Donkey\";\"The_Friendly_Beasts\""}
{"question": "if you have a serious injury in Bangladesh, would you probably dial a Fibonacci number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"July_2016_Dhaka_attack\";\"Telephone_numbers_in_Bangladesh\";\"Catholic_Church_in_Bangladesh\";\"Apostolic_Nunciature_to_Bangladesh\""}
{"question": "hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hydrogen", "extracted_entity": "\"Magic_number_(physics)\";\"Magic_number_(physics)\";\"Atomic_Energy_Central_School\";\"Bigg_Boss_(Hindi_season_12)\""}
{"question": "is Great Pyramid of Giza the last wonder of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Great_Pyramid_of_Giza\";\"Giza_pyramid_complex\";\"Egyptian_pyramids\";\"Ancient_Egyptian_architecture\""}
{"question": "do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Non-English_versions_of_The_Simpsons\";\"Dubbing_(filmmaking)\";\"Anglophone_Cameroonian\";\"Instructions_Not_Included\""}
{"question": "will Elijah Cummings vote for Joe Biden in the next presidential elections?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Faithless_electors_in_the_2016_United_States_presidential_election\";\"2016_United_States_presidential_election\";\"American_Independent_Party\";\"Elijah_Cummings\""}
{"question": "did Joan Crawford guest star on  JAG (TV series)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Joan", "extracted_entity": "\"Joan_Crawford_filmography\";\"India_Adams\";\"Joan_Crawford\";\"India_Adams\""}
{"question": "can an African Elephant get pregnant twice in a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Elephant;\"African_forest_elephant\";Elephant;\"African_bush_elephant\""}
{"question": "would a sofer be a bad job for a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Why_Don't_You_Get_a_Job?\";\"Asa_Keisar\";\"Vegetarianism_and_religion\";\"The_Sneeze_(blog)\""}
{"question": "is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Commercial_driver's_license\";\"Commercial_driver's_license\";\"Driving_in_the_United_States\";\"Driver's_licenses_in_the_United_States\""}
{"question": "was Jackson Pollock straight edge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Jackson_Pollock\";\"Jackson_Pollock\";\"Convergence_(Pollock)\";\"Jackson_Pollock\""}
{"question": "do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Goldstone_Deep_Space_Communications_Complex\";\"Goldstone_Deep_Space_Communications_Complex\";\"Goldstone_Deep_Space_Communications_Complex\";\"Goldstone_Deep_Space_Communications_Complex\""}
{"question": "would a fungal life-form be threatened by a pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Fungicide_use_in_the_United_States\";\"Bronze_disease\";\"Bronze_disease\";Copper"}
{"question": "do urban legends always have to occur in cities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Urban_legend\";\"Urban_legend\";\"Urban_legend\";\"Urban_legend\""}
{"question": "has Freemasonry been represented on the Moon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Freemasonry;Freemasonry;\"Freemasonry_in_the_Philippines\";\"History_of_Freemasonry\""}
{"question": "is Hamlet more common on IMDB than Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Parody;\"To_Be_or_Not_to_Be_(book)\";\"2_B_R_0_2_B\";\"Typographical_error\""}
{"question": "coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Model_Wife\";\"Here_Come_the_Co-Eds\";WAGs;\"Various_Positions\""}
{"question": "was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Mrs._Doubtfire\";\"Mrs._Doubtfire\";\"Madame_Doubtfire\";\"Lisa_Jakub\""}
{"question": "can people die from brake failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Regenerative_brake\";\"Brake_fade\";\"Sudden_unintended_acceleration\";\"Brake_(film)\""}
{"question": "if it socially acceptable to wear an icon depicting crucifixion? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Acculturation;\"Instrument_of_Jesus'_crucifixion\";\"Judas_(Lady_Gaga_song)\";Iconodulism"}
{"question": "could a monarch butterfly rule a kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Monarch_butterfly\";\"Monarch_butterfly\";\"Monarch_butterfly\";\"Monarch_flycatcher\""}
{"question": "can vitamin C rich fruits be bad for health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Fruitarianism;Coulure;\"Negative-calorie_food\";\"Vitamin_deficiency\""}
{"question": "did breakdancing grow in popularity during WW2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"German_code_breaking_in_World_War_II\";Break-through;Break-through;\"German_code_breaking_in_World_War_II\""}
{"question": "would it be unusual to see frost in September in Texas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Climate_of_Texas\";\"Goodyear,_Arizona\";\"Climate_of_Texas\";\"Climate_of_Texas\""}
{"question": "are Aldi's foods discounted due to being out of date?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Eat_This,_Not_That\";\"Store_brand\";\"Eat_This,_Not_That\";Ralphs"}
{"question": "would a recruit for the United States Marine Corps be turned away for self harm?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Joseph_Steffan\";\"Gary_Rader\";\"United_States_Marine_Corps_Recruit_Training\";\"Zoe_Dunning\""}
{"question": "can you order an Alfa Romeo at Starbucks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Caffe\u0300_sospeso\";Starbucks;\"Frappe\u0301_coffee\";\"Instant_coffee\""}
{"question": "will Chick Fil A be open on Halloween 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chick-fil-A;Chick-fil-A;\"Chick-fil-A_Kickoff_Game\";Chick-fil-A"}
{"question": "do drag kings take testosterone to look masculine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Drag_queen\";\"Exo\u0301tico\";\"Drag_king\";\"Drag_pageantry\""}
{"question": "are pennies commonly used in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pitching_pennies\";Penney;\"Pitching_pennies\";\"Canadian_dollar\""}
{"question": "could the Pope be on an episode of Pimp My Ride?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"Weight_Gain_4000\";\"A_Scause_for_Applause\";\"The_Papal_Chase\";\"The_Papal_Chase\""}
{"question": "can E6000 cure before a hoverboard finishes the Daytona 500? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"21st_Century_Cures_Act\";\"Emergency_Preservation_and_Resuscitation\";\"Emergency_Preservation_and_Resuscitation\";\"Tricorder_X_Prize\""}
{"question": "does Nigella Lawson care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "N", "extracted_entity": "\"Nigella_Lawson\";\"Nigella_Lawson\";\"Nigella_Lawson\";\"Nigella_Lawson\""}
{"question": "would Cuba Libre consumption help with insomnia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Corruption_in_Cuba\";Cocaine;\"Havana_syndrome\";\"Rationing_in_Cuba\""}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"One-Eyed_Jacks\";\"John_Wallace_Crawford\";\"Thom_Hatch\";\"The_Jack_Bull\""}
{"question": "would someone on Venus be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Transit_of_Venus\";Venusians;Venus;\"Atmosphere_of_Venus\""}
{"question": "was Jean Valjean imprisoned due to hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jean_Valjean\";\"The_Prisoner_of_Chillon\";\"The_Prisoner_of_Chillon\";\"Jean_Valjean\""}
{"question": "is Europa (moon) name origin related to Amunet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Europa_(consort_of_Zeus)\";\"Europa_(moon)\";Ataegina;\"Europa_(consort_of_Zeus)\""}
{"question": "does Adobe Suite have video game engine coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Adobe", "extracted_entity": "\"Adobe_Director\";\"Adobe_Creative_Suite\";LiveType;\"Adobe_Soundbooth\""}
{"question": "could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"French_Embassy,_Tokyo\";\"Thirty_Minutes_over_Tokyo\";\"Gallery_Fake\";\"Tokyo_Cancelled\""}
{"question": "would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"United_States_Naval_Academy\";\"United_States_Military_Academy\";\"United_States_Military_Academy\";\"United_States_Air_Force_Academy\""}
{"question": "is strep throat harmless to singer Rita Ora after her 2020 tonsilitis surgery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ocrelizumab;Tonsillitis;\"Eosinophilic_granulomatosis_with_polyangiitis\";\"Eloesser_flap\""}
{"question": "would an Alfa Romeo vehicle fit inside a barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Carriage_house\";\"Vehicle_Assembly_Building\";\"Carriage_house\";Dovecote"}
{"question": "gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "g", "extracted_entity": "\"The_Knightly_Tale_of_Gologras_and_Gawain\";\"J._R._R._Tolkien's_Riders_of_Rohan\";\"King_Arthur_(2004_film)\";\"The_dragon_(Beowulf)\""}
{"question": "could two newborn American Black Bear cubs fit on a king size bed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"She's_Having_a_Baby\";\"Zoo_Atlanta\";\"The_Teddy_Bears\";\"Asian_black_bear\""}
{"question": "are coopers required in the beverage industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Are", "extracted_entity": "\"Cooper_(profession)\";\"Cooper_(profession)\";\"A&K_Cooperage\";\"Cooper_(profession)\""}
{"question": "was Superhero fiction invented in the digital format?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"DC_Comics\";\"Superhero_fiction\";Newuniversal;Kryptonian"}
{"question": "does Antarctica have a lot of problems relating to homelessness?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Firefighting_in_Antarctica\";\"Crime_in_Antarctica\";\"Antarctica_cooling_controversy\";\"Antarctica_cooling_controversy\""}
{"question": "does an individual oceanographer study many sciences?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Oceanography;\"Yoshitaka_Ota\";\"Nereus_Program\";\"Marine_Science_Technician\""}
{"question": "is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Andromache_Karakatsanis\";\"Roberta_Jamieson\";\"British_Columbia_Human_Rights_Tribunal\";\"Suzanne_Co\u0302te\u0301\""}
{"question": "did Julio Gonzalez like acetylene?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Selena_Roberts\";\"He\u0301ctor_Lavoe\";\"Alex_Rodriguez\";\"Manny_Alexander\""}
{"question": "is the letter D influenced by the shape of ancient doors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Automatic_door\";\"Door_of_Prophecies\";Cardea;\"Gibbs_surround\""}
{"question": "is \"A Tale of Two Cities\" a parody of the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"A_Tale_of_Two_Cities\";\"A_Tale_of_Two_Cities\";\"A_Tale_of_Two_Cities\";\"A_Tale_of_Two_Cities\""}
{"question": "was the death of Heath Ledger caused by his work on The Dark Knight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Heath_Ledger\";\"Heath_Ledger\";\"Heath_Ledger\";\"Spike_Milligan\""}
{"question": "did Donatello use a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Donatello;\"Fofa\u0303o_(character)\";\"Oo\u0308phoi\";\"Lupe_Fiasco\""}
{"question": "did Rahul Dravid ever kick a field goal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fake_field_goal\";\"Fake_field_goal\";\"Rahul_Dravid\";\"Rahul_Dravid\""}
{"question": "do you often hear Marco Polo's name shouted near water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Marco_Polo_(game)\";\"The_Sea,_the_Sea\";\"Marco_Polo_(game)\";\"Marco_Polo\""}
{"question": "are people more likely than normal to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Thermal_burn\";\"Heat_stroke\";Burn;\"Thermal_burn\""}
{"question": "is an espresso likely to assuage fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Anaphrodisiac;Catharsis;Espresso;Anaphrodisiac"}
{"question": "will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"September_2025_lunar_eclipse\";\"December_2028_lunar_eclipse\";\"Glamanand_Supermodel_India_2018\";\"September_2025_lunar_eclipse\""}
{"question": "would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_White_Negro\";\"The_Anti-Capitalistic_Mentality\";Misanthropy;\"The_Abolition_of_Work\""}
{"question": "did a gladiator kill his opponent with a shotgun?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Execution_by_firing_squad\";Gladiator;\"A_Gunfight\";Gladiator"}
{"question": "did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Montezuma_Castle_National_Monument\";\"Vellore_Fort\";\"Grizzly_Man\";\"Bear_attack\""}
{"question": "could B be mistaken for an Arabic numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "B;B;\"ISO_3166-2:BH\";Betacism"}
{"question": "can COVID-19 spread to maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "COVID", "extracted_entity": "\"Telstar_19V\";\"Flight_control_modes\";\"Yellow_flag_(contagion)\";\"Maritime_call_sign\""}
{"question": "is 500GB USB device enough to save 10 hours of Netflix shows a day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Streaming_media\";\"USB_flash_drive\";Flexplay;\"File_Streaming_Technology\""}
{"question": "do hornets provide meaningful data for oceanographers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Navigational_database\";\"Air_traffic_control\";\"Radar_chart\";Meaconing"}
{"question": "would Michael Phelps be good at pearl hunting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Pearl_hunting\";\"Miscovich_emeralds_hoax\";\"Zack_Pearlman\";\"Chuck_Testa\""}
{"question": "at Christmastime, do some films remind us that groundhog day is approaching?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Groundhog_Day\";\"Groundhog_Day_(film)\";\"25_Days_of_Christmas\";\"Groundhog_Day\""}
{"question": "was Great Recession the period of severest unemployment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Great_Recession\";\"Great_Recession_in_the_United_States\";\"Great_Recession\";\"Great_Recession\""}
{"question": "could Katharine Hepburn have ridden the AirTrain JFK?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Jefferson_Airplane\";\"Lauren_Hutton\";\"Scooter_(motorcycle)\";\"Inauguration_of_John_F._Kennedy\""}
{"question": "was Walt Disney able to email his illustrations to people living far away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Walt_Disney\";\"Walt_McDougall\";\"If_You_Had_Wings\";\"Walt_Disney\""}
{"question": "can a person who knows only English read Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Orthographies_and_dyslexia\";\"Rudolf_Flesch\";\"Please_Teach_Me_English\";\"Japanese_language_education_in_the_United_States\""}
{"question": "would three commas be sufficient for displaying US 2018 GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"GDP_deflator\";\"Historical_GDP_of_China\";\"Scatter_plot\";\"Consumer_Expenditure_Survey\""}
{"question": "would Brian Warner be a good singer for a soul music band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Contours\";\"Brian_Wilson\";\"James_Dean_(songwriter)\";\"Harry_Balk\""}
{"question": "could someone listen to the entire U2 debut studio album during an episode of Peppa Pig?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Zoo_TV_Tour\";\"Zoo_TV_Tour\";\"More_Crap\";\"PopMart_Tour\""}
{"question": "did Hanuman ever experience an orgasm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"U._G._Krishnamurti\";\"Pi\u0304ti\";Kundalini;Orgasm"}
{"question": "would you find a tibia beside parsley on a holiday plate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Modern_Cookery_for_Private_Families\";\"Altare_della_Patria\";Rutabaga;\"Aunts_Aren't_Gentlemen\""}
{"question": "does someone from Japan need a passport to go to a Nordic country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Swedish_nationality_law\";\"International_child_abduction_in_Japan\";\"Immigration_to_Japan\";\"Nationality_law\""}
{"question": "would a black widow woman have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pe", "extracted_entity": "\"The_Peaches\";\"Girl_with_Peaches\";\"Ammi_majus\";\"Girl_with_Peaches\""}
{"question": "does a person using tonsure have hair at the top of their scalp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hair_removal\";Tonsure;Tonsure;Tonsure"}
{"question": "would Terence Tao outperform Eminem in a math competition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Terence_Tao\";\"Ego_Trip's_The_(White)_Rapper_Show\";\"Shaq_Vs.\";\"Crosby_Loggins\""}
{"question": "do any video games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ending-Man_Terminator\";\"Slot_machine\";\"Redemption_game\";\"Slot_machine\""}
{"question": "is number of different US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Hong_Kong\";\"Hong_Kong_legislative_elections\";Quorum;\"Elections_in_Hong_Kong\""}
{"question": "can lettuce result in spontaneous abortion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Abortion;Abortion;\"Septic_abortion\";Lettuce"}
{"question": "has mummification in the Andes been prevented by rainfall?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Chinchorro_mummies\";\"Mummy_Juanita\";\"Muisca_mummification\";\"Mummy_Juanita\""}
{"question": "are some Do It Yourself projects potentially lethal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Do-it-yourself_biology\";\"Do_it_yourself\";\"Do_it_yourself\";\"Do_it_yourself\""}
{"question": "was King Kong (2005 film) the lead actress's biggest box office role?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Naomi", "extracted_entity": "\"King_Kong_(2005_film)\";\"King_Kong_(2005_film)\";\"King_Kong_(1976_film)\";\"King_Kong_(2005_film)\""}
{"question": "would a bodybuilder choose maize over chicken breast for dinner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Maya_maize_god\";\"The_Ramen_Girl\";\"Cucurbita_texana\";\"Chicken_65\""}
{"question": "karachi was a part of Alexander the Great's success?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "kar", "extracted_entity": "\"History_of_Karachi\";\"Wars_of_Alexander_the_Great\";\"Demographic_history_of_Karachi\";\"The_Great_Game\""}
{"question": "do guitarists need both hands to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Guitar;\"Double_stop\";Vibraphone;\"Three-hand_effect\""}
{"question": "does Kenny G hold the qualifications to be a tax collector?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kenny", "extracted_entity": "\"Jeff_Wooller\";\"Kenny_Alexander_(businessman)\";\"Steve_Vizard\";\"George_Magan,_Baron_Magan_of_Castletown\""}
{"question": "in baseball, is a \"Homer\" named after the poet Homer who wrote the Odyssey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Homer_at_the_Bat\";Kalamos;\"Ernest_Thayer\";\"Homer,_New_York\""}
{"question": "is a felony jury enough people for a Bunco game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bunco;Bunco;Jury;\"Hung_jury\""}
{"question": "is letter C crucial to spelling the two most common words in English language?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "C;\"English-language_spelling_reform\";\"English_phonology\";Phonotactics"}
{"question": "do most middle class families have butlers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Butler;Butler;\"Great_house\";\"The_Butler\""}
{"question": "is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Jeremiah_Horrocks\";\"Later_life_of_Isaac_Newton\";\"Westminster_Abbey\";\"Isaac_Newton\""}
{"question": "did Mickey Mouse appear in a cartoon with Bugs Bunny in 1930?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Rabbit_Romeo\";\"Bugs_Bunny_Rides_Again\";\"Rabbit_Romeo\";\"Tease_for_Two\""}
{"question": "did Jackson 5 members exceed number in The Osmonds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"The_Jackson_5\";\"The_Jackson_5\";\"The_Jackson_5\";\"The_Jackson_5\""}
{"question": "can you get a ride on Amtrak to the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Adventure_Express\";\"Derren_Brown's_Ghost_Train\";\"Grim_Fandango\";\"Oriental_Desert_Express\""}
{"question": "are all limbs required for jujutsu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jujutsu;Jujutsu;\"Jujutsu_techniques\";\"Jujutsu_techniques\""}
{"question": "would The Dark Knight be appropriate for a preschool class?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sorcerer's_Son\";\"Could_It_Be..._Satan?\";\"Nexo_Knights\";Grimoire"}
{"question": "did Jesus know anyone who spoke Estonian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Yestonians;\"Ignace_Lepp\";Intermovement;\"Johannes_Karhapa\u0308a\u0308\""}
{"question": "does 2015 have more unlucky Friday's than usual?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"One_of_Our_Thursdays_is_Missing\";\"Weekend_effect\";Thursday;\"Doomsday_rule\""}
{"question": "is Tony Bennett's middle name shared by a former UFC champion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Junior_dos_Santos\";\"Dan_Duva\";\"Charles_Bennett_(fighter)\";\"Anthony_Small\""}
{"question": "do you need both hot and cold water to peel a tomato?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Tomato_soup\";Tomato;\"Sun-dried_tomato\";\"'Ota_'ika\""}
{"question": "is Newt Gingrich's nickname a type of Reptile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hellbender;\"Oxyjulis_californica\";Botaurus;\"Rhithrogena_germanica\""}
{"question": "do spider wasps have eight legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spider;Arachnid;\"Spider_anatomy\";\"Spider_behavior\""}
{"question": "does the New York Public Library sell Alpo products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"New_York_Public_Library\";\"Alpo_(pet_food)\";\"Alpo_(pet_food)\";\"New_York_Public_Library\""}
{"question": "can petroleum jelly be used as fuel in a car?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Petroleum_jelly\";\"Petroleum_jelly\";\"Petroleum_jelly\";\"Wood_gas_generator\""}
{"question": "can Aerosmith legally drive in the carpool lane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bootleg_turn\";\"Handbrake_turn\";\"Bootleg_turn\";\"Underwater_cycling\""}
{"question": "are Durian fruits an olfactory delight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Durian;\"Synsepalum_dulcificum\";\"Pelargonium_odoratissimum\";\"Melon_fly\""}
{"question": "could a wandering albatross fly from Bucharest to New York City without a rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Lavinium;\"Wandering_albatross\";\"Orteig_Prize\";\"Steve_Fossett\""}
{"question": "do some people soak in olive oil and water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Mimouna;\"Olive_oil\";\"Holy_Sponge\";\"Olive_oil_extraction\""}
{"question": "do depressed people travel to the Golden Gate Bridge often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Slugging;\"Golden_Gate_Bridge\";\"Suicides_at_the_Golden_Gate_Bridge\";\"Suicides_at_the_Golden_Gate_Bridge\""}
{"question": "can members of the Green Party of England and Wales vote in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Green_Party_of_the_United_States\";\"Political_parties_in_the_United_States\";\"Green_Party_of_the_United_States\";\"Green_Party_of_the_United_States\""}
{"question": "is Poseidon similar to the god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Neptune_(Marvel_Comics)\";Hephaestus;\"Son_of_Vulcan\";\"4660_Nereus\""}
{"question": "is Glycol something United Airlines would buy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Rexine;\"United_Airlines\";\"Beta_cloth\";\"Aircraft_dope\""}
{"question": "does a person suffering from Thalassophobia enjoy oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Thalassotherapy;Thalassotherapy;\"Nicolas_Bouvier\";Leptospirosis"}
{"question": "can a Bengal cat survive eating only pancakes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Bengal_slow_loris\";\"Trapped_(2016_Hindi_film)\";\"Bengal_slow_loris\";\"Bengal_slow_loris\""}
{"question": "does it seem like the Gorillaz is composed of more members than they have?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Gor", "extracted_entity": "Gorillaz;Gorillaz;\"The_Gorillas\";Gorillaz"}
{"question": "could Rich and Morty be triggered for children of alcoholics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Diathesis\u2013stress_model\";Epigenetics;\"Antisocial_personality_disorder\";\"Excited_delirium\""}
{"question": "was the Japanese street aesthetic once illuminated by noble gasses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Atelier_Bow-Wow\";\"Japanese_painting\";\"Toyohara_Kunichika\";\"Motomachi,_Yokohama\""}
{"question": "in isopropyl alcohol, is the solubility of salt low?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Phenformin;\"Salt_(chemistry)\";Perfluorodecalin;\"Saline_water\""}
{"question": "could you watch all of JAG in six months?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jagga_Jasoos\";\"Happy_Bhag_Jayegi\";\"Zindagi_Na_Milegi_Dobara\";\"Robert_Rippberger\""}
{"question": "do silicone suits make judo difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Sauna_suit\";G-suit;\"Training_masks\";\"Brazilian_jiu-jitsu\""}
{"question": "will The Exorcist stimulate limbic system?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Assist_(Scientology)\";Necromania;\"Hemifacial_spasm\";Neurostimulation"}
{"question": "was P. G. Wodehouse's favorite book The Hunger Games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"The_Hunger_Games\";\"The_Hunger_Games_(novel)\";\"The_Hunger_Games_(novel)\";\"The_Hunger_Games_(film_series)\""}
{"question": "would J.K Rowling's top sellers be on a fantasy shelf?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "are some types of pancakes named after coins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pancake;\"Pancake_house\";Pancake;Pancake"}
{"question": "can Paprika be made without a dehydrator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Paprika;\"Paprika_oleoresin\";Paprika;Paprika"}
{"question": "in Doctor Who, did the war doctor get more screen time than his successor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"War_Doctor\";\"War_Doctor\";\"The_Doctor_(Doctor_Who)\";\"The_Doctor_(Doctor_Who)\""}
{"question": "was Mesopotamia part of what is now China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"East_Turkestan\";\"History_of_Xinjiang\";\"Migration_to_Xinjiang\";Xinjiang"}
{"question": "could Arnold Schwarzenegger hypothetically defeat Haf\u00fe\u00f3r Bj\u00f6rnsson in a powerlifting competition if both are at their peak strength?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jo\u0301n_Pa\u0301ll_Sigmarsson\";\"Sport_in_Iceland\";\"Two_Hands_Anyhow\";\"Haf\u00feo\u0301r_Ju\u0301li\u0301us_Bjo\u0308rnsson\""}
{"question": "was the British car, the Mini, the first car manufactured?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Mini;\"Bond_Minicar\";Mini;Mini"}
{"question": "will you see peach blossoms and Andromeda at the same time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"The_Peaches\";\"Cherry_Blossoms_(film)\";\"Two_Tahitian_Women\";\"National_Cherry_Blossom_Festival\""}
{"question": "can The Hobbit be read in its entirety in four minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Lord_of_the_Rings_(film_series)\";\"Akhand_Path\";\"Steve_Woodmore\";\"Alphabet_fu\u0308r_Lie\u0300ge\""}
{"question": "is number of stars in Milky Way at least ten times earth's population?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Milky_Way\";\"Milky_Way\";\"Milky_Way\";\"Milky_Way\""}
{"question": "could Christopher Walken enlist in the United States Marine Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher", "extracted_entity": "\"Christopher_George\";\"Chris_Jay\";\"Hispanics_in_the_United_States_Marine_Corps\";\"Chris_Jay\""}
{"question": "is white light the absence of color?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "White;White;\"Shades_of_white\";\"Whiteout_(weather)\""}
{"question": "can Jabberwocky be considered a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Jabberwocky_sentence\";Jabberwocky;Jabberwocky;\"Jabberwocky_sentence\""}
{"question": "would Eye surgery on a fly be in vain?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Chicken_eyeglasses\";\"Chicken_eyeglasses\";\"Eyestalk_ablation\";\"The_Fly_(1958_film)\""}
{"question": "should you ask a neighbor for candy on New Year's Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Christmas_pickle\";\"Christmas_pickle\";\"A_Christmas_Eve\";\"Saint_Nicholas_Day\""}
{"question": "are all types of pottery safe to cook in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Earthenware;Earthenware;Pottery;Stoneware"}
{"question": "can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Adrammelech;Abdastartus;Pndapetzim;\"Kingdom_Come:_Deliverance\""}
{"question": "does the United States of America touch the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Geography_of_the_United_States\";\"United_States_Indo-Pacific_Command\";\"United_States_Indo-Pacific_Command\";\"Boundaries_between_the_continents_of_Earth\""}
{"question": "would Donald Duck be allowed into most grocery stores?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Donald's_Ostrich\";\"Donald_Duck_universe\";\"Harry_Duncan\";\"Donald_Duck_universe\""}
{"question": "is it best to avoid kola nuts with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Horse_colic\";Kosambari;\"Garcinia_kola\";\"Peanut_allergy\""}
{"question": "would 7 zucchini's satisfy potassium USDA daily recommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"7_Up\";\"Overall_nutritional_quality_index\";\"Kraft_Dinner\";\"Special_K\""}
{"question": "was the Treaty of Versailles settled over blueberry scones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Eden_Agreement\";\"Treaty_of_Versailles_(1757)\";\"Treaty_of_Versailles_(1757)\";\"Treaty_of_Versailles\""}
{"question": "will the producer of Easy Rider become an octogenarian in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Bachelor_(season_22)\";\"BoJack_Horseman\";\"Coke_Studio_Pakistan_(season_11)\";\"Addictive_Nature\""}
{"question": "does Osama bin Laden put a wafer on his tongue every Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Brother_Ali\";\"Zaghloul_El-Naggar\";\"Zakat_al-Fitr\";\"Beliefs_and_ideology_of_Osama_bin_Laden\""}
{"question": "can you taste Law & Order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Where_the_Water_Tastes_Like_Wine\";\"I_Drink_Your_Blood\";Taste;Taste"}
{"question": "are queen bees unnecessary for growing apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Queen_bee\";Beekeeping;\"Queen_bee\";\"Queen_bee\""}
{"question": "does a bumblebee have to worry about spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"It's_Tough_to_Be_a_Bug!\";Bumblebee;Bumblebee;Bumblebee"}
{"question": "can the Department of Defense perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_States_Air_Force_Pararescue\";\"United_States_Armed_Forces\";\"United_States_Army_Parachute_Team\";\"United_States_Armed_Forces_School_of_Music\""}
{"question": "can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Optical_lift\";\"\u02bbOumuamua\";\"\u02bbOumuamua\";\"Sokol_Eshelon\""}
{"question": "was Land of Israel in possession of an Islamic empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_Israel\";\"Ottoman_Empire\";\"Ottoman_Arabia\";\"Ottoman_Empire\""}
{"question": "do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Languages_of_Africa\";\"Dyula_people\";\"Culture_of_Africa\";\"Portuguese_language_in_Africa\""}
{"question": "could the Toyota Stadium sit a tenth of the population of Gotheburg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ullevi;\"Toyota_Park\";\"Toyota_Park\";\"Toyota_Park\""}
{"question": "does the word swastika have meaning in sanskrit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Sauwastika;Swastika;Swastika;Swastika"}
{"question": "were there greater landslides than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Landslide_(board_game)\";\"Landslide_(board_game)\";\"1984_United_States_presidential_election_in_Utah\";\"Great_Blue_Hill_eruption_prank\""}
{"question": "can you hunt Iberian wolves in the Southern United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bear_hunting\";\"Bear_hunting\";\"Florida_black_bear\";\"Wolf_hunting\""}
{"question": "is hanging a viable execution method on a ship at sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Suicide_by_hanging\";Hanging;\"Suicide_by_hanging\";Keelhauling"}
{"question": "is pig meat considered inedible within the cuisine of Hawaii?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Native_cuisine_of_Hawaii\";\"Dog_meat\";\"Native_cuisine_of_Hawaii\";\"Hawaiian_Poi_Dog\""}
{"question": "are the events of Star Trek: The Next Generation in the history of the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Generation;\"The_Five_Ages_of_the_Universe\";\"History_of_the_world\";\"Age_wave\""}
{"question": "if you're pregnant, might you be recommended ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Gingerol;Ginger;Gingering;\"Ginkgo_biloba\""}
{"question": "does The Jungle Book contain racist subtext?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Jungle_Book\";\"The_Jungle_Book\";\"Law_of_the_jungle\";\"The_Jungle_Book\""}
{"question": "does the Prime Minister of the United Kingdom have poor job security?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Small_government\";\"Cabinet_of_the_United_Kingdom\";\"Political_positions_of_David_Cameron\";\"Political_positions_of_Jeremy_Corbyn\""}
{"question": "would Gomer Pyle salute a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"William_Prescott\";\"Bunker_Hill_Monument\";\"William_Prescott\";\"James_Harden_Daugherty\""}
{"question": "when en route from China to France, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Peking_to_Paris\";Puyi;Honghuzi;\"Altitude_sickness\""}
{"question": "could you brew beer from start to finish in the month of September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "Brewing;Brewing;\"Great_Northern_Brewing_Company\";\"Beer_style\""}
{"question": "is Freya a combination of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "is", "extracted_entity": "Theia;\"Asteria_(Titaness)\";\"Asteria_(Titaness)\";Harpy"}
{"question": "in most Mennonite homes, would children know of The Powerpuff Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Freedomites;\"The_Powerpuff_Girls_Movie\";\"The_Powerpuff_Girls\";\"Babes_in_the_Wood_murders_(Pine_Grove_Furnace)\""}
{"question": "would a sophist use an \u00e9p\u00e9e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Sophist;\"E\u0301pe\u0301e\";Sophist;\"Charles-Michel_de_l'E\u0301pe\u0301e\""}
{"question": "did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"James,_son_of_Zebedee\";\"Paul,_Apostle_of_Christ\";\"Paul,_Apostle_of_Christ\";\"Paul,_Apostle_of_Christ\""}
{"question": "do sand cats avoid eating all of the prey of eels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Sand_tiger_shark\";\"Brown_catshark\";\"Alligator_gar\";\"Tiger_shark\""}
{"question": "was Muhammed a member of the Uniting Church in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"James_Udy\";\"John_Mallison\";\"Wesleyan_Methodist_Church_of_Australia\";\"James_Udy\""}
{"question": "would the historic Hattori Hanz\u014d admire Naruto?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Honda_Tadakatsu\";\"Hattori_Hanzo\u0304\";\"Hattori_Hanzo\u0304\";\"Nogi_Maresuke\""}
{"question": "should you wrap a gift for a mother of a stillborn in stork wrapping paper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Easter_basket\";\"Ring_a_Ring_o'_Roses\";\"Father_Christmas\";\"Father_Christmas\""}
{"question": "does crucifixion violate US eighth amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Haskell_County,_Oklahoma\";\"Salazar_v._Buono\";\"Eighth_Amendment_to_the_United_States_Constitution\";Adultery"}
{"question": "would a TMNT coloring book have pizza in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Screaming_Yellow_Zonkers\";\"Eskimo_Joe's\";\"Vic_Dibitetto\";\"Simms_Taback\""}
{"question": "could Sugar Ray Robinson box if he stole in Iran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Cryptocurrency;\"Operation_Jackpot_(drug_investigation)\";\"Asher_Karni\";\"Bomb_Iran\""}
{"question": "would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Something_Up_There_Likes_Me\";\"Something_Up_There_Likes_Me\";\"Something_Up_There_Likes_Me\";\"U-Friend_or_UFO?\""}
{"question": "can a banana get a virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Banana_bunchy_top_virus\";\"Banana_Xanthomonas_wilt\";\"Banana_freckle\";\"Banana_streak_virus\""}
{"question": "is the use of the word Gypsy by non-Romani people considered okay?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Romani_people\";\"Romani_language\";\"Romani_people_in_Romania\";\"Romani_people\""}
{"question": "are implants from an ORIF surgery affected by the magnetic field of the Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Implant_(medicine)\";\"Safety_of_magnetic_resonance_imaging\";\"Magnetic_implant\";Osteolysis"}
{"question": "was Subway involved in a pedophilia scandal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Subway_(restaurant)\";\"History_of_the_New_York_City_Subway\";\"New_York_City_Subway\";\"New_York_City_Subway\""}
{"question": "did Jack Dempsey have most title fight wins in either of his weight classes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Jack_Dempsey\";\"Philadelphia_Jack_O'Brien\";\"Jack_Dempsey\";\"Philadelphia_Jack_O'Brien\""}
{"question": "did Bill Nye vote for Franklin Delano Roosevelt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Wendell_Willkie\";\"Franklin_Delano_Roosevelt_Jr.\";\"Franklin_D._Roosevelt\";\"Criticism_of_Franklin_D._Roosevelt\""}
{"question": "is grief always obvious when it is being experienced?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Anticipatory_grief\";Grief;Grief;Grief"}
{"question": "did either side score a touchdown during the Football War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Turning_point_of_the_American_Civil_War\";\"Football_War\";\"Football_War\";\"Turning_point_of_the_American_Civil_War\""}
{"question": "is there a Yeti associated with Disney theme parks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Yetisports;Yeti;Yeti;Yeti"}
{"question": "did Zorro carve his name into items regularly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"El_Gran_Carnaval_de_San_Pedro\";Yacatecuhtli;Yacatecuhtli;Zorro"}
{"question": "did Subway have a sex offender as a spokesperson?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jared", "extracted_entity": "\"Jared_Fogle\";\"Miss_Subways\";\"Jared_Fogle\";Manspreading"}
{"question": "could Christopher Nolan's movies finance Cyprus's entire GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cyprus;\"Cinema_of_Cyprus\";\"Turkish_economic_boom_of_the_2000s\";\"Economy_of_Cyprus\""}
{"question": "is Steve Carell's character on The Office portrayed as one with tremendous leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Steve_Carell\";\"Steve_Carell\";\"The_Office_(U.S._TV_series)\";\"The_Office_(U.S._season_8)\""}
{"question": "could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dominican_Order\";\"Dominican_Order\";\"Monastic_State_of_the_Knights_Hospitaller\";\"Papal_infallibility\""}
{"question": "can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Snow_White_and_the_Seven_Dwarfs_(1937_film)\";\"Killer_Queen_(video_game)\";\"Mario_Sports_Mix\";\"Paul_Grice\""}
{"question": "does the texture of leaves remain the same independent of their coloring changing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "Evergreen;Leaf;Leaf;\"Exfoliation_(botany)\""}
{"question": "can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Free_Bible_Students\";\"Cotton_Valley,_Louisiana\";\"Free_Bible_Students\";\"Elohim_City,_Oklahoma\""}
{"question": "would Doctor Strange like the Pittsburgh Steelers logo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Logos_and_uniforms_of_the_Pittsburgh_Steelers\";\"Pittsburgh_Steelers\";\"Logos_and_uniforms_of_the_Pittsburgh_Steelers\";\"Logos_and_uniforms_of_the_Pittsburgh_Steelers\""}
{"question": "would Iris (mythology) and Hermes hypothetically struggle at a UPS job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Menippe_and_Metioche\";\"Icarus_paradox\";\"Menippe_and_Metioche\";\"The_Arcadians_(musical)\""}
{"question": "is pi in excess of square root of 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pi", "extracted_entity": "\"Square_root_of_5\";\"Square_root_of_5\";\"Proof_that_22/7_exceeds_\u03c0\";Pi"}
{"question": "would it be uncommon for a high schooler to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bad_Luck_Brian\";\"Yellow_journalism\";BlitzMail;\"Yellow_journalism\""}
{"question": "could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Caligula's_Giant_Ship\";\"Merchant_ship\";\"Vamoose_(steamship)\";\"Tokio_Express\""}
{"question": "does the human stomach destroy a bee if ingested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Western_honey_bee\";\"Malpighamoeba_mellificae\";Botulism;\"Nosema_apis\""}
{"question": "is blonde hair green eyed Sara Paxton considered a Latino?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Asia_Carrera\";\"Jackie_Cruz\";\"Jackie_Cruz\";\"Christina_Milian\""}
{"question": "are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Photosynthesis;Photosynthesis;Photosynthesis;Photosynthesis"}
{"question": "can you see live harbor seals in Washington DC?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Lincoln_Park_Zoo\";\"Pier_39\";\"Richmond,_California\";\"Marine_life_of_New_York\u2013New_Jersey_Harbor_Estuary\""}
{"question": "was story of Jesus inspired by Egyptian myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Christ_myth_theory\";\"Christ_myth_theory\";\"Christ_myth_theory\";\"Christ_myth_theory\""}
{"question": "has CNES planted a French flag on the lunar surface?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Patrouille_de_France\";\"National_symbols_of_France\";\"French_Algeria\";\"Flag_desecration\""}
{"question": "do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Denialism;Denialism;\"Modern_flat_Earth_societies\";\"Myth_of_the_flat_Earth\""}
{"question": "can Iowa be hidden in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Most_Secret\";\"Chambercombe_Manor\";Denehole;Denehole"}
{"question": "can you house a giant squid at Soldier Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Guantanamo_Bay_Naval_Base\";\"Guantanamo_Bay_Naval_Base\";\"Camp_Wellfleet\";\"Guantanamo_Bay_Naval_Base\""}
{"question": "was hippie culture encouraged by the government in the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Counterculture;Hippie;Counterculture;\"History_of_the_hippie_movement\""}
{"question": "would Mickey Mouse blend in with the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"United_States_Army_Special_Forces_in_popular_culture\";Goofy;\"Mickey_Mouse_universe\";\"212th_Coast_Artillery_(United_States)\""}
{"question": "does penicillin cure a learning disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Methylphenidate;Ampicillin;Neuroenhancement;Penicillamine"}
{"question": "is it dark is Basel during the day in Los Angeles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"San_Francisco\";\"Carnival_of_Basel\";\"San_Francisco_fog\";\"Night_photography\""}
{"question": "is honey associated with queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Honey_bee\";\"Honey_bee\";\"Honey_bee\";\"Queen_bee\""}
{"question": "was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Corn", "extracted_entity": "\"Beulah_Rebecca_Hooks_Hannah_Tingley\";\"Cornelia_Fort\";\"Florence_Jaffray_Harriman\";\"Winnifred_Sprague_Mason_Huck\""}
{"question": "does a giant green lady stand in New York Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pacifica_(statue)\";\"Liberty_Island\";\"Liberty_Island\";\"New_York_Bay\""}
{"question": "could chives be mistaken for grass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Common_linnet\";Pansy;Chives;Clematis"}
{"question": "could a two-year old win a Scrabble tournament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"David_Howell_(chess_player)\";\"Are_You_an_Egghead?\";\"Are_You_an_Egghead?\";\"Chess_prodigy\""}
{"question": "is Y2K relevant to the plot of The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Interpretations_of_Fight_Club\";\"Genealogies_of_a_Crime\";\"Y2K:_The_Game\";\"Feraliminal_Lycanthropizer\""}
{"question": "is Sirius part of a constellation of an animal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sirius_in_fiction\";Sirius;Sirius;\"The_Sirius_Mystery\""}
{"question": "would alligator best saltwater crocodile in hypothetical Lake Urmia battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Lake_Urmia\";\"Are_You_an_Egghead?\";\"Battle_of_the_Abas\";\"Lake_Urmia\""}
{"question": "would a Common warthog starve in a greenhouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Water_stagnation\";Eichhornia;Bracken;SMEDI"}
{"question": "is purchasing food for a Lolcat unnecessary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mess_of_pottage\";\"Prison_commissary\";\"Mishloach_manot\";\"Meal_voucher\""}
{"question": "do some psychotherapy patients have no mental illness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Psychotherapy;\"Schizoid_personality_disorder\";\"Postpartum_psychosis\";\"Bipolar_II_disorder\""}
{"question": "did Harry Houdini appear on Chris Angel Mindfreak?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Evilspeak;\"Wiccan_(comics)\";\"Criss_Angel\";\"Warlock_(New_Mutants)\""}
{"question": "could  jockey win Triple Crown between Eid al-Fitr endpoints?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Triple_Crown_of_Thoroughbred_Racing_(United_States)\";\"Triple_Crown_of_Thoroughbred_Racing_(United_States)\";\"Triple_Crown_of_Thoroughbred_Racing_(United_States)\";\"Triple_Crown_of_Thoroughbred_Racing_(United_States)\""}
{"question": "would a vegetarian be able to eat something at Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Chick-fil-A;Chick-fil-A;Chick-fil-A;Chick-fil-A"}
{"question": "does the Pixar film Brave feature Scottish people?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Brave_(2012_film)\";\"Brave_(2012_film)\";\"Brave_(2012_film)\";Braveheart"}
{"question": "do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"On_the_Banks_of_Plum_Creek\";\"Kawuneeche_Valley\";\"Leaf_miner\";\"Birch_leafminer\""}
{"question": "do most college students own a fax machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Autograph;Mimeograph;\"Punched_card\";Fax"}
{"question": "would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paul", "extracted_entity": "\"Planned_community\";\"Planned_community\";\"Urban_design\";\"Augustus_Pugin\""}
{"question": "could a delicious recipe be made with The Onion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Acquacotta;\"Blooming_onion\";\"French_onion_soup\";\"Onion_ring\""}
{"question": "could a markhor give birth three times in a single year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Markhor;\"The_Neanderthal_Parallax\";\"Male_pregnancy\";Wraeththu"}
{"question": "would a model be likely to frequently enjoy the menu at Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Casual_Sex?\";\"Darcy_Sterling\";\"Suzanne_Pirret\";MyFreeCams.com"}
{"question": "does the history of Europe include the age of dinosaurs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Geological_history_of_Europe\";\"Geological_history_of_Europe\";Paleogene;Cenozoic"}
{"question": "was John George Bice's birthplace near Cornwall?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Bicton_House,_Devon\";\"Bicton_House,_Devon\";\"Buxton,_Maine\";\"John_Halifax,_Gentleman\""}
{"question": "do white blood cells outnumber red blood cells in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"White_blood_cell\";\"White_blood_cell\";Human;\"White_blood_cell\""}
{"question": "was Edward II crucial to England's victory at Battle of Falkirk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Treaty_of_Edinburgh\u2013Northampton\";\"Edward_II_of_England\";\"Robert_the_Bruce\";\"Edward_II_of_England\""}
{"question": "did Holy Land belong to Adamu's tribe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Holy_Land\";Kahndaq;\"Tribe_of_Shabazz\";\"Horn_of_Africa\""}
{"question": "does Ahura Mazda have a rivalry with Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Twins_in_mythology\";\"Ahura_Mazda\";\"Ahura_Mazda\";\"Ahura_Mazda\""}
{"question": "is an Eastern chipmunk likely to die before seeing two leap years?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sonoma_chipmunk\";De-extinction;Narwhal;Reindeer"}
{"question": "would the owners of the company Peter Griffin works for need barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Grainger_Games\";\"Grainger_Games\";Weatherfield;\"Anthony_Habgood\""}
{"question": "is the CIA part of the Department of Defense?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_States_Secret_Service\";\"United_States_Secret_Service\";\"Central_Intelligence_Agency\";\"Directorate_of_Operations_(CIA)\""}
{"question": "was Harry Potter and the Philosopher's Stone popular during the great depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Harry_Potter_and_the_Philosopher's_Stone_(film)\";\"Harry_Potter_and_the_Philosopher's_Stone_(film)\";\"Great_Depression_in_the_United_Kingdom\";\"The_Fabulous_Philosopher's_Stone\""}
{"question": "is a slime mold safe from cerebral palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "Phisoderm;\"Flubber_(material)\";\"Non-Newtonian_fluid\";\"Contagium_vivum_fluidum\""}
{"question": "is British Airways the air force of the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "British", "extracted_entity": "\"Royal_Air_Force\";\"British_Overseas_Airways_Corporation\";\"Royal_Navy\";\"History_of_British_Airways\""}
{"question": "has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Nobel_Charitable_Trust\";\"Nobel_Prize\";\"Nobel_Prize\";\"Nobel_Prize_controversies\""}
{"question": "are aggressive bumblebees suicidal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "g", "extracted_entity": "\"Fear_of_bees\";Bumblebee;Bumblebee;Bumblebee"}
{"question": "is Edward Snowden in hiding from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Edward_Snowden\";\"Edward_Snowden_asylum_in_Russia\";\"Edward_Snowden\";\"Edward_Snowden\""}
{"question": "can the Great Depression be treated with Prozac?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pro", "extracted_entity": "\"Listening_to_Prozac\";Psoriasis;Fluoxetine;\"Chronic_obstructive_pulmonary_disease\""}
{"question": "can photography be considered abstract art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Abstract_photography\";\"Op_art\";\"Abstract_photography\";\"Op_art\""}
{"question": "could a bee hummingbird balance a scale with a single pea on it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Hummingbird;Bee;\"Bee_hummingbird\";\"Bombus_ternarius\""}
{"question": "is Disney associated with Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Anaheim,_California\";\"Southern_California\";\"Greater_Los_Angeles\";\"Los_Angeles_County,_California\""}
{"question": "did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Judith_Rodin\";\"Karl_Weintraub\";\"Judith_Rodin\";\"Catharine_Beecher\""}
{"question": "is a doctorate required to teach at a SUNY School?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Doctor_of_Education\";\"Doctor_of_Education\";\"Master's_degree\";\"Doctor_of_Education\""}
{"question": "does the swastika have positive uses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Swastika;Swastika;Swastika;Sauwastika"}
{"question": "does Nintendo's link ever see an astronomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Gamerz_Heaven\";\"Poke\u0301mon:_Jirachi\u2014Wish_Maker\";\"Poke\u0301mon_Channel\";\"If_I_See_You_in_My_Dreams\""}
{"question": "has Elon Musk's hairline changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Human_hair_color\";\"Tanis_Half-Elven\";\"Human_hair_color\";\"Batman:_Anarky\""}
{"question": "do American teams in National Hockey League outnumber Canadian teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"National_Hockey_League_rivalries\";\"National_Hockey_League_rivalries\";\"National_Hockey_League_rivalries\";\"National_Hockey_League_rivalries\""}
{"question": "did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Teri_Hatcher\";\"Teri_Hatcher\";\"Teri_Hatcher\";\"Teri_Hatcher\""}
{"question": "were all the materials to make a cannon known during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Technological_history_of_the_Roman_military\";\"Wuwei_Bronze_Cannon\";\"Greek_and_Roman_artillery\";\"Greek_and_Roman_artillery\""}
{"question": "is Cookie Monster's diet Paleo friendly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cookie_diet\";\"Barq's\";Lactitol;\"Oatmeal_Crisp\""}
{"question": "would it be impossible to seat every Chief Justice of the United States on a Boeing 737?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Boeing_747\";\"Hugh_John_Casey\";\"Boeing_747\";\"JetBlue_Flight_292\""}
{"question": "could Toyota stadium house people suffering homelessness in Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Toyota_Park\";\"Toyota_Park\";\"Toyota_Park\";\"Comerica_Park\""}
{"question": "can a martyr saint have been excommunicated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Justin_Martyr\";\"Heroic_virtue\";\"Seraphim_Chichagov\";\"Bartolo_Longo\""}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Smell_of_Reeves_and_Mortimer\";\"Scrooge_(1935_film)\";\"Scrooge,_or,_Marley's_Ghost\";\"Ebenezer_Scrooge\""}
{"question": "does a Disney princess on Broadway have red hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Disney_Princess\";\"Hair_(musical)\";\"Disney_Princess\";\"Disney_Princess\""}
{"question": "could Steven Spielberg send emails as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Sean_Lennon\";\"Facilitated_communication\";\"Shiva_Ayyadurai\";\"Letter_to_His_Father\""}
{"question": "does Felix Potvin have a position on a dodgeball team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fe\u0301lix_Potvin\";\"Fe\u0301lix_Potvin\";\"Vladimir_Potkin\";\"Fe\u0301lix_Potvin\""}
{"question": "would someone with leukophobia enjoy looking at the Flag of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Modern_display_of_the_Confederate_flag\";\"Modern_display_of_the_Confederate_flag\";\"Flag_of_the_United_States\";\"Gun_culture_in_the_United_States\""}
{"question": "would 1943-S penny be good for making silverware?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a", "extracted_entity": "\"Penny_(United_States_coin)\";\"Sterling_silver\";\"Penny_(United_States_coin)\";\"Penny_(United_States_coin)\""}
{"question": "did mongoose come from later period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Josephoartigasia_monesi\";\"Black_mongoose\";Rhinoceros;\"Indian_rhinoceros\""}
{"question": "would it be possible to fit a football field in Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Torero_Stadium\";\"Virginia_Beach_Sportsplex\";\"Colley_Track/Soccer_Complex\";\"CenturyLink_Field\""}
{"question": "is Issac Newton often associated with a red fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Flower_of_Kent\";Sumac;\"Newton_Wonder\";\"Newtons_(cookie)\""}
{"question": "would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Kobe_Steel\";\"Transfield_Shipping_Inc_v_Mercator_Shipping_Inc\";\"Labor_market_of_Japan\";Mitsui"}
{"question": "can you buy spinal cord at Home Depot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Unlocking_the_Truth\";YourMechanic;\"Jerry_Harvey_(inventor)\";Thumbplay"}
{"question": "are some chiropractic manipulations dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Chiropractic;\"Chiropractic_controversy_and_criticism\";\"Chiropractic_controversy_and_criticism\";\"Chiropractic_treatment_techniques\""}
{"question": "were some people afraid of New Years Day coming in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Christmas_Eve_(2015_film)\";\"Do_They_Know_It's_Christmas?\";Christmas;\"Christmas_Eve\""}
{"question": "did travelers sing sea shanties on the Oregon Trail?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Sea_shanty\";\"Sea_shanty\";\"Sea_shanty\";\"Sea_shanty\""}
{"question": "do members of the Supreme Court of the United States have longer terms than most senators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"United_States\";\"Term_of_office\";\"Term_limits_in_the_United_States\";\"Political_positions_of_Donald_Trump\""}
{"question": "paleography hypothetically helps to understand Cthulhu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "pale", "extracted_entity": "Paleoradiology;Paleoradiology;Paleoradiology;Paleoradiology"}
{"question": "is surfing popular in Des Moines, Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Des_Moines,_Washington\";\"Tourmaline_Surfing_Park\";\"Steamer_Lane\";\"Long_Beach,_California\""}
{"question": "is Statue of Unity hypothetically more level with Statue of Liberty than Lighthouse of Alexandria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Statue_of_Liberty\";\"Hudson_Heights,_Manhattan\";\"The_Colossus_of_Rhodes_(film)\";\"Statue_of_Liberty_National_Monument\""}
{"question": "is CEO of Nissan an internationally wanted fugitive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Carlos", "extracted_entity": "\"FBI_Ten_Most_Wanted_Fugitives\";\"Aaron_Davidson\";\"Mariano_Jinkis\";\"Volkswagen_emissions_scandal\""}
{"question": "has type of political association Pompey had with Caesar influenced reality TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Posca_(Rome_character)\";\"Quintus_Valerius_Pompey\";\"Quintus_Valerius_Pompey\";\"Personal_life_of_Cicero\""}
{"question": "would Rime of the Ancient Mariner make a good sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Rime_of_the_Ancient_Mariner\";\"The_Rime_of_the_Ancient_Mariner\";\"Samuel_Taylor_Coleridge\";\"Oral-formulaic_composition\""}
{"question": "do Snow White dwarves best The Hobbit dwarves in battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Second_Battle_of_Beruna\";\"Battle_of_the_Hornburg\";\"First_Battle_of_Beruna\";\"Second_Battle_of_Beruna\""}
{"question": "can the history of art be learned by an amoeba?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Instruction_of_Amenemope\";\"Instruction_of_Amenemope\";\"Luba_art\";\"Education_in_ancient_Greece\""}
{"question": "can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Toyota_FCHV\";\"Urban_S\";\"Corvus_Energy\";\"Corvus_Energy\""}
{"question": "walt Disney dominated his amusement park peers at Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Walt", "extracted_entity": "\"Walt_Disney\";\"Walt_Disney:_Hollywood's_Dark_Prince\";\"Walt_Disney:_Hollywood's_Dark_Prince\";\"Criticism_of_The_Walt_Disney_Company\""}
{"question": "would human race go extinct without chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Cursed_for_Gold\";\"Ethylene_glycol\";Chlorobium;\"Amazon_rubber_boom\""}
{"question": "is the average bulk carrier ideal for transporting bromine at room temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"LNG_carrier\";\"Ore-bulk-oil_carrier\";\"Vacuum_insulated_evaporator\";\"Gas_carrier\""}
{"question": "can depression be mistaken for laziness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Laziness;\"History_of_depression\";\"History_of_depression\";\"Collective_depression\""}
{"question": "did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Easy_Rider\";\"Easy_Rider\";\"Bette_Davis\";\"Easy_Rider\""}
{"question": "did pirates who had scurvy need more Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Etiology;Scurvy;Scurvy;Scurvy"}
{"question": "could the leader of Heaven's Gate save images in JPEG format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Breath_of_Fire_III\";Guwange;Stillsong;\"Breath_of_Fire_IV\""}
{"question": "does the FDA require sell by dates using Roman Numerals?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Mandatory_labelling\";\"Test_and_hold\";\"Test_and_hold\";\"Consumer_Packaging_and_Labeling_Act\""}
{"question": "would Kylee Jenner ask for no cream in her coffee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sonnet_116\";\"Nevertheless,_she_persisted\";\"Senna_occidentalis\";\"Nevertheless,_she_persisted\""}
{"question": "does Nicole Kidman know any Scientologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nicole", "extracted_entity": "\"Jesus_in_Scientology\";\"Jada_Pinkett_Smith\";\"Nicole_Kidman\";\"Faith_Salie\""}
{"question": "is Lionel Richie related to Sheila E?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Lionel_Richie\";\"Lionel_Richie\";\"Peter_Michael_Escovedo\";\"Sheila_E.\""}
{"question": "was a nuclear bomb used in the Napoleonic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"France_and_weapons_of_mass_destruction\";\"Pressure_cooker_bomb\";\"France_and_weapons_of_mass_destruction\";\"Iraq_and_weapons_of_mass_destruction\""}
{"question": "is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cristo\u0301va\u0303o_da_Gama\";Catoblepas;\"No_Ordinary_Hero:_The_SuperDeafy_Movie\";\"The_Flyting_of_Dumbar_and_Kennedie\""}
{"question": "can you buy a fair trade laptop?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Pawnbroker;Laptop;Pawnbroker;\"LAN_gaming_center\""}
{"question": "was Noah concerned with buoyancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Noah", "extracted_entity": "\"Neutral_buoyancy\";Buoyancy;Buoyancy;Buoyancy"}
{"question": "is a watchmaker likely to be able to fix an Apple Watch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Clockmaker;Watchmaker;Watchmaker;\"Apple_Watch\""}
{"question": "are sables related to wolverines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Coywolf;Sable;Coywolf;\"Silver_fox_(animal)\""}
{"question": "are some Brazilian Navy ships built in Britian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Brazilian_Navy\";\"Brazilian_Naval_School\";\"Brazilian_Navy\";\"Forte_de_Sa\u0303o_Lourenc\u0327o\""}
{"question": "could Carl Friedrich Gauss speak to someone 100 miles away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Daniel_Dunglas_Home\";\"Daniel_Dunglas_Home\";\"E\u0301tienne_Lombard\";\"Bell_Homestead_National_Historic_Site\""}
{"question": "in Hey Arnold, did any characters stay on a porch all the time?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Hey_Arnold!\";\"Hey_Arnold!\";\"Hey_Arnold!\";\"Hey_Arnold!\""}
{"question": "has Johns Hopkins University always treated subjects ethically?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Johns_Hopkins_University\";\"Johns_Hopkins_University\";\"Johns_Hopkins_University\";\"Johns_Hopkins_University\""}
{"question": "is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bloodstain_pattern_analysis\";\"Bloodstain_pattern_analysis\";\"Bloodstain_pattern_analysis\";\"Movat's_stain\""}
{"question": "do black-tailed jackrabbits fear the European wildcat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"European_wildcat\";\"White-tailed_jackrabbit\";\"British_big_cats\";Wildcat"}
{"question": "has Kelly Clarkson outsold season 4 American Idol winner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Carrie", "extracted_entity": "\"American_Idol_(season_4)\";\"American_Idol_(season_4)\";\"American_Idol_(season_4)\";\"American_Idol_(season_4)\""}
{"question": "is Hermes equivalent to the Roman god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hermes_(sculpture)\";\"Hermes_(Marvel_Comics)\";\"Hermes_(Marvel_Comics)\";\"Mercury_Man\""}
{"question": "can an elite runner circle the Pyrenees in one day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mauro_Prosperi\";\"Kevin_Lin\";\"Mauro_Prosperi\";Pedestrianism"}
{"question": "is the Golden eagle considered a scavenger bird?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Peregrine_falcon\";\"The_Peregrine_Fund\";\"Golden_eagle\";\"Golden_eagle\""}
{"question": "can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Keyboard_layout\";\"Dvorak_Simplified_Keyboard\";\"Romanian_keyboard_layout\";\"Keyboard_layout\""}
{"question": "are grapes essential to winemaking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Winemaking;Grape;Must;\"History_of_the_wine_press\""}
{"question": "would the Cookie Monster decline an offer of free Keebler products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Moody_Foodie\";\"E-I-E-I-(Annoyed_Grunt)\";\"Medicinal_Fried_Chicken\";\"The_Heartbroke_Kid\""}
{"question": "did Jeremy Irons master sweep picking as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Zack_Grumet\";\"John_Wayne_Gacy\";\"Robert_Black_(serial_killer)\";\"Aaron_Banks\""}
{"question": "for Hostas to look their best, do they need lots of chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Turacoverdin;Musophaga;Cestrum;\"Gymnothorax_pictus\""}
{"question": "has spinach been a source of power in a comic movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Castor_oil\";Spinach;\"You_Can't_Do_That_on_Television\";Illeism"}
{"question": "can you see the Statue of Freedom from the Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Statue_of_Liberty\";\"Statue_of_Liberty\";\"Statue_of_Liberty\";\"Statue_of_Freedom\""}
{"question": "does Ludacris have Greek heritage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lycomedes;\"Hippolytus_(son_of_Theseus)\";Hippolyta;\"Alexander_Helios\""}
{"question": "will bumblebees derail the United States presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Political_Google_bombs_in_the_2004_U.S._Presidential_election\";Bumblebee;\"Bombus_huntii\";\"Bird_strike\""}
{"question": "does Rahul Dravid belong to the family Gryllidae?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gryllus;\"Gryllus_firmus\";Gryllacrididae;Gryllacrididae"}
{"question": "is Metallica protective over their music?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Come_Out_and_Play_(song)\";Metallica;\"Invasion_of_Privacy_(album)\";Beatallica"}
{"question": "does Dean Cain have less days to birthday than Will Ferrell every 4th of July?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stephen_Colbert_(character)\";\"I_Still_Remember\";\"Can't_Forget_About_You\";\"Dean_Cain\""}
{"question": "is Lord Voldemort associated with a staff member of Durmstrang?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Lord_Voldemort\";\"Toby_Esterhase\";\"Draco_Malfoy\";\"Ministry_of_Magic\""}
{"question": "are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Descent_from_Genghis_Khan\";\"Descent_from_Genghis_Khan\";\"Descent_from_Genghis_Khan\";Borjigin"}
{"question": "is starving Hamas agent eating pig bad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cooking_in_the_Danger_Zone\";\"Contagion_(film)\";\"Blockade_of_the_Gaza_Strip\";\"Meal,_Ready-to-Eat\""}
{"question": "would Ringo Starr avoid the pot roast at a restaurant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ringo_Starr\";\"Ringo_Starr\";Gauloises;\"Ringo_Starr\""}
{"question": "was ship that recovered Apollo 13 named after a World War II battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "VF-13;\"USS_The_Sullivans_(DD-537)\";\"Apollo_13\";\"USS_Cavallaro_(APD-128)\""}
{"question": "are both founders of Ben & Jerry's still involved in the company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ben_&_Jerry's\";\"Ben_&_Jerry's\";\"Ben_&_Jerry's\";\"Ben_Cohen_(businessman)\""}
{"question": "is a krabby patty similar to a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "Cheeseburger;Cheeseburger;\"Patty_melt\";\"Timeline_of_United_States_inventions_(1890\u20131945)\""}
{"question": "did Beethoven enjoy listening to EDM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Mozart_effect\";Chillwave;\"Good_Vibrations\";\"Mozart_effect\""}
{"question": "is it true that gay male couples cannot naturally reproduce?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Celibacy_syndrome\";\"Mixed-orientation_marriage\";Pseudogamy;\"He_never_married\""}
{"question": "could an American confuse breakfast in British cuisine for dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sandwich,_Kent\";Sandwich;\"Eliza_Acton\";\"The_Art_of_Cookery_Made_Plain_and_Easy\""}
{"question": "can Kit & Kaboodle hypothetically help someone past the Underworld gates?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Help!..._It's_the_Hair_Bear_Bunch!\";\"The_Rescuers\";\"Midnight_Rescue!\";Catnapped!"}
{"question": "are two cans of Campbell's Soup a day good for hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Campbell_Soup_Company\";\"Campbell_Soup_Company\";\"Chicken_soup\";Soup"}
{"question": "are there any official American knights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"American_Cardinals_Dinner\";\"Authorized_foreign_decorations_of_the_United_States_military\";\"Military_Order_of_the_Loyal_Legion_of_the_United_States\";\"Polish_Legion_of_American_Veterans\""}
{"question": "is the Foreign and Commonwealth Office a European political agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Foreign_and_Commonwealth_Office\";\"Representative_of_the_European_Union,_London\";\"Minister_of_State_for_Europe\";\"Foreign_and_Commonwealth_Office\""}
{"question": "do gorillas fight with panda bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bear_attack\";\"Animals_in_professional_wrestling\";Bear;\"Kuma_and_Panda\""}
{"question": "would lumberjacks get full after eating three dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Three_men_make_a_tiger\";\"Three_men_make_a_tiger\";\"Monkeys_in_Chinese_culture\";\"Jolt_Cola\""}
{"question": "is being 5 year Capital One Venture member more cost effective than being in Church of Satan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Incel;\"Flag_Building\";\"Louis_Farrakhan\";\"Venture_capital_trust\""}
{"question": "tata Hexa can accomodate every Spice Girl?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"RJ_Ophelia\";Samsaya;\"Anuja_Chauhan\";\"Melody_Hossaini\""}
{"question": "could common warthog be useful for scrimshaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Common_warthog\";\"Common_warthog\";Wart-biter;\"Common_warthog\""}
{"question": "is breast cancer associated with a ribbon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Evelyn_Lauder\";\"Pink_ribbon\";\"Pink_ribbon\";\"Pink_ribbon\""}
{"question": "would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Nuno_Gomes_(diver)\";\"Nuno_Gomes_(diver)\";\"Nuno_Gomes_(diver)\";\"Pascal_Bernabe\u0301\""}
{"question": "is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Seal_hunting\";\"Seal_hunting\";\"Harp_seal\";\"Seal_hunting\""}
{"question": "was a Tiny House ceiling out of Osama bin Laden's reach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Majlis_al_Jinn\";\"Osama_bin_Laden's_compound_in_Abbottabad\";\"Osama_bin_Laden's_house_in_Khartoum\";Ceiling"}
{"question": "can you conduct surveillance from a teddy bear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Cellphone_surveillance\";Surveillance;Surveillance;Surveillance"}
{"question": "does table tennis use prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "does", "extracted_entity": "\"Polite_number\";Primero;Mahjong;\"Three_player_mahjong\""}
{"question": "was the Carnation Revolution the deadliest revolution in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Carnation_Revolution\";\"Carnation_Revolution\";\"Carnation_Revolution\";\"Carnation_Revolution\""}
{"question": "is the Muslim world hostile to Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Anti-Zionism;\"Religious_anti-Zionism\";\"Islamic\u2013Jewish_relations\";\"War_against_Islam_conspiracy_theory\""}
{"question": "is the saltwater crocodile less endangered than the European otter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Giant_otter\";\"Eurasian_otter\";Otter;\"Eurasian_otter\""}
{"question": "can someone from New England profit by growing coffee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Planter_class\";\"History_of_coffee\";\"Coffee_in_world_cultures\";\"Black_Coffee_(2007_film)\""}
{"question": "could a dichromat probably easily distinguish chlorine gas from neon gas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Neon_lighting\";\"Neon_lamp\";\"Color_blindness\";Neon"}
{"question": "nATO doesn't recognize double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Crossed_hands_(gesture)\";\"Crossed_hands_(gesture)\";\"National_flag\";\"Enlargement_of_NATO\""}
{"question": "did Julius Caesar read books on Pharmacology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ancient_Rome_and_wine\";Farmakonisi;\"Medicine_in_ancient_Rome\";\"Medicine_in_ancient_Rome\""}
{"question": "does water have viscosity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Viscosity;Friction;Water;\"Properties_of_water\""}
{"question": "was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Los", "extracted_entity": "\"Venues_of_the_1984_Summer_Olympics\";\"Los_Angeles_Stadium_at_Hollywood_Park\";\"Grand_Olympic_Auditorium\";\"Grand_Olympic_Auditorium\""}
{"question": "would you spend less on your food at Aldi than at Whole Foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"Whole_Foods_Market\";\"Whole_Foods_Market\";\"Whole_Foods_Market\";Lunchables"}
{"question": "are Scottish people Albidosi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Fabada_asturiana\";\"Falkirk_F.C.\";\"Fabada_asturiana\";Alheira"}
{"question": "are System of a Down opposed to globalization?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Negotiated_cartelism\";\"Globalization_and_Its_Discontents\";\"Polarity_(international_relations)\";\"Globalization_and_Its_Discontents\""}
{"question": "was Alexander the Great baptized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Russia;\"Vladimir_the_Great\";\"Mikhail_Gorbachev\";\"In_the_Footsteps_of_Alexander_the_Great\""}
{"question": "did Demi Lovato's ancestors help turn maize into popcorn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Maize;Popcorn;Maize;\"Seed_ball\""}
{"question": "would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "\"Lionhead_(goldfish)\";\"Common_goldfish\";Goldfish;\"Freshwater_aquarium\""}
{"question": "is Pearl Harbor the mythical home of a shark goddess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pearl_Harbor\";\"Pearl_Harbour,_New_Zealand\";\"Pearl_Harbour,_New_Zealand\";\"Halawa,_Hawaii\""}
{"question": "would Elon Musk be more likely to know about astrology than physics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Elon_Musk:_Tesla,_SpaceX,_and_the_Quest_for_a_Fantastic_Future\";\"Elon_Musk\";\"Elon_Musk:_Tesla,_SpaceX,_and_the_Quest_for_a_Fantastic_Future\";\"Elon_Musk\""}
{"question": "can paresthesia be caused by a white pigment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "p", "extracted_entity": "\"Retinitis_pigmentosa\";\"Allergic_conjunctivitis\";\"Dermatopathia_pigmentosa_reticularis\";\"White_dot_syndromes\""}
{"question": "could Charlie Bucket be a hotel manager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Bucket_List\";\"How_to_Marry_a_Millionaire_(TV_series)\";\"George_Webb_(actor)\";\"Prince_Monolulu\""}
{"question": "can children be soldiers in the US Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Military_recruitment\";\"Children_in_the_military\";\"Children_in_the_military\";\"Children_in_the_military\""}
{"question": "is Jack Black's height enough to satisfy Coronavirus distancing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Julius_No\";\"Tall_Man_(Phantasm)\";\"Jack_Radcliffe\";\"Black_Dynamite\""}
{"question": "are peaches best eaten when firm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pe", "extracted_entity": "\"Peach_(fruit)\";Lotus-eaters;Peach;Peach"}
{"question": "was Charles Manson's body unwanted?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Unwanted\";\"Charles_Manson\";Circumcision;\"Charles_F._Gunther\""}
{"question": "was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Suicide_of_Kurt_Cobain\";\"Kurt_Cobain\";\"Suicide_of_Kurt_Cobain\";\"Courtney_Love\""}
{"question": "can a human eat an entire 12-lb roast turkey in an hour? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Turkey_fryer\";\"This_is_why_you're_fat\";\"Turkey_in_a_Can\";\"Jerusalem_mixed_grill\""}
{"question": "would a house full of aloe vera hypothetically be ideal for Unsinkable Sam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Albizia_saman\";\"Albizia_saman\";\"Wartime_Farm\";\"The_Alan_I_W_Frank_House\""}
{"question": "would dual-energy X-ray absorptiometry be useful if performed on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Dual-energy_X-ray_absorptiometry\";\"Dual-energy_X-ray_absorptiometry\";\"Dual-energy_X-ray_absorptiometry\";\"Dual-energy_X-ray_absorptiometry\""}
{"question": "would Harvey Milk have approved of Obama?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Harvey_Milk\";\"Harvey_Milk_Day\";\"Public_image_of_Mitt_Romney\";\"Barack_Obama_citizenship_conspiracy_theories\""}
{"question": "would a jumping spider need over half a dozen contact lenses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Spider;\"Glass_Spider_Tour\";Mysterio;\"Spider_anatomy\""}
{"question": "are seasons of Survivor surpassed by number of Ancient Greek letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Survivor:_Millennials_vs._Gen_X\";\"Survivor_Greece\";\"Survivor_(U.S._TV_series)\";\"Survivor_(franchise)\""}
{"question": "do citizens of Cheshire sing La Marseillaise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Song_of_the_Falklands\";\"God_Save_the_Queen\";\"La_Marseillaise\";\"March_of_the_Malvinas\""}
{"question": "are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Anorexia_nervosa\";Diabulimia;Glucagonoma;\"Bulimia_nervosa\""}
{"question": "in teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Management_of_depression\";\"Management_of_depression\";Antidepressant;Antidepressant"}
{"question": "could eating Chinook salmon help Ryan Reynolds?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chin", "extracted_entity": "\"Brant_Pinvidic\";\"Ryan_Knighton\";\"Homaro_Cantu\";\"Brant_Pinvidic\""}
{"question": "would kaffir lime be good in a White Russian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Kaffir_lime\";\"Kaffir_lime\";Papirovka;Papirovka"}
{"question": "is entire Common Era minuscule to lifespan of some trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Old-growth_forest\";\"Old-growth_forest\";\"Old-growth_forest\";\"Treefall_gap\""}
{"question": "did Charlemagne have a bar mitzvah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Char", "extracted_entity": "Hanukkah;\"Bar_and_Bat_Mitzvah\";\"Bar_and_Bat_Mitzvah\";\"Bar_and_Bat_Mitzvah\""}
{"question": "did Monty Python write the Who's on First sketch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Monty_Python_Instant_Record_Collection\";\"Monty_Python's_Previous_Record\";\"The_Colonel_(Monty_Python)\";\"Who's_on_First?\""}
{"question": "in geometry terms, is the Royal Observatory in Greenwich similar to a yield sign?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Crux;\"Royal_Observatory,_Greenwich\";\"Meridian_circle\";\"Prime_meridian_(Greenwich)\""}
{"question": "did Sugar Ray Robinson win a fight against Canelo Alvarez?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Carmen_Basilio\";\"Sugar_Ray_Leonard_vs._Roberto_Dura\u0301n_II\";\"Sugar_Ray_Robinson\";\"Sugar_Ray_Robinson\""}
{"question": "can an asteroid be linked with virginity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"16_Psyche\";\"16_Psyche\";Volcanology;\"Asteroid_mining\""}
{"question": "would a Yeti be likely to have prehensile limbs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Yeti;Yeti;Yeti;\"Oriental_Yeti\""}
{"question": "is groundhog day used as a global season indicator? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Groundhog_Day\";\"Groundhog_Day\";\"Weather_lore\";\"Groundhog_Day\""}
{"question": "did mercenaries fight for England in the Glorious Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Recruitment_in_the_British_Army\";\"Scots_Army\";\"Hessian_(soldier)\";\"Swiss_mercenaries\""}
{"question": "was the sable depicted in Marvel comics anthropomorphic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mercury_Sable\";\"Silver_Sable\";\"Silver_Sable\";\"Jon_Sable\""}
{"question": "are twinkies considered artisan made products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Artisanal_food\";\"Artisanal_food\";\"Craft_service\";\"Craft_production\""}
{"question": "were mollusks an ingredient in the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Purple;\"Koufonisi_(Crete)\";Muricidae;Purple"}
{"question": "is Christianity better for global warming than Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Collision_(2009_film)\";\"Christianity_and_Neopaganism\";Satanism;\"Collision_(2009_film)\""}
{"question": "was Lord Voldemort taught by Professor Dumbledore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Albus_Dumbledore\";\"T._P._Wiseman\";\"Albus_Dumbledore\";Professor"}
{"question": "would costumes with robes and pointy hats be helpful for Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Illusory_truth_effect\";Baldric;\"Modern_dress\";\"Brian_Feldman_(artist)\""}
{"question": "are the knights in the Medieval Times show not authentic knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Authenticity_(reenactment)\";\"Medieval_art\";\"Authenticity_(reenactment)\";\"Knights_and_Knaves\""}
{"question": "could William Franklyn-Miller win a 2020 Nascar Cup Series race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"It_Can't_Happen_Here\";\"Monster_Energy_NASCAR_Cup_Series\";\"Monster_Energy_NASCAR_Cup_Series\";\"2018_Monster_Energy_NASCAR_Cup_Series\""}
{"question": "did Al-Farabi ever meet Mohammed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al", "extracted_entity": "\"Fadl_ibn_Abbas\";\"Warith_Deen_Mohammed\";\"Abd_al-Fattah_Abu_Ghudda\";\"Khalid_Sheikh_Mohammed\""}
{"question": "did the Pearl Harbor attack occur during autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Attack_on_Pearl_Harbor\";\"Attack_on_Pearl_Harbor\";\"Attack_on_Pearl_Harbor\";\"Japan_during_World_War_II\""}
{"question": "have the Israelis played the Hammerstein Ballroom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Palestinian_rocket_attacks_on_Israel\";\"Boycotts_of_Israel\";\"Sands_Hotel_and_Casino\";\"King_David_Hotel\""}
{"question": "if you're reducing salt intake, are olives a healthy snack?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ol", "extracted_entity": "\"Healthy_diet\";\"Mediterranean_diet\";\"Deep_frying\";\"Mediterranean_diet\""}
{"question": "do pediatricians perform abortions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Neonatology;Neonatology;\"Natural_childbirth\";\"Fetal_surgery\""}
{"question": "could casualties from deadliest war rival France's population?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Napoleonic_Wars_casualties\";\"Civilian_casualty_ratio\";War;\"Civilian_casualty_ratio\""}
{"question": "does Lorem ipsum backwards fail to demonstrate alliteration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Lorem_ipsum\";\"Diaeresis_(prosody)\";\"Lorem_ipsum\";\"Speech_error\""}
{"question": "can you see Stonehenge from a window in Dusseldorf?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Manhattanhenge;\"Tina_Juretzek\";\"The_Eye_of_Silence\";\"John_Constable\""}
{"question": "can Reiki be stored in a bottle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ryu\u0304gu\u0304-jo\u0304\";\"162173_Ryugu\";\"Ryu\u0304gu\u0304-jo\u0304\";Suikinkutsu"}
{"question": "would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Toyota", "extracted_entity": "\"Toyota_Supra_in_motorsport\";\"Toyota_Supra\";\"Car_of_Tomorrow\";\"Car_of_Tomorrow\""}
{"question": "would Tom Cruise ever insult L. Ron Hubbard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tom_Cruise:_Unauthorized\";\"Tom_Cruise_Purple\";\"Tom_Cruise:_Unauthorized\";\"Tom_Cruise\""}
{"question": "does Lemon enhance the flavor of milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Lemon_liqueur\";Buttermilk;Lemon;Saponarin"}
{"question": "was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Harry_Potter_and_the_Chamber_of_Secrets_(film)\";\"Harry_Potter_and_the_Chamber_of_Secrets\";\"Harry_Potter_(film_series)\";\"The_House_with_a_Clock_in_Its_Walls_(film)\""}
{"question": "is the United States the largest exporter of Fair Trade products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"North_American_Free_Trade_Agreement\";\"Foreign_trade_of_the_United_States\";\"Fair_trade_cocoa\";\"Fair_Trade_USA\""}
{"question": "is Snickers helpful for weight loss?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "Simeticone;MealEnders;Hyoscine;Simeticone"}
{"question": "could the Jackson 5 play a full game of rugby with each other?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"All_Together_Now_(The_Farm_song)\";\"All_Together_Now_(The_Farm_song)\";\"East_Africa_rugby_union_team\";\"The_New_Edition_Story\""}
{"question": "did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Marvin_Gay_Sr.\";\"Marvin_Gaye\";\"Death_of_Marvin_Gaye\";\"Death_of_Marvin_Gaye\""}
{"question": "did the founders of the biggest city in Orange County, California speak Italian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Andrew_Glassell\";\"Andrew_Glassell\";\"Tony_Pizzo\";\"Joseph_Libbey_Folsom\""}
{"question": "is an ammonia fighting cleaner good for pet owners?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Working_Cats_Program\";\"Skunks_as_pets\";\"Pit_bull\";\"Domestic_rabbit\""}
{"question": "could you buy Hershey's Kisses in red foil with farthings after 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hershey's_Kisses\";\"Hershey's_Kisses\";\"Hershey's_Kisses\";\"Hershey's_Kisses\""}
{"question": "could morphine cure HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Kumdang-2;Kumdang-2;Ubhejane;Kemron"}
{"question": "would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Quiet", "extracted_entity": "\"Studio_C\";\"Ayodele_Awojobi\";\"The_Quiet_Game\";\"Dead_Man_on_Campus\""}
{"question": "would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Natural_Park_of_Penyal_d'Ifac\";Eurowheel;\"Parque_Ross\";\"Giant's_Causeway\""}
{"question": "would the chef at La Grenouille find salsa to be a strange request?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"SMAP\u00d7SMAP\";\"Hiroyuki_Sakai\";\"That's_What_I_Like_(Bruno_Mars_song)\";\"Sabrina_Ghayour\""}
{"question": "was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Michael_Crichton\";\"Michael_Chiklis\";\"George_W._Gregory\";\"William_J._Donovan\""}
{"question": "would toast for a vegan have margarine instead of butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Toast;\"Reuben_sandwich\";\"More-with-Less_Cookbook\";Cenovis"}
{"question": "did any cultures associate celery with death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Celery;Celery;Celery;Capnomancy"}
{"question": "are emus related to elks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Re'em\";Horse;\"American_mountain_deer\";\"Re'em\""}
{"question": "is the Forbidden City host to a wooden rollercoaster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Harry_Potter_and_the_Forbidden_Journey\";\"Haunted_attraction_(simulated)\";\"Harry_Potter_and_the_Forbidden_Journey\";\"The_Wizarding_World_of_Harry_Potter_(Universal_Studios_Japan)\""}
{"question": "are slime lilies in a different scientific family than asparagus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Asparagales;Nymphaeaceae;\"Families_of_Asparagales\";Liliaceae"}
{"question": "in order to work in district management, does one need a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Car_guard\";\"Valet_boy\";\"Car_guard\";Houseboy"}
{"question": "can you buy Reddit at Walmart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Loopt;PayAnywhere;Uniregistry;Sendle"}
{"question": "if you have black hair and want red hair, do you need bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Human_hair_color\";Red;\"Red_hair\";\"Human_hair_color\""}
{"question": "is a curling iron necessary in curling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Curling;\"Hair_iron\";Curling;Horseshoe"}
{"question": "was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Old_Mother_Riley_in_Paris\";\"Something_Fresh\";\"Something_Fresh\";\"Something_Fresh\""}
{"question": "was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Mascarene_parrot\";\"History_of_the_Pacific_Islands\";\"History_of_Tuvalu\";\"History_of_New_Zealand\""}
{"question": "did Lamarck and Darwin agree about the origin of species diversity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Common_descent\";\"On_the_Origin_of_Species\";\"On_the_Origin_of_Species\";\"On_the_Origin_of_Species\""}
{"question": "if you were at an Apple store, would most of the computers be running Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "most", "extracted_entity": "\"Apple_Store\";\"Apple_Store\";Ubuntu;\"ITunes_Store\""}
{"question": "can you make an MP3 from the Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Soundtracker_(music_streaming)\";\"MP3_player\";\"Shorten_(file_format)\";\"Tidal_(service)\""}
{"question": "are there any chives hypothetically good for battling vampires?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chives;Cimaruta;Wamphyri;Dragaera"}
{"question": "could a Diwali celebration feature a crustacean?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dahi_Handi\";\"Celebration_(Alaska_festival)\";\"Eid_al-Fitr\";Boita"}
{"question": "do astronomers write horoscopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Horoscope;\"Celestial_cartography\";Horoscope;\"Meteorological_astrology\""}
{"question": "is the name of a mythical creature also the name of a Small Solar System body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mesoplanet;\"Small_Gods\";Mesoplanet;Plutoid"}
{"question": "did Medieval English lords engage in fair trade with peasants?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Competition_law\";\"Gift_economy\";\"History_of_competition_law\";Churl"}
{"question": "are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Natural_gas_prices\";\"Economic_effects_of_the_Deepwater_Horizon_oil_spill\";\"Lowering_Gasoline_Prices_to_Fuel_an_America_That_Works_Act_of_2014\";\"North_Dakota_oil_boom\""}
{"question": "do anatomical and symbolic hearts look remarkably different?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "anatom", "extracted_entity": "\"Heart_(symbol)\";\"Heart_(symbol)\";\"Heart_(symbol)\";\"Heart_(symbol)\""}
{"question": "are you more likely to find bipolar disorder in a crowd than diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Diabulimia;\"Diabetes_mellitus\";\"Diabetic_hypoglycemia\";\"Gestational_diabetes\""}
{"question": "does having lip piercings lead to more expensive dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dental_implant\";\"Human_tooth\";Malocclusion;\"Dental_floss\""}
{"question": "are most mall Santa Claus actors white?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Most", "extracted_entity": "\"Santa_Claus_in_film\";\"White_Liar\";\"Christmas_in_Wonderland\";\"White_&_Nerdy\""}
{"question": "does the Taco Bell kitchen contain cinnamon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Flavored_syrup\";\"Blue_Bell_Creameries\";Tiramisu;\"Old-fashioned_doughnut\""}
{"question": "could white rice go rancid before sesame seeds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gibberellin;\"White_rice\";\"Mieum_(food)\";\"Ammi_majus\""}
{"question": "can you listen to the entire Itunes song catalog in one year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"ITunes_Store\";\"ITunes_Store\";\"ITunes_Store\";\"ITunes_Store\""}
{"question": "do mollymawks live where albatrosses cannot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mollymawk;Mollymawk;Swallow;\"Atlantic_yellow-nosed_albatross\""}
{"question": "would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hay_diet\";\"Abby_Rosmarin\";\"Abby_Rosmarin\";\"Non-celiac_gluten_sensitivity\""}
{"question": "are Mayors safe from harm from the federal government?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Citizens_for_Self-Governance\";\"Citizens_for_Self-Governance\";\"United_States_Conference_of_Mayors\";\"Cities_for_Climate_Protection_program\""}
{"question": "is it normal to see a red panda in Shanghai outside of a zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Beijing_Zoo\";\"Ling_Ling_(giant_panda)\";\"Nishiyama_Zoo\";\"Panda_diplomacy\""}
{"question": "would a human following a hyena diet be unwelcome at a vegan festival?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Going_Ape\";Man-eater;\"Buddhist_vegetarianism\";\"Hungry_ghost\""}
{"question": "can DRL Racer X drone get across Brooklyn Bridge in 18 seconds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Drone_Racing_League\";\"Electric_drag_racing\";\"Drone_Racing_League\";\"CO2_dragster\""}
{"question": "would Temujin hypothetically be jealous of Charlemagne's conquests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Poher;Naimon;\"Huon_of_Bordeaux\";\"Le_Pe\u0300lerinage_de_Charlemagne\""}
{"question": "would a bodybuilder enjoy wearing a cast for several weeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Flex_Lewis\";\"Flex_Lewis\";\"Doug_Tracht\";\"Eric_Bana\""}
{"question": "can you carry a Chrysler in a laptop bag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Dodge_Caravan\";\"Dodge_Charger\";\"So_Round,_So_Firm,_So_Fully_Packed\";\"Commercial_Utility_Cargo_Vehicle\""}
{"question": "is Home Depot a one stop shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Home_Depot\";\"The_Home_Depot\";\"The_Home_Depot\";QuikTrip"}
{"question": "is the Holy Land important to Eastern religions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Religions_of_the_ancient_Near_East\";\"Church_of_the_East\";\"Middle_East\";\"History_of_Eastern_Christianity\""}
{"question": "is immersion in virtual reality testable on cnidarians before humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Immersion_(virtual_reality)\";\"Applications_of_virtual_reality\";\"VR_Kanojo\";\"Immersion_(virtual_reality)\""}
{"question": "did Jack Dempsey ever witness Conor McGregor's fights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Reg_Gorman\";\"Conor_McGregor\";\"Conor_McGregor\";\"Exhibition_fight\""}
{"question": "are black and white prison uniforms made to resemble a zebra?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Prison_uniform\";\"Battle_Dress_Uniform\";Zebra;\"Ice_Station_Zebra\""}
{"question": "would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Infant_Bacchus\";\"Bacchus_(grape)\";\"Bacchus_(grape)\";Bacchanalia"}
{"question": "did Malcolm X avoid eating ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Malcolm_X\";Egham;\"Green_Eggs_and_Ham\";\"Ham_and_eggs\""}
{"question": "would a snakebite hypothetically be a threat to T-1000?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tapsnake;\"Human_uses_of_reptiles\";T-1000;\"Snake_Plissken\""}
{"question": "is there a Harry Potter character named after Florence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Florence_Craye\";\"Florence_Craye\";\"Florence_Kate_Upton\";\"Jeeves_and_the_Feudal_Spirit\""}
{"question": "could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Azerbaijan", "extracted_entity": "\"Where_Are_We_Now?\";\"1944_(song)\";\"Menin\u0301_Qazaqstanym\";\"Azerbaijan_in_the_Eurovision_Song_Contest_2009\""}
{"question": "was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Cathedral_of_the_Archangel\";\"Vassian_Patrikeyev\";\"Cathedral_of_the_Archangel\";\"Postnik_Yakovlev\""}
{"question": "can Poland Spring make money in the Sahara?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"2000s_commodities_boom\";\"2000s_commodities_boom\";\"Arab_Spring\";\"Poland_and_the_International_Monetary_Fund\""}
{"question": "do people with mood disorders need permanent institutionalization?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Psychiatric_hospital\";\"Treatment_of_mental_disorders\";\"Bipolar_disorder\";\"Mental_disorder\""}
{"question": "will Dustin Hoffman likely vote for Trump in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Most_Likely_to_Murder\";\"Most_Likely_to_Murder\";\"Michael_Avenatti\";\"The_Manchurian_Candidate\""}
{"question": "can your psychologist say hello to you while you are out at the supermarket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Will_You_Please_Be_Quiet,_Please?\";\"Miracles_of_the_Namiya_General_Store\";\"How_About_a_Friendly_Shrink?\";\"Walmart_greeter\""}
{"question": "did Johann Sebastian Bach influence heavy metal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Heavy_metal_music\";\"Heavy_metal_music\";\"Heavy_metal_music\";\"Heavy_metal_music\""}
{"question": "are flag of Gabon colors found in rainbow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Flag_of_Gabon\";\"Flag_of_Gabon\";\"Flag_of_Gabon\";\"National_flag\""}
{"question": "does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "C-SPAN;\"Wireless_network\";\"Communications_satellite\";\"COSMOS_(telecommunications)\""}
{"question": "can a grey seal swim in the same water as the subject of Moby Dick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cetacean_surfacing_behaviour\";\"Pseudobiceros_hancockanus\";\"Clearwater_Marine_Aquarium\";\"Human_cannibalism\""}
{"question": "did Modern Family win a Slammy award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Modern", "extracted_entity": "\"Modern_Family\";\"Integrity_(Modern_Family)\";\"Mother's_Day_(Modern_Family)\";\"Family_Portrait_(Modern_Family)\""}
{"question": "would Mount Wycheproof be a breeze for Edmund Hillary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Helm_Wind\";\"Brett_Kirk\";\"USCGC_Northwind_(WAGB-282)\";\"Edmund_Hillary\""}
{"question": "does Canada have a relationship with a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Canada", "extracted_entity": "\"History_of_monarchy_in_Canada\";\"Monarchy_of_Canada\";\"Canadian_sovereignty\";\"Canada\u2013New_Zealand_relations\""}
{"question": "would a compact disc melt in magma?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hyaloclastite;\"Lava_dome\";\"Lid_tectonics\";Cryovolcano"}
{"question": "is Sea World hazardous to leopard seal's health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sea", "extracted_entity": "\"Leopard_tortoise\";\"Ocean_Conservancy\";\"Cetacean_surfacing_behaviour\";\"Leopard_seal\""}
{"question": "was being a mail carrier considered one of the most dangerous jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Expressman;\"Mail_carrier\";\"Mail_and_wire_fraud\";\"Postal_worker\""}
{"question": "were Depeche Mode heavily influenced by blues music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Origins_of_the_blues\";Blues;Blues;Blues"}
{"question": "do people with swallowing disorders need high viscosity drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Glucagon_rescue\";\"Sweetened_beverage\";Mannitol;\"Glucagon_rescue\""}
{"question": "can whole genome sequencing be used for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Whole_genome_sequencing\";\"Whole_genome_sequencing\";\"Whole_genome_sequencing\";\"Whole_genome_sequencing\""}
{"question": "is Anakin Skywalker from Star Wars associated with the color black?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Atramentum;\"Redskin_(slang)\";Black;Black"}
{"question": "do you need to schedule separate preventive healthcare and sickness visits? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Concomitant_drug\";\"Doctor's_visit\";\"General_medical_examination\";\"Preventive_healthcare\""}
{"question": "was the original Metroid groundbreaking for its polygons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Truncated_icosahedron\";Freescape;Icosagon;\"Truncated_icosahedron\""}
{"question": "would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Organizational_structure_of_Jehovah's_Witnesses\";\"Doomsday_rule\";\"Watch_Tower_Society_unfulfilled_predictions\";\"The_Three_Weeks\""}
{"question": "are any of the words that CAPTCHA stands for palindromes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "HuntBar;\"Browser_hijacking\";\"Browser_hijacking\";\"Browser_hijacking\""}
{"question": "is the Jurassic era a tourist destination?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Jurassica;Jurassica;Jurassica;Jurassica"}
{"question": "can a dolphin keep a diary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Cetacean_surfacing_behaviour\";\"Common_bottlenose_dolphin\";\"Cetacean_surfacing_behaviour\";\"Oceanic_dolphin\""}
{"question": "is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sayyid_Al-Qemany\";\"Religious_views_of_Muhammad_Ali\";\"Cat_Stevens'_comments_about_Salman_Rushdie\";\"Muhammad_Ali\""}
{"question": "does Carl Linnaeus share the same final resting place as Michael Jackson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Lonesome_George\";\"David_Livingstone\";\"Carl_Linnaeus\";\"John_W._N._Watkins\""}
{"question": "could Eddie Murphy dial 911 in a car as a young child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Streetball_in_Puerto_Rico\";\"Nikko_Jenkins\";\"Keron_Thomas\";\"Killer_on_the_Road\""}
{"question": "do flying fish have good eyesight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Accipitridae;\"Bird_vision\";\"Flying_fish\";\"Bird_vision\""}
{"question": "does Justin Bieber vote in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Vote_for_the_Worst\";\"Justin_Bieber\";Vevo;\"Justin_Bieber\""}
{"question": "did Ice make people rich?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ice_famine\";\"Ice_trade\";\"Ice_trade\";\"Ice_(TV_series)\""}
{"question": "is J.D. Salinger's most successful work influential to killers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"J._D._Salinger\";\"Hocus_Pocus_(novel)\";\"Closing_Time_(novel)\";\"Salinger_v._Random_House,_Inc.\""}
{"question": "is it impossible for pigs to use pig latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Quod_licet_Iovi,_non_licet_bovi\";\"Quod_licet_Iovi,_non_licet_bovi\";\"Dog_Latin\";\"Contraction_(grammar)\""}
{"question": "can the theory of cultural hegemony explain global warming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cultural_imperialism\";\"Cultural_imperialism\";\"Cultural_imperialism\";\"Cultural_imperialism\""}
{"question": "is March named after Jupiter's son in Roman mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "is", "extracted_entity": "March;\"Jupiter_(mythology)\";Proserpina;\"Martius_(month)\""}
{"question": "can furniture be made of hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hair_prosthesis\";\"Resin_wicker\";\"Human_hair_growth\";Hair"}
{"question": "is SnapCap an example of a retail store?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Outlet_store\";Shopbop;\"Outlet_store\";Shoplet"}
{"question": "did Rand Paul frequently swim in Lake Michigan during his undergraduate years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Rand_Paul\";\"Terry_Rand\";\"Terry_Rand\";\"Mike_Barrowman\""}
{"question": "would Hannah Nixon be proud of Richard Nixon following the Watergate scandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hannah_Milhous_Nixon\";\"Hannah_Milhous_Nixon\";\"Miss_Otis_Regrets\";\"Miss_Otis_Regrets\""}
{"question": "would Tony Stark be considered a polymath?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Polygenism;\"Plastic_Man\";Polygenism;Ateliosis"}
{"question": "is Supreme Court of the United States analogous to High Courts of Justice of Spain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Supreme_Court_of_the_United_States\";\"Supreme_Court_of_the_United_States\";\"Supreme_Court_of_the_United_States\";\"Supreme_Court_of_the_United_States\""}
{"question": "does Amtrak run from NYC directly to the Moai location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Am", "extracted_entity": "\"Penn_Station_Access\";\"New_York_City\";\"Jamaica_station\";\"New_York_high-speed_rail\""}
{"question": "does Ronda Rousey avoid BBQ restaraunts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ronda_Rousey\";\"Ronda_Rousey\";\"Ronda_Rousey\";\"Bring_It_On:_All_or_Nothing\""}
{"question": "can someone in Uberlandia work for Mitsubishi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Dekasegi;Dekasegi;\"Mechanic:_Resurrection\";\"Claudio_Uberti\""}
{"question": "do oak trees have leaves during winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Oak", "extracted_entity": "Marcescence;Oak;\"Quercus_palustris\";Oak"}
{"question": "is Argon near Neon on the periodic table of elements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Neon;Argon;Argon;\"Neon_compounds\""}
{"question": "does the Constitution of the Philippines copy text from the British constitution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Bible_translations_into_the_languages_of_the_Philippines\";\"Constitution_of_the_Philippines\";\"Constitution_of_the_Philippines\";\"Magna_Carta\""}
{"question": "did Ada Lovelace die tragically young for her era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ada_Lovelace\";\"Ada_Lovelace\";\"Ada_Lovelace\";\"Ada_Lovelace\""}
{"question": "is dyslexia the most common intellectual disability in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dyslexia;\"Intellectual_disability\";\"Intellectual_disability\";\"Intellectual_disability\""}
{"question": "is the referee at a soccer match highly visible against the field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Touch_match_officials\";\"Assistant_referee_(association_football)\";\"Referee_(association_football)\";\"Goalkeeper_(association_football)\""}
{"question": "does Family Guy take place on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Family_Guy\";\"Family_Guy\";\"Best_of_the_West\";\"The_American_West\""}
{"question": "could someone in the Canary Islands fish for largemouth bass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Canary_damsel\";\"Canary_damsel\";\"Cynoglossus_canariensis\";\"Cynoglossus_canariensis\""}
{"question": "did the Presidency of Bill Clinton conclude with his impeachment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Impeachment_of_Bill_Clinton\";\"Impeachment_of_Bill_Clinton\";\"Bill_Clinton\";\"Bill_Clinton_pardon_controversy\""}
{"question": "did Marco Polo travel with Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Marco_Polo\";\"Marco_Polo\";\"Niccolo\u0300_and_Maffeo_Polo\";\"The_Travels_of_Marco_Polo\""}
{"question": "could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Big_Man_on_Hippocampus\";\"This_Is_Your_Brain_on_Drugs\";DreamWorksTV;\"Steve_Cutts\""}
{"question": "will a 2 Euro coin float across the Red Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"General_Grot-Rowecki\";\"Crossing_the_Red_Sea\";\"European_migrant_crisis\";\"Freedom_Flotilla_II\""}
{"question": "have any murderers outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Kane_&_Lynch:_Dead_Men\";\"No_More_Kings\";\"The_Streak_(wrestling)\";\"Kane_&_Lynch:_Dead_Men\""}
{"question": "can the US branch of government that has power over the military also have the power to veto?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Legislative_veto_in_the_United_States\";Veto;\"Legislative_veto\";\"Legislative_veto_in_the_United_States\""}
{"question": "are Brussels sprout particularly good for adrenal fatigue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Brussels_sprout\";\"Grape_and_raisin_toxicity_in_dogs\";\"Grape_and_raisin_toxicity_in_dogs\";Coumestrol"}
{"question": "can oysters be used in guitar manufacturing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Pearloid;\"Guitar_pick\";Pearloid;\"Zemaitis_Guitars\""}
{"question": "are parodies of the President of the United States illegal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Doublespeak;MSTing;Doublespeak;\"Discrediting_tactic\""}
{"question": "can lobster breathe in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Southern_marsupial_mole\";\"Sea_turtle\";\"Deep_sea_creature\";Aquanaut"}
{"question": "is Phobos part of the Andromeda galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Andromeda_Galaxy\";\"Andromeda_Galaxy\";\"Andromeda_XXI\";\"Andromeda_Galaxy\""}
{"question": "were all of Heracles's children present for his funeral pyre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Revelers_Vase\";\"Whom_the_Gods_Would_Destroy\";\"House_of_the_Tragic_Poet\";\"The_Suppliants_(Euripides)\""}
{"question": "can Africanized bees be considered multicultural?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "African", "extracted_entity": "\"Africanized_bee\";\"Africanized_bee\";\"Africanized_bee\";\"Africanized_bee\""}
{"question": "is nickel dominant material in US 2020 nickels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Nickel;\"Nitinol_60\";\"Nickel_(United_States_coin)\";\"Nickel_(United_States_coin)\""}
{"question": "did Leonardo da Vinci lack contemporary peers in his home city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Leonardo_da_Vinci\";\"Museo_Ideale_Leonardo_da_Vinci\";\"John_Vinci\";\"Leonardo_da_Vinci\""}
{"question": "was there fear leading up to the year 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tokophobia;\"Anthropophagous_2000\";\"Baby_81_incident\";\"Ambidextrous_(novel)\""}
{"question": "can you buy furniture and meatballs in the same store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hardware_store\";\"Assortment_strategies_(retail)\";\"Jim_McIngvale\";\"Closeout_(sale)\""}
{"question": "during the Cuban revolution, did the US experience a population boom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Cuban_immigration_to_the_United_States\";\"Tourism_in_Cuba\";\"Cuban_exile\";\"Cuban_immigration_to_the_United_States\""}
{"question": "could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Superstar_Effect\";\"Usain_Bolt\";\"Usain_Bolt\";\"Usain_Bolt\""}
{"question": "can a copy of The Daily Mirror sustain a campfire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Harrow_Observer\";\"E._V._Lucas\";\"Gale_&_Polden\";\"Shrewsbury_Chronicle\""}
{"question": "would Bugs Bunny harm an olive tree in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Juglans_regia\";\"A_Hare_Grows_in_Manhattan\";\"Looney_Tunes:_Rabbits_Run\";\"Olive_fruit_fly\""}
{"question": "did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Batman_(1989_film)\";\"Dirty_Harry_round\";Murlynd;\"Batman_(1989_film)\""}
{"question": "can actress Danica McKellar skip astronaut education requirements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She", "extracted_entity": "\"Alien_of_extraordinary_ability\";\"Mae_Jemison\";\"Joan_Higginbotham\";\"Yvonne_Cagle\""}
{"question": "can you transport a coin along a sea of mercury?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Charon;Charon;\"Hippo_water_roller\";Ferry"}
{"question": "are the founders of Skype from Asia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Daou_Technology_Inc.\";Realme;ImaHima;HungryGoWhere"}
{"question": "was the tenth Amendment to the Constitution written using Pitman shorthand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Pitman_shorthand\";\"John_Pitman_(judge)\";\"United_States_labor_law\";\"Tenth_Amendment_to_the_United_States_Constitution\""}
{"question": "can you save every HD episode of Game of Thrones on Samsung Galaxy A10e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "YouTube;PlayOn;\"Tearaway_Unfolded\";Twitch.tv"}
{"question": "does Super Mario protagonist hypothetically not need continuing education classes in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"JumpStart_Advanced_2nd_Grade\";\"Marge_Be_Not_Proud\";\"Re-Animator:_The_Musical\";\"Plankton_and_Karen\""}
{"question": "would a body builder prefer an elk burger over a beef burger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"UGG_(brand)\";\"Travis_Rice\";\"Lanny_McDonald\";\"Frick_and_Frack\""}
{"question": "will NY Stock Exchange closing bell be heard in Universal Music Group's headquarters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Closing_Bell\";\"Closing_Bell\";\"Universal_Music_Group\";\"Record_Store_Day\""}
{"question": "could Saint Peter watch television?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Television_Today\";\"Chair_of_Saint_Peter\";\"Saint_Peter\";\"Jesus_of_Nazareth_(miniseries)\""}
{"question": "are all United States Aldi locations owned by the same company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Aldi;Aldi;Aldi;\"Sarpino's_Pizzeria\""}
{"question": "was the first Vice President of the United States an Ottoman descendant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John", "extracted_entity": "\"Adlai_Stevenson_II\";\"James_S._Sherman\";\"Russell_Benjamin_Harrison\";\"Vice_President_of_the_United_States\""}
{"question": "would it be safe to have a jackfruit thrown at your head?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jack", "extracted_entity": "\"Spring-heeled_Jack\";\"Jack-o'-lantern\";\"Spring_training\";\"Jacko_hoax\""}
{"question": "can a woman on average have a baby before wheat seed blooms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "can", "extracted_entity": "\"Pregnancy_rate\";\"Pregnancy_rate\";Infertility;Pregnancy"}
{"question": "did Donald Trump come up with the idea for the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Othmar_Ammann\";\"Harbor_Defenses_of_New_York\";\"Trump_Plaza_(Jersey_City)\";\"New_York_Harbor\""}
{"question": "did Saint Augustine use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Julian_calendar\";\"Revised_Julian_calendar\";\"Byzantine_calendar\";\"Julian_calendar\""}
{"question": "do Koalas prefer Eucalyptus over meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Koala;Koala;\"Lesser_kudu\";\"Kangaroo_meat\""}
{"question": "are goats found on abyssal plains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Syrian_wild_ass\";Shasu;\"Cashmere_goat\";\"Altai_Mountain_goat\""}
{"question": "are gorillas closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gorilla;Gorilla;Human;Gorilla"}
{"question": "do Star Wars fans say \"beam me up\" often?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Gamesters_of_Triskelion\";\"Scotty_(Star_Trek)\";\"Star_Trek:_The_Animated_Series\";\"Beam_me_up,_Scotty\""}
{"question": "does USA fail separation of church and state in multiple ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Separation_of_church_and_state_in_the_United_States\";\"Separation_of_church_and_state_in_the_United_States\";\"Separation_of_church_and_state_in_the_United_States\";\"Separation_of_church_and_state_in_the_United_States\""}
{"question": "could Bob Marley's children hypothetically win tug of war against Kublai Khan's children?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Children_of_General_Hospital\";\"Children_of_the_Lamp\";\"Children_of_the_Storm\";\"Captain_Marvel_Jr.\""}
{"question": "does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Festival;Unbirthday;\"Alice_in_Wonderland_(1951_film)\";\"Alice_in_Wonderland_(franchise)\""}
{"question": "is dessert eaten before breakfast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Rice_pudding\";Dessert;\"Tea_(meal)\";\"History_of_breakfast\""}
{"question": "was Krishna skilled at using the bow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Arjuna;Krishna;\"Earth_Maiden_Arjuna\";\"Indian_martial_arts\""}
{"question": "was Aristotle a member of the House of Lords?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Chaerephon;\"William_Lok\";Chaerephon;\"Alexander_Popham\""}
{"question": "would Ibn Saud tolerate salsa music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Salsa_music\";\"Salsa_music\";\"Salsa_music\";\"Salsa_music\""}
{"question": "would a responsible bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Texas_Lady\";\"Rickey_(cocktail)\";\"Vinegar_Hill,_Brooklyn\";\"Derek_Brown_(mixologist)\""}
{"question": "is Bactrian Camel most impressive animal when it comes to number of humps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Bactrian_camel\";\"Wild_Bactrian_camel\";Dromedary;Camel"}
{"question": "do giraffes require special facilities at zoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Giraffe_Centre\";Giraffe;\"Reticulated_giraffe\";\"The_Ranch_Resort\""}
{"question": "can a wheelbarrow full of starch kill hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Gutierrezia_microcephala\";\"Bismuth_subsalicylate\";\"Oxalis_glabra\";\"Halogeton_glomeratus\""}
{"question": "can a snake wear a snowshoe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Chaps;\"Extended_Cold_Weather_Clothing_System\";\"Snake_charming\";\"Colorado_State_University\u2013Pueblo\""}
{"question": "can a false pope become a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Charlatan;\"Foolishness_for_Christ\";\"Foolishness_for_Christ\";\"Saint_Faith\""}
{"question": "does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "D", "extracted_entity": "Zugspitze;Saoner;Aachen;\"Climate_of_San_Diego\""}
{"question": "can dementia be cured with a cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Alzheimer's_disease_research\";\"Degenerative_disc_disease\";\"Preiser_disease\";Dementia"}
{"question": "can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Shipwreck_Falls\";\"Skyride_(Six_Flags_Great_Adventure)\";\"Shipwreck_Falls\";\"Six_Flags_Magic_Mountain\""}
{"question": "would Dave Chappelle pray over a Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dave", "extracted_entity": "\"Cat_Stevens'_comments_about_Salman_Rushdie\";\"Quran_desecration\";\"Chaminda_Vaas\";\"Dove_World_Outreach_Center_Quran-burning_controversy\""}
{"question": "is a jellyfish safe from atherosclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Chironex_fleckeri\";\"Cyanea_nozaki\";\"The_Word_Alive\";\"Box_jellyfish\""}
{"question": "is the Riksdag a political entity in Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"L\u00f8gting\";\"L\u00f8gting\";Riksdag;\"Dissolution_of_the_union_between_Norway_and_Sweden\""}
{"question": "did Linnaeus edit Darwin's draft of Origin of Species?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"On_the_Origin_of_Species\";\"Natural_Selection_(manuscript)\";\"Carl_Linnaeus\";\"On_the_Origin_of_Species\""}
{"question": "is Hermione Granger eligible for the Order of the British Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Knight;\"Doreen_Ashburnham-Ruffner\";\"Order_of_Merit\";Dame"}
{"question": "was Saudi Aramco started due to an assassination?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Saudi_Aramco\";\"Hisham_Nazer\";\"Saudi_Arabia\";\"Abdullah_S._Jum'ah\""}
{"question": "should a Celiac sufferer avoid spaghetti?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Diverticulitis;Colistin;\"Otitis_media\";\"Otitis_media\""}
{"question": "does Olympia Washington share name with Hephaestus's workshop location?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vulcano;\"Olympia_(comics)\";\"Asterix_at_the_Olympic_Games\";\"Olympia_(comics)\""}
{"question": "could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Time_(magazine)\";\"The_New_Day_(newspaper)\";\"Time_(magazine)\";\"Solar_eclipse_of_August_21,_2017\""}
{"question": "does Jason have anything in common with Dr. Disrespect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dr_DisRespect\";\"Jason_Morgan_(General_Hospital)\";\"Dr_DisRespect\";\"Dr_DisRespect\""}
{"question": "is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Urairat_Soimee\";\"Childbirth_in_Japan\";\"Cervical_cancer\";\"Mirai_Nagasu\""}
{"question": "are ropes required to operate a frigate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mooring_(watercraft)\";\"Capstan_(nautical)\";Jackline;\"Nautical_cable\""}
{"question": "did Brad Peyton need to know about seismology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Terry_Wallace_(geophysicist)\";\"Forensic_seismology\";\"Forensic_seismology\";\"Bruce_Bolt\""}
{"question": "is Ludacris in same music genre as 2000's Binaural?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Genre-Specific_Xperience\";\"Vocal_trance\";\"Trip_hop\";Trilenium"}
{"question": "would LeBron James hypothetically glance upwards at Yuri Gagarin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ico-D;\"Attempted_assassination_of_Leonid_Brezhnev\";\"To_the_Stars:_The_Autobiography_of_George_Takei\";\"Leonid_Chernovetskyi\""}
{"question": "do you need a farmer to make a circuit board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Stripper_(agriculture)\";Plough;\"Baling_wire\";Hoe-farming"}
{"question": "did Gandhi watch the television show Bonanza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Randee_Heller\";\"Bedtime_for_Bonzo\";Bonanza;\"Michael_McDonald_(comedian)\""}
{"question": "is Shakespeare famous because of the infinitive form?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Infinitive;\"Vocative_case\";\"Infinitive_(Ancient_Greek)\";\"Infinitive_(Ancient_Greek)\""}
{"question": "would the crew of Apollo 15 have difficulty riding a unicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Unicycle;\"Apollo_15\";\"Manual_handling_of_loads\";Unicycle"}
{"question": "do Squidward Tentacles and Alan Greenspan have different musical passions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Squidward_Tentacles\";\"Jon_and_Al_Kaplan\";\"Overboard_(a_cappella)\";\"Jon_and_Al_Kaplan\""}
{"question": "can a minor replicate the double-slit experiment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Double-slit_experiment\";\"Double-slit_experiment\";\"Double-slit_experiment\";\"Double-slit_experiment\""}
{"question": "can an anchovy born in 2020 survive 25th US census?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"AquAdvantage_salmon\";\"A_Star_Is_Born_Again\";\"Dolly_(sheep)\";Anencephaly"}
{"question": "do you have to put on glasses to read a QR code?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Q-switching;\"Expert_Infantryman_Badge\";\"Eye_vein_verification\";\"Corrective_lens\""}
{"question": "would Seroquel be the first treatment recommended by a doctor to someone with depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "S", "extracted_entity": "\"Electroconvulsive_therapy\";Sertraline;Quetiapine;\"Travel_medicine\""}
{"question": "do women often need new shoes during their pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"All_God's_Children_Need_Traveling_Shoes\";\"Mule_(shoe)\";\"All_God's_Children_Need_Traveling_Shoes\";\"Fuck-me_shoes\""}
{"question": "would Persephone be a good consultant to a landscape architect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Per", "extracted_entity": "\"Kathryn_Gustafson\";\"Kathryn_Gustafson\";\"Hearst_Castle\";\"Boughton_House\""}
{"question": "is University of Pittsburgh easier to enter than FBI?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Nationality_Rooms\";\"2012_University_of_Pittsburgh_bomb_threats\";\"2012_University_of_Pittsburgh_bomb_threats\";\"2012_University_of_Pittsburgh_bomb_threats\""}
{"question": "will the small intenstine break down a cotton ball?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Intrauterine_device\";\"Intrauterine_device\";\"Hormonal_IUDs\";\"Tranquillizer_gun\""}
{"question": "do the Eskimos sunbathe frequently?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Shadowy_Men_on_a_Shadowy_Planet\";\"The_Clicquot_Club_Eskimos\";\"Shadowy_Men_on_a_Shadowy_Planet\";\"The_Clicquot_Club_Eskimos\""}
{"question": "was the French Revolution televised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"France_Te\u0301le\u0301visions\";\"France_24\";\"France_Te\u0301le\u0301visions\";\"Television_in_France\""}
{"question": "is the most expensive color in the world Blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Blue_Is_the_Warmest_Colour\";Blue;\"Blue_Is_the_Warmest_Color_(comics)\";\"Blue_Is_the_Warmest_Colour\""}
{"question": "did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ros", "extracted_entity": "\"Rosalind_Franklin_and_DNA\";\"Rosalind_Franklin\";\"Rosalind_Franklin_and_DNA\";\"Rosalind_(education_platform)\""}
{"question": "can you substitute the pins in a bowling alley lane with Dustin Hoffman's Oscars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "CinemaSins;Blooper;Pinsetter;\"Bowling_for_Votes\""}
{"question": "did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Hundred_Years'_War\";\"Hundred_Years'_War\";\"Hundred_Years'_War\";\"Hundred_Years'_War\""}
{"question": "is the Sea of Japan landlocked within Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sea_of_Japan\";\"Geography_of_Japan\";\"Sea_of_Japan_naming_dispute\";\"Childbirth_in_Japan\""}
{"question": "is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Pumpkin_Spice_Latte\";Cinnamon;\"Japanese_incense\";\"Blade_grinder\""}
{"question": "will Lhamo Thondup be considered by Catholic Church to be a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Nguye\u0302\u0303n_Ngo\u0323c_Tho\u031b\";\"Ogyen_Trinley_Dorje\";\"Orgyen_Kusum_Lingpa\";\"Ogyen_Trinley_Dorje\""}
{"question": "would someone in Boston not receive the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "CFMT-DT;\"Gilbert_Perreault\";\"Professional_sports_in_Canada\";\"Humanist_Party_of_Ontario\""}
{"question": "is Christmas always celebrated on a Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Christmas_and_holiday_season\";\"Twelve_Days_of_Christmas\";Christmas;\"Christmas_traditions\""}
{"question": "are pancakes typically prepared in a pot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pancake;Pancake;Pancake;\"Crepe_maker\""}
{"question": "has Ringo Starr been in a relatively large number of bands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ring", "extracted_entity": "\"Ringo_Starr\";\"Ringo_Starr\";\"Ringo_Starr_&_His_All-Starr_Band\";\"Ringo_Starr\""}
{"question": "did Richard III know his grandson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Richard", "extracted_entity": "\"Richard_III_of_England\";\"Richard_III_of_England\";\"Richard_of_Shrewsbury,_Duke_of_York\";\"William_Montagu,_1st_Earl_of_Salisbury\""}
{"question": "can Kane challenge Joe Biden in this year's primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Potomac_primary\";\"2012_United_States_presidential_debates\";\"2020_Democratic_Party_presidential_primaries\";\"2008_Democratic_Party_vice_presidential_candidate_selection\""}
{"question": "did Stone Cold Steve Austin wrestle in three different centuries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Stone_Cold_Steve_Austin\";\"Stone_Cold_Steve_Austin\";\"History_of_WWE\";\"King_of_the_Ring_(1998)\""}
{"question": "do embalmed bodies feel different at funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Embalming;Embalming;Embalming;Embalming"}
{"question": "are leaves from coca good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "are", "extracted_entity": "Coca;Coca;\"Hypoxic_ventilatory_response\";Coca"}
{"question": "did the Beatles write any music in the Disco genre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Disco;\"Hip_hop_music\";\"Lo-fi_music\";\"Dance_music\""}
{"question": "is Nine Inch Nails a good guest for students in earliest grade to take Iowa tests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mandatory_Brunch_Meeting\";\"Iowa_State_Cyclones_football\";\"Britt,_Iowa\";Trinitones"}
{"question": "did Harvey Milk ever run for governor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Harvey", "extracted_entity": "\"Milk_(film)\";\"Milk_(film)\";\"Harvey_Milk\";\"Harvey_Milk\""}
{"question": "is a bengal fox likely to see the Superbowl?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Weekend_Surprise\";\"Bengal_tiger\";\"Figment_(Disney)\";\"Murrah_buffalo\""}
{"question": "could all Tahiti hotels hypothetically accommodate US D-Day troops?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "was Amazon involved in the lunar landing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Amazon", "extracted_entity": "\"Sinking_of_the_Rainbow_Warrior\";\"Amazons_Attack!\";Amazonomachy;\"Diana_Trujillo\""}
{"question": "could the Spice Girls compete against \u017dRK Kumanovo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Spice_Girls\";\"Pasha_Kovalev\";\"Popstars_(German_TV_series)\";Honeyz"}
{"question": "are saltwater crocodiles related to alligators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "Alligatoridae;Alligatoridae;\"River_dolphin\";Alligatoridae"}
{"question": "would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Trepopnea;\"Pneumonic_plague\";\"Hypoadrenocorticism_in_dogs\";\"Nevoid_basal-cell_carcinoma_syndrome\""}
{"question": "has every astronaut survived their space journey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Astronaut:_The_Last_Push\";\"Astronaut:_The_Last_Push\";\"Expedition_20\";\"Time_in_Advance\""}
{"question": "can any person with a driver's license work in transport of aviation fuel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Commercial_pilot_license\";\"Medical_escort\";\"Pilot_boat\";\"Maritime_pilot\""}
{"question": "could a snowy owl survive in the Sonoran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Sonoran_Desert\";\"Sonoran_pronghorn\";\"Abert's_squirrel\";\"Heber-Overgaard,_Arizona\""}
{"question": "can you buy chlorine at a dollar store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Roger_Christie\";\"99_Cents_Only_Stores\";\"Variety_store\";\"Dollar_Tree\""}
{"question": "is the voice of the Genie from Disney's Aladdin still alive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Robin", "extracted_entity": "\"Genie_(Disney)\";\"Genie_(Disney)\";\"Aladdin_(Disney_character)\";\"Aladdin_(Disney_character)\""}
{"question": "was the amount of spinach Popeye ate unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Popeye;Popeye;Popeye;Lard"}
{"question": "would an explosion at a gunpowder storage facility result in a supersonic shock wave?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Explosion;\"Boiler_explosion\";\"Boiler_explosion\";\"Effects_of_nuclear_explosions\""}
{"question": "could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_Hersheypark\";\"DelGrosso's_Amusement_Park\";\"Rim_Runner\";\"HangTime_(roller_coaster)\""}
{"question": "can a person be diagnosed with a Great Depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Major_depressive_disorder\";\"Major_depressive_disorder\";\"Major_depressive_disorder\";\"Major_depressive_disorder\""}
{"question": "could Godzilla have been killed by the Tohoku earthquake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Godzilla_(2014_film)\";\"Godzilla_(1954_film)\";\"Godzilla_(2014_film)\";\"Godzilla_vs._King_Ghidorah\""}
{"question": "is retail a job anybody can be suited for?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mr._Handyman\";\"Personal_shopper\";\"Shop_fitting\";\"I_Can_Get_It_for_You_Wholesale\""}
{"question": "does cell biology teach about the life cycle of Al Qaeda?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cell_biology\";\"Cell_biology\";\"Cell_biology\";\"Cell_biology\""}
{"question": "did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Les_Demoiselles_d'Avignon\";\"Jose\u0301_Luis_Sampedro\";\"Sabana_Grande,_Caracas\";\"Legacy_of_Che_Guevara\""}
{"question": "has Burger King  contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Fight_for_$15\";\"Bambi_effect\";\"Get_Outdoors_Georgia\";\"History_of_Burger_King\""}
{"question": "would a snake have reasons to fear a honey badger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Honey_badger\";Elaphe;\"Fear_of_bees\";Aposematism"}
{"question": "do most people only memorize slightly over half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"ZIP_Code\";\"ZIP_Code\";\"ZIP_Code\";\"ZIP_Code\""}
{"question": "does Happy Gilmore Productions CEO own a Torah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Shraga_Simmons\";\"Aviv_Nevo\";\"Jonathan_Greenstein\";\"Ronald_Perelman\""}
{"question": "does Mercury make for good Slip N Slide material?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mercury", "extracted_entity": "Relay;\"Powder_coating\";\"Dry_lubricant\";\"RTV_silicone\""}
{"question": "is RoboCop director from same country as Gaite Jansen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Marc_Koninckx\";\"Cinema_of_the_Netherlands\";Costa-Gavras;\"Gariep_Dam\""}
{"question": "was the Parc des Princes fully operational during June of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Parc_des_Princes\";\"Disneyland_Resort\";\"Walt_Disney_Studios_Park\";\"Parc_des_Princes\""}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Secret_World_of_Santa_Claus\";\"Christmas_elf\";Elf;\"The_Elf_on_the_Shelf\""}
{"question": "do worshipers of Shiva make a pilgrimage to the Holy Land?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Hajj;\"Religious_tourism\";\"2006_Hajj_stampede\";Pilgrimage"}
{"question": "was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Presidency_of_Woodrow_Wilson\";\"Divided_government_in_the_United_States\";\"Peacemakers:_The_Paris_Peace_Conference_of_1919_and_Its_Attempt_to_End_War\";\"Presidency_of_Woodrow_Wilson\""}
{"question": "was it typical to see Johnny Cash on stage in a rainbow-colored outfit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Johnny_Cash\";\"Dressed_in_Black:_A_Tribute_to_Johnny_Cash\";\"Polka_dot\";\"Johnny_Cash\""}
{"question": "is slitting your wrists an unreliable suicide method?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Suicide_methods\";\"Suicide_methods\";\"Suicide_methods\";\"Suicide_methods\""}
{"question": "would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Huna_(New_Age)\";\"Hawaiian_hoary_bat\";\"Obesity_in_the_Pacific\";\"Manu'a_Project\""}
{"question": "is Christopher Nolan indebted to Bob Kane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Christopher_Nolan\";\"Alfred_Pennyworth\";Batwoman;\"Coins_in_the_Bible\""}
{"question": "did Kim Il-sung network on LinkedIn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Song_of_General_Kim_Jong-il\";\"Voice_of_Korea\";\"Kim_Jong-il\";\"Song_of_General_Kim_Il-sung\""}
{"question": "could all People's Volunteer Army hypothetically be transported on Symphony of the Seas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Help_for_Heroes\";\"Help_for_Heroes\";Mexeflote;\"Operation_Torch\""}
{"question": "is Bern a poor choice for a xenophobic Swiss citizen to live?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "n", "extracted_entity": "\"Expo_64\";\"German_immigration_to_Switzerland\";Bern;\"Pakistanis_in_Switzerland\""}
{"question": "would a vegan prefer a natural bongo drum over a synthetic one?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"An_Instinct_for_the_Kill\";\"Bongo_drum\";\"Bata\u0301_drum\";Tanbou"}
{"question": "is an inappropriate lullaby Love Song from November 11, 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Fever_(Little_Willie_John_song)\";\"Sweet_Lullaby\";\"Homecoming_(Kanye_West_song)\";\"Miley_Cyrus_&_Her_Dead_Petz\""}
{"question": "does Orange County, California require airplanes to be quiet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Noise_regulation\";\"Orange_County,_California\";\"Quiet_Spike\";\"John_Wayne_Airport\""}
{"question": "are kayaks used at the summit of Mount Everest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Expedition_Everest\";\"Wang_Jing_(mountaineer)\";\"Porter_(carrier)\";Kayak"}
{"question": "would someone on a keto diet be able to eat Dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Whoonga;\"5_A_Day\";Konzo;\"Ketogenic_diet\""}
{"question": "was Raphael's paintings influenced by the country of Guam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Camille_Pissarro\";\"Camille_Pissarro\";\"New_Zealand_art\";\"Paul_Jacoulet\""}
{"question": "is it normal to find parsley in multiple sections of the grocery store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Katz's_Delicatessen\";\"Shoppers_Food_&_Pharmacy\";\"Shoppers_Food_&_Pharmacy\";\"Variety_store\""}
{"question": "was the subject of Parsifal taken from British folklore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Serendipity;\"Emma_Orczy\";Percival;\"Maid_Marian\""}
{"question": "do drummers need spare strings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "drum", "extracted_entity": "\"Drum_kit\";\"Drum_kit\";\"Marching_band\";\"Drum_kit\""}
{"question": "would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Cactus_plugging\";\"Astrophytum_myriostigma\";Cactus;Cactus"}
{"question": "is there a jukebox musical about a sweet transvestite from Transexual, Transylvania?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Transylmania;\"Sweet_Transvestite\";\"Sweet_Transvestite\";\"Neopaganism_in_Hungary\""}
{"question": "did Malcolm X use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Malcolm_X\";XeTeX;\"Malcolm_X\";XeTeX"}
{"question": "would the fastest tortoise win a race against a Chicago \"L\"?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "would", "extracted_entity": "\"1/2_+_1/4_+_1/8_+_1/16_+_\u22ef\";\"L.A._Beast\";\"Seattle_Slew\";Lagahoo"}
{"question": "does Hammurabi's Code violate Christians Golden Rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Code_of_Hammurabi\";\"Code_of_Hammurabi\";\"Code_of_Hammurabi\";Impalement"}
{"question": "does the Roman god Vulcan have a Greek equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Vulcan", "extracted_entity": "\"Vulcan_(mythology)\";\"Son_of_Vulcan\";\"Vulcan_(mythology)\";Vulcanops"}
{"question": "can the Dalai Lama fit in a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tibet;\"Architecture_in_Tibet\";Chunseong;\"Tashi_Wangdi\""}
{"question": "is watermelon safe for people with a tricarboxylic acid allergy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "water", "extracted_entity": "\"Cicuta_maculata\";\"Trifoliate_orange\";Watermelon;\"Prunella_vulgaris\""}
{"question": "is a Chinchilla breed of felis catus a type of rodent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Chlamydophila_felis\";\"Jaliscan_cotton_rat\";\"Feline_calicivirus\";\"Chlamydophila_felis\""}
{"question": "can printing books in kanji instead of the Roman alphabet save trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Japanese_input_methods\";Washi;\"Romanization_of_Japanese\";Mojibake"}
{"question": "is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Audi", "extracted_entity": "\"Enhanced_full_rate\";\"Surround_sound\";\"10.2_surround_sound\";\"Plus_8\""}
{"question": "would a Rockette look odd with a moustache? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dieudonne\u0301_M'bala_M'bala\";\"What_Is_a_Man_Without_a_Moustache?\";\"What_Is_a_Man_Without_a_Moustache?\";Transformativeness"}
{"question": "would Hodor hypothetically be a good math mathematician?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Apostolos_Doxiadis\";\"Vainglory_(video_game)\";\"Tajemnica_Statuetki\";\"Michael_Dorff\""}
{"question": "would Hades and Osiris hypothetically compete for real estate in the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Gods_of_Egypt_(film)\";\"Osiris_(Marvel_Comics)\";\"Osiris_myth\";\"The_Contendings_of_Horus_and_Seth\""}
{"question": "would a German Shepherd be welcome in an airport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Gabriele_Rollnik\";Shepherd;\"The_Shepherd\";\"Guinea_pig\""}
{"question": "does a person need a college degree to become a bartender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Master_craftsman\";\"Doctor_of_Law\";\"Doctor_of_Philosophy\";Dietitian"}
{"question": "is capsaicin associated with cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Capsaicin;Capsaicin;Capsaicin;Capsaicin"}
{"question": "would Arnold Schwarzenegger be unable to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Arnold_Schwarzenegger\";\"Political_career_of_Arnold_Schwarzenegger\";\"Alec_Baldwin\";\"Political_career_of_Arnold_Schwarzenegger\""}
{"question": "when Hugh Jackman was a teacher, would he have taught The Great Gatsby?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hugh_Millais\";\"Will_Hay\";\"W._G._Grace\";\"John_Hodgkin_(tutor)\""}
{"question": "is Fiat Chrysler associated with Japanese cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Japanese_domestic_market\";\"Japanese_domestic_market\";\"VIP_style\";\"Manufacturing_in_Japan\""}
{"question": "can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Jeff_Buckley\";\"Katell_Keineg\";\"Malachi_Ritscher\";\"Death_by_coconut\""}
{"question": "would many meals heavy in brussels sprouts benefit someone on Coumadin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Brussels_sprout\";\"Brussels_sprout\";Coulure;\"Mirepoix_(cuisine)\""}
{"question": "is it safe to share silverware with an HIV positive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Misconceptions_about_HIV/AIDS\";\"Needle_sharing\";\"Needle_sharing\";\"Pin_prick_attack\""}
{"question": "do movies always show nerds as the losers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Slacker_(film)\";\"Goodbye,_20th_Century!\";\"The_Losers_(film)\";\"Boys_Don't_Cry_(film)\""}
{"question": "are eagles and young bears both used as labels for skills-training youth groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cub_Scout\";\"Beaver_Scouts_(Scouting_Ireland)\";\"Wolf_Cubs_(Baden-Powell_Scouts'_Association)\";\"Cub_Scouts_(The_Scout_Association)\""}
{"question": "could ancient Tony Bennett have a baby in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Boss_Baby\";\"She's_Having_a_Baby\";\"Small_World:_An_Academic_Romance\";\"Damian_Hurley\""}
{"question": "can a lifeboat rescue people in the Hooke Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Robert_William_Hook\";\"Lifeboat_(rescue)\";\"Rescue_(UK_TV_series)\";\"Fishguard_Lifeboat_Station\""}
{"question": "would a stool be useful for a Lusotitan to reach the top of an almond tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Stool_(seat)\";\"S\u030cupelka\";\"Close_stool\";Mallakhamba"}
{"question": "would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Kingmaker\";Blackadder;\"The_Kingmaker\";\"Manor_House_Hotel\""}
{"question": "would somebody leave reiki with bruises?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Rurouni_Kenshin:_Reflection\";\"Ko\u0304taro\u0304_Makarito\u0304ru!\";\"Chu\u0304shingura\";\"One_Missed_Call_(2003_film)\""}
{"question": "is there a popular Broadway character who is a missionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Elder", "extracted_entity": "\"Missionary:_Impossible\";\"He\u0301lio_Pestana\";\"The_Missionary\";\"The_Book_of_Mormon_(musical)\""}
{"question": "would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Millionaire;Billionaire;\"Jeff_Bezos\";\"Philanthropy_in_the_United_States\""}
{"question": "are months based on the solar cycle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Month;\"The_Months\";\"Diurnal_cycle\";\"Diurnal_cycle\""}
{"question": "do many fans of J.K Rowling know who Alan Rickman is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Alan_Rickman\";\"Jon_Gilbert_(bibliographer)\";\"Geeks_OUT\";\"Alan_Rickman\""}
{"question": "could someone in a coma experience fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sleep_paralysis\";\"Sleep_paralysis\";Phobia;\"Sleep_paralysis\""}
{"question": "could Johnny Carson's children fill out a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Johnny_Carson\";\"Johnny_Lujack\";\"Johnny_Carson\";\"Johnny_Ritchey\""}
{"question": "would downloading Mario 64 on an emulator be legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "\"Virtual_Console\";Emulator;\"1964_(emulator)\";\"Nintendo_3DS\""}
{"question": "are Tom and Jerry featured in a ride at Disneyland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pixar_Play_Parade\";\"Disneyland_Monorail_System\";\"Six_Flags_Magic_Mountain\";\"Pixar_Pal-A-Round\""}
{"question": "can a blind person tell time by Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dipleidoscope;\"Dialling_(mathematics)\";\"Big_Ben\";\"Big_Ben\""}
{"question": "are sea turtles enjoying life during quarantine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Cultural_depictions_of_turtles\";\"Cetacean_surfacing_behaviour\";\"Cultural_depictions_of_turtles\";\"Green_sea_turtle\""}
{"question": "has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "this", "extracted_entity": "\"You_Can't_Kill_Stephen_King\";\"Don_King_(boxing_promoter)\";\"Don_King_(boxing_promoter)\";\"Charles_Manson\""}
{"question": "does Paulo Coelho's wife make a living through speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Velho_Chico\";\"Paulo_Coelho\";\"Se_Eu_Fosse_Voce\u0302\";\"Paulo_Coelho\""}
{"question": "is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Fabian_Strategy\";\"The_Barefoot_Executive\";\"As\u0327k_Laftan_Anlamaz\";Saktyd"}
{"question": "is Final Fantasy VI closer to beginning than end of its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Final_Fantasy_VI\";\"Final_Fantasy_VI\";\"Star_Wars_sequel_trilogy\";\"Final_Fantasy_VI\""}
{"question": "were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Apollo_17\";\"Canceled_Apollo_missions\";\"Apollo_13_(film)\";\"Apollo_13\""}
{"question": "is chaff produced by hydropower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"MV_Chauncy_Maples\";\"Cruachan_Power_Station\";Welland;Tillicoultry"}
{"question": "is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Relinquishment_of_United_States_nationality\";\"Travel_document\";\"Visa_Waiver_Program\";Passport"}
{"question": "is Garfield known for hating italian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Garfield;\"Garfield's_Thanksgiving\";Garfield;\"Garfield_(character)\""}
{"question": "is basil safe from Hypervitaminosis D?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Mrs._Burns_lemon_basil\";\"Mrs._Burns_lemon_basil\";Basil;\"Welcome_to_Collinwood\""}
{"question": "does a kangaroo incubate its offspring?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Red_kangaroo\";Kangaroo;Kangaroo;\"Ursine_tree-kangaroo\""}
{"question": "did J. Edgar Hoover take his calls in Langley, Virginia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Davison_Army_Airfield\";\"The_Private_Files_of_J._Edgar_Hoover\";\"Gerald_Ford\";\"Alexandria,_Virginia\""}
{"question": "is Elijah part of a Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Elijah;\"Simchat_Torah\";\"Zahle\u0301\";Judaism"}
{"question": "can a cheetah generate enough force to topple Big Show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Defying_Gravity_(song)\";\"Superhuman_strength\";\"Texas_Twister\";\"Tembo_the_Badass_Elephant\""}
{"question": "could Buzz Aldrin have owned a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Buzz_Aldrin\";\"Buzz_Aldrin\";\"Harry_Mendell\";\"Clarence_Hansell\""}
{"question": "was Mercedes-Benz associated with the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Political_views_of_Adolf_Hitler\";\"Ideology_of_the_SS\";Ahnenerbe;\"Automotive_industry_in_Germany\""}
{"question": "did Johnny Carson win enough Emmy's to fill a carton if Emmy's were eggs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Miskel_Spillman\";\"American_game_show_winnings_records\";\"More_Crap\";\"John_Carpenter_(game_show_contestant)\""}
{"question": "can bottlenose dolphins hypothetically outbreed human women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Homosexual_behavior_in_animals\";Dolphin;\"Bottlenose_dolphin\";\"Evolution_of_cetaceans\""}
{"question": "if a baby was born on Halloween would they be a Scorpio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Infanticide;\"Moon_in_Scorpio\";\"I_Don't_Want_to_Be_Born\";\"Al's_Baby\""}
{"question": "could the Great Wall of China connect the Dodgers to the White Sox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Chinese_home_run\";\"Chinese_home_run\";\"Forbidden_City\";\"Great_Wall_of_China\""}
{"question": "would a honey badger fit inside an oven?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Honey_badger\";\"Honey_badger\";\"Honey_badger\";\"Honey_badger\""}
{"question": "did Heracles famous labors exceed a baker's dozen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Labours_of_Hercules\";\"Labours_of_Hercules\";Heracles;\"Labours_of_Hercules\""}
{"question": "does Guam have a state capital?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Guam;\"Haga\u030atn\u0303a,_Guam\";\"District_Court_of_Guam\";\"Guam_Congress_Building\""}
{"question": "are Disney's seven dwarves the original ones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Seven_Dwarfs\";\"Snow_White_and_the_Seven_Dwarfs_(1937_film)\";\"7_Dwarves_\u2013_Men_Alone_in_the_Wood\";\"Diamonds_and_Toads\""}
{"question": "does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Abdul", "extracted_entity": "\"Khalil_el-Moumni\";\"Yusuf_Nazzal\";\"Muhammad_Yunus_Nawandish\";\"Kunj_Yusuf_Pasha\""}
{"question": "is Jesse W. Moore a potential recipient of a Snoopy-themed award from NASA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Silver_Snoopy_award\";\"Lady_Bird_Johnson_Environmental_Award\";\"Alan_Shepard\";\"United_States_Astronaut_Badge\""}
{"question": "has Gorillaz creator been in more bands than Bernard Sumner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bernard_Sumner\";Spacehog;\"Bernard_Sumner\";Gorillaz"}
{"question": "is it safe to eat kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Ergocalciferol;\"Slow_cooker\";\"Kidney_disease\";Kidney"}
{"question": "is zoology unconcerned with strigoi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "zo", "extracted_entity": "\"Zastrozzi,_The_Master_of_Discipline\";\"Santo_Stefano_Island\";\"Diphtheritic_stomatitis\";\"International_Code_of_Zoological_Nomenclature\""}
{"question": "did the phone Alexander Graham Bell use have call waiting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Telephone_call\";\"Long-distance_calling\";\"Bell_(satellite)\";\"Long-distance_calling\""}
{"question": "were paparazzi directly responsible for the death of Amy Winehouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Linda_Kasabian\";\"Tate_murders\";\"Tate_murders\";\"Mary_Brunner\""}
{"question": "on August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Busy_Tonight\";\"The_Tonight_Show\";\"The_Tonight_Show_Starring_Jimmy_Fallon\";\"The_Tonight_Show\""}
{"question": "are more watermelons grown in Brazil than Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Agriculture_in_Brazil\";\"Coffea_canephora\";\"Agriculture_in_Brazil\";\"Genetically_modified_crops\""}
{"question": "does store bought milk have cream at the top?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Milk_Marketing_Board\";Ensure;Milk;Ensure"}
{"question": "could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Old_Bay_Seasoning\";\"Cunard_Line\";\"Boston_Fruit_Company\";\"Cunard_Line\""}
{"question": "did eggs need to be kept cold in the middle ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Eggnog;Axe;\"Medieval_cuisine\";\"History_of_breakfast\""}
{"question": "would Topa Inca Yupanqui have encountered the western honey bee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Topa_Inca_Yupanqui\";\"Topa_Inca_Yupanqui\";\"Topa_Inca_Yupanqui\";Pachacuti"}
{"question": "is \"A Tale of Two Cities\" a popular science novel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_City_&_the_City\";\"The_City_&_the_City\";\"Tales_of_the_City\";\"Changing_Places\""}
{"question": "was Amy Winehouse a fan of Star Wars: Rogue One?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Rogue_One\";\"Flaming_(Internet)\";\"Rhona_Mitra\";\"Geek_girl\""}
{"question": "can shooting bald eagle get a person more prison time than Michael Vick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"True_threat\";\"Chuck_Rosenberg\";\"Bald_eagle\";\"Cornealious_Michael_Anderson_III\""}
{"question": "could you drive a Rowe 550 to the 2008 Summer Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Britannia_Yacht_Club\";\"Rowe_Racing\";\"Venues_of_the_2000_Summer_Olympics\";\"Aston_Martin\""}
{"question": "can you go water skiing on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Venus_Lake\";Zorbing;\"Venus_Lake\";Venus"}
{"question": "would a Wolverine and a Lynx be hard to tell apart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wolverine;Lynx;\"European_badger\";Wolf"}
{"question": "could Plato have agreed with the beliefs of Jainism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Platonism;Plato;Relativism;Philosophy"}
{"question": "do bald eagles nest on Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"Bald_eagle\";\"Bald_Head_Island,_North_Carolina\";\"Bald_eagle\";\"Bald_eagle\""}
{"question": "could James Brown's ex-wives hold a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Real_tennis\";\"Sport_in_Scotland\";\"Elizabeth_Town_&_Country_Club\";\"Partners_in_Crime_(short_story_collection)\""}
{"question": "should Peter Griffin be an expert at the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Peter", "extracted_entity": "\"John_Rousmaniere\";\"Adaptive_expertise\";\"John_Rousmaniere\";\"Morton_W._Coutts\""}
{"question": "did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Charles_Manson\";\"Appetite_for_Destruction\";\"Dennis_Wilson\";\"Olivia_Newton_Bundy\""}
{"question": "is there any absolute way to prevent abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "\"Partial-Birth_Abortion_Ban_Act\";\"Birth_control\";\"Unsafe_abortion\";\"Instillation_abortion\""}
{"question": "is Saturn named after king of gods in Greek mythology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Saturn_(mythology)\";\"Saturn_(song)\";\"Saturn_(song)\";Saturn"}
{"question": "were any of despised Pope Alexander VI's descendants canonized?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Canonization_of_the_Romanovs\";\"Canonization_of_the_Romanovs\";\"Pope_Alexander_VI\";\"Canonization_of_the_Romanovs\""}
{"question": "would it be difficult to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Terraforming_of_Venus\";\"Venus_snow\";\"Colonization_of_Venus\";\"Colonization_of_Venus\""}
{"question": "can chemicals in onion help create a thermonuclear bomb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Mustard_oil_bomb\";\"Mustard_oil_bomb\";ANFO;\"Incendiary_device\""}
{"question": "are anchovies associated with Italian food?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "anch", "extracted_entity": "\"Anchovies_as_food\";\"Anchovies_as_food\";Anchovy;\"Peruvian_anchoveta\""}
{"question": "did Operation Barbarossa or Barbarossa's last expedition succeed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Operation", "extracted_entity": "\"Order_of_battle_for_Operation_Barbarossa\";\"Operation_Barbarossa\";\"Operation_Barbarossa\";\"Operation_Barbarossa\""}
{"question": "has numerology helped shape hotel layouts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Numerology;Numerology;Numerology;Numerology"}
{"question": "did The Who have to cancel tours due to World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Who_Tour_1966\";\"Reorganization_of_occupied_dioceses_during_World_War_II\";\"Magic_Bus:_The_Who_on_Tour\";\"Wartime_League\""}
{"question": "can an art dealer buy Boeing 737-800 with a Da Vinci painting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Annibale_Berlingieri\";\"Annibale_Berlingieri\";\"Larry_Gagosian\";\"Salvator_Mundi_(Leonardo)\""}
{"question": "would a crocodile survive longer in Great Salt Lake than alligator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Saltwater_crocodile\";\"Common_basilisk\";Dilophosaurus;Arowana"}
{"question": "would a clouded leopard encounter an awake pangolin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Leopard_attack\";\"Asiatic_lion\";\"Sinai_agama\";\"Leopard_attack\""}
{"question": "does acupuncture cause pain in many people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Acupuncture;Acupuncture;Acupuncture;Acupuncture"}
{"question": "are twins always born during the same year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Twin;\"Conjoined_twins\";Twin;\"Conjoined_twins\""}
{"question": "could a nymph tick pass through a standard hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Knockout_punch\";\"Knockout_punch\";\"Leather_punch\";\"Ticket_punch\""}
{"question": "would WWF be angrier if you killed koala instead of black swan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Pride_(Five_Finger_Death_Punch_song)\";\"Would_You_Rather_(film)\";\"Sheena_(film)\";\"Yinka_Dene_Alliance\""}
{"question": "does New Year's Day always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Thursday;\"Thanksgiving_(United_States)\";\"Ash_Wednesday\";\"Public_holidays_in_Germany\""}
{"question": "is a pottery kiln inappropriate for use with glass blowing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pottery;Kiln;Kiln;Kiln"}
{"question": "do people of the Iyer caste eat meat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Meat;Iyer;\"Jain_vegetarianism\";\"Cattle_slaughter_in_India\""}
{"question": "do carpenters understand geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stereotomy_(descriptive_geometry)\";\"The_Carpenters\";\"The_Carpenters\";\"Steel_detailer\""}
{"question": "did the crew of Apollo 15 take pictures of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Mount_Sharp\";\"Mount_Sharp\";\"Mount_Sharp\";\"Apollo_15\""}
{"question": "are human footprints absent from Mount Sharp?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Skookum_cast\";\"Prehistoric_Trackways_National_Monument\";\"1790_Footprints\";Albertosaurus"}
{"question": "could Tom Cruise explain mental auditing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tom", "extracted_entity": "\"Being_Tom_Cruise\";\"Tom_Cruise:_Unauthorized\";\"Tom_Cruise\";\"Tom_Cruise\""}
{"question": "is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Chinatown,_Milan\";\"Alfa_Romeo_MiTo\";\"Chinatown,_Milan\";\"Wuling_Rongguang\""}
{"question": "would drinking a glass of lemonade provide Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Lemonade;\"Vitamin_C\";\"Vitamin_C\";\"Cranberry_juice\""}
{"question": "does a game engine have a fuel injector?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Ignition_Factor\";\"Massively_multiplayer_online_game\";\"Active_Fuel_Management\";\"Multiplayer_online_battle_arena\""}
{"question": "did J. D. Salinger ever ask his father for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Flowers_of_St._Francis\";\"William_Peter_Blatty\";\"J._D._Salinger\";Mondegreen"}
{"question": "were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ancient_Egyptian_technology\";\"Ancient_Greek_technology\";\"Ancient_Egyptian_technology\";\"Ancient_technology\""}
{"question": "is the Joker in a healthy romantic relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Love-Ins\";\"Love_&_Sex\";\"Nontraditional_Love\";\"Love_&_Sex\""}
{"question": "would an adherent of Zoroastrianism consult the Quran for religious guidance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Criticism_of_Zoroastrianism\";\"Religion_in_pre-Islamic_Arabia\";Zoroastrianism;\"Islamic_culture\""}
{"question": "would a diet of ice eventually kill a person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ice_famine\";\"Ice_famine\";\"Beyond_the_Ice_Limit\";Cold"}
{"question": "can binary numbers and standard alphabet satisfy criteria for a strong password?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Password;\"Password_strength\";\"Strong_cryptography\";\"Password_psychology\""}
{"question": "is Phobos (moon) name origin similar to Roman god Pavor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Turms;Turms;\"Pluto_(mythology)\";\"Pluto_(mythology)\""}
{"question": "would members of Blue Lives Matter support every element of Grand Theft Auto III?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Thunderiders;\"Fallout_4\";\"3_Percenters\";\"SOCOM_U.S._Navy_SEALs:_Fireteam_Bravo_3\""}
{"question": "will twenty pea pods contents cover entire chess board?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Four-player_chess\";Pea;\"Chess_on_a_Really_Big_Board\";\"Pea_galaxy\""}
{"question": "was the Joker an enemy of the Avengers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Alternative_versions_of_Joker\";\"Alternative_versions_of_Joker\";\"Red_Hood\";Catwoman"}
{"question": "could Christopher Nolan borrow pants from Danny Devito?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "DeVito/Verdi;\"Mango_(Saturday_Night_Live)\";\"Body_inflation\";\"Huntington_Hartford\""}
{"question": "could you watch Naruto and Puzzle Place on the same channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Arashi;IQiyi;\"The_Puzzle_Place\";Naruto"}
{"question": "is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Ezaki_Glico\";\"LG_Electronics\";\"L_Tower\";\"LG_Electronics\""}
{"question": "was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Colossus_of_Rhodes_(Dali\u0301)\";\"Acropolis_of_Athens\";\"The_Colossus_of_Rhodes_(Dali\u0301)\";\"Olympic_Mountains\""}
{"question": "does Fraktur have a sordid history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fraktur_(folk_art)\";\"Fraktur_(folk_art)\";\"Fraktur_(folk_art)\";\"The_Sentimental_Agents_in_the_Volyen_Empire\""}
{"question": "would a hedgehog avoid animals without a spinal cord?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Over_the_Hedge_(film)\";Beagle;\"Feist_(dog)\";\"Beagle_Brigade\""}
{"question": "can you worship Ahura Mazda at a mosque?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sujud;Hajj;Mosque;\"Sacred_architecture\""}
{"question": "would a moose hypothetically be too much for a minotaur to devour whole?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ogdens'_Nut_Gone_Flake\";\"Cervalces_latifrons\";\"Cervalces_latifrons\";\"Cervalces_latifrons\""}
{"question": "is Gandalf hypothetically a formidable foe for Charmed's Barbas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Knightly_Tale_of_Gologras_and_Gawain\";\"Mabon_ap_Modron\";\"Sir_Launfal\";Gandalf"}
{"question": "do suburbs encourage the use of cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Automotive_city\";\"Urban_sprawl\";\"Effects_of_the_car_on_societies\";\"Ve\u0301lib'\""}
{"question": "could Al Capone have read works from the Harlem Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"African-American_literature\";\"John_Gilmore_(writer)\";\"Achille_Peretti_(artist)\";\"Frank_Samperi\""}
{"question": "can pancreas removal cause bankruptcy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pan", "extracted_entity": "\"Pancreas_transplantation\";Pancreaticoduodenectomy;Pancreatectomy;Hypopituitarism"}
{"question": "is shrimp prevalent in Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ethiopian_cuisine\";\"Ethiopian_cuisine\";\"African_cuisine\";\"Israeli_cuisine\""}
{"question": "does Marco Rubio have a close relationship with Allah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Q-Tip_(musician)\";\"Cat_Stevens\";\"Islam_and_cats\";\"Islam_and_cats\""}
{"question": "does The Doctor keep his ship in his childhood home?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"A_Night_in_Sickbay\";\"A_Night_in_Sickbay\";\"A_Ship_Comes_In\";\"Doctor's_Orders_(Star_Trek:_Enterprise)\""}
{"question": "did Archduke Franz Ferdinand of Austria participate in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Archduke_Franz_Ferdinand_of_Austria\";\"Archduke_Franz_Ferdinand_of_Austria\";\"Archduke_Franz_Ferdinand_of_Austria\";\"Allies_of_World_War_I\""}
{"question": "is a beard is moss that grows on a human?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Beard;Beard;Pogonophobia;Goatee"}
{"question": "is there radiation where Nikola Tesla once worked?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Tesla_Science_Center_at_Wardenclyffe\";\"Wardenclyffe_Tower\";\"Nikola_Tesla\";\"Tesla_Experimental_Station\""}
{"question": "would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Mary,_mother_of_Jesus\";Panagia;\"Council_of_Ephesus\";\"Church_of_Mary\""}
{"question": "could one Amazon share ever buy twenty year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Netflix;Netflix;Netflix;Netflix"}
{"question": "would you have to wear a coat when on Phobos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Positive_pressure_personnel_suit\";\"The_A-Team\";\"Private_(rank)\";\"Moonbase_Alpha_(Space:_1999)\""}
{"question": "is Cambodia too expensive for 2020 richest billionaire to own?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Billionaire\";\"The_Billionaire\";NagaCorp;NagaCorp"}
{"question": "did Emma Stone pursue a higher education?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Emma_Slade\";\"Winifred_Goldring\";\"Emma_Stone\";\"Emma_Stone\""}
{"question": "was San Antonio the site of a major battle in the 19th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_San_Antonio\";\"Fiesta_San_Antonio\";\"Siege_of_Be\u0301xar\";\"Battle_of_the_Alamo\""}
{"question": "can the United Nations Framework Convention on Climate Change be held at the Javits Center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_Nations_Climate_Change_conference\";\"United_Nations_Climate_Change_conference\";\"2013_United_Nations_Climate_Change_Conference\";\"United_Nations_Climate_Change_conference\""}
{"question": "can music be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Music_and_political_warfare\";\"Heavy_metal_music\";\"Martial_music\";\"Music_in_psychological_operations\""}
{"question": "could an ocelot outrun a kindergartner? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Joint-eater;Xaltotun;\"Will-o'-the-wisp\";\"Ork_(folklore)\""}
{"question": "was Elizabeth II the Queen during the Persian Gulf War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "1990;\"Gulf_War\";\"Gulf_War\";\"Margaret_Thatcher\""}
{"question": "would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Robert_Couteau\";\"Robert_Couteau\";\"Jim_Lewis_(astrologer)\";Sepharial"}
{"question": "did Woodrow Wilson consider Blacks to be equal members of society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"American_Equal_Rights_Association\";\"All_men_are_created_equal\";\"Miscegenation:_The_Theory_of_the_Blending_of_the_Races,_Applied_to_the_American_White_Man_and_Negro\";\"American_Equal_Rights_Association\""}
{"question": "if your electric stove has a glass top, should you use cast iron skillets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cast-iron_cookware\";\"Cast-iron_cookware\";\"Barbecue_grill\";Wok"}
{"question": "can soup be eaten with the hands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "soup", "extracted_entity": "Soup;Soup;\"Cellophane_noodles\";\"Japanese_cuisine\""}
{"question": "did Cleopatra have ethnicity closer to Egyptians than Greeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ancient_Egyptian_race_controversy\";\"Early_life_of_Cleopatra\";\"Ancient_Egyptian_race_controversy\";\"Ancient_Egyptian_race_controversy\""}
{"question": "does the judo rank system reach the triple digits?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Rank_in_Judo\";\"Rank_in_Judo\";\"Judo_in_Canada\";Judo"}
{"question": "was the son of Tsar Nicholas a daredevil?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Ivan_Vorpatril\";\"Kraven_the_Hunter_(Alyosha_Kravinoff)\";\"Nicholas_Fandorin\";\"Ivan_Vorpatril\""}
{"question": "did Bill Gates achieve Latin honors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bill_Gates\";\"Bill_Gates\";\"Bill_Gates\";\"Bill_Clinton\""}
{"question": "has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bill_Cosby_Is_Not_Himself_These_Days\";\"Bill_Cosby_Is_Not_Himself_These_Days\";\"Two_Bad_Neighbors\";\"No_Time_for_Sergeants\""}
{"question": "if Goofy were a pet, would he need heartworm prevention?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Superfudge;\"Just_Stupid!\";\"Goofy_Groceries\";\"Popeye_no_Eigo_Asobi\""}
{"question": "did Richard Wagner support the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Wagner_controversies\";\"Wagner_controversies\";\"Nietzsche_contra_Wagner\";\"Richard_Wagner\""}
{"question": "was historical Dracula from a town in Bucharest?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Romanians;\"Bran_Castle\";\"Vlad_Dracul_House\";\"Vlad_Dracul_House\""}
{"question": "is electricity necessary to balance an account in Microsoft Excel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Electricity_meter\";\"Electric_power\";\"Electric_power\";\"Electricity_meter\""}
{"question": "did Pink Floyd have a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"San_Tropez_(song)\";\"Roxanne_(song)\";\"Les_Lilas\";\"Country_Song_(Pink_Floyd_song)\""}
{"question": "has the creator of Futurama lived in multiple centuries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"King_Kull_(DC_Comics)\";Futurama;\"Infinite_Man\";Omni-Man"}
{"question": "is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cosesaurus;\"Bone_Sharps,_Cowboys,_and_Thunder_Lizards\";Sternum;Polyonax"}
{"question": "did Elizabeth I of England own any viscose fiber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Fustian;\"Serge_(fabric)\";Paduasoy;\"Bridget_Chaworth\""}
{"question": "could a cat ride Panzer VIII Maus tank missile from Barcelona to Madrid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"ETF_Ride_Systems\";\"Helico\u0301pteros_del_Sureste\";\"Barcelona\u2013El_Prat_Airport\";\"Force_Majeure_(film)\""}
{"question": "did Jack Dempsey fight the current WBC heavyweight champion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Jack_Dempsey_vs._Luis_A\u0301ngel_Firpo\";\"Frank_Shamrock\";\"HBO_World_Championship_Boxing\";\"Rocky_Fratto\""}
{"question": "would a cauliflower farmer prosper at a latitude of 75\u00b0 N?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Roswell_Garst\";Farmer;\"Stripper_(agriculture)\";\"Vincent_Kosuga\""}
{"question": "would a student in eleventh grade be unable to run for president of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Channing_E._Phillips\";\"Charles_R._Doty\";\"Harrison_Bergeron_(film)\";\"Voter_suppression\""}
{"question": "was Amy Winehouse familiar with Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Brexit;Brexit;\"Brexit_in_popular_culture\";\"Brexit_in_popular_culture\""}
{"question": "can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_Kingdom_in_the_Eurovision_Song_Contest_2019\";\"United_Kingdom_in_the_Eurovision_Song_Contest_2019\";\"Lucie_Jones\";\"Sound_of...\""}
{"question": "are Scottish people descended from Mary, Queen of Scots part French?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Mary,_Queen_of_Scots\";\"Mary_Queen_of_Scots_(2013_film)\";\"History_of_Scottish_Gaelic\";\"Scottish_Americans\""}
{"question": "are quadrupeds represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Shagai;\"Chinese_zodiac\";\"Graphic_pejoratives_in_written_Chinese\";\"Chinese_zodiac\""}
{"question": "is the Louvre in billionaire George Soros's price range?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Groupe_du_Louvre\";\"Louvre_Pyramid\";\"Le_Louvre:_The_Palace_&_Its_Paintings\";\"Louvre_Pyramid\""}
{"question": "would a duke hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Tsarevich_Ivan_Ivanovich_of_Russia\";\"The_Tsar's_Bride_(opera)\";\"Morganatic_marriage\";\"Grand_Duke_Paul_Alexandrovich_of_Russia\""}
{"question": "can a jet plane be made without society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"History_of_aviation\";\"Men_Without_Wings\";\"Pilot_(V)\";\"Flying_car\""}
{"question": "would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Jim_Thorpe\";\"Jimmy_Feigen\";\"Joey_Cheek\";\"Bonnie_St._John\""}
{"question": "could Ryan Crouser throw a bengal fox with ease?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Carrom_ball\";Macuahuitl;\"Chocks_Away\";\"Rescue_of_Bat_21_Bravo\""}
{"question": "would a physician be unlikely to recommend Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Reiki;Reiki;Reiki;Reiki"}
{"question": "can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Revenue_Equalization_Reserve_Fund\";\"Emergency_Economic_Stabilization_Act_of_2008\";\"Revenue_Equalization_Reserve_Fund\";Microcredit"}
{"question": "can Burundi's communicate with citizens of New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Communications_in_Burundi\";\"Media_of_Burundi\";\"KISS_NB\";\"KISS_NB\""}
{"question": "is Morocco an ideal location for water skiing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Taghazout;Agadir;\"Solar_power_in_Morocco\";\"Tourism_in_Morocco\""}
{"question": "did Al Capone carry a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al", "extracted_entity": "\"Richard_Martino\";\"Tim_Cappello\";\"Richard_Martino\";Carcano"}
{"question": "was Mark Twain a struggling inventor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Mark_Twain\";\"Robert_Quackenbush\";\"The_Adventures_of_Mark_Twain_(1944_film)\";\"Robert_Quackenbush\""}
{"question": "is B's place in alphabet same as Prince Harry's birth order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "B;\"Type_B_Cipher_Machine\";\"A\u030a\";B"}
{"question": "would a vegan eat a traditional Paella dish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"American_Jewish_cuisine\";Kashrut;Paella;Pagash"}
{"question": "is cow methane safer for environment than cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Zero-emissions_vehicle\";\"Zero-emissions_vehicle\";\"United_States_emission_standards\";\"Zero-emissions_vehicle\""}
{"question": "was Darth Vader monogamous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Darth_Vader\";\"Skywalker_family\";Sith;\"Darth_Vader\""}
{"question": "were there footprints on the moon in 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Footprints_on_the_Moon_(1969_film)\";\"Moon_landing_conspiracy_theories\";\"Moon_landing_conspiracy_theories\";\"Footprints_on_the_Moon_(1969_film)\""}
{"question": "does Pantheon in Paris have a unique name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Panthe\u0301on\";\"Panthe\u0301on\";Paris;\"Pantheon,_Rome\""}
{"question": "can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Northern_Epirote_Declaration_of_Independence\";\"Greek_Muslims\";\"History_of_the_Jews_in_Greece\";\"Stefanos_Sarafis\""}
{"question": "would a Beaver's teeth rival that of a Smilodon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Smilodon;\"Wabamun_Lake\";\"Beaver_dam\";Smilodon"}
{"question": "can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tropicana_Field\";\"Pimlico_Race_Course\";\"Tropicana_Field\";\"Tropicana_Field\""}
{"question": "do calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Whers;\"Calico_cat\";\"Drain_fly\";\"Viola_pedata\""}
{"question": "are all cucumbers the same texture?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Cucumber_mosaic_virus\";\"Cucumber_mosaic_virus\";\"Cucumber_sandwich\";Cucumber"}
{"question": "would Library of Alexandria need less shelf space than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Library_of_Alexandria\";\"Library_of_Alexandria\";\"Library_of_Alexandria\";\"Library_of_Alexandria\""}
{"question": "would a spider wasp be more effective than a bullet ant to stop a criminal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Spider_wasp\";\"If_Looks_Could_Kill_(film)\";Latrodectus;\"Spider_wasp\""}
{"question": "would a triples tandem bike support Apollo 15 crew?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Apollo", "extracted_entity": "\"Tandem_bicycle\";\"Apollo_15\";\"Tandem_bicycle\";\"Tandem_bicycle\""}
{"question": "if one of your feet is in a leg cast, should the other be in a sandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Sandal;\"Stiletto_heel\";\"Orthopedic_cast\";Slingback"}
{"question": "did Klingons appear in the movie The Last Jedi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Languages_in_Star_Wars\";\"Max_Rebo_Band\";\"Max_Rebo_Band\";\"Kalmyk_Oirat\""}
{"question": "can you find Depala's race in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hunt_Bowman\";Zendikar;\"Races_and_factions_of_Warcraft\";\"Races_and_factions_of_Warcraft\""}
{"question": "would a Dodo hypothetically tower over Ma Petite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tiny_Topsy\";Dodo;Dodo;\"Kokomo_Jr.\""}
{"question": "can the Very Large Telescope observe the largest mountain on Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Mount_Everest_webcam\";\"Mount_Sharp\";\"Mons_Huygens\";\"Mount_Everest_webcam\""}
{"question": "was Morris County named after a chief justice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Lewis_Morris_(governor)\";\"Robert_Hunter_Morris\";\"Lewis_Morris_(governor)\";\"Richard_J._Hughes\""}
{"question": "is the Gujarati script the same category of script as Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kan", "extracted_entity": "\"Gujarati_alphabet\";\"Gujarati_language\";\"Gujarati_alphabet\";\"Gujarati_grammar\""}
{"question": "did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "France;\"Around_the_Moon\";\"Orbital_mechanics\";\"Napole\u0301on_(coin)\""}
{"question": "would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"300_Club\";\"Degree_day\";Cryoseism;\"January_2017_European_cold_wave\""}
{"question": "if Martin Luther did one theses a day would he run out in half a year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Ninety-five_Theses\";\"Martin_Luther\";\"Ninety-five_Theses\";\"Martin_Luther\""}
{"question": "are honey badgers and hyenas anatomically dissimilar? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hyaena;Hyaena;Hyena;Chimpanzee"}
{"question": "did Al Pacino act in a movie during World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Al_Pacino\";\"Secret_Agent_of_Japan\";\"Al_Lettieri\";\"Al_Pacino\""}
{"question": "could you watch a new Seinfeld episode every day for a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"You're_Getting_Old\";\"2000_Year_Old_Man\";Seinfeld;\"You're_Getting_Old\""}
{"question": "did Barack Obama participate in the Reformation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Reformation_Day\";\"Mormon_Reformation\";Reformation;Reformation"}
{"question": "did President William Howard Taft read DC Comics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hulbert_Taft\";\"Publication_history_of_Superman\";\"Hulbert_Taft\";\"Charles_Phelps_Taft\""}
{"question": "can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Wakaliwood;\"Can't_Stop_(Red_Hot_Chili_Peppers_song)\";\"Ski_wax\";\"Matt_Wachter\""}
{"question": "are there Americans still enlisted in the Confederate States Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Confederate_States_Army\";\"Confederate_States_of_America\";\"Confederate_States_Army\";\"Confederate_States_Army\""}
{"question": "are right wing Amreicans opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Islamo-leftism;\"Ila_al-Amam_(Morocco)\";\"Ila_al-Amam_(Morocco)\";\"Arab_Nationalist_Movement\""}
{"question": "can a sea turtle play tennis using a tennis racket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Beach_tennis\";\"Beach_tennis\";Turtle;\"Deck_tennis\""}
{"question": "in a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Shuttlecock;\"Andy_Lennon\";\"American_kestrel\";\"Swallowtail_butterfly\""}
{"question": "is shoe soup innocuous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Ghost_word\";\"The_History_of_Little_Goody_Two-Shoes\";\"Ina_Garten\";\"Mess_of_pottage\""}
{"question": "did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Chen_Kenmin\";\"Tsumasaburo\u0304_Bando\u0304\";\"Propaganda_in_Japan_during_the_Second_Sino-Japanese_War_and_World_War_II\";\"History_of_Japanese_cuisine\""}
{"question": "did Holy Saturday 2019 have special significance to pot smokers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"19_Kislev\";Sabbath;\"Bon_Air_Presbyterian_Church\";Sabbath"}
{"question": "do you need lactobacillus to make pickles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Lactobacillus_pontis\";Lactobacillus;\"Lactobacillus_pontis\";Lactobacillus"}
{"question": "would a Gray Whale fit easily in an above ground pool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Beluga_whale\";\"Kaunaoa_Bay\";\"It_Came_from_Beneath_the_Sea\";Whale"}
{"question": "will a celibate cleric likely suffer a stoning in Somalia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hudud;Stoning;Stoning;\"Sahar_Gul\""}
{"question": "could Little Women have been a book read by veterans of the civil war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Little", "extracted_entity": "\"Louisa_May_Alcott\";\"Amos_Bronson_Alcott\";\"Little_Men\";\"Louisa_May_Alcott\""}
{"question": "is MIX a word and a roman numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "MIX;MIX;MIX;\"Blend_word\""}
{"question": "would Downton Abbey finale viewership defeat every Kazakhstan citizen in tug of war?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"The_Wenlock_Arms\";\"You_Win_or_You_Die\";\"Prized_Apart\";\"Tom_Clancy's_EndWar\""}
{"question": "was dynamite used during Middle Ages warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Dynamite;\"Gunpowder_artillery_in_the_Middle_Ages\";\"Early_thermal_weapons\";\"Tunnel_warfare\""}
{"question": "is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "P", "extracted_entity": "\"Michael_Vick\";\"The_Michael_Vick_Project\";\"Michael_Vick\";\"Michael_Vick\""}
{"question": "can Kate Gosselin's household fill out a Bandy team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"One_for_the_Team\";\"The_F_Word_(2013_film)\";\"Team_Robespierre\";\"Team_Robespierre\""}
{"question": "is the tree species that the name Leipzig refers to an evergeen tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "Eckbach;\"Leipzig,_North_Dakota\";\"Leipzig,_North_Dakota\";Tetradium"}
{"question": "will Gremlins sequels tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gremlins;\"The_Matrix_Revolutions\";\"Gremlins_2:_The_New_Batch\";\"The_Matrix\""}
{"question": "is Snow White an example of good consent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Civility;\"By_Common_Consent\";\"Sexual_consent\";\"Covariation_model\""}
{"question": "is MF Doom a Fantastic Four villain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Doctor_Doom\";\"Doctor_Doom\";\"Fantastic_Four_in_film\";\"Frightful_Four\""}
{"question": "would most grand masters know what the French Defense is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"D'Artagnan_and_Three_Musketeers\";\"Louis-Nicolas_Davout\";Rossignols;\"History_of_Freemasonry_in_France\""}
{"question": "are there options for students who struggle to understand the writing style of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Free_writing\";\"Othello_error\";Otium;\"Reader's_theatre\""}
{"question": "would an environmentalist advocate for preventing domestic canine reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Trap\u2013neuter\u2013return\";\"Dayton_Hyde\";\"Human\u2013canine_bond\";\"Trap\u2013neuter\u2013return\""}
{"question": "could a monolingual American read Lenovo's native name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Digraphia;\"Johann_Martin_Schleyer\";\"Rudolf_Flesch\";Hypercorrection"}
{"question": "can 1980 United States presidential election result be considered a photo finish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"1980_United_States_presidential_election_in_Nevada\";\"1980_United_States_presidential_election\";\"1980_United_States_presidential_election\";\"1980_Democratic_Party_presidential_primaries\""}
{"question": "did Cynthia Powell celebrate a silver anniversary with John Lennon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Julian_Lennon\";\"Julian_Lennon\";\"Cynthia_Lennon\";\"Cynthia_Lennon\""}
{"question": "is it possible for biologist Rachel Carson to have flown to the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"RV_Rachel_Carson_(2017)\";\"Ann_Pellegreno\";\"Under_a_Jarvis_Moon\";\"Amelia_Earhart\""}
{"question": "did Confederate States Army influence West Point fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Union_Army\";\"United_States_Military_Academy\";\"United_States_Military_Academy\";\"Continental_Army\""}
{"question": "would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Will_Ferrell\";\"Will_Any_Gentleman...?\";\"Will_Any_Gentleman...?\";\"Will_Any_Gentleman...?\""}
{"question": "is the Mona Lisa in the same museum as the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Culture_of_Paris\";\"Art_in_Paris\";\"Mona_Lisa\";\"Mona_Lisa_(Prado's_version)\""}
{"question": "does the land in close proximity to beaver dams suffer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Beaver_dam\";\"Beaver_dam\";Dam;\"Beaver_dam\""}
{"question": "was Jackson Pollock trained by Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Leonardo", "extracted_entity": "\"Jackson_Pollock\";\"Jackson_Pollock\";\"Leonardo_Drew\";\"Jackson_Pollock\""}
{"question": "did Francois Mitterrand ever meet Barak Obama while they both held the position of President?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bill_Clinton\";\"French_presidential_debates\";\"Benjamin_Netanyahu\";\"Foreign_policy_of_the_Franc\u0327ois_Mitterrand_administration\""}
{"question": "would Carrie Poppy be likely to trust a psychic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Poppy_Montgomery\";\"Poppy_Montgomery\";\"Watching_the_Detectives_(film)\";\"Rumours_(Glee)\""}
{"question": "would stargazers prefer binoculars over a telescope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Victor_J._Stenger\";Binoculars;Binoculars;\"Star_hopping\""}
{"question": "is it possible to binge entire EastEnders series without water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"EastEnders_in_popular_culture\";\"Oasis_(drink)\";\"Hannah_Waterman\";\"Banana_Sprite_challenge\""}
{"question": "was Mother Theresa a follower of atheism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She", "extracted_entity": "\"Edith_Stein\";\"Theosophical_mysticism\";\"Hinduism_in_the_West\";\"Theosophical_mysticism\""}
{"question": "is the BBC World Service hosted in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"BBC_World_Service\";\"BBC_World_Service\";\"BBC_World_Service\";\"BBC_World_Service\""}
{"question": "did Melania Trump have same profession as Olga Kurylenko?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Melania", "extracted_entity": "\"Katia_Elizarova\";\"Yulia_Makarenkova\";\"Nataliya_Nikonorova\";\"Maria_Alyokhina\""}
{"question": "was Robert Downey Jr. a good role model as a young man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"John_G._Thomas\";\"Dr._No_(film)\";\"Robert_Downey_Jr.\";\"Robert_Downey_Jr.\""}
{"question": "should a finished website have lorem ipsum paragraphs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Lorem_ipsum\";\"Lorem_ipsum\";\"Lorem_ipsum\";\"Lorem_ipsum\""}
{"question": "would Jacques Du\u00e8ze have been friends with Richard Dawkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jacques", "extracted_entity": "\"Charles_Pe\u0301guy\";\"Ernest_Ange_Duez\";\"Ernest_Ange_Duez\";\"Jacques_Chardonne\""}
{"question": "would Gordon Ramsey use uranium as a seasoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Uraninite;\"Operation_Grapple\";\"Operation_Grapple\";\"Operation_Sandwedge\""}
{"question": "does Robert De Niro use a microscope at work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Robert_Emerson_(scientist)\";\"Thomas_Wedgwood_(photographer)\";\"The_Eiger_Sanction_(film)\";\"Comparison_microscope\""}
{"question": "is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ian_Fishback\";\"Ian_Fishback\";\"Paula_Jones\";\"Political_Google_bombs_in_the_2004_U.S._Presidential_election\""}
{"question": "are pirate lieutenants like navy lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Petty_officer\";\"Seaman_status_in_United_States_admiralty_law\";Midshipman;\"Young_gentlemen\""}
{"question": "does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Open_aortic_surgery\";\"Open_aortic_surgery\";\"Percutaneous_aortic_valve_replacement\";\"Double_aortic_arch\""}
{"question": "does Homer Simpson need two hands worth of fingers to count to 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ant_&_Dec's_Push_the_Button\";\"Ant_&_Dec's_Push_the_Button\";\"Brother,_Can_You_Spare_Two_Dimes?\";Finger-counting"}
{"question": "can you see the moon in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Walking_on_the_Moon\";\"Die_Another_Day\";\"British_television_Apollo_11_coverage\";\"I_See_the_Moon\""}
{"question": "is the Asian black bear multicolored?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Giant_panda\";\"Asian_black_bear\";\"Japanese_giant_hornet\";\"Asian_giant_hornet\""}
{"question": "can United States Secretary of State do crimes in U.K. without being arrested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nited", "extracted_entity": "\"Threatening_government_officials_of_the_United_States\";\"U.S._Customs_and_Border_Protection\";\"U.S._Immigration_and_Customs_Enforcement\";\"Bureau_of_Diplomatic_Security\""}
{"question": "did Alice's Adventures in Wonderland inspire Macbeth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"King_Duncan\";Banquo;\"Macbeth,_King_of_Scotland\";Macbeth"}
{"question": "could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Adverse_health_effects_from_lunar_dust_exposure\";\"Adverse_health_effects_from_lunar_dust_exposure\";\"Adverse_health_effects_from_lunar_dust_exposure\";\"Adverse_health_effects_from_lunar_dust_exposure\""}
{"question": "was England directly involved in the Arab-Israeli conflict?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Arab\u2013Israeli_conflict\";\"Arab_League_and_the_Arab\u2013Israeli_conflict\";\"Arab_League\";\"Israel\u2013United_Kingdom_relations\""}
{"question": "can an adult male stand on top Donatello's bronze David and touch the Sistine Chapel ceiling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"David_(Donatello)\";\"David_(Donatello)\";\"David_(Michelangelo)\";\"David_(Donatello)\""}
{"question": "was Dioskourides a lapidary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Dionysios_Skylosophos\";\"The_King_Must_Die\";\"Hippocrates,_father_of_Peisistratos\";\"Hippocrates,_father_of_Peisistratos\""}
{"question": "would a CEO typically clean the toilets in a company's building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Janitor;\"Executive_compensation_in_the_United_States\";\"John_Henry_Patterson_(NCR_owner)\";\"Executive_suite\""}
{"question": "can you purchase General Motors products at a movie theater?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"History_of_cinema_in_the_United_States\";\"Movie_theater\";\"AMC_Theatres\";\"Movie_theater\""}
{"question": "did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Xena:_Warrior_Princess_(video_game)\";\"Xena:_Warrior_Princess\";\"Xena:_Warrior_Princess\";\"Xena:_Warrior_Princess\""}
{"question": "could all of the people who pass through 30th Street Station every day fit in Dorton Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"White_River_Amphitheatre\";\"United_Center\";\"White_River_Amphitheatre\";\"Improv_Everywhere\""}
{"question": "can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Karl_Tu'inukuafe\";\"Makerita_Urale\";\"What_Can_I_Do_(The_Corrs_song)\";\"Hiroshi_Fujiwara\""}
{"question": "could all the unemployed people due to 1933 Great Depression fit in Tiger Stadium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tiger_Stadium_(Detroit)\";\"Tiger_Stadium_(Detroit)\";\"Tiger_Stadium_(Detroit)\";\"Tiger_Stadium_(LSU)\""}
{"question": "would a duck ever need a Caesarean section?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Lord_Love_a_Duck\";\"Caesarean_section\";\"Caesarean_section\";\"Caesarean_section\""}
{"question": "is nickel a better payout than mercury if given a dollar per atomic number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Simon\u2013Ehrlich_wager\";Nickel;Manganese;\"Big_Nickel\""}
{"question": "does Pikachu like Charles Darwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pikachu;Pikachu;Pikachu;Pikachu"}
{"question": "can Simon Cowell vote for the next Supreme Court judge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Appointment_and_confirmation_to_the_Supreme_Court_of_the_United_States\";\"Appointment_and_confirmation_to_the_Supreme_Court_of_the_United_States\";\"Appointments_Clause\";\"Nuclear_option\""}
{"question": "is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Stomatitis;\"Dental_fluorosis\";Probiotic;\"Otitis_media\""}
{"question": "is US route 1 dominated by historically red states?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"U.S._Route_1\";\"U.S._Route_1\";\"U.S._Route_1\";\"U.S._Route_1\""}
{"question": "can a cell fit inside of a shoebox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dog_crate\";\"Glove_compartment\";\"Battery_holder\";\"Dog_crate\""}
{"question": "is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Tobacco_smoking\";\"Smokeless_tobacco\";\"History_of_nicotine_marketing\";\"The_Tale_of_Benjamin_Bunny\""}
{"question": "can Tame Impala's studio band play a proper game of Hot Potato?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Go-go;\"Jo\u0301nsi\";\"Alex_Minoff\";\"Animal_Kingdom_(band)\""}
{"question": "is the title of Shirley Bassey's 1971 diamond song a true statement?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Undisputed_Truth\";\"The_Undisputed_Truth\";\"The_Undisputed_Truth\";\"The_Undisputed_Truth\""}
{"question": "would P. G. Wodehouse be taught in second grade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"P._G._Wodehouse\";\"P._G._Wodehouse_minor_characters\";\"P._G._Wodehouse_minor_characters\";\"P._G._Wodehouse_minor_characters\""}
{"question": "can Curiosity take samples of rocks from Lacus Temporis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Lacus_Temporis\";\"Lacus_Temporis\";\"Capri_Mensa\";\"Margaritifer_Sinus_quadrangle\""}
{"question": "is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ho", "extracted_entity": "\"From_G's_to_Gents_(season_1)\";\"The_Bachelorette_(season_3)\";\"Megan_Wants_a_Millionaire\";\"I_Love_Money_(season_1)\""}
{"question": "would Phineas and Ferb enjoy winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Phineas_and_Ferb_Christmas_Vacation\";\"Phineas_and_Ferb\";\"Phineas_and_Ferb_(season_3)\";\"Phineas_and_Ferb:_Quest_for_Cool_Stuff\""}
{"question": "could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"TGS_Hates_Women\";\"TGS_Hates_Women\";\"Somebody_to_Love_(30_Rock)\";\"Jane_Krakowski\""}
{"question": "can children become lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Direct_commission_officer\";Lieutenant;\"Second_lieutenant\";\"Lieutenant_commander_(United_States)\""}
{"question": "could you drive from New England to a Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"You_Enjoy_Myself\";\"One_If_By_Land,_Two_If_By_Sea_Restaurant\";Carhop;\"Mall_Madness\""}
{"question": "do seven McDonald's hamburgers exceed USDA recommended fat allowance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Five_Guys\";\"McDonald's_Israel\";\"Topps_Meat_Company\";\"Criticism_of_fast_food\""}
{"question": "was Mozart accused of stealing from Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Karl_Alois,_Prince_Lichnowsky\";\"Udo_Proksch\";\"Wagner_(film)\";\"Mozart:_The_Conspirators_of_Prague\""}
{"question": "will Justin Bieber take over Mike Pence's position in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_President_Show\";\"The_End_of_Twerk\";\"Mone\u0301t_X_Change\";\"Charlie_Walk\""}
{"question": "did any citizen of San Antonio vote for Boris Johnson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"2013_San_Antonio_mayoral_election\";\"2011_San_Antonio_mayoral_election\";\"Midland_County,_Texas\";\"Lyndon_B._Johnson\""}
{"question": "does being good at guitar hero make you a good guitarist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Guitar_Hero:_Warriors_of_Rock\";\"Guitar_Hero_(video_game)\";\"Guitar_Hero:_Aerosmith\";\"DJ_Hero\""}
{"question": "can Roman numerals fill the normal number of Sudoku box options?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Buku_Sudoku\";Sudoku;\"Brain_Age:_Train_Your_Brain_in_Minutes_a_Day!\";\"Mathematics_of_Sudoku\""}
{"question": "is a Cassowary safer pet than a crane?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cassowary;Cassowary;Cassowary;\"Common_crane\""}
{"question": "was proofreading Edgar Allan Poe works lucrative?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ghostwriter;\"Essay_mill\";\"Literary_forgery\";Self-publishing"}
{"question": "was Hillary Clinton's deputy chief of staff in 2009 baptised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hillary_Clinton's_tenure_as_Secretary_of_State\";\"Presidency_of_Barack_Obama\";\"Confirmations_of_Barack_Obama's_Cabinet\";\"Hillary_Clinton's_tenure_as_Secretary_of_State\""}
{"question": "can dessert be made with vegetables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "Dessert;\"Cholera_(food)\";\"Cholera_(food)\";\"Pavlova_(cake)\""}
{"question": "would Othello be Shakespeare's play to buy Scheherazade most time with king?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Obey_the_Time\";\"The_Durutti_Column\";\"William_Shakespeare\";\"The_Vagabond_King\""}
{"question": "did Jon Brower Minnoch suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Jon_Brower_Minnoch\";\"Jon_Brower_Minnoch\";\"Jon_Brower_Minnoch\";\"Nek_minnit\""}
{"question": "does ancient Olympics crown fail to hide tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Mask;Trophy;\"Crown_of_Immortality\";\"Mikimoto_Ko\u0304kichi\""}
{"question": "can someone with dermatitis be a hand model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hand_model\";\"Hand_model\";\"Hand_model\";\"Body_painting\""}
{"question": "is it bad to have lactic acid in your body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Lactic_acidosis\";\"Pyruvate_carboxylase_deficiency\";\"Lactic_acid\";Lactitol"}
{"question": "are cucumbers often found in desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Cucumis_humifructus\";Cucumber;Cucurbita;Cucumber"}
{"question": "are blue lips normal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Blue", "extracted_entity": "\"Blue_Lips\";\"Zahra's_Blue_Eyes\";\"Blue_Lips\";\"Blue_Lips_(short_film)\""}
{"question": "can someone with celiac disease have potato vodka?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"James_Lind\";Gin;Gingivitis;Vodka"}
{"question": "are deaf people left out of enjoying music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Don't_Lose_The_Music\";\"Music-specific_disorders\";\"Loud_music\";\"Hearing_loss\""}
{"question": "phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ph", "extracted_entity": "\"Around_the_World_in_80_Days_(Michael_Palin_book)\";\"Around_the_World_in_80_Days_(miniseries)\";\"Around_the_world_sailing_record\";\"Around_the_World_in_80_Days_(2004_film)\""}
{"question": "would Cardi B. benefit from soy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Card", "extracted_entity": "Soybean;Vegetarianism;\"Soylent_(meal_replacement)\";Soybean"}
{"question": "are the names of The Powerpuff Girls alliterative? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Powerpuff_Girls\";\"The_Powerpuff_Girls\";\"Blossom,_Bubbles_and_Buttercup\";\"Double_Take_(group)\""}
{"question": "did the leader of Heaven's Gate consider himself a prophet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hadraniel;\"Heaven's_Gate_(religious_group)\";\"Heaven's_Gate_(religious_group)\";Hadraniel"}
{"question": "would King Leonidas have succeeded with an army the size of Mozart's compositions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Leonidas_I\";\"Leonidas_I\";\"Leonidas_Squadron\";Sparta"}
{"question": "do Elementary School students typically need graphing calculators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Graphing_calculator\";\"Graphing_calculator\";\"Graphing_calculator\";\"Word_problem_(mathematics_education)\""}
{"question": "can Family of Barack Obama ride comfortably in 2020 Jaguar F Type?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Evolution_(ride)\";\"Jaguar_F-Pace\";\"Larry_Ellison\";\"Andrea_Orcel\""}
{"question": "could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Goddess_Girls\";Apollo;\"Giants_(Greek_mythology)\";\"D'Artagnan_and_Three_Musketeers\""}
{"question": "does the Eighth Amendment to the United States Constitution protect freedom of speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Eighth_Amendment_to_the_United_States_Constitution\";\"Fighting_words\";\"Eighth_Amendment_to_the_United_States_Constitution\";\"Eighth_Amendment_to_the_United_States_Constitution\""}
{"question": "do most fans follow Katy Perry for gospel music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Crossover_music\";\"Carly_Smithson\";\"Crossover_music\";\"Katy_Perry\""}
{"question": "does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Dinner_Party_(Seinfeld)\";\"Paula's_Party\";\"The_Dinner_Party_(Seinfeld)\";\"Fred_Weintraub\""}
{"question": "are rainbows devoid of color made by mixing yin and yang colors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Complementary_colors\";Rainbow;\"Complementary_colors\";\"Structural_coloration\""}
{"question": "did Christopher Columbus sail representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher", "extracted_entity": "\"Christopher_Columbus\";\"The_Discovery_of_America_by_Christopher_Columbus\";\"Origin_theories_of_Christopher_Columbus\";\"Origin_theories_of_Christopher_Columbus\""}
{"question": "can an Asian black bear use chopsticks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kung_Fu_Bear\";\"Kung_Fu_Bear\";\"ClayFighter_63\u2153\";\"Asian_black_bear\""}
{"question": "did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Coen_brothers\";\"Coen_brothers\";\"Coen_brothers\";\"Coen_brothers\""}
{"question": "would a Durian be dangerous if it fell on your head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dur", "extracted_entity": "Durian;Durian;\"Durian_Durian\";\"Durian_Durian\""}
{"question": "can nitric acid break the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Tonite_(explosive)\";\"N2O:_Nitrous_Oxide\";\"Nitrile_rubber\";\"N2O:_Nitrous_Oxide\""}
{"question": "does autopilot rely on fossil fuels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Autopilot;Autopilot;Autogyro;Powerback"}
{"question": "were muskets used in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Pacific_War\";\"Pacific_War\";\"Pacific_War\";\"Pacific_War\""}
{"question": "do tourists prefer Tuvalu to Niue?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Vaiaku_Lagi_Hotel\";\"Tourism_in_New_Zealand\";Tahiti;\"Aha_Oe_Feii?\""}
{"question": "could you windsurf in Puerto Rico during Hurricane Maria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hurricane_Maria\";\"Hurricane_Maria\";\"Meteorological_history_of_Hurricane_Maria\";\"Hurricane_Maria\""}
{"question": "did Northwest Airlines' longevity surpass Betty White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Northwest_Airlines\";\"Northwest_Airlines\";\"Elgen_Long\";\"Linda_Finch\""}
{"question": "is myocardial infarction a brain problem?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Myocardial_infarction\";\"Myocardial_infarction\";\"Myocardial_infarction\";\"Myocardial_infarction\""}
{"question": "is Newspeak considered very straightforward?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "is", "extracted_entity": "\"Scoop_(news)\";News;\"Newspeak_(programming_language)\";News"}
{"question": "was Pope Alexander VI's origin country least represented in papal history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ope", "extracted_entity": "\"Pope_Alexander_VI\";\"Pope_Alexander_VI\";\"Albanians_in_Italy\";\"History_of_the_papacy\""}
{"question": "would you find olives at a heladeria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hechsher;Leccino;Pellana;\"Kalamata_olive\""}
{"question": "do people celebrate Earth Day with a ceremonial tire fire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Bonfire;Bonfire;\"Regional_variations_of_barbecue\";\"Earth_religion\""}
{"question": "did France win the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"French_Revolution\";\"French_Revolution\";\"French_Revolution\";\"French_Revolution\""}
{"question": "does menthol make cigarettes less addictive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tobacco_marketing_targeting_African_Americans\";\"Nandrolone_decanoate\";Flutamide;Sildenafil"}
{"question": "was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "\"J._D._Salinger\";\"Anthony_Quinn\";\"The_Catcher_in_the_Rye\";\"The_Catcher_in_the_Rye\""}
{"question": "could Reza Shah be related to Queen Elizabeth I?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hamid_Reza_Pahlavi\";\"Hamid_Reza_Pahlavi\";\"Gholamreza_Pahlavi\";\"Patrick_Ali_Pahlavi\""}
{"question": "is it possible that June got its name from mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": "\"June_(given_name)\";June;\"June_(given_name)\";June"}
{"question": "is it possible to get killed walking to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Time-lapse_photography\";\"Physics_of_the_Impossible\";\"Time-lapse_photography\";\"Asteroid_impact_avoidance\""}
{"question": "is Canon Inc. a Kabushiki gaisha?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Canon", "extracted_entity": "\"Kabushiki_gaisha\";Kodo-kai;\"Misako_Aoki\";\"Kabushiki_gaisha\""}
{"question": "can a diamond float on water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Treasure_Under_Glass\";\"MS_Sea_Diamond\";\"Titanium_ring\";\"Prince_Rupert's_drop\""}
{"question": "does horseradish have a fetlock?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Horseradish;Horseradish;Horseradish;Horseradish"}
{"question": "can you hide a pet macaque under your desk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"ZhuZhu_Pets\";\"Pet_cemetery\";\"Green_iguana_in_captivity\";\"ZhuZhu_Pets\""}
{"question": "can Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh", "extracted_entity": "\"Peter_Leek\";\"Josh_Hose\";\"Jeremy_McClure\";\"Josh_Hose\""}
{"question": "would fans of Jonathan Larson be unaware of HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Online_shaming\";Anonymity;\"Nepotism_(The_Office)\";\"Nobody_Likes_Onions\""}
{"question": "would Carolina Reaper decrease sales if added to all US salsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Carolina_Reaper\";\"Carolina_Reaper\";\"Carolina_Reaper\";\"Carolina_Foods\""}
{"question": "will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Dominion_6.19_in_Osaka-jo_Hall\";\"Bacon_Deluxe\";\"2013_CrossFit_Games\";\"Six_Degrees_of_Kevin_Bacon\""}
{"question": "was the Eiffel tower used as a symbol of the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Eiffel_Tower\";\"Eiffel_Tower\";\"German_military_administration_in_occupied_France_during_World_War_II\";\"Flag_of_France\""}
{"question": "does James Watson believe that Africans are inferior to Europeans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Zachary_Macaulay\";\"Scientific_racism\";Polygenism;\"Scientific_racism\""}
{"question": "is The Joy of Painting TV show still producing new episodes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Joy_of_Painting\";\"The_Joy_of_Painting\";\"The_Joy_of_Painting\";\"The_Joy_of_Painting\""}
{"question": "is the skull formed as one whole bone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Skull_fracture\";Head;\"Skull_fracture\";Skull"}
{"question": "was  Godfrey of Bouillon an Islamaphobe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Hassen_Chalghoumi\";Speraindeo;\"Gregory_the_Patrician\";\"Saint_Ernest\""}
{"question": "are lengths measured in metres in the UK?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Swimming_pool\";\"Foot_(unit)\";Metre;\"How_Long_Is_the_Coast_of_Britain?_Statistical_Self-Similarity_and_Fractional_Dimension\""}
{"question": "do quadragenarian's have little memory capacity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tetraplegia;Tetraplegia;\"Super_recogniser\";Pachygyria"}
{"question": "did Jackie Kennedy wear Dolce & Gabbana to her husband's inauguration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Happy_Birthday,_Mr._President\";\"First_inauguration_of_Barack_Obama\";\"Inauguration_of_Donald_Trump\";\"A_Tour_of_the_White_House_with_Mrs._John_F._Kennedy\""}
{"question": "did Larry King sign the Magna Carta?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Martin_Luther_King_Jr._authorship_issues\";\"Bernice_King\";\"B.B._King\";\"Larry_King\""}
{"question": "will a Euro sink in water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Eurotas_(river)\";EuroChem;\"Euro_Aquae\";\"Eurotas_(river)\""}
{"question": "would Iggy Pop travel with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ig", "extracted_entity": "\"Ubiquitous_Synergy_Seeker\";\"Friends_(Justin_Bieber_and_BloodPop_song)\";\"Bam_Margera_Presents:_Where_the_\u266f$&%_Is_Santa?\";\"Long_Way_2_Go\""}
{"question": "is Drew Carey important to the history of wrestling?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pat_Patterson_(wrestler)\";\"Drew_Carey\";\"Drew_Carey\";\"Drew_Carey\""}
{"question": "did Jay-Z ever collaborate with Louis Armstrong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ella_Fitzgerald_and_Louis_Armstrong_collaborations\";\"Jazzy_Jay\";Jay-Z;Jay-Z"}
{"question": "is CAS number 8009-03-8 harmful for a rash?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Antimicrobial_surface\";Tacrolimus;\"Maculopapular_rash\";Viperin"}
{"question": "would a model be appropriate to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Anne", "extracted_entity": "\"Anne_Parillaud\";\"La_Femme_Nikita\";\"La_Femme_Nikita\";\"La_Femme_Nikita\""}
{"question": "would Michael J Fox qualify for the Army Rangers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"De'Aaron_Fox\";\"Michael_Ford_(gridiron_football)\";\"Kenny_Phillips\";\"Michael_Sam\""}
{"question": "do salmon mate in the Caspian Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Salmon;\"Caspian_seal\";\"Caspian_kutum\";\"Caspian_seal\""}
{"question": "could Chuck Norris ride a horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Budweiser_Clydesdales\";\"Norris_Locomotive_Works\";\"Spanky_Spangler\";\"Paul_Stader\""}
{"question": "can Ford F-350 tow entire Yale University student body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Berkeley_Robotics_and_Human_Engineering_Laboratory\";\"Carnegie_Mellon_University_traditions\";\"Eckerd_College_Search_and_Rescue\";\"Tug_of_war\""}
{"question": "are any of J.K. Rowling's books in the genre of And Then There Were None?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"And_Then_There_Were_None\";\"And_Then_There_Were_None\";\"And_Then_There_Were_None_(play)\";\"The_Sea_Shall_Not_Have_Them\""}
{"question": "was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"You're_a_Good_Sport,_Charlie_Brown\";\"You're_a_Good_Man,_Charlie_Brown\";\"He's_Your_Dog,_Charlie_Brown\";\"You're_a_Good_Sport,_Charlie_Brown\""}
{"question": "would an actuary be confused about what prime numbers are?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Random_digit_dialing\";\"Abundant_number\";\"Double_counting_(fallacy)\";\"Faulty_generalization\""}
{"question": "was 1941 Operation Barbarossa related to The Crusades?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Operation_Barbarossa\";\"Axis_and_Soviet_air_operations_during_Operation_Barbarossa\";\"Operation_Barbarossa\";\"Operation_Barbarossa\""}
{"question": "can ham make a cut seal up quicker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Rip_cut\";\"Lame_(kitchen_tool)\";\"No-knead_bread\";Ham"}
{"question": "can Chinese mountain cat survive in the orbit? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Monkeys_and_apes_in_space\";\"Chinese_mountain_cat\";\"Soyuz_(spacecraft)\";\"Mission:_Space\""}
{"question": "are there special traffic laws associated with funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Vignette_(road_tax)\";\"Funeral_Rule\";\"Funeral_Rule\";\"Highways_England_Traffic_Officer_Service\""}
{"question": "did George Washington drive a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"2005_Cadillac_DTS_Presidential_State_Car\";\"An_American_Revolution\";\"The_Dukes_of_Hazzard\";\"Julian_Morrow\""}
{"question": "is metal a type of folk music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Viking_metal\";\"Folk_metal\";\"Viking_metal\";\"Viking_metal\""}
{"question": "did Neanderthals use arithmetic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Arithmetic;Arithmetic;\"Ancient_Egyptian_mathematics\";\"Ancient_Egyptian_mathematics\""}
{"question": "would East India Company prefer China's modern trade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"East_India_Company\";\"Old_China_Trade\";\"East_India_Company\";\"East_India_Company\""}
{"question": "could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Chain_letter\";\"The_Trevor_Project\";\"Chain_letter\";\"Devon_Hamilton_and_Hilary_Curtis\""}
{"question": "would World War II have been the same without Alan Turing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "without", "extracted_entity": "\"Alan_Turing\";\"Alan_Turing_Memorial\";\"Turing_machine\";\"Alan_Turing\""}
{"question": "would General Zod prefer an iPhone over a Samsung Galaxy S4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Zig-Zag_(company)\";\"Jewcan_Sam\";\"HTC_Desire_Z\";\"Fashion_Beast\""}
{"question": "can 200 men end to end cover Great Pyramid of Giza's base?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Five_Greatest_Warriors\";\"French_campaign_in_Egypt_and_Syria\";\"Delta_Force:_Land_Warrior\";\"Battle_of_the_Nek\""}
{"question": "does meat from cows fed only grass taste more like wild game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Beefalo;Beefalo;\"Cattle_feeding\";Beefalo"}
{"question": "did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Alan_Turing_Memorial\";\"John_Leech_(politician)\";\"Alan_Turing_Memorial\";\"Chemical_castration\""}
{"question": "do people associate greyhounds with the movie 'Homeward Bound'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Homeward_Bound:_The_Incredible_Journey\";\"Homeward_Bound_(song)\";\"Greyhound_(film)\";\"See_a_man_about_a_dog\""}
{"question": "would it be unusual to find a yellow perch in the Red Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Yellow_perch\";\"Sulfurhead_aulonocara\";\"Yellow-edged_lyretail\";\"Common_yellowthroat\""}
{"question": "can every digit in Pi be memorized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Pi;Piphilology;Pi;Pi"}
{"question": "is Norman Oklahoma named after a viking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Eric_Northman\";\"Oklahoma_runestones\";\"Rogers_(surname)\";\"Curmsun_Disc\""}
{"question": "did Alfred Hitchcock include internet slang in his films?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Robert_Bloch\";\"Robert_Bloch\";\"Minced_oaths_in_media\";Euphemism"}
{"question": "did Supernatural break 2001 CW debuting shows seasons record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_WB\";\"Supernatural_(U.S._TV_series)\";\"Supernatural_(U.S._TV_series)\";\"Supernatural_(U.S._TV_series)\""}
{"question": "would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Compassion_(Doctor_Who)\";\"Fourth_Doctor\";\"Timewyrm:_Genesys\";\"Timewyrm:_Genesys\""}
{"question": "would Jean Harris's victim have avoided lentils?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Disappearance_of_Eloise_Worledge\";\"Murder_of_Michele_MacNeill\";\"Andre\u0301_Dallaire\";\"Death_of_Joan_Robinson_Hill\""}
{"question": "would an art dealer prize a print of a Van Goh? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Han_van_Meegeren\";\"Larry_Gagosian\";\"Interchange_(de_Kooning)\";\"Posthumous_fame_of_Vincent_van_Gogh\""}
{"question": "is it comfortable to wear sandals outside Esperanza Base?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "\"Esperanza_Base\";Sandal;\"Esperanza_Base\";\"Jump_boot\""}
{"question": "does welding with acetylene simulate the temperature of a star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Starlite;\"Atomic_hydrogen_welding\";\"Oxygen-burning_process\";\"Silicon-burning_process\""}
{"question": "is pickled cucumber ever red?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Red_cabbage\";\"Campari_tomato\";\"Campari_tomato\";Cucumber"}
{"question": "is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"MyNetworkTV_telenovelas\";Soapnet;\"Television_in_the_United_States\";\"MyNetworkTV_telenovelas\""}
{"question": "do any Islamic dominated countries have a Starbucks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Starbucks;\"Coffee_culture\";\"Coffee_production_in_Ethiopia\";Oxfam"}
{"question": "was Achilles a direct descendent of Gaia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gaia;Neoptolemus;Aeacus;Achilles"}
{"question": "was Noah associated with a dove?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Doves_as_symbols\";\"Doves_as_symbols\";\"Noah's_Ark\";\"The_Pigeon_and_the_Dove\""}
{"question": "will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Detroit_People_Mover\";Reshoring;\"Detroit_People_Mover\";\"Downtown_Eastside\""}
{"question": "are Saturn's famous rings solid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Rings_of_Saturn\";Saturn;Saturn;\"Rings_of_Saturn\""}
{"question": "would menu at Chinese Starbucks be familiar to an American?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Hidekazu_Tojo\";\"Papa_John's_Pizza\";Glocalization;\"Sushi_Mizutani\""}
{"question": "would Lee Sedol understand the complexities of the Sicilian Defence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Sicilian_Defence\";\"Sicilian_Defence\";\"Sicilian_Defence\";\"Sicilian_Defence\""}
{"question": "would a northern fur seal pass a driving test?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Baikal_seal\";\"Seal_hunting\";\"Northern_fur_seal\";\"Seal_hunting\""}
{"question": "was Hundred Years' War a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hundred_Years'_War\";\"Hundred_Years'_War\";\"Hundred_Years'_War\";\"Hundred_Years'_War\""}
{"question": "did Nine Inch Nails inspire Aretha Franklin's sound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Nine_Inch_Nails\";\"Turpentine_(song)\";\"Nine_Inch_Nails\";\"Nine_Inch_Nails\""}
{"question": "does giant panda have colors that differ from yin yang?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Giant_panda\";\"Red_panda\";\"Red_panda\";\"Red_panda\""}
{"question": "would vegans consider chickpeas for a tuna substitute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Tuna;\"Got_Mercury?\";Tuna;Veganism"}
{"question": "do the directors of The Matrix advocate for transgender rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "RespectAbility;RespectAbility;\"Resolution_(talent_agency)\";\"Pantsuit_Nation\""}
{"question": "is Dungeons and Dragons a game well suited for solo play?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dungeons", "extracted_entity": "Gamebook;\"Dungeons_&_Dragons_Tactics\";\"Desktop_Dungeons\";\"Dungeons_&_Dragons\""}
{"question": "would you buy bananas for tostones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tostones;\"Cooking_banana\";Tostones;Tostones"}
{"question": "are birds important to badminton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Bird;Badminton;Peafowl;\"Human_uses_of_birds\""}
{"question": "if you're running focal fossa, are you using linux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Linux_kernel\";\"Granular_Linux\";Folx;Linux"}
{"question": "is jalapeno heat outclassed by Bhut jolokia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bhut_jolokia\";\"Kjetil_Jansrud\";\"Red_Savina_pepper\";\"Vikas_Krishan_Yadav\""}
{"question": "would a goblin shark eat at Crossroads Kitchen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Goblin_Market\";\"Goblin_Market\";\"Goblin_Market\";Joint-eater"}
{"question": "is Lines on the Antiquity of Microbes briefer than any haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Lines_on_the_Antiquity_of_Microbes\";Diatribe;Platitude;Platitude"}
{"question": "could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Condeep;Enwave;\"Ris\u00f8r_Underwater_Post_Office\";\"Crystal_Island\""}
{"question": "would Kurt Cobain have benefited from Project Semicolon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Five_Dollar_Bob's_Mock_Cooter_Stew\";\"Project_Semicolon\";\"Mentors_(band)\";\"Project_Semicolon\""}
{"question": "did Douglas Adams use email as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Douglas_Adams\";\"Douglas_Adams\";\"Douglas_Adams\";\"Ted_Patrick\""}
{"question": "could Goofy have counted nine planets in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Barney_Google_and_Snuffy_Smith\";Goofy;\"A_Statue_for_Father\";\"Winsor_McCay\""}
{"question": "could Javier Sotomayor jump over the head of the average giraffe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Zorro;\"A_Place_Called_Chiapas\";\"La_Ciudad_Blanca\";\"Rafael_Go\u0301mez_Ortega\""}
{"question": "can you avoid internet trolls on reddit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Slashdot_effect\";Slashdot;\"Google_penalty\";\"Google_penalty\""}
{"question": "is Ganymede in the Milky Way galaxy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Globular_cluster\";\"Omega_Centauri\";\"Andromeda_Galaxy\";\"Milky_Way\""}
{"question": "does Billy Graham support agnosticism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Billy_Graham\";\"Billy_Graham\";\"Franklin_Graham\";\"Billy_Graham\""}
{"question": "is euphoria associated with drug addiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Euphoria;Euphoria;Euphoria;Euphoria"}
{"question": "can the Moscow Kremlin fit inside Disney Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Izmaylovo_District\";\"Cinderella_Castle\";\"Moscow_Kremlin\";\"Cinderella_Castle\""}
{"question": "did the Cherokee people send a delegation to oppose allotment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Cherokee_removal\";\"Cherokee_Commission\";\"Cherokee_removal\";\"Cherokee_removal\""}
{"question": "would an Orthodox Presbyterian object to 1700s judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Scottish_religion_in_the_eighteenth_century\";\"Qualified_Chapel\";\"English_coffeehouses_in_the_17th_and_18th_centuries\";\"Plain_dress\""}
{"question": "would Robert Wadlow tower over a German Shepherd?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"William_Wallace\";\"Where_Eagles_Dare\";\"Lyndhurst,_Hampshire\";\"William_Compton,_1st_Earl_of_Northampton\""}
{"question": "is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ys", "extracted_entity": "\"A_Business_Career\";\"The_Accountant_(2016_film)\";\"Carman_George_Blough\";Actuary"}
{"question": "would a lullaby be enough to wake Hellen Keller up?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Wake_Me_Up_(Avicii_song)\";\"Wake_Me_Up_(Avicii_song)\";\"Wake_Up_(ClariS_song)\";\"Wake_Me_Up_(Avicii_song)\""}
{"question": "would an average American Public University be welcoming to Ku Klux Klan members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ku_Klux_Klan\";\"Ku_Klux_Klan_titles_and_vocabulary\";\"Secret_societies_at_Duke_University\";\"U.S._Klans\""}
{"question": "do the Egyptian pyramids look the same from outside as they did when new?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Environmental_issues_in_Egypt\";\"Environmental_issues_in_Egypt\";\"Abbasid_architecture\";\"Egyptian_Building\""}
{"question": "is popular science used to peer review papers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Scholarly_peer_review\";\"Scholarly_peer_review\";\"Scholarly_peer_review\";\"Scholarly_peer_review\""}
{"question": "would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "\"Handel's_Last_Chance\";\"Handel's_Last_Chance\";Gandalf;\"John_Braham\""}
{"question": "could Snoopy transmit rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sno", "extracted_entity": "\"Snooper_and_Blabber\";\"Cryptic_bat_rabies\";\"Rabies_transmission\";Smell-O-Vision"}
{"question": "did the Wall Street Crash of 1929 hurt the stocks of robotics companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Wall_Street_Crash_of_1929\";\"John_J._Raskob\";\"Wall_Street_Crash_of_1929\";\"Cornering_the_market\""}
{"question": "can I find my home with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Find_Me_a_Home\";\"A_Map_of_Home\";\"Where_Is_My_Friend's_Home\";\"Find_Me_a_Home\""}
{"question": "is the QWERTY keyboard layout meant to be slow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Dvorak_Simplified_Keyboard\";\"Typographical_error\";QWERTY;\"Thumb-shift_keyboard\""}
{"question": "is dysphoria around one's pelvis treatable without surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Shoulder_dystocia\";\"Shoulder_dystocia\";\"Elbow_dysplasia\";\"Hip_dysplasia\""}
{"question": "does Southwest Airlines use bulk carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Southwest_Airlines\";\"Southwest_Airlines\";\"Southwest_Airlines\";\"History_of_Southwest_Airlines\""}
{"question": "is helium the cause of the Hindenburg explosion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hindenburg_disaster\";\"Noble_gas\";\"Hindenburg_disaster\";\"Hindenburg_disaster\""}
{"question": "were French people involved in the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Foreign_enlistment_in_the_American_Civil_War\";\"France_in_the_American_Revolutionary_War\";\"French_weapons_in_the_American_Civil_War\";\"France_and_the_American_Civil_War\""}
{"question": "could Edward Snowden join MENSA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Electric_Guest\";\"Edward_Norton\";\"Edward_Downes\";\"Air_America_(radio_network)\""}
{"question": "did the 23rd amendment give Puerto Ricans the right to vote for president?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Twenty-fourth_Amendment_to_the_United_States_Constitution\";\"Twenty-second_Amendment_to_the_United_States_Constitution\";\"Twenty-second_Amendment_to_the_United_States_Constitution\";\"Twenty-fourth_Amendment_to_the_United_States_Constitution\""}
{"question": "did Elle Fanning play an essential part in ending apartheid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ordeal_by_Innocence\";W.E.;\"Elleke_Boehmer\";\"Dorothy_Hewett\""}
{"question": "could Jamie Brewer have attended the United States Naval Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jamie", "extracted_entity": "\"Johnny_Lechner\";\"RJ_Brewer\";\"Jerauld_Wright\";\"Jason_Duboe\""}
{"question": "if someone is lactose intolerant, do they have to avoid cream?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Lactose_intolerance\";\"Lactose_intolerance\";\"Lactose_intolerance\";\"Lactose_intolerance\""}
{"question": "can numerologists become members of Royal Society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Royal_Numismatic_Society\";\"Royal_Fellow_of_the_Royal_Society\";\"Royal_Society\";\"Royal_Society\""}
{"question": "could pickled cucumbers from 1,000 years ago be good still?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Judean_date_palm\";Janjucetus;\"Digitaria_exilis\";Stillbay"}
{"question": "is the most recent Democrat President in the US known for his painting practice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael", "extracted_entity": "\"Michael_Shane_Neal\";\"Michael_Shane_Neal\";\"Dan_Dunn_(painter)\";\"Michael_Murphy_(sculptor)\""}
{"question": "do black swan cygnets typically know both of their genetic parents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Swan;\"Roan_(horse)\";\"Mute_swan\";\"Champagne_gene\""}
{"question": "would Edward II of England have been born without Vikings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Edward_the_Confessor\";\"Church_of_St._Edward_the_Martyr,_Brookwood\";\"Edward_II_of_England\";\"Edward_the_Confessor\""}
{"question": "do workers at Nissan's headquarters eat with chopsticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Manual_keigo\";Robatayaki;\"Yatai_(food_cart)\";Shinsekai"}
{"question": "does Hanuman have some of the same duties as Athena?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Han", "extracted_entity": "\"Sanat_Kumara\";Rati;Hermathena;\"Vakri_grahas\""}
{"question": "does title of Van Morrison's most played song apply to a minority of women worldwide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"More_Than_a_Woman_(Bee_Gees_song)\";\"You_Are_the_Woman\";\"More_Than_a_Woman_(Bee_Gees_song)\";\"A_Woman's_Worth\""}
{"question": "did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Eugene_Allen\";\"Eugene_Allen\";\"Eugene_Allen\";\"Eugene_Allen\""}
{"question": "could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Hammond_Circus_Train_Wreck\";\"Indiana_World_War_Memorial_Plaza\";\"Oldtimer_Grand_Prix_(Salzburgring)\";\"Modena_Park_2017\""}
{"question": "can a rabbi save the soul of a Christian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bikur_cholim\";\"Yaakov_Israel_Ifargan\";\"Rabbi_Akiva\";\"Rabbi_Akiva\""}
{"question": "would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Inferno_(Dante)\";\"Divine_Comedy\";\"Divine_Comedy\";\"Divine_Comedy\""}
{"question": "can a student from Smithtown's Cleary School understand the speech of a French person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "is the Berlin University of the Arts a Baroque period relic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "Kulturforum;Kulturforum;\"Bauhaus_University,_Weimar\";\"New_Bauhaus_Museum_Weimar\""}
{"question": "are paratroopers good at mountain rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Mountain_rescue\";\"Mountain_rescue\";\"Green_Light_Teams\";\"Hostage_Rescue_Team\""}
{"question": "should wool be hand washed only?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Woolen;Shatnez;Shatnez;\"Cashmere_wool\""}
{"question": "do people put creatures from the Black Sea on their pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sirene;\"Pleurotus_ostreatus\";\"Pleurotus_ostreatus\";Campania"}
{"question": "is Pan a symbol of virtue and virginity in women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pan_(god)\";\"Pan_(god)\";Androgyny;\"History_of_human_sexuality\""}
{"question": "is Nine Inch Nails's lead singer associated with David Lynch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Nine_Inch_Nails_live_performances\";\"Scot_Coogan\";\"Nine_Inch_Nails_live_performances\";\"Matt_Fallon\""}
{"question": "if you bottle your own milk, would there be cream on top of it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Clotted_cream\";\"Cream_soda\";\"Egg_cream\";\"Cream_soda\""}
{"question": "would Constitution of the United States paper offend PETA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_States_v._Progressive,_Inc.\";\"The_Federalist_Papers\";\"Clamshell_Alliance\";\"Liberty_pole\""}
{"question": "was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Expedition_Everest\";\"Mount_Everest\";\"Timeline_of_Mount_Everest_expeditions\";\"1953_British_Mount_Everest_expedition\""}
{"question": "is Albany, Georgia the most populous US Albany?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Albany,_Georgia\";\"Albany_metropolitan_area,_Georgia\";\"Albany,_Georgia\";\"Georgia_(U.S._state)\""}
{"question": "can Curiosity (rover) kill a cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Curiosity_(rover)\";\"Curiosity_(rover)\";\"Curiosity_(rover)\";\"Curiosity_killed_the_cat,_but_satisfaction_brought_it_back\""}
{"question": "are the Great Lakes part of an international border?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Anglican_Diocese_of_the_Great_Lakes\";\"Great_Lakes\";\"Great_Lakes\";\"Lake_Michigan\""}
{"question": "did Karl Marx influence the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Ideology_of_the_Communist_Party_of_China\";\"Mao_Zedong\";\"Ideology_of_the_Communist_Party_of_China\";\"Ideology_of_the_Communist_Party_of_China\""}
{"question": "does American Independence Day occur during autumn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Independence_Day_(United_States)\";\"Independence_Day_(United_States)\";Juneteenth;\"Christmas_in_July\""}
{"question": "has the Holy Grail been featured in at least five films?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Holy_Grail\";\"Epic_film\";\"The_Ten_Commandments_(1923_film)\";\"Knights_Templar_in_popular_culture\""}
{"question": "can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spearfishing;Fishfinder;\"Spearfish_torpedo\";\"Acoustic_homing\""}
{"question": "are hippos dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ey", "extracted_entity": "\"They_Shoot_Horses,_Don't_They?_(novel)\";Hippopotamus;\"Pygmy_hippopotamus\";Hippopotamus"}
{"question": "would it be impossible to get to Burning Man on the Mayflower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Burning_Land\";\"City_of_Ashes\";\"How_to_Stop_an_Exploding_Man\";\"20_Million_Miles_to_Earth\""}
{"question": "is the US Secretary of State similar to an administrative secretary of an office?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"United_States_Department_of_State\";\"United_States_Department_of_State\";\"United_States_Secretary_of_State\";\"United_States_Secretary_of_State\""}
{"question": "would Stephen King fans be likely to own an image of a clown?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Evil_clown\";\"Andre_the_Giant_Has_a_Posse\";\"Andre_Miripolsky\";\"2016_clown_sightings\""}
{"question": "is snoring a sign of good breathing while sleeping?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sn", "extracted_entity": "Snoring;Snoring;Snoring;\"Sleep_and_breathing\""}
{"question": "did Mozart ever buy anything from Dolce & Gabbana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Wolfgang_Amadeus_Mozart\";\"Annibale_Berlingieri\";\"Annibale_Berlingieri\";\"Piero_Manzoni\""}
{"question": "did any Golden Globe winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Brendan_Kyle_Hatcher\";VEISHEA;\"Westminster_College_(Missouri)\";\"Bill_Clinton\""}
{"question": "did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Carlos_Gracie\";\"Rolls_Gracie\";\"Gracie_Humaita\u0301\";\"Gracie_jiu-jitsu_ranking_system\""}
{"question": "have any members of the 2020 British royal family allegedly committed a felony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"2007_royal_blackmail_plot\";\"Unlawful_Killing_(film)\";\"2007_royal_blackmail_plot\";\"David_Armstrong-Jones,_2nd_Earl_of_Snowdon\""}
{"question": "can first letter row of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Diaeresis_(diacritic)\";\"Syriac_alphabet\";Apostrophe;\"Diaeresis_(diacritic)\""}
{"question": "does March begin on the same day of the week as February during leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Determination_of_the_day_of_the_week\";\"Determination_of_the_day_of_the_week\";February;\"Determination_of_the_day_of_the_week\""}
{"question": "would a Drow tower over The Hobbit's hero?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Peregrin_Took\";Gandalf;\"Cynon_ap_Clydno\";Gandalf"}
{"question": "are Big Ben's bells currently rung on their normal schedule at the Palace of Westminster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Greenwich_Time_Signal\";\"Big_Ben\";\"Big_Ben\";\"Big_Ben\""}
{"question": "is it dangerous to consume chlorine when mixed with sodium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Water_chlorination\";\"Drinking_water_quality_in_the_United_States\";Diarrhea;\"Chinese_alchemical_elixir_poisoning\""}
{"question": "was Al-Farabi a student of the Great Sheikh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Al-Suyuti;Al-Farabi;Al-Farabi;\"Abu_Madyan\""}
{"question": "can telescopes hear noise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Ultrasound;\"Astronomical_radio_source\";\"Astronomical_radio_source\";\"Microwave_auditory_effect\""}
{"question": "would Roman Gallic Wars army struggle to build the pyramids faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Punic_Wars\";\"Punic_Wars\";\"Punic_Wars\";\"Second_Punic_War\""}
{"question": "can a software engineer work during a power outage?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Power_cycling\";\"Redundancy_(engineering)\";Downtime;\"Power_cycling\""}
{"question": "would Goofy hypothetically enjoy Nylabone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Go", "extracted_entity": "\"Leisure_Suit_Larry\";\"Destination:_Imagination\";\"Neurotically_Yours\";\"Imaginationland_Episode_III\""}
{"question": "was a USB flash drive used in The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"USB_flash_drive\";\"Bernoulli_Box\";Cilice;\"USB_flash_drive\""}
{"question": "can surgery prevent an existential crisis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Orsan_plan\";\"Operating_room_management\";\"Aortic_aneurysm\";\"Aortic_aneurysm\""}
{"question": "is an internet connection essential for someone using Chrome OS?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chrome", "extracted_entity": "Chromebook;\"Chrome_OS\";Chromebook;Chromebook"}
{"question": "does butter industry survive cow extinction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Milk_fever\";\"United_States_raw_milk_debate\";\"Milk_fever\";\"Dairy_cattle\""}
{"question": "is a Coca plant farm likely to be found in Yakutsk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Coca", "extracted_entity": "\"Amur_Oblast\";Turpan;Dzungaria;\"Yarkant_County\""}
{"question": "does Siri know geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Marjorie_Rice\";\"Marjorie_Rice\";\"Gigliola_Staffilani\";\"Gigliola_Staffilani\""}
{"question": "in star rating systems, is 5 stars considered good?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Star_(classification)\";\"Star_(classification)\";\"Star_(classification)\";\"Star_(classification)\""}
{"question": "has Billy Joel sold out Astana Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Billy_Joel\";\"Billy_Joel\";\"Khan_Shatyr_Entertainment_Center\";\"Billy_Joel_in_Concert\""}
{"question": "did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Solomon_in_Islam\";\"Solomon_in_Islam\";Solomon;Pharisees"}
{"question": "can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Ableton_Live\";Ableton;\"Ableton_Live\";Amaysim"}
{"question": "was Surfing popular when pogs came out?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "surfing", "extracted_entity": "Waveski;\"History_of_surfing\";\"History_of_surfing\";\"Surf_culture\""}
{"question": "does Hades have a loose grip on the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Pluto_(mythology)\";\"Deep_sea_community\";\"Pluto_(Marvel_Comics)\";\"Greek_underworld\""}
{"question": "was Dorothea Wendling from same place Porsche originated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Neckarsulm;\"Porsche_family\";\"Louise_Pie\u0308ch\";\"Johanna_Olbrich\""}
{"question": "are there mental disorders you can hide?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Fear_Nothing\";\"Hidden_personality\";\"The_Pride_You_Hide\";Secrecy"}
{"question": "can a Muslim eat a McRib sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"International_availability_of_McDonald's_products\";\"International_availability_of_McDonald's_products\";McArabia;\"International_availability_of_McDonald's_products\""}
{"question": "did Julia Roberts practice blast beats as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Fiona_Apple\";Kerli;\"Ananda_Lewis\";\"Meghan_Trainor\""}
{"question": "do Muslims have a different idea of Seraphim than Christians?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Atheist_Manifesto:_The_Case_Against_Christianity,_Judaism,_and_Islam\";\"Islam_and_antisemitism\";\"Apophatic_theology\";\"Palestinian_Israelitism\""}
{"question": "would nickel boil in the outer core of the earth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Outer_core\";Nickel;\"Outer_core\";Nickeline"}
{"question": "did the Paramount leader produce Titanic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Raise_the_Titanic_(film)\";\"RMS_Titanic_in_popular_culture\";\"David_MacDonald_(director)\";\"Raise_the_Titanic_(film)\""}
{"question": "has Aretha Franklin ever collaborated with a suicidal person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Are", "extracted_entity": "\"You're_Never_Alone_with_a_Schizophrenic\";\"Melody_Beattie\";\"Drug_use_in_music\";\"Three_Days_(Jane's_Addiction_song)\""}
{"question": "would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Harry_Potter_and_the_Philosopher's_Stone\";\"History_of_atheism\";\"Pendragon_(role-playing_game)\";\"Harry_Potter_and_the_Philosopher's_Stone\""}
{"question": "is capturing giant squid in natural habitat impossible with no gear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Giant_squid\";\"Deadliest_Catch\";\"Steve_O'Shea\";\"Giant_squid\""}
{"question": "would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Catello_di_Rosso_Gianfigliazzi\";\"Inferno_(Dante)\";Odysseus;\"Vanni_Fucci\""}
{"question": "did Andy Warhol influence Art Deco style?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Andy_Warhol\";\"Andy_Warhol\";\"Andy_Warhol\";\"Andy_Warhol\""}
{"question": "are any minor league baseball teams named after felines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"1926_World_Series\";\"Harry_Frazee\";\"Fred_McGriff\";\"Phillie_Phanatic\""}
{"question": "do Apollo and Baldur share similar interests?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Baldr;\"Alcor_(star)\";\"Balder_(comics)\";\"Balder_(comics)\""}
{"question": "would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Saddam_Hussein\";Shawar;\"Saladin_Governorate\";\"Failed_Iraqi_peace_initiatives\""}
{"question": "is August a winter month for part of the world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Year_Without_a_Summer\";August;\"Christmas_in_July\";\"Indian_summer\""}
{"question": "did Naruto escape the Temple of Doom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Naruto_Shippuden:_Legends:_Akatsuki_Rising\";\"Naruto_Shippuden:_Legends:_Akatsuki_Rising\";Naruto;\"Naruto_Shippuden:_Legends:_Akatsuki_Rising\""}
{"question": "is the Louvre's pyramid known for being unbreakable? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Unbreakable_(TV_series)\";\"Ine\u0301quilibre\";\"Ine\u0301quilibre\";\"Unbreakable_(Birds_of_Tokyo_song)\""}
{"question": "is breakdancing safe for people with tendonitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Clavicle_fracture\";\"Dupuytren's_contracture\";Lanyard;\"Thumb_break\""}
{"question": "did Christopher Columbus go to Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Christopher_Columbus\";\"Amundsen's_South_Pole_expedition\";\"Amundsen's_South_Pole_expedition\";\"History_of_Antarctica\""}
{"question": "was Dr. Seuss a liar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dr", "extracted_entity": "\"An_Honest_Liar\";\"Lying_Jim_Townsend\";\"An_Honest_Liar\";\"Why_Would_I_Lie?\""}
{"question": "is 1936 Summer Olympics venue too small for a Superbowl crowd?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"1936_NFL_Championship_Game\";\"1936_NFL_Championship_Game\";\"Cairo_International_Stadium\";\"1936_Summer_Olympics\""}
{"question": "does bull shark bite hurt worse than crocodile bite?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bull_shark\";\"Bull_shark\";\"Bull_shark\";\"Bull_shark\""}
{"question": "could a white belt defeat Jon Jones in a Brazilian jiu-jitsu match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Vinicius_Magalha\u0303es_(Draculino)\";\"Mark_Schultz_(wrestler)\";\"Mixed_martial_arts\";\"John_Will\""}
{"question": "would Communist Party of the Soviet Union hypothetically support Trickle Down Economics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Trickle-down_economics\";\"Trickle-down_economics\";\"Economy_of_the_Soviet_Union\";\"Trickle-up_effect\""}
{"question": "lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "lil", "extracted_entity": "\"Lil_Wayne\";\"Lil_Wayne\";\"Lil_Wayne_albums_discography\";\"Lil_Wayne_albums_discography\""}
{"question": "is it unusual to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Happy_hardcore\";\"Violently_Happy\";\"Funeral_Party\";\"Happy_(band)\""}
{"question": "could a cow produce Harvey Milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Smallpox_vaccine\";\"Ubre_Blanca\";\"Dairy_cattle\";\"Frozen_bovine_semen\""}
{"question": "do people still see Henry Ford's last name often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Henry_Ford\";\"Lord,_Mr._Ford\";\"Henry_Ford_II\";\"History_of_Ford_Motor_Company\""}
{"question": "would a thesis paper be unusual to assign to kindergartners? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kinder_der_Landstrasse\";\"Martin_Heidegger_and_Nazism\";\"Gestalt_qualities\";\"Naomi_Weisstein\""}
{"question": "did George W. Bush grow up speaking Cantonese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "George", "extracted_entity": "\"George_W._Bush\";\"George_H._W._Bush\";\"Billy_Bush\";\"George_Hu\""}
{"question": "was Eve involved in an incestuous relationship?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Relationship_of_Eve_Polastri_and_Villanelle\";\"Eve_(U.S._TV_series)\";\"Eve_(The_X-Files)\";\"Eve_(U.S._TV_series)\""}
{"question": "can the Communist Party of the Soviet Union get a perfect all kill?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Collective_leadership_in_the_Soviet_Union\";\"Democracy_and_Totalitarianism\";\"Communist_Party_of_the_Soviet_Union\";\"Soviet_of_the_Union\""}
{"question": "was Ariana Grande inspired by Imogen Heap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Imogen_Cunningham\";\"Nicki_Minaj\";\"Imogen_Heap\";\"Betty_Boo\""}
{"question": "will Futurama surpass the number of episodes of The Simpsons by the end of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Futurama:_Worlds_of_Tomorrow\";\"Mystery_Science_Theater_3000\";Futurama;\"Wet_Hot_American_Summer:_Ten_Years_Later\""}
{"question": "would Bonanza marathon end before WWE Heat marathon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"WWE_Fastlane\";\"Miami_Marathon\";\"Ultraman_(endurance_challenge)\";\"Bear_100_Mile_Endurance_Run\""}
{"question": "would a greyhound be able to outrun a greyhound bus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Greyhound_racing\";\"R_U_Faster_Than_a_Redneck?\";\"Grey_Rabbit\";\"Intercity_bus_service\""}
{"question": "did Tony Bennett have more children than he had wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tony_Bennett\";\"Tony_Bennett\";\"Tony_Bennett\";\"Tony_Bennett\""}
{"question": "is it safe to use Ammonia with Clorox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Ammoniacum;\"Acetic_acid_(medical_use)\";\"Ammonia_solution\";Ammoniacum"}
{"question": "is Bill Gates the wealthiest of the Baby Boomers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ll", "extracted_entity": "\"Bill_Gates\";\"Bill_Gates\";\"Bill_Gates\";\"Bill_Gates\""}
{"question": "did Switzerland support the United States in the Spanish\u2013American War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Spanish\u2013American_War\";\"Spain_and_the_American_Revolutionary_War\";\"Spanish\u2013American_War\";\"Spanish\u2013American_War\""}
{"question": "is Rosemary outclassed as plant found in most song titles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Bookends_(album)\";\"Love_Grows_(Where_My_Rosemary_Goes)\";\"Love_Grows_(Where_My_Rosemary_Goes)\";\"Rosemary_Lane_(song)\""}
{"question": "did Methuselah live at least 800 years as long as Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Meth", "extracted_entity": "Methuselah;Methuselah;Methuselah;Methuselah"}
{"question": "did any of Maya Angelou's children follow in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Angel", "extracted_entity": "\"Maya_Rudolph\";\"Maya_Angelou\";\"Maya_Angelou\";\"Teena_Marie\""}
{"question": "is Alistair Darling in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Scotland_Decides:_Salmond_versus_Darling\";\"Scotland_Decides:_Salmond_versus_Darling\";\"2014_Scottish_independence_referendum\";\"Alistair_Darling\""}
{"question": "would it be difficult for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "ami", "extracted_entity": "\"Ayako_Imoto\";\"Ayako_Imoto\";\"Ayako_Imoto\";\"Tateyama_Kurobe_Alpine_Route\""}
{"question": "could an NBA game be completed within the span of the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Six-Day_War\";\"Six-Day_War\";\"Six-Day_War\";\"Six-Day_War\""}
{"question": "would Arnold Schwarzenegger have a hard time picking up a red fox in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Red_Forman\";\"Charlton_Heston\";\"Red_Forman\";\"Millhouse_(film)\""}
{"question": "did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Palestinian_stone-throwing\";\"Stones_for_the_Rampart\";\"History_of_Albania\";\"Albanian_Civil_War\""}
{"question": "do all cancer patients get disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Malignant_multiple_sclerosis\";\"Spastic_cerebral_palsy\";Glioblastoma;\"Lung_cancer\""}
{"question": "was the father of social security system serving in the white house during the Panic of 1907?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "\"Edwin_E._Witte\";\"Social_security\";\"Francis_Townsend\";\"Social_Security_(United_States)\""}
{"question": "would the host of The Colbert Report be likely to vote for Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stephen_Colbert_(character)\";\"Donald_Trump\";\"Stephen_Colbert\";\"The_Late_Show_with_Stephen_Colbert\""}
{"question": "would Felicity Huffman vote for Mike DeWine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Faithless_elector\";\"Mike_DeWine\";\"2005_debate_on_nuclear_option_(United_States_Senate)\";\"Kwame_Kilpatrick\""}
{"question": "are Naruhito's ancestors the focus of Romance of the Three Kingdoms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Genpei_War\";\"The_Tale_of_Ho\u0304gen\";\"The_Twelve_Kingdoms\";\"The_Tale_of_Genji\""}
{"question": "would Recep Tayyip Erdo\u011fan be unfamiliar with b\u00f6rek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Recep_Tayyip_Erdog\u0306an\";\"Recep_Tayyip_Erdog\u0306an\";\"Block_of_Wikipedia_in_Turkey\";\"Recep_Tayyip_Erdog\u0306an\""}
{"question": "did Richard Wagner compose the theme songs for two television series?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Roger_Wagner\";\"Richard_Rodgers\";\"Alan_Wagner\";\"What's_Opera,_Doc?\""}
{"question": "are the colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Flag_of_France\";\"Flag_of_Mauritania\";\"Flag_of_Mauritania\";\"Flag_of_the_Western_European_Union\""}
{"question": "would Matt Damon be afraid of parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Matt", "extracted_entity": "\"Not_Afraid\";\"Kenrick_Monk\";\"Who's_Afraid_of_Virginia_Woolf?_(film)\";\"Such_Men_Are_Dangerous\""}
{"question": "does Long John Silver's serve sea otter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Long_John_Silver's\";\"Long_John_Silver's\";\"Dry_Dock_Brewing_Company\";\"Long_John_Silver's\""}
{"question": "does United Airlines have a perfect operation record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_Airlines\";\"United_Airlines\";\"United_Airlines\";\"Southwest_Airlines\""}
{"question": "is it wise to feed a Snickers bar to a poodle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pogue;\"Bambi_effect\";\"Time_to_Pee!\";\"Yes\u2013no_question\""}
{"question": "can someone sell their time through the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Pay_what_you_want\";\"Choice_Properties_REIT\";\"Straw_purchase\";\"Sandra_Rinomato\""}
{"question": "does Zelda Williams have any cousins on her father's side?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Zelda_Williams\";\"Quincy_Jones\";\"Draco_Malfoy\";\"Reva_Shayne\""}
{"question": "could Casio's first invention be worn around the ankle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Original_Car_Shoe\";\"The_Original_Car_Shoe\";\"Snap_fastener\";\"Metal_corset\""}
{"question": "would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Alice_Ormsby-Gore\";\"Alice_Ormsby-Gore\";\"Dannielynn_Birkhead\";\"Eric_Clapton\""}
{"question": "is the Federal Reserve a quick walk from Space Needle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Space_Needle\";\"Space_Needle\";\"Space_Needle\";Glitch"}
{"question": "did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Helen_Palmer_(author)\";\"Ida_Henrietta_Hyde\";\"Mary_Baker_Eddy\";\"Ginger_Rogers\""}
{"question": "were the Ten commandments the part of the bible that Jewish people do not believe in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Ten_Commandments\";\"Ten_Commandments_in_Catholic_theology\";\"Ten_Commandments\";\"Ten_Commandments\""}
{"question": "would Modafinil be effective in completing a suicide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mod", "extracted_entity": "Modafinil;Modafinil;Modafinil;Modafinil"}
{"question": "did US President during Spanish-American War suffer similar demise to Abraham Lincoln?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Spanish\u2013American_War\";\"Timeline_of_the_Spanish\u2013American_War\";\"The_Lincoln_Train\";\"Spanish\u2013American_War\""}
{"question": "does Post Malone have a fear of needles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Arrowette;\"Post_Malone\";\"Gil_Grissom\";\"Peter_Hathaway_Capstick\""}
{"question": "hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"African_bush_elephant\";\"Zufari:_Ride_into_Africa!\";\"Elephas_celebensis\";\"Taylor_KO_Factor\""}
{"question": "can a New Yorker get their eyes checked by Rand Paul legally?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Draize_test\";\"Green_Card_Test\";\"Green_Card_Test\";\"Draize_test\""}
{"question": "would Hapshetsut be considered a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "H", "extracted_entity": "Thagyamin;Supayagyi;\"Tshering_Yangdon\";\"Smim_Htaw_Buddhaketi\""}
{"question": "does Sockington enjoy onions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sockington;Sockington;\"Terry_Fuckwitt\";Sockington"}
{"question": "did the Gunpowder plot eliminate Mary, Queen of Scots bloodline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Gunpowder_Plot\";\"Mary,_Queen_of_Scots\";\"Mary,_Queen_of_Scots\";\"Gunpowder,_Treason_&_Plot\""}
{"question": "did goddess Friday is named after despise felines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Triskaidekaphobia;Triskaidekaphobia;\"Sylvia_(given_name)\";\"Sylvia_(given_name)\""}
{"question": "would a blooming onion be possible with a shallot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Allium_fistulosum\";\"Tree_onion\";\"Blooming_onion\";Shallot"}
{"question": "is there a popular Disney character made from living ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ice_Age_(2002_film)\";\"Frozen_(2013_film)\";\"Ice_Age:_A_Mammoth_Christmas\";\"Ice_King\""}
{"question": "could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Stanislav_Kurilov\";\"Escape_from_Alcatraz_(film)\";\"Escape_from_Alcatraz_(film)\";\"Stanislav_Kurilov\""}
{"question": "would an Evander Holyfield 2020 boxing return set age record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ev", "extracted_entity": "\"Evander_Holyfield\";\"Evander_Holyfield_vs._George_Foreman\";\"William_Hughes_(born_1998)\";\"Evander_Holyfield_vs._George_Foreman\""}
{"question": "can methane be seen by the naked eye?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Methane;\"Atmospheric_methane\";Methane;Tholin"}
{"question": "are all students guaranteed lunch at school in the US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Summer_Lunch_Programs_in_Public_Libraries\";\"School_meal_programs_in_the_United_States\";\"National_School_Lunch_Act\";\"National_School_Lunch_Act\""}
{"question": "did Maroon 5 go on tour with Nirvana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mar", "extracted_entity": "\"Maroon_V_Tour\";\"Maroon_V_Tour\";\"Maroon_5\";\"Maroon_5\""}
{"question": "are pancakes a bad snack for cats?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bacon_mania\";\"Pancake_house\";\"Bacon_mania\";\"Pancake_house\""}
{"question": "would a modern central processing unit circuit chip fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Drive_bay\";Motherboard;\"Home_computer\";Motherboard"}
{"question": "would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hogwarts;\"Lord_Voldemort\";\"Lord_Voldemort\";Hogwarts"}
{"question": "is silicon important in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Silicon", "extracted_entity": "\"Silicon_Valley\";\"Silicon_Valley\";\"Silicon_Valley\";\"Silicon_Valley\""}
{"question": "did Rumi spend his time in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "R", "extracted_entity": "\"Ramakrishna's_samadhi\";\"Ramakrishna's_samadhi\";Ataraxia;\"Religious_ecstasy\""}
{"question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tlaxcala;Cactus;Zacatecas;Tejano"}
{"question": "is ABBA's 1970's genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Blaxploitation;\"Hard_bop\";\"Mary_J._Blige\";\"African-American_music\""}
{"question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"The_Invisible_Man\";\"To_See_the_Invisible_Man\";\"The_Invisible_Man_(1933_film)\";\"Holtzman_effect\""}
{"question": "did P. G. Wodehouse like the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"P._G._Wodehouse\";\"P._G._Wodehouse\";\"P._G._Wodehouse\";\"P._G._Wodehouse\""}
{"question": "could amoebas have played a part in the Black Death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Young_Sherlock_Holmes\";\"Black_Egyptian_hypothesis\";\"The_Pyramid_(film)\";\"Ascalapha_odorata\""}
{"question": "did anyone in the 1912 election take a majority of the popular vote?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"1912_United_States_elections\";\"1912_United_States_presidential_election_in_Ohio\";\"1912_United_States_presidential_election_in_Pennsylvania\";\"1912_United_States_presidential_election_in_New_York\""}
{"question": "does parsley sink in milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pars", "extracted_entity": "Parlick;Parmigiano-Reggiano;\"Butter_in_a_Lordly_Dish\";\"Mastitis_in_dairy_cattle\""}
{"question": "would someone go to San Francisco for a nature escape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "San", "extracted_entity": "\"Escape_from_Alcatraz_(triathlon)\";\"Escape_from_Alcatraz_(triathlon)\";\"Escape_from_Alcatraz_(triathlon)\";\"June_1962_Alcatraz_escape_attempt\""}
{"question": "did the iPhone usher in the scientific revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_iPhone\";IPhone;\"History_of_iPhone\";\"History_of_iPhone\""}
{"question": "were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Four_Horsemen_of_the_Apocalypse\";\"Sydney_Bristow\";\"Biblical_Magi\";\"Horsemen_(film)\""}
{"question": "do people with DID have a good memory?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Recall_(memory)\";\"Eyewitness_memory\";\"Methods_used_to_study_memory\";\"Super_recogniser\""}
{"question": "can a carrot receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Uterus_transplantation\";\"Uterus_transplantation\";\"Uterus_transplantation\";\"Organ_transplantation\""}
{"question": "does Darth Vader's character resemble Severus Snape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Darth_Vader\";\"Darth_Vader\";\"Hasbro_Darth_Vader_Voice_Changer\";\"Darth_Vader\""}
{"question": "can spiders help eggplant farmers control parasites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Social_immunity\";\"Social_immunity\";\"Social_immunity\";\"Social_immunity\""}
{"question": "was John Lennon known to be a good friend to Sasha Obama?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"We_Love_You\";\"Paul_Saltzman\";\"Garden_Party_(Rick_Nelson_song)\";\"The_Beatles_Experience\""}
{"question": "did the population of the Warsaw Ghetto record secret police on cell phones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Telephone_tapping_in_the_Eastern_Bloc\";\"Telephone_tapping_in_the_Eastern_Bloc\";\"Secret_police\";\"Surveillance_issues_in_smart_cities\""}
{"question": "do children send their Christmas letters to the South Pole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Father_Christmas_Letters\";\"Santa_Claus\";\"Santa_Claus\";\"The_Father_Christmas_Letters\""}
{"question": "do all crustaceans live in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Crustacean;Crustacean;Crustacean;Crustacean"}
{"question": "does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Venus_Equilateral\";\"James_Webb_Space_Telescope\";\"Hubble_Space_Telescope\";\"The_Seance_Spectre\""}
{"question": "does Robert Downey Jr's Marvel Cinematic Universe character survive the Infinity War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Darkseid;\"Crisis_on_Infinite_Earths\";\"Crisis_on_Infinite_Earths\";\"Crisis_on_Infinite_Earths\""}
{"question": "did Secretariat win a Formula One championship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Secretariat_(horse)\";\"Secretariat_(horse)\";\"Secretariat_(film)\";\"Secretariat_(horse)\""}
{"question": "can Billie Eilish afford a Porsche?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hanna_Louisa_Bissiw\";\"Billie_the_Vision_and_the_Dancers\";\"She's_Got_All_the_Friends_That_Money_Can_Buy\";\"Ruth_Buzzi\""}
{"question": "is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a", "extracted_entity": "\"Cruise_West\";\"Halloween_Spooktacular\";\"Halloween_Spooktacular\";\"Newport_Beach_Christmas_boat_parade\""}
{"question": "do most religious people in Quebec refer to the Quran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Freedom_of_religion_in_Canada\";\"Islam_in_Canada\";\"Islamophobia_in_Canada\";\"Que\u0301bec_Identitaire\""}
{"question": "do members of NFL teams receive infantry training?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Training_camp_(National_Football_League)\";\"Training_camp_(National_Football_League)\";\"Training_camp_(National_Football_League)\";\"History_of_the_National_Football_League\""}
{"question": "is it normal for people to sing when the YMCA is mentioned?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Do_You_Feel_Like_We_Do\";YMCA;\"By_a_Waterfall\";\"Let_'Em_In\""}
{"question": "does 50 Cent get along with Jeffrey Atkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"50_Cent\";\"50_Cent\";\"50_Cent\";\"Piggy_Bank_(song)\""}
{"question": "are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"True_Faith_and_Allegiance\";\"Pledge_of_Allegiance\";\"Pledge_of_Allegiance\";\"Pledge_of_Allegiance\""}
{"question": "did H.G. Wells' \"War of the Worlds\" include cosmic rays?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_War_of_the_Worlds_(1953_film)\";\"The_War_of_the_Worlds_(1953_film)\";\"The_War_of_the_Worlds\";\"War_of_the_Worlds_\u2013_The_True_Story\""}
{"question": "do skeletons have hair?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Dreadlocks;\"Lovelock_Cave\";Dreadlocks;Dreadlocks"}
{"question": "does conservatism repulse Blaire White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mary_Whitehouse\";\"The_Reactionary_Mind\";White;\"The_White_Queen_(TV_series)\""}
{"question": "would Swiss Guard defeat the Marines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Special_Forces_Command_(Switzerland)\";\"History_of_the_United_States_Marine_Corps\";\"History_of_the_United_States_Marine_Corps\";\"History_of_the_United_States_Marine_Corps\""}
{"question": "can a firewall protect against a short circuit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Firewall_(construction)\";\"Firewall_(construction)\";\"Firewall_(computing)\";\"Radiant_barrier\""}
{"question": "can Hulk's alter ego explain atomic events?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Thunderbolt_Ross\";\"Hulk_(comics)\";\"Red_Hulk\";Zor-El"}
{"question": "does US brand Nice depend on Western honey bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Western_honey_bee\";\"Western_honey_bee\";\"Western_honey_bee\";\"Beekeeping_in_the_United_States\""}
{"question": "would Benito Mussolini hypothetically play well in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He", "extracted_entity": "\"Lou_Campanelli\";\"Dino_Meneghin\";\"History_of_basketball\";\"Bill_Bradley\""}
{"question": "would the author of Little Women have remembered the ratification of the 13th Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Women's_Suffrage_and_Western_Women's_Fashion_through_the_early_1900's\";\"Ada_Langworthy_Collier\";\"Louisa_May_Alcott\";\"Anya_Seton\""}
{"question": "do bodies movie during hanging?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hung_Up\";\"Body_art\";\"Body_Party\";\"God_Lives_Underwater\""}
{"question": "could Hurricane Harvey catch a Peregrine falcon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"USS_Peregrine_(AM-373)\";\"Peregrine_falcon\";\"Gulfstream_Peregrine\";\"Bird_strike\""}
{"question": "is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Aloe;\"International_Aloe_Science_Council\";\"Aloe_vera\";\"Aloe_vera\""}
{"question": "could ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mixed-sex_sports\";\"Types_of_tennis_match\";\"Bagel_(tennis)\";\"Steffi_Graf\""}
{"question": "can you swim to Miami from New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Swim_Miami\";\"Diana_Nyad\";Miami;\"Miami_Marathon\""}
{"question": "could the children of Greek hero Jason hypothetically fill a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Boy_on_a_Dolphin\";\"The_Belles_of_St._Trinian's\";\"Child_of_a_Dream\";\"Ari_Lehman\""}
{"question": "could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Brazilian_battleship_Minas_Geraes\";\"Battle_of_Actium\";\"Brazilian_battleship_Minas_Geraes\";\"Battle_of_Actium\""}
{"question": "can you find Bugs Bunny at Space Mountain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Go_God_Go\";\"Holes_(film)\";\"Flagstaff_station\";\"Bugs_Bunny_in_Double_Trouble\""}
{"question": "did a Polish poet write sonnets about Islamic religion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Crimean_Sonnets\";\"Romanticism_in_Poland\";\"The_Crimean_Sonnets\";\"The_Crimean_Sonnets\""}
{"question": "is Hanuman associated with a Norse god?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Hel_(being)\";\"Vaikuntha_Chaturmurti\";\"Religion_in_Iceland\";Hanuman"}
{"question": "does Iphone have more iterations than Samsung Galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "IPhone;IPhone;IPhone;IPhone"}
{"question": "can the city of Miami fit inside Uppsala?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Stockholm;\"Sweden_Solar_System\";Uppsala;\"Va\u0308rmlands_nation\""}
{"question": "was King Arthur at the beheading of Anne Boleyn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"King_Arthur_(opera)\";\"Treason_in_Arthurian_legend\";\"Gawain_(opera)\";\"Le_Morte_d'Arthur\""}
{"question": "can you transport a primate in a backpack?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Brachiation;\"Hudo_(Scouting)\";Packraft;\"Animal_locomotion\""}
{"question": "did Christopher Columbus break the fifth commandment in Christianity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "did", "extracted_entity": "\"Ten_Commandments_in_Catholic_theology\";\"First_seven_ecumenical_councils\";Desecration;\"Council_of_Chalcedon\""}
{"question": "do most high school head coaches make as much as the Head Coach at NCSU?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Professors_in_the_United_States\";\"College_Board\";\"College_athletics_in_the_United_States\";\"North_Carolina_Tar_Heels_men's_basketball\""}
{"question": "has Justin Timberlake ever written a song about Britney Spears?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Justin", "extracted_entity": "\"Britney_Spears\";\"Cry_Me_a_River_(Justin_Timberlake_song)\";\"Cry_Me_a_River_(Justin_Timberlake_song)\";\"Sometimes_(Britney_Spears_song)\""}
{"question": "could Eddie Hall hypothetically deadlift the world's largest cheeseburger?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Wham-O;\"Bill_Keeler\";\"Cheeseburger_(wrestler)\";Slopper"}
{"question": "did Johann Sebastian Bach leave his first wife for his second wife?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Maria_Barbara_Bach\";\"Anna_Magdalena_Bach\";\"Johann_Sebastian_Bach\";\"Anna_Magdalena_Bach\""}
{"question": "is the Mona Lisa based on a real person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Lifelike_experience\";\"I,_Mona_Lisa\";\"Mona_Lisa_replicas_and_reinterpretations\";\"I,_Mona_Lisa\""}
{"question": "does ABBA have similar gender configuration to The Mamas & The Papas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Mamas_&_the_Papas\";\"The_Mamas_&_the_Papas\";\"The_Mamas_&_the_Papas\";\"ABBA_(album)\""}
{"question": "is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Emerald_Pool\";\"Rahway_River\";\"Torch_Lake_(Antrim_County,_Michigan)\";Rainbowing"}
{"question": "is Mozambique Drill an easy shot for United States Army Ranger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Mozambique_Drill\";\"Mozambique_Drill\";\"Mozambique_Drill\";\"Mozambique_Drill\""}
{"question": "have rhinoceroses been killed to improve human sex lives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Poaching;\"Wildlife_smuggling\";\"Animal_sexual_behaviour\";\"Black_rhinoceros\""}
{"question": "is the Easter Bunny popular in September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Easter_Bunny\";\"Easter_Bunny\";\"Easter_Bunny\";Easter"}
{"question": "were veterans of the War in Vietnam (1945\u201346) given free education by the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Education_in_the_Soviet_Union\";\"Education_in_Russia\";\"Re-education_camp_(Vietnam)\";\"Education_in_the_Soviet_Union\""}
{"question": "can a Kia Rio fit inside the Oval Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kia_Rio\";\"Situation_Room\";\"Rio_Carbon\";\"Chicago_Air_Shower_Array\""}
{"question": "are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Anti-Catholicism;\"Martyr_Saints_of_China\";\"Christianity_in_China\";\"Chinese_Orthodox_Church\""}
{"question": "would someone buying crickets be likely to own pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Crickets_as_pets\";\"Crickets_as_pets\";\"Crickets_as_pets\";\"Crickets_as_pets\""}
{"question": "is Bugs Bunny known for carrying a root vegetable around with him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Beanstalk_Bunny\";\"Bugs_Bunny\";\"Buddy's_Bug_Hunt\";\"Lumber_Jack-Rabbit\""}
{"question": "would moon cakes be easy to find in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Mooncake;Mooncake;Mooncake;Mooncake"}
{"question": "does the Red Sea have biblical significance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Red_Sea\";\"Red_Sea\";\"Red_Sea\";\"Crossing_the_Red_Sea\""}
{"question": "could a newborn look over the top of a fully grown horseradish plant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Chad_Donovan\";\"Horse_culture_in_Mongolia\";\"Billy_Smart_Jr.\";\"Martha's_Son\""}
{"question": "will a rock float in the atmosphere of Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Iceberg;Micrometeorite;Micrometeoroid;Ice"}
{"question": "would three newborn kittens fit on a standard Amtrak coach seat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Metro_Richmond_Zoo\";Kitten;\"Sam_Bailey\";\"Baby_transport\""}
{"question": "would a loudspeaker be useful for most Gallaudet students?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Do_You_Feel_Like_We_Do\";\"Can_You_Hear_Their_Voices?\";\"Gentlemen_Hall\";Megaphone"}
{"question": "is Bucharest located south of Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Buch", "extracted_entity": "\"Cairo\u2013Alexandria_desert_road\";\"Cairo\u2013Alexandria_desert_road\";Khatatba;Khatatba"}
{"question": "do Bing (search engine) searches earn the searcher more than competitors do?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bing", "extracted_entity": "\"Search_neutrality\";\"Google_bomb\";Pay-per-click;\"Search_advertising\""}
{"question": "was Alaska part of the Northern Army during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Alaska_Territorial_Guard\";\"American_Expeditionary_Force,_Siberia\";\"United_States_Army_Alaska\";\"Allied_intervention_in_the_Russian_Civil_War\""}
{"question": "will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Abaddon;Abaddon;Abaddon;Satan"}
{"question": "do all shooting sports involve bullets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Shooting_sports\";\"Shooting_sports\";\"Shooting_sports\";\"Shooting_sports\""}
{"question": "is Edgar Allan Poe obscure in the world of horror fiction?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Edgar", "extracted_entity": "\"Weird_fiction\";\"Marc_Olden\";\"Marc_Olden\";\"Edgar_Allan_Poe\""}
{"question": "would Achilles dominate Legolas in a hypothetical fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Achilles", "extracted_entity": "\"Ajax_the_Great\";Hero;Iliad;Odysseus"}
{"question": "would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Crucifixion;\"Crucifixion_and_Last_Judgement_diptych\";Triclavianism;Longinus"}
{"question": "can a prime number be represented by the number of days in a week?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Round_number\";\"Abundant_number\";\"Round_number\";Sexagesimal"}
{"question": "can Planned Parenthood tell your University that you have Herpes simplex virus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Merciad\";\"You_Kent_Always_Say_What_You_Want\";\"Cameron_Crazies\";\"The_Merciad\""}
{"question": "can the Palace of Westminster tell time in the dark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Time_signal\";Time;\"Palace_of_Westminster\";\"Sandringham_time\""}
{"question": "can Arnold Schwarzenegger deadlift an adult Black rhinoceros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"M._Bison\";Ubhejane;\"ClayFighter_63\u2153\";\"Chouseishin_Gransazer\""}
{"question": "did Jesus go to school to study railroad engineering?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Thomas_Wenski\";\"John_Hardon\";\"Michael_Joseph_Murphy\";\"Kurt_Yaghjian\""}
{"question": "could a white cockatoo have lived through the entire Thirty Years' War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cockatoo;\"Carnaby's_black_cockatoo\";\"White_cockatoo\";Cockatoo"}
{"question": "would most school children in New York be wearing jackets on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Groundhog_Day\";\"Groundhog_Day\";\"Groundhog_Day\";\"We're_Not_from_Here\""}
{"question": "do children's bicycles often have extra wheels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Training_wheels\";Tricycle;\"Training_wheels\";Carousel"}
{"question": "are all Wednesdays in a year enough to read Bible 15 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Friday_the_13th\";\"Triennial_cycle\";\"Church_attendance\";\"Biblical_Sabbath\""}
{"question": "could Bernie Sanders visit the Metropolitan Museum of Art twenty times for under two hundred dollars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ethel_Scull_36_Times\";\"Ethel_Scull_36_Times\";\"Cultural_depictions_of_Elvis_Presley\";\"I._M._Pei\""}
{"question": "has Nikola Tesla's name inspired multiple brands?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Nikola_Motor_Company\";\"Nikola_Tesla_in_popular_culture\";\"Nikola_Tesla\";\"Tesla,_Inc.\""}
{"question": "is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Pirates_of_the_Caribbean:_The_Curse_of_the_Black_Pearl\";\"Shantae_and_the_Pirate's_Curse\";\"Raiders_of_the_Lost_Ark\";\"Ark:_Survival_Evolved\""}
{"question": "is the Matrix a standalone movie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"The_Matrix\";\"The_Matrix\";\"The_Matrix_Revolutions\";\"The_Matrix_(franchise)\""}
{"question": "could JPMorgan Chase give every American $10?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Publishers_Clearing_House\";\"Chase_Bank\";\"JPMorgan_Chase\";\"JPMorgan_Chase\""}
{"question": "will the Stanford Linear Accelerator fit on the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Transbay_Tube\";\"University_Link_tunnel\";\"Beacon_Hill_station_(Sound_Transit)\";\"Golden_Gate_Bridge\""}
{"question": "have Jamie Lee Curtis been the subject of fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jonathan_Lee_Riches\";\"Fake_news_website\";Uncooked;\"Jamie_Lee_Curtis\""}
{"question": "would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Socotra_Rock\";Apoxyomenos;\"The_Poseidon_Project\";\"Statue_of_Liberty\""}
{"question": "does New York Harbor sit on a craton without volcanic activity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Geology_of_New_York_(state)\";\"Smartville_Block\";\"Earthquake_activity_in_the_New_York_City_area\";\"Geography_of_New_York_City\""}
{"question": "was United Airlines blameless in worst crash in history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"United_Express_Flight_6291\";\"United_Airlines_Flight_93\";\"United_Airlines_Flight_736\";\"LeRoy_Homer_Jr.\""}
{"question": "did Polar Bears roam around in Ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Polar_bear\";\"Panthera_spelaea\";\"History_of_lions_in_Europe\";Kosmopoisk"}
{"question": "did Queen Elizabeth I read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Faerie_Queene\";\"May_Sarton\";\"History_of_the_Puritans_under_Queen_Elizabeth_I\";Gloriana"}
{"question": "is there historic graffiti on Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Roman_graffiti\";\"Ancient_Maya_graffiti\";\"Porta_Alchemica\";Graffiti"}
{"question": "does Adam Sandler skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Uz\u030cgave\u0307ne\u0307s\";\"Easter_egg\";\"Shrove_Tuesday\";\"Christmas_Guy\""}
{"question": "would a sesame seed be mistaken for a wood frog egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Sesame;Jackfruit;\"The_Hazel-nut_Child\";Tinda"}
{"question": "is Capricorn the hypothetical zodiac sign of Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Capricorn_(astrology)\";\"Negative_sign_(astrology)\";\"Negative_sign_(astrology)\";\"Cancer_(astrology)\""}
{"question": "is Nicole Kidman ideal choice to play Psylocke based on height and weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "icole", "extracted_entity": "\"Dominika_Wolski\";\"Naureen_Zaim\";\"Naureen_Zaim\";\"Julie_Strain\""}
{"question": "is a paraplegic suitable for conducting an orchestra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Derek_Paravicini\";\"Derek_Paravicini\";\"Concetta_M._Tomaino\";\"British_Paraorchestra\""}
{"question": "would you hire someone with dyscalculia to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Groma_surveying\";\"Construction_surveying\";Surveying;\"Paul_J._Turek\""}
{"question": "are there enough people in the Balkans to match the population of Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Romani_people_in_Serbia\";Kosovo;\"Serbian_diaspora\";Serbia"}
{"question": "does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pan_(god)\";\"Pan_(god)\";\"Don't_Give_Up_the_Sheep\";\"Pan_(god)\""}
{"question": "can the Persian Gulf fit in New Jersey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Geography_of_New_Jersey\";\"2011\u201312_Strait_of_Hormuz_dispute\";\"Persian_Gulf\";\"2011\u201312_Strait_of_Hormuz_dispute\""}
{"question": "did the swallow play a role in a famous film about King Arthur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"King_Arthur_Was_a_Gentleman\";\"The_Kid_Who_Would_Be_King\";\"King_Arthur_(2004_film)\";\"Phillip_Glasier\""}
{"question": "could Durian cause someone's stomach to feel unwell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dur", "extracted_entity": "Februus;\"Disease_in_Imperial_Rome\";Februus;\"Kuru_(disease)\""}
{"question": "do people take laxatives because they enjoy diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Fletcher's_Laxative\";Laxative;\"Fletcher's_Laxative\";\"Heat_intolerance\""}
{"question": "would someone on antidepressants need to be cautious of some citrus fruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Grapefruit\u2013drug_interactions\";\"Grapefruit\u2013drug_interactions\";\"Grapefruit\u2013drug_interactions\";\"Dog_collar\""}
{"question": "was Daniel thrown into the lion's den in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Daniel_in_the_lions'_den\";\"Daniel_in_the_lions'_den\";\"Book_of_Daniel\";\"Daniel_(biblical_figure)\""}
{"question": "can a Toyota Supra make a vlog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Internet_bot\";Slingbox;Privoxy;Sudo"}
{"question": "did brother of Goofy creator's employer commit an excommunicable offense?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Bullwinkle_J._Moose\";\"Nobody_Likes_Onions\";\"Mr._Burns\";\"Prick_Up_Your_Ears_(Family_Guy)\""}
{"question": "did any of religions in which Himalayas are sacred originate in 19th century?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Religion_in_Asia\";\"Indian_religions\";Sikh;Sikhism"}
{"question": "could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Open_Loop_New_York\";\"Hogwarts_Express_(Universal_Orlando_Resort)\";\"Fantasy_flight\";\"Universal_Studios_Florida\""}
{"question": "would an oil painter avoid reds from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "oil", "extracted_entity": "\"Alexander_John_Drysdale\";\"Drip_painting\";\"Moorten_Botanical_Garden_and_Cactarium\";\"Donny_Johnson\""}
{"question": "is coal needed to practice parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Coal;Coalite;\"Coal_mining\";\"Coal_seam_fire\""}
{"question": "can food be cooked in the cosmic microwave background?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Microwave_oven\";\"Microwave_oven\";\"Microwave_oven\";Microwave"}
{"question": "is Central Park Zoo located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Central_Park_Zoo\";\"Miami_Seaquarium\";\"Central_Park_Zoo\";\"Central_Park_Zoo\""}
{"question": "is Michael an unpopular name in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Michael_&_Michael_Have_Issues\";\"Michael_the_Black_Man\";\"Michael_the_Black_Man\";\"Michael_&_Michael_Have_Issues\""}
{"question": "is it common for women to have moustaches?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Moustache;Moustache;Moustache;Moustache"}
{"question": "is dopamine snorted nasally by drug users?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cocaine;Cocaine;\"Recreational_use_of_nitrous_oxide\";Cocaine"}
{"question": "can I ski in Steamboat Springs, Colorado in August?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Steamboat_Springs,_Colorado\";\"Steamboat_Ski_Resort\";\"Steamboat_Ski_Resort\";\"Steamboat_Springs,_Colorado\""}
{"question": "could the surface of Europa fry an egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Europa_(moon)\";\"Europa_(moon)\";\"Colonization_of_Europa\";\"Colonization_of_Europa\""}
{"question": "can too many oranges cause diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Effects_of_Hurricane_Irma_in_Florida\";\"Citrus_tristeza_virus\";\"Orange_(fruit)\";\"Citrus_canker\""}
{"question": "would someone in CHE101 require a Maya Angelou book?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Repent_Amarillo\";\"The_Possessed_(comics)\";\"The_Coven_(film)\";\"This_Isn't_What_It_Looks_Like\""}
{"question": "can I build a house on an asteroid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"To_Build_a_Home\";\"To_Build_a_Home\";Igloo;\"Monolithic_dome\""}
{"question": "can you get Raclette in YMCA headquarters city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Saint-Nazaire;\"Harlem_Shake_(meme)\";\"Rush_Hour_3\";\"Undercover_Brother\""}
{"question": "is a fairy more prevalent in world myths than a valkyrie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Fairy_Queen\";Valkyrie;Valkyrie;Fairy"}
{"question": "does Nicole Kidman despise Roman Josi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"JoJo_Siwa\";\"Roman_Reloaded_(song)\";\"Samantha_Bee\";\"JoJo_Siwa\""}
{"question": "are looks the easiest way to tell rosemary from lavender? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rose", "extracted_entity": "Cordierite;\"Black_Is_the_Color_(of_My_True_Love's_Hair)\";Litmus;\"Love_Grows_(Where_My_Rosemary_Goes)\""}
{"question": "are all twins the same gender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Twin;\"Conjoined_twins\";Twin;\"Multiple_birth\""}
{"question": "could Sainsbury's buy Tesco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tesco;\"Sainsbury's\";\"Sainsbury's\";Tesco"}
{"question": "is the Greek alphabet as common as Sumerian cuneiform?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Cuneiform;\"Old_Persian_cuneiform\";\"Hittite_cuneiform\";\"Cuneiform_(Unicode_block)\""}
{"question": "would Jesus understand the Easter Bunny?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Christology;\"Jesus_Christ_Superstar\";\"Assumption_of_Mary\";\"What_would_Jesus_do?\""}
{"question": "would Avengers Comics be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Comic_book\";\"Midtown_Comics\";SkyMall;\"Recalled_comics\""}
{"question": "would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Glynn_Turman\";\"Han_Solo\";\"David_Gautreaux\";\"Han_Solo\""}
{"question": "were any members of Canidae in Aesop's Fables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Pseudolynchia_canariensis\";Canidae;\"Fig_wasp\";\"Mole_snake\""}
{"question": "does Snoopy look like Chance from Homeward Bound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Shaggy_Rogers\";\"Journey_into_Imagination_with_Figment\";\"Figment_(Disney)\";\"Homeward_Bound:_The_Incredible_Journey\""}
{"question": "did Martin Luther believe in Satan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Martin_Luther\";\"Martin_Luther\";Lutheranism;Lutheranism"}
{"question": "could Barron Trump have lived through the Mexican Revolution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cesar_Millan\";\"William_Alexander_Morgan\";\"Chocolat_(2016_film)\";\"La\u0301zaro_Ca\u0301rdenas\""}
{"question": "is the current Chief Justice of the United States forbidden from buying alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"John_Roberts\";\"Chief_Justice_of_the_United_States\";\"Supreme_Court_of_the_United_States\";\"United_States_Court_of_Appeals_for_the_District_of_Columbia_Circuit\""}
{"question": "would Columbus have discovered Durian trees during his 1492 expedition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"White_Amazonian_Indians\";\"White_Amazonian_Indians\";\"Rodrigo_de_Triana\";\"Araucaria_cunninghamii\""}
{"question": "could a camel fit in a dog house?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Zarafa_(giraffe)\";Caravanserai;Ghorfa;\"Hybrid_camel\""}
{"question": "did Immanuel Kant ever meet the 14th president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Horace_Kallen\";\"Corliss_Lamont\";\"Princeton_University\";\"Nicholas_Murray_Butler\""}
{"question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Runts;\"Krispy_Kreme\";\"Special_K\";\"LRP_ration\""}
{"question": "will a person survive a fever of NY's highest recorded temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Deep_hypothermic_circulatory_arrest\";\"Clinical_death\";Hypothermia;Hypothermia"}
{"question": "was latest Republican governor of New Jersey as of 2020 heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"New_Jersey_in_the_21st_century\";\"New_Jersey_in_the_21st_century\";\"Governor_of_New_Jersey\";\"Elections_in_New_Jersey\""}
{"question": "did Dale Jr.'s father crash his car due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dale", "extracted_entity": "\"Murray_Chotiner\";\"Randy_D._Johnson\";\"Dale_Coyne_Racing\";\"Death_of_Dale_Earnhardt\""}
{"question": "would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Late_Night_with_Conan_O'Brien\";\"If_They_Could_See_Us_Now\";\"The_Tonight_Show_with_Conan_O'Brien\";\"Conan_(talk_show)\""}
{"question": "should you bring your own bags to Aldi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Packed_lunch\";\"Tiffin_carrier\";\"Birte_Glang\";\"Caffe\u0300_sospeso\""}
{"question": "is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Nobunaga_Concerto_(film)\";\"Saito\u0304_Hajime_(Rurouni_Kenshin)\";\"Zomahoun_Idossou_Rufin\";\"Daisuke_Ryu\""}
{"question": "could you make the kitchen 'holy trinity' without celery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Holy_trinity_(cuisine)\";\"Trinity_College,_Cambridge\";\"Holy_trinity_(cuisine)\";Churchkhela"}
{"question": "can a snake swallow an M60 Patton?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Blue_poison_dart_frog\";\"Python_molurus\";\"Jellyfish_Eyes\";Frogfish"}
{"question": "would an American feel lost due to language barriers at Disneyland Paris?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Julia_Alvarez\";Hypocognition;Hypocognition;\"Mia_Love\""}
{"question": "did Christopher Columbus condone multiple deadly sins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Seven_deadly_sins\";\"Seven_deadly_sins\";\"Seven_deadly_sins\";\"Seven_deadly_sins\""}
{"question": "would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Last_Temptation_of_Christ_(film)\";\"Death_Defying_Acts\";\"Richard_Dawkins\";\"Richard_Dawkins\""}
{"question": "should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"First-generation_college_students_in_the_United_States\";Science;Clairvoyance;Mediumship"}
{"question": "would an owl monkey enjoy a strawberry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Frugivore;\"Burrowing_Owl_Estate\";\"Burrowing_Owl_Estate\";\"Oh_Say_Can_You_Say?\""}
{"question": "do beeswax alternatives to cling wrap use plsatic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Nonwoven_fabric\";\"Semen_extender\";\"Polyurethane_laminate\";Plastisol"}
{"question": "are Brian Cranston and Saoirse Ronan's combined Emmy Awards a prime number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"14th_Satellite_Awards\";\"Daytime_Emmy_Award_for_Outstanding_Special_Guest_Performer_in_a_Drama_Series\";\"Daytime_Emmy_Award_for_Outstanding_Lead_Actor_in_a_Drama_Series\";\"38th_Daytime_Emmy_Awards\""}
{"question": "does Soylent use Pea for their source of protein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Soy_yogurt\";Soybean;Soybean;\"Soybean_sprout\""}
{"question": "does the United States Department of Education oversee services benefiting undocumented migrants? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"United_States_Department_of_Education\";\"School_Improvement_Grant\";\"United_States_Department_of_Education\";\"United_States_Department_of_Education\""}
{"question": "should cactus soil always be damp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "c", "extracted_entity": "Cactus;Cactus;Cactus;Cactus"}
{"question": "was disc jockey Jay Thomas enemies with Clayton Moore?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Jay_Thomas\";Jeezy;\"Meek_Mill\";\"Clay_Davis\""}
{"question": "are there people who are men who experience menstruation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Men_who_have_sex_with_men\";Menopause;\"Men_who_have_sex_with_men\";Menstruation"}
{"question": "would a retail associate envy the retailer's CEO's pay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Employee_retention\";\"Retail_loss_prevention\";\"Customer_Loyalty_(The_Office)\";\"Patronage_concentration\""}
{"question": "is a person with St. Vitus's Dance likely to win a ballet competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Vitus;\"Christopher_Gattelli\";\"Sydenham's_chorea\";\"So_You_Think_You_Can_Dance_(Belgian_and_Dutch_TV_series)\""}
{"question": "would a pacifist be opposed to hunting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Pacifism;Pacifism;Pacificism;Pacifism"}
{"question": "did Jane Austen suffer from middle child syndrome?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jane", "extracted_entity": "\"Reception_history_of_Jane_Austen\";\"Elizabeth_Jenkins_(author)\";\"Jane_Eyre\";\"Timeline_of_Jane_Austen\""}
{"question": "do hyenas appear in a Broadway musical?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Oh_What_a_Circus\";\"Magical_Mystery_Tour_(film)\";\"Oh_What_a_Circus\";\"Ooh_La_La_(Faces_song)\""}
{"question": "is calling ABBA the Swedish Beatles a preposterous claim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Isn't_It_a_Pity\";\"Beatles_(song)\";\"All_for_the_Beatles\";\"Carl_Magnus_Palm\""}
{"question": "can an emu chase a bogan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Boggart;Boggart;\"Shug_Monkey\";Boggart"}
{"question": "would the Ku Klux Klan welcome Opal Tometi into their group?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"Alleluia!_The_Devil's_Carnival\";\"The_Hateful_Eight\";\"She-Devils_on_Wheels\";\"Tura_Satana\""}
{"question": "was the Mentalist filmed in black and white?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fear_in_the_Night_(1972_film)\";\"Blindfold_(1966_film)\";\"Images_(film)\";\"Shadow_of_a_Doubt\""}
{"question": "did Mike Tyson train to use the gogoplata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Gogoplata;Gogoplata;Gogoplata;Gogoplata"}
{"question": "does the JPEG acronym stand for a joint committee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Joint_Photographic_Experts_Group\";\"Joint_parliamentary_committee\";\"Joint_parliamentary_committee\";\"Joint_committee\""}
{"question": "could a Jujutsu expert hypothetically defeat a Janissary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Jujutsu;Jujutsu;Jujutsu;\"Jo\u0304\""}
{"question": "could boolean algebra be described as binary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Boolean_algebra\";\"Binary_operation\";\"Two-element_Boolean_algebra\";\"Boolean_algebra\""}
{"question": "would Iceland lose to Amazon in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Icesave_dispute\";\"Whaling_in_Iceland\";\"Icelandic_outvasion\";Auctionata"}
{"question": "was Florence Nightingale's death more painful than Saint Peter's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Nightingale_(fairy_tale)\";\"Tuberculosis_in_human_culture\";\"Florence_Nightingale\";\"Marino_Faliero\""}
{"question": "are a dozen pickles good for easing hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Pickled_cucumber\";\"Spatini_sauce\";\"Ju\u0304rokucha\";\"Placebo-controlled_study\""}
{"question": "can Larry King's ex-wives form a water polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Larry_King_(tennis)\";\"Husbands_(TV_series)\";\"Larry_King_(tennis)\";\"Water_Polo_Yankees\""}
{"question": "did Hamlet's author use email?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Micropoetry;\"Terry_Pratchett\";\"Edgar_Wallace\";Hamlet"}
{"question": "would Garfield like canid food?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He", "extracted_entity": "Garfield;\"Garfield's_Thanksgiving\";\"Garfield:_A_Tail_of_Two_Kitties\";\"Garfield:_A_Tail_of_Two_Kitties\""}
{"question": "did Helen Keller ever read a novel by J. K. Rowling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Helen_Keller\";\"Thelma_Wood\";\"Helen_Corke\";\"No_Crystal_Stair\""}
{"question": "was the AK-47 used in the Seven Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Smith_&_Wesson_M76\";\"Comparison_of_the_AK-47_and_M16\";\"Smith_&_Wesson_M76\";\"Seven_Years'_War\""}
{"question": "would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Nancy_Drew\";\"Nancy_Drew\";\"The_Hardy_Boys/Nancy_Drew_Mysteries\";\"The_Hardy_Boys/Nancy_Drew_Mysteries\""}
{"question": "are there multiple Star Wars TV shows produced by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Star_Wars_prequel_trilogy\";\"Star_Wars:_Clone_Wars_(2003_TV_series)\";Disney+;\"Star_Wars_sequel_trilogy\""}
{"question": "did origin dynasty of Go precede Jia Sidao?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"Go_dynasty\";\"Go_dynasty\";\"Go_dynasty\";\"Go_dynasty\""}
{"question": "do Australians ride Kangaroos to work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Drover_(Australian)\";\"National_Ride_to_Work_Day\";Bullocky;\"Jackaroo_(trainee)\""}
{"question": "will every resident of Los Angeles County, California go to Jehovah Witnesses's heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ticket_to_Heaven\";\"Jehovah's_Witnesses_beliefs\";\"Kiavah_Wilderness\";\"Kiavah_Wilderness\""}
{"question": "are the Vietnamese people a great untapped resource for NBA players?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Corruption_in_Vietnam\";\"Gambling_in_Vietnam\";\"Corruption_in_Vietnam\";\"Corruption_in_Vietnam\""}
{"question": "did the Berlin Wall prevent any athletes from competing in the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"1936_Summer_Olympics\";\"Berlin_36\";\"1936_Summer_Olympics\";\"Basketball_at_the_1936_Summer_Olympics\""}
{"question": "is Route 66 generally unknown to Americans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Route", "extracted_entity": "\"U.S._Route_66\";\"U.S._Route_66\";\"U.S._Route_66\";\"U.S._Route_66\""}
{"question": "would a Bulgarian priest eat a four-course meal on Christmas Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bulgarian_cuisine\";\"Christmas_Eve\";Koliva;Zapivka"}
{"question": "would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Plessy_v._Ferguson\";\"Plessy_v._Ferguson\";\"Plessy_v._Ferguson\";\"Plessy_v._Ferguson\""}
{"question": "is Shiva's divine dance an ancient physical fitness pose?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "h", "extracted_entity": "Nataraja;Nataraja;Natarajasana;Nataraja"}
{"question": "is Pig Latin related to real Latin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Quod_licet_Iovi,_non_licet_bovi\";\"Dog_Latin\";\"Quod_licet_Iovi,_non_licet_bovi\";\"Pig_Latin\""}
{"question": "does the Dalai Lama believe in the divine barzakh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Ram_Bahadur_Bomjon\";\"Drenpa_Namkha\";\"Avalokites\u0301vara\";\"Dalai_Lama\""}
{"question": "will the torso be safe from blows to the largest and smallest bones in body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Scapular_fracture\";Bone;\"Brachial_plexus\";\"If_We_Are_the_Body\""}
{"question": "did Spiderman fight against Falcon in the MCU?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Spider-Man_in_video_games\";\"Spider-Man_Versus_Kraven_the_Hunter\";\"Spider-Man:_The_Video_Game\";\"Spider-Man_(2002_film)\""}
{"question": "would Alexander Hamilton have known about koalas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Koala;Koala;Moa;Koala"}
{"question": "are tumors in the lymph nodes ignorable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hemangioblastoma;Lymphangioma;Lymphangiomatosis;Lymphoma"}
{"question": "did the Qwerty keyboard layout predate computers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "QWERTY;\"Keyboard_layout\";\"Keyboard_layout\";Typewriter"}
{"question": "can crane slamdunk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Chimney_crane\";\"Alligator_shear\";\"Alligator_shear\";\"Bulk-handling_crane\""}
{"question": "is a railroad engineer needed during NASCAR events?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Pit_stop\";\"NASCAR_rules_and_regulations\";\"Railroad_engineer\";\"Pit_stop\""}
{"question": "would Kelly Clarkson's voice shake glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Walking_on_Broken_Glass\";\"Shattered_Glass_(Britney_Spears_song)\";\"Shattered_Glass_(Britney_Spears_song)\";\"Think_About_It_(Melanie_C_song)\""}
{"question": "does an organ donor need to be dead to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Organ_transplantation\";\"Transplantable_organs_and_tissues\";\"Organ_procurement\";\"Liver_transplantation\""}
{"question": "did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Walt_Disney_Company\";\"The_Jungle_Book_(1967_film)\";\"Walt_Disney\";\"Walt_Disney_Animation_Studios\""}
{"question": "is Fiat Chrysler gaining a new overall corporate identity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Chrysler_(brand)\";\"Fiat_Chrysler_Automobiles\";\"History_of_Chrysler\";\"Fiat_Chrysler_Automobiles\""}
{"question": "is ID required to get all medications from all pharmacies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Drug_coupon\";Pharmacy;\"Medical_prescription\";\"Medical_prescription\""}
{"question": "is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Robert_Edward_Lee_(sculpture)\";\"Nancy_Pelosi\";\"Black_Lives_Matter\";\"Black_Lives_Matter\""}
{"question": "would it be common to find a penguin in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sandfly_Bay\";\"Sandfly_Bay\";\"Little_penguin\";\"Curio_Bay\""}
{"question": "is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Raggare;\"Frugal_innovation\";\"Dobol_Trobol:_Lets_Get_Redi_2_Rambol!\";\"Maruti_Suzuki\""}
{"question": "is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mouse_for_Sale\";\"Cryptocurrency_bubble\";\"Mickey_Mouse\";\"Daffy_Dilly\""}
{"question": "during the pandemic, is door to door advertising considered inconsiderate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "during", "extracted_entity": "\"False_advertising\";Doorbuster;Fearmongering;\"Out-of-home_advertising\""}
{"question": "would early Eastern Canadian Natives language have use of the letter B?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Great_Lakes_Algonquian_syllabics\";\"Potawatomi_language\";\"Ojibwe_language\";\"Ojibwe_writing_systems\""}
{"question": "should someone prone to jealousy be in a polyamorous relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "Polyamory;Polyamory;\"Polygyny_in_animals\";\"Polygyny_in_animals\""}
{"question": "can eating grapefruit kill besides allergies or choking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Grape", "extracted_entity": "\"Grapefruit_juice\";\"Grapefruit_diet\";Grapefruit;\"Grape_and_raisin_toxicity_in_dogs\""}
{"question": "do Shivambu practitioners believe ammonia is unhealthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Aghor_Yoga\";Gomutra;Haumai;Gomutra"}
{"question": "does a Starbucks passion tea have ginger in it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hawaij;\"Ginger_tea\";\"Hibiscus_tea\";\"Hibiscus_tea\""}
{"question": "was The Jackson 5 bigger family band than The Isley Brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"The_Jackson_5\";\"The_Jackson_5\";\"The_Jackson_5\";\"The_Jackson_5\""}
{"question": "does Mercury help detect coronavirus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Mercury_(element)\";Echovirus;\"Human_coronavirus_NL63\";\"Mercury_poisoning\""}
{"question": "would someone typically confuse a sweet potato with a pineapple?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Princess_Caraboo\";Eggcorn;\"Ugli_fruit\";\"Alex_(parrot)\""}
{"question": "would a rabbi worship martyrs Ranavalona I killed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Pulsa_diNura\";\"Pope_Pius_XII_and_the_Holocaust\";\"Pai_Ma\u0304rire\";\"Martyrdom_of_Pionius\""}
{"question": "are there tearjerkers about United Airlines flights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"United_Airlines\";\"United_Airlines\";\"History_of_United_Airlines\";\"United_Airlines_Flight_93\""}
{"question": "would James Cotton's instrument be too strident for a smooth jazz band?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jazz_guitarist\";\"Earl_Young_(drummer)\";\"Pete_Candoli\";\"Slide_guitar\""}
{"question": "is Oculudentavis more dangerous than Allosaurus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al", "extracted_entity": "Allosaurus;Allosaurus;Torvosaurus;\"Odontolabis_cuvera\""}
{"question": "would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Mausoleum_of_Genghis_Khan\";\"Mausoleum_of_Genghis_Khan\";\"Mausoleum_of_Genghis_Khan\";\"Genghis_Khan\""}
{"question": "is Cholera alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Vibrio_cholerae\";Cholera;\"Vibrio_cholerae\";Cholera"}
{"question": "does Magnus Carlsen enjoy KFC?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Carlsberg_Lighthouse\";\"Christianssands_Bryggeri\";\"Carlsberg_Group\";\"Ha\u0301karl\""}
{"question": "should oysters be avoided by people with ADHD?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Otitis_media\";\"History_of_psychosurgery\";Overfishing;SeaChoice"}
{"question": "is it expected that Charla Nash would be anxious near a gorilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Fatal_Attractions_(TV_series)\";\"Vivienne_de_Watteville\";\"Emotion_in_animals\";\"The_Mind_of_an_Ape\""}
{"question": "can amoebas get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Amoebozoa;Amoebiasis;\"Acute_myeloid_leukemia\";\"Entamoeba_histolytica\""}
{"question": "does Snoop Dogg advocate a straight edge lifestyle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Straight_edge\";Normcore;\"The_Straight_Edge_Society\";Blaxploitation"}
{"question": "is menthol associated with Christmas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Menthol;Menthone;Nemenhah;Menthol"}
{"question": "would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Gerald_Howarth\";\"Political_views_of_Christopher_Hitchens\";\"Stephen_Whittle\";\"John_Stubbs\""}
{"question": "did Fran\u00e7ois Mitterrand serve under Napoleon Bonapart in the French army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Pierre-Louis_Dupas\";\"Pierre-Louis_Dupas\";\"Jean-Charles_Pichegru\";\"Franc\u0327ois_Se\u0301verin_Marceau\""}
{"question": "do ants outperform apes on language ability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Carpenter_ant\";Ant;\"Carpenter_ant\";\"Temnothorax_albipennis\""}
{"question": "is Cantonese spoken in Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Culture_of_Hong_Kong\";\"Chinese_people_in_Japan\";\"Japanese_Chinese_cuisine\";\"Japanese_language\""}
{"question": "would a birdwatcher pursue their hobby at a Philadelphia Eagles game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Birdwatching;Birdwatching;\"John_Bugas\";Hawkwatching"}
{"question": "do shrimp taste best when cooked for a long time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Neocaridina_davidi\";\"Dried_shrimp\";\"Seafood_boil\";\"Sea_urchin\""}
{"question": "did Columbus obtain his funding from the rulers of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Merchant_Prince\";Karelsprivilege;\"Byzantine_economy\";\"Bardi_family\""}
{"question": "can a minotaur hypothetically injure a tibia playing football?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Footballer's_ankle\";\"Tibia_shaft_fracture\";\"Falcon_(sport)\";\"Falcon_(sport)\""}
{"question": "did the Royal Air Force fight in the Boxer Rebellion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Military_history_of_New_Zealand\";\"Royal_New_Zealand_Air_Force\";\"History_of_the_Royal_Air_Force\";\"Imperial_Japanese_Navy_Air_Service\""}
{"question": "are monks forbidden from engaging in warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Excommunication;\"Conscientious_objector\";\"Freedom_of_religion_in_China\";\"Buddhist_ethics\""}
{"question": "does Metallica use Soulseek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Soul", "extracted_entity": "\"Soulshine_(song)\";\"Soulshine_(song)\";Soulsearcher;Soulsearcher"}
{"question": "would the operating system of a Samsung Galaxy 1 sound edible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Samsung_Galaxy_Express_2\";\"Samsung_Galaxy_Express\";\"Samsung_Galaxy\";\"Vocaloid_2\""}
{"question": "can the President of Mexico vote in New Mexico primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"United_States_presidential_primary\";\"United_States_presidential_primary\";\"United_States_presidential_nominating_convention\";\"2016_United_States_presidential_election_in_New_Mexico\""}
{"question": "could a firewall be destroyed by a hammer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Water_hammer\";\"Water_hammer\";\"Wave_pounding\";\"Battering_ram\""}
{"question": "is radioactive waste a plot device for many shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Radioactive_Man_(The_Simpsons_episode)\";\"Radioactive_Man_(The_Simpsons_episode)\";\"Radioactive_Man_(The_Simpsons_episode)\";\"Radioactive_Chicken_Heads\""}
{"question": "are raw carrots better for maximizing vitamin A intake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Carrot;Carrot;\"Carrot_juice\";\"Tomato_pomace\""}
{"question": "are all characters in Legend of Robin Hood fictional?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Robin_Hood:_The_Legend_of_Sherwood\";\"Robin_Hood:_The_Legend_of_Sherwood\";\"Robin_Hood_(DC_Comics)\";\"The_Merry_Adventures_of_Robin_Hood\""}
{"question": "can Amtrak's Acela Express break the sound barrier?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Acela_Express\";\"Acela_Express\";\"Acela_Express\";\"Acela_Express\""}
{"question": "can Vice President of the United States kill with impunity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he", "extracted_entity": "\"Suicide_by_cop\";Interdiction;\"Moise\u0301s_Giroldi\";Countersurveillance"}
{"question": "is Brooklyn known for its bread products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Breadbasket;Polly-O;\"Henry_S._Levy_and_Sons\";\"Henry_S._Levy_and_Sons\""}
{"question": "can children be hurt by jalapeno peppers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Street_children_in_Latin_America\";\"Jalapen\u0303o\";\"Jalapen\u0303o\";\"2008_United_States_salmonellosis_outbreak\""}
{"question": "would a cattle farmer be useful to a drum maker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Marching_Men\";\"Marching_Men\";\"Job_for_a_Cowboy\";\"Cattle_King\""}
{"question": "did Pedubastis I know Japanese people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Japanese_Wife\";\"Jo_Barrett\";\"Gay_Japan_News\";\"Bamboo_English\""}
{"question": "can Cyril Ramaphosa become Secretary General of NATO?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Cyril_Ramaphosa\";\"Colombo_Plan\";\"Cyril_Ramaphosa\";\"United_Nations_Secretary-General_selection\""}
{"question": "could an elephant easily defeat a male macaque?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Southern_elephant_seal\";\"The_Macomber_Affair\";\"Marsupial_lion\";\"The_Short_Happy_Life_of_Francis_Macomber\""}
{"question": "would the trees in Auburn, New York be changing colors in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Autumn_leaf_color\";\"Autumn_leaf_color\";\"Autumn_leaf_color\";\"Fraxinus_americana\""}
{"question": "would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Olympic_Stadium_(Montreal)\";\"City_of_Rock\";\"Saputo_Stadium\";\"Toronto_Rock\""}
{"question": "are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "True", "extracted_entity": "\"Crystal_healing\";\"Quartz_clock\";\"Adverse_health_effects_from_lunar_dust_exposure\";\"Marine_chronometer\""}
{"question": "did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Unusual_types_of_gramophone_records\";\"Unusual_types_of_gramophone_records\";Gretsch;\"Gramophone_Company\""}
{"question": "can atheism surpass Christianity in American black communities by 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Atheism_in_the_United_States\";\"Black_people_and_Mormonism\";\"Atheism_in_the_African_diaspora\";Atheism"}
{"question": "did James Watson's partner in studying the double helix outlive him? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Double_Helix_Medal\";\"James_Watson\";\"Jerry_Donohue\";\"Rosalind_(education_platform)\""}
{"question": "does the central processing unit usually have a dedicated fan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fan_(machine)\";\"Computer_fan\";\"Storage_heater\";\"Computer_fan\""}
{"question": "was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tickle_Me_Elmo\";\"Tickle_Me_Elmo\";Blaxploitation;\"Kickin'_It_Old_Skool\""}
{"question": "are sesame seeds glued onto hamburger buns?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Screaming_Yellow_Zonkers\";Sesame;\"Screaming_Yellow_Zonkers\";\"Screaming_Yellow_Zonkers\""}
{"question": "were the Great Pyramids built by a theocratic government?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Great_Pyramid_of_Giza\";\"The_Myth_of_the_Machine\";\"Ancient_Egypt\";Pyramid"}
{"question": "would it be wise to bring a robusto into Central Park Zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Feral_rhesus_macaque\";\"Reticulated_giraffe\";\"The_Maryland_Zoo_in_Baltimore\";\"San_Diego_Zoo_Global\""}
{"question": "do restaurants associate meatballs with the wrong country of origin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"A\u0300_la_zingara\";\"Mystery_meat\";\"Tastes_like_chicken\";\"Mystery_meat\""}
{"question": "can eating your weight in celery prevent diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Diabulimia;\"Diabetes_mellitus\";\"Diabetes_mellitus\";\"Healthy_diet\""}
{"question": "does Mario use mushrooms to run faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"The_Super_Mario_Bros._Super_Show!\";\"Mario_&_Luigi:_Superstar_Saga\";\"Mario_Party_Advance\";\"Captain_Toad:_Treasure_Tracker\""}
{"question": "are goldfish more difficult to care for than isopods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Common_goldfish\";Goldfish;Goldfish;Goldfish"}
{"question": "is the rise of agriculture attributed to rivers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Irrigation;\"History_of_agriculture\";\"River_delta\";\"River_valley_civilization\""}
{"question": "were number of states in Ancient Greece underwhelming compared to US states in 1900?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"History_of_modern_Greece\";\"History_of_modern_Greece\";\"History_of_modern_Greece\";\"History_of_modern_Greece\""}
{"question": "did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Byzantine_calendar\";\"Byzantine_calendar\";\"Byzantine_calendar\";\"Greek_Old_Calendarists\""}
{"question": "has Cesar Millan ever tamed a short-eared dog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cesar_Millan\";\"Picardy_Spaniel\";Obelix;\"French_Bulldog\""}
{"question": "can a chess board be converted to a Shogi board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Shogi;\"Chigorin_Chess\";Trishogi;Hexshogi"}
{"question": "could R. Kelly write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Russell_Kelly\";\"Kelly_Oxford\";\"Rhett_Miller\";\"Thomas_Rhett\""}
{"question": "could the moon fit inside the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Bathyscaphe_Trieste\";\"Phobos_(moon)\";\"Lake_Vostok\";\"Moon_pool\""}
{"question": "can paratroopers be used in a vacuum?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Frogman;Paratrooper;\"Hostage_Rescue_Team\";\"Underwater_Demolition_Team\""}
{"question": "can I hold Bing in a basket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Sneakbox;\"Wringer_(magic_trick)\";\"Hold_Anything\";\"Oh_Say_Can_You_Say?\""}
{"question": "did the Nepalese Civil War take place near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Nepalese_Civil_War\";\"Nepalese_Civil_War\";\"People's_war\";\"Nepalese_Civil_War\""}
{"question": "is clementine pith highly sought after?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "c", "extracted_entity": "Clementia;\"Tono_Maria\";\"Clementine_Rose\";\"Clementine_(series)\""}
{"question": "would a Rabbi celebrate Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Thanksgivukkah;\"Hanukkah_bush\";Kiddush;\"Chabad_messianism\""}
{"question": "are psychiatric patients welcome to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"United_States_Air_Force_Medical_Service\";\"United_States_Air_Force_Medical_Service\";\"United_States_military_chaplains\";\"United_States_Air_Force_Medical_Service\""}
{"question": "did Ivan the Terrible use the Byzantine calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Byzantine_calendar\";\"Byzantine_calendar\";\"Byzantine_calendar\";\"Byzantine_calendar\""}
{"question": "can you get a fever from consuming meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Classical_swine_fever\";\"Effects_of_global_warming_on_humans\";\"Bulimia_nervosa\";\"Heat_cramps\""}
{"question": "can Viper Room concert hypothetically be held at National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Bomb_Factory\";\"The_Bomb_Factory\";\"The_Bomb_Factory\";\"The_Viper_Room\""}
{"question": "would you be likely to see storks at a baby shower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Milky_stork\";\"St._Paul,_Alaska\";\"Bearded_seal\";\"Nevus_flammeus_nuchae\""}
{"question": "would Methuselah hypothetically hold a record in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Methuselah;Methuselah;Methuselah;Methuselah"}
{"question": "as of 2020 have more women succeeded John Key than preceded him?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Women's_suffrage_in_New_Zealand\";\"Prime_Minister_of_New_Zealand\";\"New_Zealand_at_the_2016_Summer_Olympics\";\"The_Guardian's_100_Best_Novels_Written_in_English\""}
{"question": "does Princess Peach's dress resemble a peach fruit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Peach_Melba\";\"Princess_Peach\";\"Peach_(color)\";\"Girl_with_Peaches\""}
{"question": "is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Scampi;\"Stephen_Colbert_(character)\";Steve-O;\"Alex_O'Loughlin\""}
{"question": "do mail carriers need multiple uniforms?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mail", "extracted_entity": "\"Tiffin_carrier\";\"Mail_carrier\";\"Mail_satchel\";\"Porter_(carrier)\""}
{"question": "did Easy Rider make a profit at the theater when it was released?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Easy_Rider\";\"Easy_Rider\";\"Prompt_Payment_and_Stealing_a_Ride\";\"The_Circus_(film)\""}
{"question": "is most store bought rice pudding made with brown rice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Most", "extracted_entity": "\"Rice_pudding\";\"Rice_pudding\";\"Rice_pudding\";\"Rice_pudding\""}
{"question": "would the chef at Carmine's restaurant panic if there was no basil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Strega_Nona\";\"Welcome_to_Collinwood\";\"The_Linguini_Incident\";\"Eating_Raoul\""}
{"question": "are Chipotle Cinnamon Pork Chops appropriate for a Seder?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chip", "extracted_entity": "\"Queso_panela\";\"Chipotle_Mexican_Grill\";\"Pimento_loaf\";Bissara"}
{"question": "could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tamanaco;Cat;\"If_Looks_Could_Kill_(film)\";\"The_New_York_Times_crossword_puzzle\""}
{"question": "does a sea otter eat spiders?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sea_otter\";\"Giant_otter_shrew\";\"Goblin_shark\";\"Portuguese_man_o'_war\""}
{"question": "is unanimously elected president's birthday a break for mail carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Powers_of_the_President_of_the_United_States\";\"O._Henry\";\"Coattail_effect\";\"Turnip_Day_Session\""}
{"question": "does Rusev have to worry about human overpopulation in his homeland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Life_and_Extraordinary_Adventures_of_Private_Ivan_Chonkin\";\"Ivan_Pavlov\";\"Ivan_Vorpatril\";\"Russian_jokes\""}
{"question": "does Buddy The Elf know anyone who works in publishing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Poul_Lange\";\"Elf_(film)\";\"Elf:_Buddy's_Musical_Christmas\";\"Noisy_Outlaws\""}
{"question": "is the Liberty Bell still in its original location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Liberty_Bell\";\"Liberty_Bell\";\"Liberty_Bell_Museum\";\"Liberty_Bell\""}
{"question": "does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"On_the_Road\";\"The_Crow_Road\";\"The_Paratrooper_of_Mechanic_Avenue\";\"Slum_tourism\""}
{"question": "does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"If_You_Build_It\";\"Half-Decent_Proposal\";\"The_Price_of_Admission\";\"If_You_Build_It\""}
{"question": "could a hundred thousand lolcats fit on a first generation iPhone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Linus_Sebastian\";\"Loon_(company)\";\"A_Boy_and_His_Atom\";\"DOK-ING_Loox\""}
{"question": "are there multiple Disney Zorro?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Disney's_Dinosaur_(video_game)\";\"Disney_Infinity:_Marvel_Super_Heroes\";\"Disney_Infinity\";\"The_Three_Caballeros\""}
{"question": "would students at Marist have to petition to get a rowing team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Somerville_College,_Oxford\";\"Vincent's_Club\";\"Vincent's_Club\";\"St_Catherine's_College_Boat_Club\""}
{"question": "does frost mean that it will be a snowy day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Snowsquall;\"Weather_lore\";Blizzard;\"Winter_storm_warning\""}
{"question": "did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Martyn_Andrews\";\"Hedgehog_in_the_Fog\";\"Boris_Johnson\";\"Alexander_Popov_(swimmer)\""}
{"question": "can a quarter fit inside of a human kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Quarter_Pounder\";\"Quarter_Pounder\";\"Quarter_Pounder\";\"Quarter_(United_States_coin)\""}
{"question": "did Alfred Nobel write a banned book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Prevention_of_Literature\";Censorship;\"Marriage_and_Morals\";\"Alfred_Nobel\""}
{"question": "could Palm Beach be held in the palm of your hand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Palm_Pistol\";\"Palm_Pistol\";\"The_Fairmont_Palm_Hotel_&_Resort\";\"Palm_Coast,_Florida\""}
{"question": "would Jason Voorhees hypothetically fail at being a martyr?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Skywire_Live\";\"Dirk_Willems\";\"Anthony_Martin_(escape_artist)\";\"Wesley_Autrey\""}
{"question": "do more Cauliflower grow in Arizona than California?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Caulanthus_californicus\";\"Croton_californicus\";\"Astragalus_oocarpus\";\"Caulanthus_californicus\""}
{"question": "is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Blackberry_winter\";\"Leaf_peeping\";\"Lammas_growth\";\"Autumn_leaf_color\""}
{"question": "does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Battle_of_Mount_Austen,_the_Galloping_Horse,_and_the_Sea_Horse\";\"War_Horse_(film)\";\"Steve_Tesich\";\"War_Horse_(film)\""}
{"question": "does a Trek 9000 require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Detachable_chairlift\";\"Overhang_(vehicles)\";\"Ski_to_Sea_Race\";\"Ford_Expedition\""}
{"question": "can a Goblin shark hypothetically ride a bike if it had limbs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"If_I_Could_Fly\";\"If_I_Could_Fly\";\"Stegosaurus_in_popular_culture\";Octopus"}
{"question": "could John Key issue an executive order in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Executive_order\";\"Executive_order\";\"Executive_order\";\"Executive_order\""}
{"question": "does Evander Holyfield eat pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Peter_Tatchell\";\"Eustace_Miles\";\"Heather_Mills\";\"Sandor_Katz\""}
{"question": "are the headquarters of All Nippon Airways near a beach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Air_Nippon_Network\";\"All_Nippon_Airways_Flight_857\";\"White_Swan_Hotel\";\"All_Nippon_Airways_Flight_60\""}
{"question": "will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Chick-fil-A;Chick-fil-A;Chick-fil-A;\"Chick-fil-A_same-sex_marriage_controversy\""}
{"question": "do hamsters provide food for any animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Golden_hamster\";Hamster;\"Turkish_hamster\";\"Chinese_hamster\""}
{"question": "can horseradish be eaten in a religious context?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Horseradish;Horseradish;\"Horse_meat\";Horseradish"}
{"question": "would Taylor Swift refer to Snoopy as oppa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "opp", "extracted_entity": "Esskeetit;\"Unorthodox_Jukebox\";\"Ahh..._The_Name_Is_Bootsy,_Baby!\";\"Bi-Polar_(Vanilla_Ice_album)\""}
{"question": "should children be kept from \"special brownies\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Beagle_Boys\";\"Book_of_Brownies\";Charmin;\"The_Poor_Kid\""}
{"question": "can monkeys use QWERTY keyboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Portuguese_keyboard_layout\";\"Arrow_keys\";QWERTY;\"PLUM_keyboard\""}
{"question": "are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Le_Jeu_des_1000_euros\";\"Le_Jeu_des_1000_euros\";\"Le_Jeu_des_1000_euros\";\"Don't_Forget_the_Lyrics!\""}
{"question": "did the Nazis use the Hammer and sickle flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Law_in_Nazi_Germany\";\"Flag_of_Germany\";\"Post\u2013World_War_II_legality_of_Nazi_flags\";\"Flag_of_Germany\""}
{"question": "are any letters directly before and after H missing from Roman numerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "H-dropping;\"Standard_Alphabet_by_Lepsius\";\"Aspirated_h\";\"Standard_Alphabet_by_Lepsius\""}
{"question": "do storks need golden toads to survive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Golden_toad\";\"Golden_toad\";\"Golden_toad\";\"Golden_toad\""}
{"question": "did compact discs make computer gaming more popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"History_of_video_games\";\"Compact_disc\";\"Cassette_deck\";\"Cassette_tape\""}
{"question": "would a teacher still have their job if they called a black student an ape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Ape_House\";\"You_Can't_Do_That_on_Television\";\"Black_people\";\"Charlie_Brown_(The_Coasters_song)\""}
{"question": "did Bill Gates help to develop the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Road_Ahead_(Bill_Gates_book)\";\"Bill_Gates\";\"Idea_Man\";\"Bill_Gates\""}
{"question": "would a Catholic priest commend someone's pride?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Alberic_Crescitelli\";\"Criticism_of_the_Catholic_Church\";\"Canonization_of_Pope_Pius_XII\";Veneration"}
{"question": "did the Social Democratic Party of Germany help Frederick II become King of Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Social_democracy\";\"History_of_the_Social_Democratic_Party_of_Germany\";\"Social_Reform_or_Revolution?\";\"Social_democracy\""}
{"question": "is Eighth Amendment to the United States Constitution popular in court?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "In", "extracted_entity": "\"Eighth_Amendment_to_the_United_States_Constitution\";\"Eighth_Amendment_to_the_United_States_Constitution\";\"Eighth_Amendment_to_the_United_States_Constitution\";\"Eighth_Amendment_to_the_United_States_Constitution\""}
{"question": "could Aretha Franklin vote for a president when her second child was born?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Aretha_Franklin\";\"Sara_Roosevelt\";\"The_President's_Daughter_(Britton_book)\";\"Ann_Nixon_Cooper\""}
{"question": "can you use the T-Mobile tuesdays app if you aren't a T-Mobile customer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Android_(operating_system)\";\"Virgin_Mobile_USA\";\"Amazon_Alexa\";\"Snap_Inc.\""}
{"question": "are classic nintendo games for emulator legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Emulator;\"Video_game_console_emulator\";\"Video_game_console_emulator\";\"Nintendo_Entertainment_System_hardware_clone\""}
{"question": "can a Sphynx cat be used for wool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sphynx_cat\";\"Sphynx_cat\";\"Donskoy_cat\";\"Sphynx_cat\""}
{"question": "do people remember Lucille Ball's winemaking as successful?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Luc", "extracted_entity": "\"Lucille_Ball\";\"Lucille_Ball\";\"Lucille_Ball\";\"Blanche_Merrill\""}
{"question": "would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Atlantic_salmon\";\"AquAdvantage_salmon\";\"Atlantic_mackerel\";\"Smoked_salmon\""}
{"question": "could Brooke Shields succeed at University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Brooke_Shields\";If....;\"Sarah_Solovay\";\"Brooke_Shields\""}
{"question": "does Neville Longbottom have more courage as a child than as an adult?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Rise_of_Neville_Chamberlain\";\"Yes,_What?\";\"Neville_Longbottom_(Fictional_Character)\";\"Neville_Longbottom_(Fictional_Character)\""}
{"question": "do Windows or Android smartphones run newer versions of Linux?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Linux_distribution\";\"Linux_distribution\";\"Linux_kernel\";Linux"}
{"question": "can the largest crustacean stretch out completely on a king-sized mattress?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Cephalopod_size\";\"Blanket_octopus\";\"Giant_oarfish\";\"Dactylogyrus_vastator\""}
{"question": "did Bruiser Brody wrestle on WWE Raw?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Donovan_Dijak\";\"Cassidy_O'Reilly\";\"Bruiser_Brody\";\"Alex_Riley\""}
{"question": "is Atlantic cod found in a vegemite sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Cod;\"Pescado_frito\";\"Cod_as_food\";\"Atlantic_cod\""}
{"question": "is an astronomer interested in drosophila?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Drosophilist;\"Science_book\";Drosophilist;\"Probably_Science\""}
{"question": "do people who smoke Djarum's like cloves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Cohoba;Jenkem;Giraffe;Djarum"}
{"question": "was Bruce Lee absent from the 1964 University of Washington graduation ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"James_Hood\";\"George_Winne_Jr.\";\"Henry_V._Graham\";\"Henry_V._Graham\""}
{"question": "does chlorine inhibit photosynthesis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Chlorosis;\"Cyanide_poisoning\";Photosynthesis;Chlorine"}
{"question": "does Amtrak operate four wheel vehicles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Passenger_car_(rail)\";\"Four-wheel_drive\";4-8-4;\"Four_Wheel_Drive\""}
{"question": "will parma ham be ready for New Year's if the pig is slaughtered in December?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Parmigiano-Reggiano;Ham;Prosciutto;Prosciutto"}
{"question": "have jokes killed more people than rats in history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Dumb_Ways_to_Die\";\"Our_Dumb_Century\";\"A_Million_Ways_to_Die_in_the_West\";\"Roadkill_cuisine\""}
{"question": "is Islamophobia against Cyprus majority religion misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Islam", "extracted_entity": "\"Greek_Cypriots\";Anti-Turkism;Anti-Turkism;\"Islam_in_Cyprus\""}
{"question": "are there multiple American government holidays during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Public_holidays_in_the_United_States_Virgin_Islands\";\"Public_holidays_in_the_United_States\";\"Public_holidays_in_the_United_States_Virgin_Islands\";\"Public_holidays_in_the_United_States\""}
{"question": "is Romeo and Juliet an unusual title to teach high schoolers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Love_Lessons_(novel)\";\"Jack_Lemmon\";Valedictorian;\"Oxford,_Cambridge_and_RSA_Examinations\""}
{"question": "does the density of helium cause voices to sound deeper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Vocal_resonation\";Phonation;Helium;\"Occlusion_effect\""}
{"question": "did Snoop Dogg refuse to make music with rival gang members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Tha_Dogg_Pound\";\"Snoop_Dogg\";\"Straight_Outta_Compton_(film)\";\"Gangsta_rap\""}
{"question": "are Leopard cats in less dire straits than Bornean Orangutan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Orangutan;\"Malayan_weasel\";Binturong;\"Malayan_weasel\""}
{"question": "did the Wehrmacht affect the outcome of the War to End All Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Morgenthau_Plan\";Wehrmacht;Denazification;\"Final_Solution\""}
{"question": "could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "has Ivan the Terrible flown to Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ivan", "extracted_entity": "\"Aeroflot_Flight_3739_(1988)\";\"Lou_and_Andy\";\"Russian_(comics)\";\"Berlin_European_UK\""}
{"question": "would a silicon shortage be bad for Intel's sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Silicon_Valley\";\"Chip_famine\";\"Chip_famine\";Intel"}
{"question": "would a Frigatebird in Ontario be a strange sight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Fireship_of_Baie_des_Chaleurs\";\"Boreal_woodland_caribou\";\"UFO_sightings_in_Canada\";\"Pickle_Lake\""}
{"question": "can cancer cause excess adrenaline production?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bone_marrow_suppression\";\"Bone_marrow_suppression\";\"Downregulation_and_upregulation\";Adrenaline"}
{"question": "can you cure hepatitis with a tonsillectomy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Tonsillectomy;Tonsillectomy;Tonsillectomy;Tonsillectomy"}
{"question": "if someone loves buffalo wings do they enjoy capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Capsaicin;\"Pocari_Sweat\";\"Velia_caprai\";\"Phytophthora_capsici\""}
{"question": "are tampons a good 24 hour solution for mentruation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Menopause;\"Hormone_replacement_therapy\";Menopause;Menopause"}
{"question": "is the Royal Air Force ensign on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"New_Zealand_Civil_Air_Ensign\";\"Royal_Air_Force_Ensign\";\"Royal_Australian_Air_Force_Ensign\";\"Royal_Air_Force_Ensign\""}
{"question": "would a customer be happy if their grocery store meat tasted like game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the", "extracted_entity": "\"Norm_Peterson\";\"Good_Burger\";\"Fun_with_Veal\";\"Pure_Food_Building\""}
{"question": "is the cuisine of Hawaii suitable for a vegan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Hawaii_(island)\";\"Genetically_modified_food_in_Hawaii\";\"Genetically_modified_food_in_Hawaii\";\"Cuisine_of_Hawaii\""}
{"question": "is it safe to wear sandals in snow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Snow_boot\";\"Snow_boot\";\"Bunny_boots\";\"The_Sandals\""}
{"question": "is the Very Large Telescope the most productive telescope in the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Very_Large_Telescope\";\"Very_Large_Telescope\";\"Science_and_technology_in_China\";\"Very_Large_Telescope\""}
{"question": "is All Purpose Flour safe for someone who has celiac disease?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Fructose_malabsorption\";\"Infant_formula\";\"Specific_carbohydrate_diet\";Flour"}
{"question": "while viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Surprise_(emotion)\";\"Jump_scare\";Adrenaline;\"Erotic_asphyxiation\""}
{"question": "should spaghetti be slick when cooked?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Spaghetti;Dashi;Dashi;Spaghetti"}
{"question": "do Sweet Potatoes prevent other plants from growing in their place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Sweet_potato\";Tomato;\"Begonia_grandis\";Subshrub"}
{"question": "was Richard III ruler of Adelaide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"William_III_of_Provence\";\"William_III_of_Provence\";\"William_III_of_Sicily\";\"Adelaide_of_Aquitaine\""}
{"question": "are Sable's a good choice of Mustelidae to weigh down a scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "S", "extracted_entity": "Mustelidae;Mustelidae;Mustelidae;Elaninae"}
{"question": "was Snoop Dogg's debut studio album released on the weekend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Snoop_Dogg\";\"Snoop_Dogg\";\"Ego_Trippin'\";\"Snoop_Dogg_discography\""}
{"question": "do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Paleontology_in_Alaska\";\"Alaskan_Malamute\";Sahuagin;\"Dwarf_sperm_whale\""}
{"question": "is Krishna similar to Holy Spirit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Krishna;\"Radha_Krishna\";Krishna;Krishna"}
{"question": "is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Early_American_editions_of_The_Hobbit\";Reversi;\"Proof-of-work_system\";\"The_Hobbit_(film_series)\""}
{"question": "does Santa Claus work during summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Santa's_workshop\";\"Santa's_workshop\";\"Santa_Claus\";\"Santa_Claus\""}
{"question": "would the average American family find Adam Sandler's home to be too small?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Erwin_Wurm\";\"Kronish_House\";\"Apartment_Therapy\";\"Single_room_occupancy\""}
{"question": "could the first European visitor to Guam been friends with Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"History_of_Guam\";\"History_of_Kiribati\";Pukapuka;\"Kala\u0304kaua's_1881_world_tour\""}
{"question": "could someone have arrived at Wrestlemania X in a Toyota Prius?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"WrestleMania_XI\";\"WrestleMania_IX\";\"WrestleMania_XX\";\"WrestleMania_X\""}
{"question": "would Republic of Korea Navy dominate Eritrea navy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Republic_of_Korea_Navy\";\"Republic_of_Korea_Navy\";\"Republic_of_Korea_Navy\";\"Republic_of_Korea_Navy\""}
{"question": "could a Gladiator's weapon crush a diamond?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Gladiator;\"Meteor_hammer\";Stiletto;Vajra"}
{"question": "was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Second_Amendment_to_the_United_States_Constitution\";\"Second_Amendment_to_the_United_States_Constitution\";\"Second_Amendment_to_the_United_States_Constitution\";\"Second_Amendment_to_the_United_States_Constitution\""}
{"question": "can Lamborghini's fastest model win a race against a Porsche 911?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Porsche_911_GT2\";\"Porsche_911\";Motorcycling;\"Porsche_911_GT1\""}
{"question": "does a mongoose have natural camouflage for desert?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Yellow_mongoose\";\"Ruddy_mongoose\";Mongoose;\"Indian_grey_mongoose\""}
{"question": "if someone is a vegan, would they eat honey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Veganism;Honey;Honey;Veganism"}
{"question": "does Jack Sparrow know any sea shantys?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Jack_Sparrow\";\"Jack_Sparrow\";\"Jack_Sparrow_(song)\";\"Jack_Sparrow_(song)\""}
{"question": "is Thanksgiving sometimes considered a day of mourning?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Thanksgiving_(United_States)\";\"Thanksgiving_(United_States)\";\"Thanksgiving_(United_States)\";\"National_Day_of_Mourning_(United_States_protest)\""}
{"question": "were there fifty English kings throughout the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Sussex_in_the_High_Middle_Ages\";\"France_in_the_Middle_Ages\";\"How_Many_Miles_to_Babylon?\";\"Gu\u00f0r\u00f8\u00f0r_Magnu\u0301sson\""}
{"question": "can rowing competitions take place indoors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Rowing_club\";\"History_of_rowing_sports\";\"Rowing_tank\";\"Rowing_tank\""}
{"question": "did Sartre write a play about Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Hell;\"Hell's_Ditch\";\"The_Marriage_of_Heaven_and_Hell\";\"Jean-Paul_Sartre\""}
{"question": "was Martin Luther same sect as Martin Luther King Jr.?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Martin", "extracted_entity": "Lutheranism;Lutheranism;\"Radical_Reformation\";\"Radical_Reformation\""}
{"question": "could Amazon afford The Mona Lisa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"If_I_Had_$1000000\";\"Money_Can't_Buy\";\"Mona_Lisa\";\"Mona_Lisa\""}
{"question": "can you find a railroad engineer on TNT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Railroad_Job\";\"John_Henry_(folklore)\";\"Salvage_(The_X-Files)\";\"Repairman_Jack\""}
{"question": "would Franz Ferdinand have survived with armadillo armor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Archduke_Franz_Ferdinand_of_Austria\";\"Iron_Cross_(film)\";\"Archduke_Franz_Ferdinand_of_Austria\";\"Assassination_of_Archduke_Franz_Ferdinand\""}
{"question": "would keelhauling be a fair punishment under the Eighth Amendment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Keelhauling;\"Eighth_Amendment_to_the_United_States_Constitution\";\"Eighth_Amendment_to_the_United_States_Constitution\";\"Roper_v._Simmons\""}
{"question": "is Bern located east of Paris?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Bern;\"La_Chaux-de-Fonds\";\"Rue_de_Berne\";\"Canton_of_Geneva\""}
{"question": "can Herpes simplex virus spread on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Herpes_simplex\";\"Herpes_simplex_virus\";\"Herpes_simplex_virus\";\"Herpes_simplex_virus\""}
{"question": "is Mixed martial arts totally original from Roman Colosseum games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mixed", "extracted_entity": "\"Martial_arts\";\"Mixed_martial_arts\";\"Mixed_martial_arts\";\"History_of_martial_arts\""}
{"question": "are those incapable of reproduction incapable of parenthood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Infertility;Infertility;Human;Fertility"}
{"question": "would Amy Winehouse's death have been prevented with Narcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Narconon;\"Sia\u0302n_Busby\";\"Norethisterone_enanthate\";\"Debora_Green\""}
{"question": "was Gandalf present at the death of Eomer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Byrhtnoth;\"O\u0301tr\";\"E\u0301omer\";\"The_Homecoming_of_Beorhtnoth_Beorhthelm's_Son\""}
{"question": "does coding rely on Boolean algebra characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"KOI_character_encodings\";ROT13;\"Polybius_square\";\"Polybius_square\""}
{"question": "will Queen Elizabeth be buried in the Pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Queen_Elizabeth_The_Queen_Mother\";\"Coronation_of_Elizabeth_II\";\"Coronation_of_Elizabeth_II\";\"Coronation_of_Elizabeth_II\""}
{"question": "can fish get Tonsillitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Tonsillitis;Tonsillitis;Tonsillectomy;Tonsillitis"}
{"question": "would Sophist's have hypothetically made good lawyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Sophist;\"SOAS_School_of_Law\";Sophist;Sophist"}
{"question": "would a pescatarian be unable to eat anchovy pizza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Il_Vegetale\";\"Il_Vegetale\";Pescetarianism;\"Chickenman_(radio_series)\""}
{"question": "could a chipmunk fit 100 chocolate chips in his mouth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"If_I_Had_$1000000\";\"Bo\u0304ku\";\"Kool-Aid_Man\";\"Super_Size_Me\""}
{"question": "does The Hague border multiple bodies of water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Germany\u2013Netherlands_border\";\"Belgium\u2013Netherlands_border\";\"Germany\u2013Netherlands_border\";\"Indonesia\u2013Malaysia_border\""}
{"question": "is Rick and Morty considered an anime?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Rick_and_Morty\";\"Rick_and_Morty\";\"Rick_and_Morty\";\"Rick_Sanchez_(Rick_and_Morty)\""}
{"question": "could a llama birth twice during War in Vietnam (1945-46)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Multiple_birth\";\"Gertrude_Van_Wagenen\";\"Pregnancy_test\";\"Multiple_birth\""}
{"question": "do some religions look forward to armageddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Waiting_for_Armageddon\";\"Waiting_for_Armageddon\";Armageddon;Armageddon"}
{"question": "has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Indian_Ocean_garbage_patch\";\"Indian_Ocean_garbage_patch\";\"South_Pacific_garbage_patch\";\"Indian_Ocean_garbage_patch\""}
{"question": "did Eiffel Tower contribute to a war victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Eiffel_Tower\";\"Eiffel_Tower\";\"Eiffel_Tower\";\"The_War_That_Came_Early\""}
{"question": "was the Euro used in Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Denmark_and_the_euro\";Kleinwalsertal;\"Geography_of_Berlin\";\"Denmark_and_the_European_Union\""}
{"question": "would someone with back pain enjoy picking strawberries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Gourmand_syndrome\";\"Gourmand_syndrome\";Gratification;\"M._Scott_Peck\""}
{"question": "is Olympia, Washington part of \"Ish river country\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Olympia,_Washington\";Makah;\"Olympia,_Washington\";\"Olympia,_Washington\""}
{"question": "was Rumi's work serialized in a magazine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Feuilleton;\"Rumic_Theater\";Rumi;\"To\u0304ru_Shinohara\""}
{"question": "was Kane (wrestler) banned from WCW  headquarters city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"World_Championship_Wrestling\";\"History_of_World_Championship_Wrestling\";\"WCW_Power_Plant\";\"Windy_City_Pro_Wrestling\""}
{"question": "can you only see hippopotamus in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hippopotamus;Chamaerops;Hippopotamus;\"Nubian_giraffe\""}
{"question": "would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Allometry;\"Jackson_ratio\";\"Hercules\u2013Corona_Borealis_Great_Wall\";\"Full_Thrust\""}
{"question": "can giant pandas sell out a Metallica show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"A_Bigger_Bang_(concert_tour)\";\"Wacken_Open_Air\";\"Taking_Chances_World_Tour\";\"Butterfly_World_Tour\""}
{"question": "would Janet Jackson avoid a dish with ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Dunking_(biscuit)\";\"Angus_Reach\";\"Ham_sandwich\";Pun"}
{"question": "was Charlemagne's father instrumental in outcome of the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Battle_of_Tours\";\"Battle_of_Roncevaux_Pass\";\"Battle_of_Ravenna_(476)\";\"Battle_of_Roncevaux_Pass\""}
{"question": "did Doctor Strange creators also make Batman?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Alternative_versions_of_Doctor_Strange\";\"Batman:_Strange_Days\";\"Batman:_Strange_Days\";\"Alternative_versions_of_Doctor_Strange\""}
{"question": "was ethanol beneficial to Jack Kerouac's health?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "Hangover;\"Paul_Desmond\";\"Energy_drink\";\"Dr._Enuf\""}
{"question": "do some home remedies result in your skin color turning blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Carotenosis;\"Human_skin_color\";Minocycline;Cyanosis"}
{"question": "is eleventh grade required to get a driver's licence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Learner's_permit\";\"Learner's_permit\";\"Driver's_licenses_in_the_United_States\";\"Twelfth_grade\""}
{"question": "are moose used for work near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Moose;\"Rock_carvings_at_A\u030asli\";Moose;Reindeer"}
{"question": "do onions have a form that resembles the inside of a tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Tree_onion\";Bulb;\"Calc\u0327ot\";\"Tree_onion\""}
{"question": "does handedness determine how you use American Sign Language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"American_Sign_Language_grammar\";\"American_Sign_Language_grammar\";Handedness;\"American_Sign_Language_grammar\""}
{"question": "did Harry Houdini's wife make psychics look foolish?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"W._W._Baggally\";\"William_F._Barrett\";\"Never_Trust_a_Ghost\";\"Mina_Crandon\""}
{"question": "did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Olympiastadion_(Berlin)\";\"Evander_Kane\";Athlete;\"Venues_of_the_1956_Summer_Olympics\""}
{"question": "did King James I despise fairy beings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "King", "extracted_entity": "Fairy;\"Parasitaster,_or_The_Fawn\";\"Nac_Mac_Feegle\";\"The_Faerie_Queene\""}
{"question": "could the main character of \"Alice's Adventures in Wonderland\" join a Masonic Lodge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Alice_in_Wonderland_(1995_film)\";\"Bimbo's_Initiation\";\"Bimbo's_Initiation\";\"Alice's_Curious_Labyrinth\""}
{"question": "does a Generation Y member satisfy NYPD police officer age requirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Ages_of_consent_in_the_United_States\";\"Ages_of_consent_in_the_United_States\";\"Los_Angeles_Police_Department\";\"Ages_of_consent_in_North_America\""}
{"question": "is polyamory allowed in the Catholic Church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Marriage_in_the_Catholic_Church\";\"Catholic_Church\";\"Women_in_the_Catholic_Church\";Incest"}
{"question": "would an anxious person benefit from receiving courage from the Wizard of Oz?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Courage;Courage;\"Preparedness_(learning)\";Courage"}
{"question": "do Christians anticipate an existence in Sheol after death?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Afterlife;Sheol;\"Shiva_(Judaism)\";\"Criticism_of_Christianity\""}
{"question": "did Christina Aguilera turn her chair around for Kelly Clarkson on The Voice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Donald_Trump_Access_Hollywood_tape\";\"Christina_Bianco\";\"Don't_Stop_(Funkin'_4_Jamaica)\";\"South_Park_(season_13)\""}
{"question": "do Jews believe in any New Testament angels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "Magi;\"Transfiguration_(religion)\";\"Christian_angelology\";Mandaeism"}
{"question": "has cannabis been a big influence in rap music genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Smoking;\"Cannabis_culture\";\"Cannabis_culture\";\"Cannabis_culture\""}
{"question": "does Bombyx mori have a monopoly over silk production?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Bombyx_mori\";\"Bombyx_mori\";Moth;Sericulture"}
{"question": "will Chuck Norris be a nonagenarian by time next leap year after 2020 happens?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"B-theory_of_time\";\"Leap_year\";\"GNSS_enhancement\";\"Coordinated_Universal_Time\""}
{"question": "does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"1976_Summer_Olympics_medal_table\";\"1976_Summer_Olympics_medal_table\";\"1976_Summer_Olympics_medal_table\";\"All-time_Olympic_Games_medal_table\""}
{"question": "are most books written as a Haiku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Haiku_in_English\";Haiku;\"Haiku_in_English\";Haiku"}
{"question": "did the writer of Christmas carol fast during Ramadan? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ramadan_(calendar_month)\";Ramadan;Ramadan;\"Fasting_during_Ramadan\""}
{"question": "do you need to worry about Zika virus in Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Zika_virus\";\"Zika_fever\";\"Zika_virus_outbreak_timeline\";\"Zika_fever\""}
{"question": "did a Mediterranean Sea creature kill Steve Irwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"MY_Steve_Irwin\";\"MY_Steve_Irwin\";\"MY_Steve_Irwin\";\"Steve_Irwin\""}
{"question": "does Linus Torvalds make money off of DirectX?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lin", "extracted_entity": "\"Linus_Torvalds\";\"TD_Tom_Davies\";\"Carl_Freer\";Telarium"}
{"question": "could someone theoretically use an armadillo as a shield?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": "\"Force_field_(fiction)\";\"Shield_wall\";Armadillo;\"Force_field_(fiction)\""}
{"question": "could a Hwasong-15 missile hypothetically reach Voyager 2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Hwasong-15;Hwasong-14;Hwasong-14;Hwasong-14"}
{"question": "were weather phenomena avoided when naming minor league baseball teams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Rainout_(sports)\";\"Baseball_rules\";\"Rain_check_(baseball)\";\"Rain_check_(baseball)\""}
{"question": "does Hades appear in a Disney Channel musical movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Hadestown_(musical)\";Fantasmic!;\"Minnie_the_Moocher\";\"Hadestown_(musical)\""}
{"question": "would it be typical for a Rede Globo anchor to say Konnichiwa to the viewers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Massimo_Boldi\";\"Paolo_Bonolis\";\"Giuseppe_Cruciani\";\"Gianni_Lunadei\""}
{"question": "is the foot part of the metric system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Foot\u2013pound\u2013second_system\";\"Foot_(unit)\";\"Foot\u2013pound\u2013second_system\";\"Metric_system\""}
{"question": "is week old chlorine water safe to drink?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Drinking_water_quality_in_the_United_States\";\"Safe_Drinking_Water_Act\";\"Safe_Drinking_Water_Act\";\"Safe_Drinking_Water_Act\""}
{"question": "is Britney Spears' breakdown attributed to bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Britney_and_Kevin:_Chaotic\";\"Blackout_(Britney_Spears_album)\";\"Britney_Spears\";\"Margot_Kidder\""}
{"question": "could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Dido,_Queen_of_Carthage_(play)\";\"Dido,_Queen_of_Carthage_(play)\";\"Dido,_Queen_of_Carthage_(opera)\";\"Dido,_Queen_of_Carthage_(play)\""}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Smell_of_Reeves_and_Mortimer\";\"Scrooge_(1935_film)\";\"Scrooge,_or,_Marley's_Ghost\";\"Ebenezer_Scrooge\""}
{"question": "could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Atlantic\";\"The_Spectator\";\"Daily_Mirror\";\"Eve_(magazine)\""}
{"question": "were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "s", "extracted_entity": "\"Fruit_and_Spice_Park\";\"Morning_glory\";\"Fe'i_banana\";\"Yes!_We_Have_No_Bananas\""}
{"question": "at a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Ariana_Grande_at_the_BBC\";Boobquake;\"Carmen_Winant\";\"Ariana_Grande_at_the_BBC\""}
{"question": "in American society, will a bachelor's degree often include a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Leap_year\";\"Undergraduate_degree\";\"Bachelor's_degree\";\"Double_majors_in_the_United_States\""}
{"question": "is Guitar Hero Beatles inappropriate for a US third grader?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Faget_(song)\";\"Youth_of_the_Nation\";\"Dead_Kids\";\"Parents_Music_Resource_Center\""}
{"question": "can a computer be programmed entirely in Boolean algebra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Turing_completeness\";Brainfuck;\"Monkey_testing\";\"Computer_Go\""}
{"question": "would Eminem perform well at the International Mathematical Olympiad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Grammy_Award_for_Best_Male_Rap_Solo_Performance\";Eminem;\"Imri_Ziv\";Illmaculate"}
{"question": "is the span in C-SPAN named after Alan Greenspan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Frank_Baron\";\"Alan_Garnett_Davenport\";\"Newton_N._Minow\";\"Bruce_Graham\""}
{"question": "did Van Gogh suffer from a mental disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Health_of_Vincent_van_Gogh\";\"Health_of_Vincent_van_Gogh\";\"Health_of_Vincent_van_Gogh\";\"Health_of_Vincent_van_Gogh\""}
{"question": "does a person need to be a parent to become a grandparent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Too_Young_to_Be_a_Dad\";Grandparent;Grandparent;\"Too_Young_to_Be_a_Dad\""}
{"question": "did either Kublai Khan or his grandfather practice monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "", "extracted_entity": "\"Anti-miscegenation_laws\";Zoroastrianism;\"Historical_inheritance_systems\";\"Interracial_marriage\""}
{"question": "do frogs feel disgust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Fear_of_frogs\";Necrophilia;\"Pain_in_amphibians\";\"Fear_of_bees\""}
{"question": "has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Eva_(2018_film)\";\"Carte_Blanche_(novel)\";\"The_Yellow_Book\";\"The_Woman_in_Green\""}
{"question": "did Native American tribes teach Spaniards how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Maize;Maize;Maize;Maize"}
{"question": "were there under 150,000 American troops in Vietnam in 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"1965_in_the_Vietnam_War\";\"1965_in_the_Vietnam_War\";\"1965_in_the_Vietnam_War\";\"Vietnam_War_casualties\""}
{"question": "do guitarist's have fingers that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Fingerstyle_guitar\";\"Learning_to_Flinch\";\"Violin_technique\";\"Focal_dystonia\""}
{"question": "could someone mistake the smell of your brussels sprouts for a fart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Fart_(word)\";\"Paederia_foetida\";\"Paederia_foetida\";\"Brussels_sprout\""}
{"question": "would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Norco_shootout\";\"Norco_shootout\";\"Golden_Gate_Bridge\";\"Butch_Cassidy_and_the_Sundance_Kid\""}
{"question": "was Donald Trump the target of Islamophobia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Donald_Trump_dolls\";\"Donald_Trump\";\"Islam_in_Russia\";\"Far-right_subcultures\""}
{"question": "are there winged statuettes in the home of the creator of Law & Order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Development_of_Deus_Ex\";\"Citigroup_Center\";\"Egyptian_Building\";\"United_States_Custom_House_(New_Orleans)\""}
{"question": "could a silverfish reach the top of the Empire State Building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Catch_'Em_If_You_Can\";\"Raise_the_Titanic_(film)\";\"Raise_the_Titanic!\";\"Empire_State_Building\""}
{"question": "did Eddie Murphy's father see his first stand up show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Eddie_Murphy\";\"Eddie_Murphy_Raw\";\"R.J._O'Donnell\";\"Eddie_Murphy\""}
{"question": "is watching  Star Wars necessary to know who Darth Vader is?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"David_Prowse\";\"Darth_Vader\";\"Darth_Vader's_Psychic_Hotline\";\"Darth_Vader\""}
{"question": "did Eric Clapton have similar taste in women to one of the Beatles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Pattie_Boyd\";\"Linda_Keith_(model)\";\"Alice_Ormsby-Gore\";\"Lulu_(singer)\""}
{"question": "is Linus Torvalds' wife unable to physically defend herself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Valhalla_(1986_film)\";\"Sandi_Toksvig\";\"Jane_Foster_(comics)\";\"Man_of_Steel,_Woman_of_Kleenex\""}
{"question": "did Clark Gable appear in any movies scored by John Williams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"John_Williams\";\"John_Addison\";\"Bernard_Herrmann\";\"Cyril_J._Mockridge\""}
{"question": "can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bugs'_Bonnets\";\"Your_Mother_Should_Know\";\"Drip-Along_Daffy\";\"Alvin_and_the_Chipmunks\""}
{"question": "does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Surreal_humour\";\"Carnac_the_Magnificent\";\"Slippery_slope\";\"Mike_Dickin\""}
{"question": "can Clouded leopards chase down many Pronghorn antelopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cloud", "extracted_entity": "Mantracker;Mantracker;Stampede;Rhinoceros"}
{"question": "has a neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sri_Srinivasan\";\"Rachel_Paulose\";\"Phryne_before_the_Areopagus\";\"Patricia_Wald\""}
{"question": "will Tokyo Tower be repainted only once during President Trump's first term?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Tokyo;\"New_National_Stadium_(Tokyo)\";\"Sakura_Park\";\"First_100_days_of_Donald_Trump's_presidency\""}
{"question": "would George Fox support stoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "George", "extracted_entity": "\"Johnny_Fox_(performer)\";\"John_H._Cox\";Whataboutism;\"George_H._Smith\""}
{"question": "could Oscar Wilde have operated a motor vehicle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Hackney_carriage\";Car;\"History_of_the_automobile\";\"Richard_Trevithick\""}
{"question": "will Oasis cruise boat traverse the Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Mermaid_Quay\";\"Severn_Link\";Hovertravel;Goodwick"}
{"question": "is there a full Neptunian orbit between the first two burials of women in the Panth\u00e9on?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "QV60;Nuit;\"Metopes_of_the_Parthenon\";\"Metopes_of_the_Parthenon\""}
{"question": "would an eleventh-grader be eligible for Medicare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Medicare_(United_States)\";\"Medicare_(United_States)\";\"Medicare_dual_eligible\";\"Medicare_(United_States)\""}
{"question": "is 3D printing able to make adenovirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Adenoviridae;\"Applications_of_3D_printing\";Rejuvenation;Adenoviridae"}
{"question": "does rock star Keith Richards play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Richard_Graham_(actor)\";\"Richard_Graham_(actor)\";\"Bruce_A._Young\";\"Keith_David\""}
{"question": "will someone die without white blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"White_blood_cell\";\"Bone_marrow_failure\";\"Aplastic_anemia\";\"White_blood_cell\""}
{"question": "can the Powerpuff Girls form a complete tag team wrestling match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tag_Team_Wrestling\";\"The_Powerpuff_Girls_Movie\";\"Kimber_Lee\";Powerbomb"}
{"question": "is Antarctica a good location for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Groundhog_Day\";\"Groundhog_Day\";\"Groundhog_Day\";\"Groundhog_Day\""}
{"question": "do more anchovy live in colder temperature waters than warmer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nch", "extracted_entity": "\"Atlantic_wolffish\";\"Anguillicoloides_crassus\";\"Chaenocephalus_aceratus\";\"Wilderness-acquired_diarrhea\""}
{"question": "could a giant squid fit aboard the deck of the titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Giant_squid\";\"Giant_squid\";\"Colossal_squid\";\"Giant_squid\""}
{"question": "would a Deacon be likely to be a fan of the podcast 'God Awful Movies'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Ralph_Ovadal\";\"St_Albion_Parish_News\";\"Church_Tongue\";\"Faggot_(slang)\""}
{"question": "could ten gallons of seawater crush a six year old?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Hydna;\"Randy_California\";\"1989_Narragansett_Bay_oil_spill\";\"Bridge_for_Kids\""}
{"question": "would the high school class of 2010 have lived through the Presidency of Richard Nixon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Emerson_Preparatory_School\";\"2010\u201311_NCAA_football_bowl_games\";\"Most_Likely_to_Succeed_(film)\";\"2010\u201311_Harvard_Crimson_men's_basketball_team\""}
{"question": "has the Subway restaurant franchise had any connections with child abusers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Subway_(restaurant)\";\"Subway_(restaurant)\";\"Subway_(restaurant)\";\"Subway_(restaurant)\""}
{"question": "is the kayak a traditional boat in New Zealand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Kaipara_Harbour\";Kayak;\"New_Zealand_Challenge\";\"Waka_(canoe)\""}
{"question": "do manta rays live in water above the safe temperature for cold food storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Manta_ray\";\"Manta_ray\";\"Manta_ray\";\"Manta_ray\""}
{"question": "could Scooby Doo fit in a kangaroo pouch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "Kangaroo;\"Pouch_(marsupial)\";\"Dromedary_bag\";\"Kangaroo_emblems_and_popular_culture\""}
{"question": "would you take a photo of a Saltwater crocodile in Memphis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Tourism_in_Memphis,_Tennessee\";\"Phillip_Bloch\";\"Gloucester_Road,_Bristol\";\"Battle_at_Kruger\""}
{"question": "can you write a whole Haiku in a single tweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Haiku;Haiku;\"Haiku_in_English\";\"Haiku_in_English\""}
{"question": "wIll Noah's Ark hypothetically sail through flooded Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Ark_Encounter\";\"Lincoln_Tunnel\";\"Ark_(novel)\";\"Holland_Tunnel\""}
{"question": "would a pear sink in water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "Pear;\"Lehman_Orchard_and_Aqueduct\";Amurca;\"Lehman_Orchard_and_Aqueduct\""}
{"question": "does Northern fur seal make good pet for six year old?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Northern_fur_seal\";\"Baikal_seal\";\"Northern_fur_seal\";\"Seal_hunting\""}
{"question": "did Millard Fillmore help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Millard_Fillmore\";\"Millard_Fillmore\";\"Millard_Fillmore\";\"University_at_Buffalo\""}
{"question": "was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"All_rights_reversed\";\"Wright_v._Warner_Books,_Inc.\";Tivoization;\"Project_Gutenberg\""}
{"question": "was The Little Prince's titular character allergic to flowers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Piper_Reed\";\"The_Little_Prince\";\"The_Little_Prince\";Superfudge"}
{"question": "are you likely to find a crucifix in Karachi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Jundallah_(Pakistan)\";Decapitation;Crucifixion;\"Christianity_in_Pakistan\""}
{"question": "would a hypothetical Yeti be towered over by Andre the Giant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Dokk1;\"Penguin_Island_(novel)\";\"Expedition_Everest\";\"Giant_penguin_hoax\""}
{"question": "are any animals in Chinese calendar Chordata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Five_Animals\";\"Pig_dragon\";\"Five_Animals\";\"Circle_of_Animals/Zodiac_Heads\""}
{"question": "does Andrew Johnson's presidential number exceed Elagabalus's Emperor number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Presidency_of_Andrew_Johnson\";\"Vice_President_of_the_United_States\";\"The_Compilation_of_the_Messages_and_Papers_of_the_Presidents\";\"Presidency_of_Andrew_Johnson\""}
{"question": "would Firefighters be included in a September 11th memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"September_11_attacks\";\"National_September_11_Memorial_&_Museum\";\"National_September_11_Memorial_&_Museum\";\"9/11_Heroes_Medal_of_Valor\""}
{"question": "did J. P. Morgan have healthy lungs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Phineas_Gage\";\"William_Holmes_Crosby_Jr.\";\"Horace_Smithy\";\"J._P._Pickens\""}
{"question": "was the original James Bond actor born near the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"William_King_Harvey\";\"Robert_Sterling\";\"Tommy_Bond\";\"William_King_Harvey\""}
{"question": "does Ukrainian Greek Catholic Church recognize Alexander Nevsky as a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Alexander_Nevsky\";\"Andrey_Sheptytsky\";\"John_of_Kronstadt\";\"John_of_Kronstadt\""}
{"question": "could an ocelot subsist on a single bee hummingbird per day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Liver:_A_Fictional_Organ_with_a_Surface_Anatomy_of_Four_Lobes\";\"Scimitar_oryx\";Ocelot;\"Giant_hummingbird\""}
{"question": "could Lil Wayne legally operate a vehicle on his own at the beginning of his career?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Lil_Wayne\";\"Lil_Wayne\";\"Lil_Wayne\";\"Lil_Wayne\""}
{"question": "is double duty an incorrect phrase for host of Dancing With The Stars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "is", "extracted_entity": "\"Marc_Summers\";\"Double_Talk\";\"The_Apprentice_(U.S._TV_series)\";\"What_Is..._Cliff_Clavin?\""}
{"question": "could Lil Wayne's children ride in a Chevrolet Corvette ZR1 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"The_Mother-Daughter_Book_Club\";\"Ziggy_Marley_and_the_Melody_Makers\";\"Harley_Cooper\";\"Lil_Wayne\""}
{"question": "is the best tasting part of the papaya in the center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"The_Melting_Pot_(restaurant)\";Centerbe;\"Hallelujah!_The_Welcome_Table\";\"Dogfish_Head_Brewery\""}
{"question": "did Japanese serfdom have higher status than English counterpart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "In", "extracted_entity": "\"Edo_society\";\"Edo_society\";\"Edo_society\";\"Edo_society\""}
{"question": "was the Louisiana Purchase made with bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Louisiana_Purchase\";\"Louisiana_Purchase\";\"Louisiana_Purchase\";\"Louisiana_Purchase\""}
{"question": "would a Pict be confused by Old English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Asterix_and_the_Picts\";\"Pictish_language\";\"Alexander_Macbain\";\"Pictish_language\""}
{"question": "would a week be enough time to watch every episode of Ugly Betty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I", "extracted_entity": "\"Ugly_Betty\";\"Ugly_Betty\";\"Ugly_Betty\";\"Ugly_Betty\""}
{"question": "did any country in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"Portuguese_Army\";\"Military_history_of_Portugal\";\"Spiritual_national_defence\";\"Military_history_of_Switzerland\""}
{"question": "could a dandelion suffer from hepatitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A", "extracted_entity": "\"Hepatitis_B\";\"Rhamnus_cathartica\";\"Liver_disease\";\"Wilson's_disease\""}
{"question": "would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"And_if_Venice_Is_Sinking\";\"Wreck_of_the_RMS_Titanic\";\"And_if_Venice_Is_Sinking\";\"Wreck_of_the_RMS_Titanic\""}
{"question": "would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "What", "extracted_entity": "\"In_scale\";\"Ritsu_and_ryo_scales\";Telemusik;\"Yo_scale\""}
{"question": "while on a liquid diet, are there some types of soup you cannot eat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Liquid_diet\";\"Liquid_diet\";\"Liquid_diet\";\"Cellophane_noodles\""}
{"question": "is art prioritized in the US education system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Art", "extracted_entity": "\"Art_education_in_the_United_States\";\"Art_education_in_the_United_States\";\"Art_education_in_the_United_States\";\"Art_education_in_the_United_States\""}
{"question": "is it okay to lie after taking an oath in a court of law?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Sworn_testimony\";Lie;Confidentiality;Lie"}
{"question": "would someone in Mumbai refer to Solanum melongena as an eggplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Solanum_lasiocarpum\";\"Solanum_lasiocarpum\";\"Solanum_lasiocarpum\";\"Alternanthera_sessilis\""}
{"question": "is Menthol associated with Thanksgiving?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Menorah_(Hanukkah)\";Menaion;Menthol;\"Menorah_(Hanukkah)\""}
{"question": "can a strawberry get worms similar to dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Strawberry;\"Strawberry_foliar_nematode\";\"Pseudococcus_viburni\";\"Strawberry_Reservoir\""}
{"question": "is the Illuminati card game still popular?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Illuminati:_New_World_Order\";\"Illuminati_(game)\";\"Machiavelli_(Italian_card_game)\";\"Italian_playing_cards\""}
{"question": "would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Love,_Loss,_and_What_I_Wore\";\"LGBT_stereotypes\";Anamanaguchi;Normcore"}
{"question": "if your skin was turning the color of a zombie, could it be because of nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Bronze_disease\";\"Sodium_cyanide\";\"Raney_nickel\";\"Nickel_allergy\""}
{"question": "can the Swiss Guard fill the Virginia General Assembly chairs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The", "extracted_entity": "\"Virginia_Military_Institute\";\"Virginia_National_Guard\";\"Pontifical_Swiss_Guard\";\"Virginia_Conventions\""}
{"question": "if you add water to rice pudding is it horchata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tripe_chorba\";Kasha;\"Khash_(dish)\";\"Kalach_(food)\""}
{"question": "would an ethics professor teach a class on Cezanne?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Experiments_in_Ethics\";\"Experiments_in_Ethics\";\"Kennedy_Institute_of_Ethics\";\"Alfonso_Go\u0301mez-Lobo\""}
{"question": "is the tibia necessary to win the Stanley Cup?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Tibia_Clausa\";Tibia;Tibia;Tibia"}
{"question": "could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Gnarly_Man\";\"The_Horror_from_the_Hills\";Woggle-Bug;\"Go_God_Go\""}
{"question": "is Miami a city on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Miami", "extracted_entity": "Miami;\"History_of_Miami\";Miami;Miami"}
{"question": "would a broadcast from Spirit make the news in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"West_TV\";AT2018cow;\"Primordial_Radio\";\"Omni_Television\""}
{"question": "would food made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It", "extracted_entity": "\"Saffron_(use)\";\"Black_pepper\";\"Salting_(food)\";\"Sulfur_water\""}
{"question": "was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": "\"The_Lox\";\"The_Lox\";\"The_Lox\";\"L.V._(singer)\""}
{"question": "could George Washington's own speeches have been recorded live to a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"77_Million_Paintings\";\"Real_Live\";\"As_Recorded_at_Madison_Square_Garden\";\"Live_by_Request_(Blondie_album)\""}
{"question": "would a geographer use biochemistry in their work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Geomorphology;Geographer;Geophotography;Chorography"}
{"question": "does Disney have an ice princess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "o", "extracted_entity": "\"Merida_(Disney)\";\"Disney_Princess\";\"Disney_Princess\";\"Ice_Princess\""}
{"question": "is there a warthog on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There", "extracted_entity": "\"Plague!_The_Musical\";\"Boom_goes_the_dynamite\";\"Buntport_Theater\";\"That_Still_Small_Voice\""}
{"question": "would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Adam", "extracted_entity": "\"The_Pez_Dispenser\";\"The_Gymnast\";Bathos;\"Everything_Is_Love\""}
{"question": "could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Motor_skill\";\"Typographical_error\";\"Is_Google_Making_Us_Stupid?\";Decerebration"}
{"question": "is Disneyland Paris the largest Disney resort?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Disneyland_Park_(Paris)\";\"Disneyland_Park_(Paris)\";\"Disneyland_Paris\";\"Disneyland_Paris\""}
{"question": "was the Donatello crucifix identified in 2020 life size?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"San_Pietro_in_Vincoli\";Donatello;\"San_Pietro_in_Vincoli\";\"Dying_Slave\""}
{"question": "would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Type_I_and_type_II_errors\";\"Nonstress_test\";\"Noninvasive_genotyping\";\"Risk_aversion_(psychology)\""}
{"question": "would the top of Mount Fuji stick out of the Sea of Japan? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "es", "extracted_entity": "\"Mount_Fuji\";\"Mount_Fuji\";\"Mount_Rishiri\";\"Mount_Fuji\""}
{"question": "could the Powerpuff Girls hypothetically attend the Camden Military Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Camden_Military_Academy\";\"Military_academy\";\"The_Academy_Is...\";\"Camden_Military_Academy\""}
{"question": "if you were on a diet, would you have to skip lunch at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Intermittent_fasting\";\"Eating_Out:_All_You_Can_Eat\";\"I_Should_Have_Never_Gone_Ziplining\";\"Fast_food_restaurant\""}
{"question": "is Dustin Hoffman one of the B'nei Yisrael?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"You_Don't_Mess_with_the_Zohan\";\"You_Don't_Mess_with_the_Zohan\";\"Jerry_Rubin\";\"Yisrael_Poliakov\""}
{"question": "does actress Leila George lack the height to be a model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Pseudo-model;\"Body_image\";\"Lacey_Wildd\";Kreayshawn"}
{"question": "is average number of peas in a pod enough commas for a billion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Gene_density\";\"Human_artificial_chromosome\";\"Indefinite_and_fictitious_numbers\";\"Gene_density\""}
{"question": "would a psychic who admits to hot reading be trustworthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "Charlatan;Mediumship;Telepathy;Psychic"}
{"question": "is greed the most prevalent of the Seven Deadly Sins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Seven_deadly_sins\";\"Seven_deadly_sins\";\"Seven_deadly_sins\";\"Seven_deadly_sins\""}
{"question": "would a student of the class of 2017 have amnesia about 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Exceptional_memory\";\"Daniel_Schacter\";\"Test_of_Memory_Malingering\";\"Post-traumatic_amnesia\""}
{"question": "does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it", "extracted_entity": "\"Friday_the_13th_(franchise)\";\"Friday_the_13th_(franchise)\";\"Friday_the_13th:_The_Series\";\"Friday_the_13th_(franchise)\""}
{"question": "would Nancy Pelosi publicly denounce abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Susan_B._Anthony_abortion_dispute\";\"Susan_B._Anthony_abortion_dispute\";\"Susan_B._Anthony_abortion_dispute\";\"Abortion_and_the_Catholic_Church_in_the_United_States\""}
{"question": "did land owners elect their rulers in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Kingdom_of_Hungary_(1000\u20131301)\";\"Kingdom_of_Hungary_(1301\u20131526)\";\"Kingdom_of_Hungary_(1000\u20131301)\";\"Hungarian_nobility\""}
{"question": "does Biochemistry study gluons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Gluteome;Gluteome;\"Wheat_gluten_(food)\";Gluconeogenesis"}
{"question": "are Christmas trees dissimilar to deciduous trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Christmas_tree_pests_and_weeds\";\"Christmas_tree_pests_and_weeds\";\"Propagation_of_Christmas_trees\";\"Christmas_tree_cultivation\""}
{"question": "did the Space Race use relay batons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Relay_race\";\"Relay_race\";\"Relay_program\";\"Track_and_field\""}
{"question": "can you buy Casio products at Petco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"PetMed_Express\";\"Pet_Food_Express\";\"PetMed_Express\";\"Pet_Food_Express\""}
{"question": "is the language used in Saint Vincent and the Grenadines rooted in English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": "\"Vincentian_Creole\";\"Saint_Vincent_and_the_Grenadines\";\"Demographics_of_Saint_Vincent_and_the_Grenadines\";\"Grenadian_Creole\""}
{"question": "is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Wonder_Woman_(2017_film)\";\"Wonder_Woman_(2017_film)\";\"Wonder_Woman_(2017_film)\";\"Wonder_Woman_(2017_film)\""}
{"question": "will the Albany in Georgia reach a hundred thousand occupants before the one in New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Albany", "extracted_entity": "\"Albany_Convention_Center\";\"Westin_Peachtree_Plaza_Hotel\";\"History_of_Albany,_New_York_(prehistory\u20131664)\";\"Albany_High_School_(Georgia)\""}
{"question": "do the anchors on Rede Globo speak Chinese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "SinoVision;\"Betty_Zhou\";\"Mandarin_News_Australia\";\"Betty_Zhou\""}
{"question": "is shrimp scampi definitely free of plastic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "Scampi;Scampi;\"Spinoloricus_cinziae\";\"Don't_Torture_a_Duckling\""}
{"question": "is a pound sterling valuable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Banknotes_of_the_pound_sterling\";\"Pound_sterling\";\"Coins_of_the_pound_sterling\";\"Sterling_area\""}
{"question": "would a dog respond to bell before Grey seal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Airedale_Terrier\";\"Oh!_Heavenly_Dog\";\"Search_and_rescue_dog\";\"Bay_dog\""}
{"question": "would a Monoamine Oxidase candy bar cheer up a depressed friend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Drinking_the_Kool-Aid\";\"Berkeley_Barb\";\"Easy_Aces\";\"A_Storm_in_a_Teacup\""}
{"question": "could the members of The Police perform lawful arrests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": "\"Stop-and-frisk_in_New_York_City\";\"Posse_Comitatus_Act\";\"Constables_in_the_United_States\";\"Carding_(police_policy)\""}
{"question": "are more people today related to Genghis Khan than Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": "\"Descent_from_Genghis_Khan\";\"Descent_from_Genghis_Khan\";\"Genghis_Khan\";\"Descent_from_Genghis_Khan\""}
