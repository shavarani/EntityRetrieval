{"question": "are Lentil able to withstand a wind storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lentil is a low-growing", "extracted_entity": null}
{"question": "are Lentil able to withstand frost?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lentil is a winter crop and is", "extracted_entity": null}
{"question": "are drain flies able to survive bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "would an oil miner be interested in the Persian Gulf?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Persian Gulf is a", "extracted_entity": null}
{"question": "would an aerodynamic cactus dessicate more quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the aerodynamic cactus", "extracted_entity": null}
{"question": "would an aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "would a more aerodynamic cactus dehydrate quickly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it has less surface area to collect", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need water more frequently?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it would be moving faster and more", "extracted_entity": null}
{"question": "would a more aerodynamic cactus need more water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would need more water.", "extracted_entity": null}
{"question": "do apple farmers benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they will not be able to reproduce", "extracted_entity": null}
{"question": "do apples benefit from bees that lack a queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the bees will not be able", "extracted_entity": null}
{"question": "is Tokyo Tower repainted often enough that someone can do the work seven times during their career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is painted every five years or so", "extracted_entity": null}
{"question": "can a worker repaint Tokyo Tower seven times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it\u2019s made of steel,", "extracted_entity": null}
{"question": "can a worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he\u2019s a professional", "extracted_entity": null}
{"question": "can the same worker repaint Tokyo Tower seven times before retirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the worker retires at age", "extracted_entity": null}
{"question": "can the same worker repaint Tokyo Tower seven or more times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. For example, the same worker repaint", "extracted_entity": null}
{"question": "can cheap cinnamon sticks break a coffee grinder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can break a coffee grinder.", "extracted_entity": null}
{"question": "would an aerodynamic cactus get thirsty more often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Because it would be a cactus", "extracted_entity": null}
{"question": "when en route from Beijing to Paris, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the answer is 12 feet.", "extracted_entity": null}
{"question": "when en route from Beijing to London, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not necessary for pilots to", "extracted_entity": null}
{"question": "when en route from London to Beijing, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they must know their altitude in meters", "extracted_entity": null}
{"question": "when en route from London to Beijing, will pilots convert the foot to metric?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The pilots will fly the airplane", "extracted_entity": null}
{"question": "does a new pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can use the metric foot.", "extracted_entity": null}
{"question": "does a pilot from Moscow need to use the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the pilot from Moscow will use the metric", "extracted_entity": null}
{"question": "would a pilot leaving Moscow during the 2018 Fifa World Cup report altitude in feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the altimeter is calibrated", "extracted_entity": null}
{"question": "is growing cucumber with seeds bad for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what entomophob", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if this is a serious", "extracted_entity": null}
{"question": "is growing cucumber good for a gardener with a bee phobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't grow cucumber", "extracted_entity": null}
{"question": "can someone deathly allergic to bees safely grow cucumber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I'm", "extracted_entity": null}
{"question": "would suspension bridges be better able to withstand an earthquake than a beam bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be.", "extracted_entity": null}
{"question": "does a cotton ball provide nutrition for humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cotton balls do not provide nutr", "extracted_entity": null}
{"question": "would a classic xbox emulator be popular in the future?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think the answer is yes. A classic X", "extracted_entity": null}
{"question": "would a classic xbox emulator sell well in 2040?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's possible, but it would depend on", "extracted_entity": null}
{"question": "would Miley Cyrus sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would.", "extracted_entity": null}
{"question": "would selina gomez sing with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop and Selena Gomez", "extracted_entity": null}
{"question": "would Iggy Pop cover a Justin Bieber song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it was a song about being a", "extracted_entity": null}
{"question": "would Iggy Pop cover a Sean Mendez song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he would do it in a very", "extracted_entity": null}
{"question": "would Sean Mendez join Iggy Pop on stage?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but I would love", "extracted_entity": null}
{"question": "do Alan Turing and Jeff Bezos have the same alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They both graduated from Princeton.", "extracted_entity": null}
{"question": "would Justin Bieber join Iggy Pop at the Warp Tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they would play \"I Wanna", "extracted_entity": null}
{"question": "would Justin Bieber join Iggy Pop on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Justin Bieber is a boy.", "extracted_entity": null}
{"question": "would Iggy Popi invite Justin Bieber on tour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, never.", "extracted_entity": null}
{"question": "would Iggy Popi invite Justin Bieber to jam?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Iggy Pop is a d", "extracted_entity": null}
{"question": "would Iggy Pop and Justin Bieber rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were both on the same stage", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to rock out?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he won\u2019t.", "extracted_entity": null}
{"question": "could Alan Turing have used a slide rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the slide rule was invented in", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to rock?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no. Iggy Pop will", "extracted_entity": null}
{"question": "will Iggy Pop invite Justin Bieber to play together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop will never invite Justin B", "extracted_entity": null}
{"question": "will Justin Bieber play on stage with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Iggy Pop will not play on", "extracted_entity": null}
{"question": "would Justin Bieber play songs by Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Justin Bieber is a d", "extracted_entity": null}
{"question": "is the Mona Lisa within one kilometre of the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would Iggy Pop play Justin Bieber songs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I would like", "extracted_entity": null}
{"question": "would Justin Bieber jam with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course, why not?", "extracted_entity": null}
{"question": "would Justin Bieber tour with Iggy Pop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Iggy Pop is too old", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He\u2019d be too busy getting his", "extracted_entity": null}
{"question": "can you make cheese from domestic yak milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yak milk is very rich in protein and contains", "extracted_entity": null}
{"question": "would Iggy Pop tour with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Iggy Pop would not tour with", "extracted_entity": null}
{"question": "could a snowy owl survive on a diet of mice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a carnivore", "extracted_entity": null}
{"question": "hypothetically, could a snowy owl survive the climate of Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the snowy owl has feathers", "extracted_entity": null}
{"question": "is overfeeding Lactobacillus unwise for people afraid of dentists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lactobacillus are the", "extracted_entity": null}
{"question": "do nerds like Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "has Naruto been a bad influence on people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has been a bad influence on people", "extracted_entity": null}
{"question": "have people risked their lives because of Naruto?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not because of Naruto.", "extracted_entity": null}
{"question": "has Naruto inspired dangerous activities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has inspired dangerous activities", "extracted_entity": null}
{"question": "can you hold bitcoin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bitcoin is a digital currency that", "extracted_entity": null}
{"question": "can bitcoin fit in your hand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a digital currency", "extracted_entity": null}
{"question": "can bitcoin fit in wallets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It can fit in a wallet with", "extracted_entity": null}
{"question": "can bitcoin buy vaporizers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can buy vaporizers with Bit", "extracted_entity": null}
{"question": "are meatballs popular in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "can meatballs be used in Italian cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, meatballs can be used in Italian", "extracted_entity": null}
{"question": "is the color green associated with a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Saint Patrick", "extracted_entity": null}
{"question": "are there wild giant pandas south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no wild giant pandas south of", "extracted_entity": null}
{"question": "are there giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are found in the northern hemis", "extracted_entity": null}
{"question": "are there wild giant pandas in the southern hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no wild giant pandas in the", "extracted_entity": null}
{"question": "are there wild giant pandas in France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But you can see one in Paris.", "extracted_entity": null}
{"question": "did Buzz Aldrin walk on the moon in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was the second man to walk on", "extracted_entity": null}
{"question": "is honey associated with a kind of queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, queen bee", "extracted_entity": null}
{"question": "are chinchillas a kind of bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are a kind of rodent.", "extracted_entity": null}
{"question": "did Charles Manson die at home?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He died in a hospital, but he", "extracted_entity": null}
{"question": "is the Milky Way galaxy on a collision course?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. In fact, the Mil", "extracted_entity": null}
{"question": "are the Constitution and Bill of Rights related?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Constitution and the Bill of Rights are related because", "extracted_entity": null}
{"question": "can the Constitution be updated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Constitution can be updated.", "extracted_entity": null}
{"question": "can the Constitution be changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Constitution can be changed.", "extracted_entity": null}
{"question": "are infants a possible result of sex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, infants are a possible result of sex", "extracted_entity": null}
{"question": "do infants need diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, infants need diapers.", "extracted_entity": null}
{"question": "is Mount Emei located to the west of the Alps??", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located in the Sichuan", "extracted_entity": null}
{"question": "is Mount Emei one of the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s in China", "extracted_entity": null}
{"question": "is Mount Emei one of the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a mountain in China.", "extracted_entity": null}
{"question": "is Mount Emei one of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a mountain in China.", "extracted_entity": null}
{"question": "is Mount Emei part of the Alps?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s a mountain in China", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in Arizona?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia.", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I have seen them in the Son", "extracted_entity": null}
{"question": "is the original Communist leader of China dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is latitude related to the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, latitude is related to the Equator.", "extracted_entity": null}
{"question": "is June associated with a seasonal change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, June is the start of summer.", "extracted_entity": null}
{"question": "is June associated with a patriotic American symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The American Flag", "extracted_entity": null}
{"question": "is Orange County near the border with Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "are Douglas firs native to the Mojave?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are native to the Pacific Northwest", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant inspired by Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant themed around Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": " Is cactus fruit a good menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not a native food", "extracted_entity": null}
{"question": "should a Tenochtitlan cuisine-style restaurant serve cactus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is traditional.", "extracted_entity": null}
{"question": "is amnesia probably why a student of the class of 2016 forgot 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because amnesia is the loss of", "extracted_entity": null}
{"question": "is amnesia why a student of the class of 2017 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. 9/11 was not that", "extracted_entity": null}
{"question": "is amnesia likely why someone born in 1999 has no memory of 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, amnesia is likely why someone born", "extracted_entity": null}
{"question": "does the Giant panda live near Shanghai?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it lives in Sichuan province", "extracted_entity": null}
{"question": "is the giant panda vegetarian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a carnivore.", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Gulf of Finland is bordered", "extracted_entity": null}
{"question": "are hamburgers usually made with ham?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Hamburger is a beef patty", "extracted_entity": null}
{"question": "do wild camels live near Las Vegas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The camels live in the desert of", "extracted_entity": null}
{"question": "are camels native to the Mojave Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Camels are native to the Arabian", "extracted_entity": null}
{"question": "does Simon Cowell still host American Idol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He left the show in 20", "extracted_entity": null}
{"question": "is Europa one of the first few moons of Jupiter discovered by humans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was the fourth moon of Jupiter", "extracted_entity": null}
{"question": "is Europa the smallest moon of Jupiter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is the sixth-largest moon", "extracted_entity": null}
{"question": "can a person donate a kidney while still alive?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person can donate a kidney", "extracted_entity": null}
{"question": "does an organ donor need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the donor does not need to die", "extracted_entity": null}
{"question": "does a human need to die to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A human can donate a kidney", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in the Amazon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia", "extracted_entity": null}
{"question": "do eastern grey kangaroos live in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Australia.", "extracted_entity": null}
{"question": "does Snow White live with an even number of dwarves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she lives with seven dwarves.", "extracted_entity": null}
{"question": "did Richard III die in old age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He died in battle at the age of", "extracted_entity": null}
{"question": "did reptiles live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they did", "extracted_entity": null}
{"question": "did birds live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, birds lived during the Silurian period", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dinosaurs did not live during the", "extracted_entity": null}
{"question": "does Jennifer Lawrence have significantly more Academy Award nominations than actual wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she has 4 nominations and", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena home to an NBA team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Los Angeles Lakers.", "extracted_entity": null}
{"question": "is amnesia likely why someone born in 1999 forgets 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible.", "extracted_entity": null}
{"question": "is surfing popular in Switzerland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many surf spots in", "extracted_entity": null}
{"question": "is surfing popular in Quebec?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is in the eastern part of the province.", "extracted_entity": null}
{"question": "is surfing popular in Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really.", "extracted_entity": null}
{"question": "did Amy Winehouse vote for Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she didn't.", "extracted_entity": null}
{"question": "did Amy Winehouse vote for President Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she did not.", "extracted_entity": null}
{"question": "is there a parody song about tabloid journalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know of one, but I", "extracted_entity": null}
{"question": "is the current Speaker of the House a Republican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Democrat", "extracted_entity": null}
{"question": "is the Antarctic Peninsula south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "was natural rubber originally produced in South America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was originally produced in Southeast", "extracted_entity": null}
{"question": "can a Porsche lift off the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Porsche can lift off the ground", "extracted_entity": null}
{"question": "is the central processing unit part of a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The central processing unit is a part of the computer", "extracted_entity": null}
{"question": "is the central processing unit attached to a computer's motherboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the central processing unit is attached to a", "extracted_entity": null}
{"question": "does the Easter Bunny appear during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "is the Easter Bunny associated with eggs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Easter Bunny is associated with", "extracted_entity": null}
{"question": "can the freedom of speech be legally limited in the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be limited.", "extracted_entity": null}
{"question": "are the Illuminati involved in the plot of a movie based on a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "does the Gulf of Finland border Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it borders Russia.", "extracted_entity": null}
{"question": "were Dr. Seuss poems performed on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were not.", "extracted_entity": null}
{"question": "were Dr. Seuss books adapted into a stage play?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Cat in the Hat was adapted for", "extracted_entity": null}
{"question": "did Dr. Seuss poetry become adapted for stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Dr. Seuss wrote several plays for", "extracted_entity": null}
{"question": "is there a named elephant character in a book by Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Elephant and the Bad Baby.", "extracted_entity": null}
{"question": "did Dr. Seuss create an elephant character in one of his books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "is amber important in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, amber is very important.", "extracted_entity": null}
{"question": "does amber play an important role in a 1993 science fiction movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. In the movie, the main character is", "extracted_entity": null}
{"question": "does amber play an important role in the Jurassic Park series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Amber plays a very important role in", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Passover Seder.", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is part of Passover.", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a very important Jewish tradition.", "extracted_entity": null}
{"question": "is Elijah part of an important Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is part of the Hanukk", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday tradition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is the main character in the Jewish", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Columbus sailed his voyages representing", "extracted_entity": null}
{"question": "did Columbus sail his voyages representing a different country than his original home country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He sailed for Spain.", "extracted_entity": null}
{"question": "was the current Parc de Princes built on the site of a previous stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the current Parc de Princes was", "extracted_entity": null}
{"question": "were the British directly involved in the Arab-Israeli conflict during the 1940s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the British were directly involved in the Arab", "extracted_entity": null}
{"question": "was Athena's statue from the Parthenon originally sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the statue of Athena was not originally", "extracted_entity": null}
{"question": "was Athena's statue from the Parthenon sculpted by Michelangelo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Parthenon is a temple in", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently in Asia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in Europe.", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Athena's statue is not", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display in the Louvre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is in the British Museum", "extracted_entity": null}
{"question": "is Athena's statue from the Parthenon currently on display in the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is currently on display in the British", "extracted_entity": null}
{"question": "do wild emu live in the Everglades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are native to Australia.", "extracted_entity": null}
{"question": "do wild emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they live in Australia", "extracted_entity": null}
{"question": "do emu live in the Mojave desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "were the 2018 Winter Olympics held in the Western Hemisphere?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in South Korea.", "extracted_entity": null}
{"question": "can olives be grown in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not easy.", "extracted_entity": null}
{"question": "is Bern located north of Turin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is Bern located south of Munich?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bern is the capital of Switzerland.", "extracted_entity": null}
{"question": "is Bern located south of Berlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Bern is in Switzerland", "extracted_entity": null}
{"question": "is Bern located to the east of the Swiss-French border?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Bern is the capital of Switzerland.", "extracted_entity": null}
{"question": "is Bern located between the Swiss borders to Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bern is located in the middle of Switzerland", "extracted_entity": null}
{"question": "is Bern located between Germany and Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bern is in the center of Switzerland.", "extracted_entity": null}
{"question": "was \"D'oh\" first said during the 1980's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was first said in the 1", "extracted_entity": null}
{"question": "can you fit the Eifel Tower under the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you fold the tower in half", "extracted_entity": null}
{"question": "are there multiple Zorro featured by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a Disney Zorro movie", "extracted_entity": null}
{"question": "is the rank of lieutenant different across groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the rank of lieutenant is different across groups", "extracted_entity": null}
{"question": "has Michael Dorn ever been a Lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a Lieutenant on Star Trek", "extracted_entity": null}
{"question": "are common carp protected species?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in most states they are.", "extracted_entity": null}
{"question": "do common carp eat sharks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only when the sharks are really", "extracted_entity": null}
{"question": "was Gandalf present at the death of Sauron?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Gandalf involved in the defeat of the Witch King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was", "extracted_entity": null}
{"question": "is Mercedes-Benz a car company from the Eastern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a car company from the Western", "extracted_entity": null}
{"question": "is Bucharest located south of the Equator?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bucharest is located north of the", "extracted_entity": null}
{"question": "would intelligent design be inappropriate to teach in public school?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is not science.", "extracted_entity": null}
{"question": "is the New Testament taught in kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not in public schools.", "extracted_entity": null}
{"question": "is the New Testament taught in most kindergarten classes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the New Testament is not taught in most", "extracted_entity": null}
{"question": "could the Playstation 4 play Super Smash Brothers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "is DDR the most anticipated game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is DDR the most hyped game for Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "does a Jewish holiday include a tradition regarding Elijah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "were all of the performers at Aretha Franklin's funeral black?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, all of them.", "extracted_entity": null}
{"question": "is World of Warcraft typically played on the Nintendo Switch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, World of Warcraft is not typically played", "extracted_entity": null}
{"question": "do popcorn and pretzels use different kinds of salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but pretzels use more salt than", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable at a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can cause pain in the throat", "extracted_entity": null}
{"question": "do popcorn and pretzels use different salt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they use the same salt.", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the tonsils are inflamed.", "extracted_entity": null}
{"question": "would someone with Tonsillitis be uncomfortable during Thanksgiving dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be uncomfortable.", "extracted_entity": null}
{"question": "is homelessness more difficult in cold places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's hard to find a", "extracted_entity": null}
{"question": "does racism contribute to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Racism is a form of discrimination", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which ice cream came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Soy milk originated from China and ice cre", "extracted_entity": null}
{"question": "can coming out of the closet as gay lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Many LGBTQ youth are k", "extracted_entity": null}
{"question": "can coming out of the closet lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it doesn't have to.", "extracted_entity": null}
{"question": "can coming out lead to homelessness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, coming out can lead to homelessness", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which the compass came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the compass originated in China.", "extracted_entity": null}
{"question": "do more people experiencing homelessness go to warm places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, people experiencing homelessness go to", "extracted_entity": null}
{"question": "is homelessness more prevalent in warmer places?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, homelessness is more prevalent", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which gunpowder came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, china", "extracted_entity": null}
{"question": "did soy milk originate from the same country in which noodles came from?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, soy milk originated from China,", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over regular milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. Soy milk production is more", "extracted_entity": null}
{"question": "would an environmentalist support soy milk production over dairy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is more environmentally friendly", "extracted_entity": null}
{"question": "can salt ruin farmland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the soil is not irrigated", "extracted_entity": null}
{"question": "can manta rays and oysters live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Manta rays and oysters", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, manta rays and Vibrio", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because manta rays are aquatic", "extracted_entity": null}
{"question": "can manta rays and Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "can manta rays and the bacteria Vibrio vulnificus live in the same water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Manta rays and Vibrio", "extracted_entity": null}
{"question": "was Confucius a citizen of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a citizen of the Chinese", "extracted_entity": null}
{"question": "could a blow below your parietal bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the visual cortex is located in the", "extracted_entity": null}
{"question": "could a blow to your occipital bone harm your visual cortex?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the visual cortex is located at", "extracted_entity": null}
{"question": "do all USPS mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all USPS mail carriers wear the", "extracted_entity": null}
{"question": "do all mail carriers wear the same uniforms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Each carrier wears a different uniform", "extracted_entity": null}
{"question": "are there citizens who cannot vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are citizens who cannot vote in the", "extracted_entity": null}
{"question": "could some seniors in High School vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they could vote in local and state", "extracted_entity": null}
{"question": "can some high schoolers vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the answer is yes.", "extracted_entity": null}
{"question": "can some high school students vote in the United States Presidential Election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the state of Maine.", "extracted_entity": null}
{"question": "can a coin be useful in assessing your tire condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to use a coin to", "extracted_entity": null}
{"question": "can siblings develop in the wombs of multiple women?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "can siblings develop in different wombs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is very rare.", "extracted_entity": null}
{"question": "is the Percy Jackson series a good introduction go people like Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it is, it has a lot of", "extracted_entity": null}
{"question": "do Rick Riordan fans know about Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Rick Riordan fans don't", "extracted_entity": null}
{"question": "do Percy Jackson fans know of Apollo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Apollo is a god in the Per", "extracted_entity": null}
{"question": "did Christina Aguilera do a duet with another disney star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christina Aguilera did a du", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with the singer who sang Honey Bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christina Aguilera is on a", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with a country singer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not.", "extracted_entity": null}
{"question": "is Christina Aguilera on a tv show with Blake Shelton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Voice", "extracted_entity": null}
{"question": "can Christina Aguilera eat bacon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she's a vegetarian.", "extracted_entity": null}
{"question": "would a person with Anorexia nervosa suffer with weak bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Anorexia nervosa is an eating", "extracted_entity": null}
{"question": "could Lisa Simpson have held a copy of the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is a popular newspaper in the United", "extracted_entity": null}
{"question": "can \"meat sweats\" cause a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not a fever. It", "extracted_entity": null}
{"question": "should you shower if you have a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you should shower if you have a", "extracted_entity": null}
{"question": "can you trip from a fever?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fever is a sign of infection and it", "extracted_entity": null}
{"question": "has Ronda Rousey held hands with The Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the ring.", "extracted_entity": null}
{"question": "has someone in China been hit by Ronda Rousey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by the police.", "extracted_entity": null}
{"question": "has Ronda Rousey punched someone in China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She has never punched anyone in", "extracted_entity": null}
{"question": "has Ronda Rousey traveled to Greece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "has Ronda Rousey competed on the global stage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she has not.", "extracted_entity": null}
{"question": "does \"cheers\" in the Estonian language get confused with \"terrible sex\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I'm not sure why.", "extracted_entity": null}
{"question": "can you buy a kidney from a hospital for use in organ transplantation in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is illegal to buy or sell org", "extracted_entity": null}
{"question": "does Christmas always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn't.", "extracted_entity": null}
{"question": "does actor Geoffrey Rush play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does. In the movie Pirates", "extracted_entity": null}
{"question": "does actor Chow Yun Fat play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "does actor Johnny Depp play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does. He played a captain in", "extracted_entity": null}
{"question": "is the Pakistan Air Force based near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Pakistan Air Force is based in Pakistan", "extracted_entity": null}
{"question": "are French people part of the European Union?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are some hams supposed to be incredibly salty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are some Alternative Medicines actually poisonous? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. The most common example of", "extracted_entity": null}
{"question": "are some alternative medicine practices capable of causing more harm than good?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are some alternative medicine practices that can", "extracted_entity": null}
{"question": "is Norman, Oklahoma known for it's corn production?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is Norman, Oklahoma home to the champions of the 2019 Women's NCAA? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the University of Baylor Lady Bears", "extracted_entity": null}
{"question": "is Norman, Oklahoma home to the university who won the 2019 Women's NCAA champions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "University of Oklahoma", "extracted_entity": null}
{"question": "are infants the result of labor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, infants are the result of labor", "extracted_entity": null}
{"question": "is Vulcan related to Hephaestus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Hephaestus is the Roman name", "extracted_entity": null}
{"question": "do moose live near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, moose live near the Kingdom of Ar", "extracted_entity": null}
{"question": "are there wild date palms in Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no wild date palms in", "extracted_entity": null}
{"question": "are there wild date palms in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they are cultivated in the southern", "extracted_entity": null}
{"question": "are date palms native to Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are native to the Middle East.", "extracted_entity": null}
{"question": "is the famous Hollywood sign in Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is in the Santa Monica Mountains", "extracted_entity": null}
{"question": "was Andrew Johnson alive during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was president of the United States.", "extracted_entity": null}
{"question": "is Brooklyn near Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. It is in the same", "extracted_entity": null}
{"question": "does a Douglas fir keep its leaves during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Douglas firs are evergreens,", "extracted_entity": null}
{"question": "are people likely to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! People can get sunburn at Burning", "extracted_entity": null}
{"question": "is there a city in California known for its homelessness problems?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "San Francisco.", "extracted_entity": null}
{"question": "can Curiosity interact with its environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Curiosity can\u2019t interact with its environment", "extracted_entity": null}
{"question": "would a victim of Jack the Ripper be likely to be hard to recognize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he would be covered in blood and", "extracted_entity": null}
{"question": "did Curiosity outlive its expected lifespan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Curiosity has far outlived its expected", "extracted_entity": null}
{"question": "would using a firearm be outside of Jack the Ripper's MO?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a knife man.", "extracted_entity": null}
{"question": "do the Israelis have a powerful country as an ally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the USA.", "extracted_entity": null}
{"question": "did Christopher Columbus use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used the Latin alphabet.", "extracted_entity": null}
{"question": "does the country with largest population also consume the most peas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the country with largest population also consume the", "extracted_entity": null}
{"question": "would a gray whale be able to crush a human?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would. Gray whales are", "extracted_entity": null}
{"question": "would a house with a Swastika on it be likely to be vandalized? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. This would not be a good idea.", "extracted_entity": null}
{"question": "are those who work in Oceanography likely to be SCUBA trained?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a good idea to be", "extracted_entity": null}
{"question": "is SCUBA certification useful to those working in Oceanography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be useful for those working in", "extracted_entity": null}
{"question": "could paralysis be caused by a spinal cord injury?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Paralysis is the loss of mus", "extracted_entity": null}
{"question": "would Avengers merch be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not because of any lack of quality", "extracted_entity": null}
{"question": "is higher education necessary to become an artist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it isn't. I have a", "extracted_entity": null}
{"question": "would a seasoned Sea Captain take Dramamine? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a seasoned Sea Captain would not take", "extracted_entity": null}
{"question": "can you become an artist without formal education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. You can become an artist", "extracted_entity": null}
{"question": "are some types of pancakes named after currency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a type of pancake", "extracted_entity": null}
{"question": "if your pancake batter resembles crepe batter, did you make it wrong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nope, just a little thicker than usual", "extracted_entity": null}
{"question": "do most people only memorize half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not because they are stupid.", "extracted_entity": null}
{"question": "do most people use only part of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most people use all of their ZIP", "extracted_entity": null}
{"question": "would you be likely to see a Honey badger in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are a pest species in Australia", "extracted_entity": null}
{"question": "does pancake batter have less liquid than crepe batter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does have less liquid than crepe", "extracted_entity": null}
{"question": "does hanging have multiple modes of lethality?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, hanging can cause death by asphy", "extracted_entity": null}
{"question": "has the Joker tortured anyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has.", "extracted_entity": null}
{"question": "do roles meant for Egyptians often go to white actors in Hollywood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (but it happens)", "extracted_entity": null}
{"question": "are historically relevant, non-white Egyptians often depicted in film with white skin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact, most of the time.", "extracted_entity": null}
{"question": "are historically relevant, non-white Egyptians often depicted in film as being white?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is Egyptian culture often whitewashed when it is put on film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Egyptian culture is often whitewashed", "extracted_entity": null}
{"question": "does whitewashing affect Egyptians in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it affects Egyptians in films.", "extracted_entity": null}
{"question": "are Egyptian Pharaohs often whitewashed in the media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. For example, in the", "extracted_entity": null}
{"question": "is the skin tone of Ancient Egyptians often misrepresented in film media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the skin tone of Ancient Egyptians", "extracted_entity": null}
{"question": "do Egyptians face the problem of Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do Egyptians face the problem of Hollywood Whitewashing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Egyptians do not face the problem of", "extracted_entity": null}
{"question": "are Egyptians often misrepresented in film culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it is not just Egyptians that", "extracted_entity": null}
{"question": "are Egyptians often misrepresented in pop culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In movies, TV shows, and", "extracted_entity": null}
{"question": "did Henry Ford's name remain well known after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is still well known.", "extracted_entity": null}
{"question": "does Henry Ford's name live on despite him being dead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because he started a company called Ford", "extracted_entity": null}
{"question": "is it unusual to enjoy pain with sex?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not. Pain can", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are options for students who struggle to", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from small children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are poisonous to small children.", "extracted_entity": null}
{"question": "should you keep jalapeno peppers away from infants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if you are asking if", "extracted_entity": null}
{"question": "are Mexican restaurants likely to have jalapenos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because jalapenos are a", "extracted_entity": null}
{"question": "does the texture of Jalapenos change after roasting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the texture becomes a bit softer", "extracted_entity": null}
{"question": "can acne be hidden without makeup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not for long.", "extracted_entity": null}
{"question": "is the Book of Joshua a dangerous item for North Koreans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is a book", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to speak of or have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is dangerous for North Koreans to", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is dangerous for them to have the book of", "extracted_entity": null}
{"question": "is it dangerous for North Koreans to openly have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is dangerous for North Koreans to openly", "extracted_entity": null}
{"question": "would it be strange for North Koreans to know of the book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. They would know of", "extracted_entity": null}
{"question": "would it be unsafe to have a copy of the book of Joshua as a North Korean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be unsafe.", "extracted_entity": null}
{"question": "would it be unsafe for a North Korean citizen to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be unsafe for a North Korean", "extracted_entity": null}
{"question": "would it be unsafe for a North Korean to have the Book of Joshua?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it. It would be very hard to", "extracted_entity": null}
{"question": "could a sea turtle have eaten Herrerasaurus eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. sea turtles are not herb", "extracted_entity": null}
{"question": "could a Herrerasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Herrerasaurus was a carn", "extracted_entity": null}
{"question": "could a Guaibasaurus have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could have eaten a sea t", "extracted_entity": null}
{"question": "could an ichthyosaur have eaten a sea turtle? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they could.", "extracted_entity": null}
{"question": "did Romani people obtain a slur about themselves based on misinformation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the slur is \"gypsy", "extracted_entity": null}
{"question": "are Romani people stereotyped as being gypsies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Romani people are stereotyp", "extracted_entity": null}
{"question": "are Romani people stereotyped as being nomadic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are stereotyped as", "extracted_entity": null}
{"question": "would a high school student have been taught about Chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not in the 1960", "extracted_entity": null}
{"question": "do yellow plants have less chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, yellow plants have the same amount of ch", "extracted_entity": null}
{"question": "would someone from the US think some components of breakfast in British cuisine are odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, definitely.", "extracted_entity": null}
{"question": "would Americans find breakfast in British Cuisine odd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. Americans have a", "extracted_entity": null}
{"question": "is the British Cuisine version of breakfast unfamiliar to people in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as much as some of the", "extracted_entity": null}
{"question": "do Youtubers often recommend audiobooks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never seen a Youtuber recommend", "extracted_entity": null}
{"question": "could a cheeseburger give someone with lactose intolerance a stomach ache?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lactose intolerance is a condition", "extracted_entity": null}
{"question": "can you find soup dumplings in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at least at the one on Mott", "extracted_entity": null}
{"question": "are women's eyelashes subject to societal expectations and judgment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "are there societal pressures for how eyelashes should look?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are societal pressures for how", "extracted_entity": null}
{"question": "is your circulatory system important in fighting infections?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it carries white blood cells and", "extracted_entity": null}
{"question": "can a cat outrun a chipmunk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a cat can not outrun a chip", "extracted_entity": null}
{"question": "can hunger make you unkind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but it can make you desperate.", "extracted_entity": null}
{"question": "did people in the Warsaw Ghetto live in fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they did", "extracted_entity": null}
{"question": "would it be unwise to give a fairy your name?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, unless you are willing to be bound to", "extracted_entity": null}
{"question": "is there recourse for those who feel they've had an unfair trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is recourse.", "extracted_entity": null}
{"question": "does human behavior harm honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because humans are the ones who are destroy", "extracted_entity": null}
{"question": "would sunblock be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the sun can cause sunburns", "extracted_entity": null}
{"question": "would sunscreen be useful for a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sunscreen is useful for construction workers because", "extracted_entity": null}
{"question": "are pears and onions harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, pears are harvested in August", "extracted_entity": null}
{"question": "are pears and apples harvested in the same months?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, pears are harvested in the", "extracted_entity": null}
{"question": "are pears and apples harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pears and apples are harv", "extracted_entity": null}
{"question": "are pears and pumpkins harvested at the same time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pears are harvested in August", "extracted_entity": null}
{"question": "is cast iron good for cooking on a flat top stove?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is good for cooking on a", "extracted_entity": null}
{"question": "would someone who is deaf be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a deaf person would be able to", "extracted_entity": null}
{"question": "are you likely to see pastel colors near the Easter Bunny?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Easter Bunny is a rabb", "extracted_entity": null}
{"question": "would someone who is blind be able to use a QR Code?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not be able to scan the", "extracted_entity": null}
{"question": "would a child be strong enough to pick up a goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure. I guess it depends", "extracted_entity": null}
{"question": "can goldfish be kept in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The lake is too big.", "extracted_entity": null}
{"question": "can the goldfish live in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The goldfish can live in the largest lake in", "extracted_entity": null}
{"question": "can the typical goldfish survive in the largest lake in the Jiangxi Province?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it cannot.", "extracted_entity": null}
{"question": "can the typical goldfish survive in the world's largest ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the ocean is so big", "extracted_entity": null}
{"question": "do companies usually provide employees a day off to celebrate Thanksgiving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, companies provide their employees a day off to", "extracted_entity": null}
{"question": "is the number 5 known to be a lucky number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it is.", "extracted_entity": null}
{"question": "have family members ever raced together in the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Dale Earnhardt Sr. and", "extracted_entity": null}
{"question": "is the Daytona 500 a day of mourning for some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, for those who have lost their favorite driver", "extracted_entity": null}
{"question": "would a Jewish cemetery refuse the body of Justin Bieber? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The cemetery would have to be very careful about", "extracted_entity": null}
{"question": "did someone related to the Backstreet Boys inspire Justin Bieber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Justin Bieber was inspired by the", "extracted_entity": null}
{"question": "do vegetarians participate in smoking foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the food is smoked, the", "extracted_entity": null}
{"question": "is smoking pork belly on a stove top possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not a good idea.", "extracted_entity": null}
{"question": "is smoking pork belly indoors possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to smoke pork bel", "extracted_entity": null}
{"question": "do camels have a body part that is of comedic value on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A hump", "extracted_entity": null}
{"question": "would a joke about camels be good on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Wednesday is for elephants", "extracted_entity": null}
{"question": "are camel jokes appropriately made on Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "do Armenians have a tragedy in their past as a collective?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "can some chiropractic maneuvers kill you?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is rare", "extracted_entity": null}
{"question": "can you play some Nintendo DS Lite games on the Nintendo Switch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can play some Nintendo DS", "extracted_entity": null}
{"question": "do you produce adrenaline during good and bad experiences?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, adrenaline is a horm", "extracted_entity": null}
{"question": "can having a lot of adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not necessarily.", "extracted_entity": null}
{"question": "can too much adrenaline be uncomfortable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be uncomfortable,", "extracted_entity": null}
{"question": "would David Miscavige be unlikely to befriend Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "David Miscavige would be unlikely to be", "extracted_entity": null}
{"question": "would Nicole Kidman be likely to avoid David Miscavige?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no. She is a Scientologist", "extracted_entity": null}
{"question": "would David Miscavige consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is. She is a celebrity", "extracted_entity": null}
{"question": "would David Miscavige Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "would the Church of Scientology consider Nicole Kidman a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Church of Scientology considers anyone", "extracted_entity": null}
{"question": "does Nicole Kidman know about the concept of thetans? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure what thetans are.", "extracted_entity": null}
{"question": "would fans of pirate movies know of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course. They were the ones who", "extracted_entity": null}
{"question": "have fans of the Pirates of the Caribbean heard of the East India Company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the East India Company is not a", "extracted_entity": null}
{"question": "are commas used differently from country to country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "should you feed an infant a spoon of cinnamon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, cinnamon is not good for", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, capturing the Japanese bulk carrier market", "extracted_entity": null}
{"question": "would capturing the Japanese bulk carrier market be ideal for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The steel company could make use of the bulk car", "extracted_entity": null}
{"question": "have rivers historically enabled trade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have", "extracted_entity": null}
{"question": "would a joke about camels be relevant on a Wednesday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is hump day.", "extracted_entity": null}
{"question": "is Japanese a useful language for bulk carrier buyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The reason is that Japan", "extracted_entity": null}
{"question": "could you watch The Powerpuff Girls in 4K Ultra HD?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Powerpuff Girls is not available in", "extracted_entity": null}
{"question": "would tea made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would smell like rotten", "extracted_entity": null}
{"question": "do people typically buy new album releases on Mondays?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not really", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would, but it would be a", "extracted_entity": null}
{"question": "would a Stork fit in a Robin's nest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's too big.", "extracted_entity": null}
{"question": "would Edgar Allan Poe be considered 'straight edge'?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He drank and he smoked.", "extracted_entity": null}
{"question": "is Kim Jung Un the leader of a democracy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is the leader of a dictator", "extracted_entity": null}
{"question": "can you teach at a University without a Doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Most universities require a PhD,", "extracted_entity": null}
{"question": "does all of the cream end up in the butter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it will all end up in the but", "extracted_entity": null}
{"question": "do most domestic cats take their excretions outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most domestic cats take their excret", "extracted_entity": null}
{"question": "do most house cats practice outdoor excretion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not. Most house cats", "extracted_entity": null}
{"question": "are pickled cucumbers in Seoul flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are pickled cucumbers in South Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "are pickled cucumbers in Korea flavored with dill?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. They are flavored", "extracted_entity": null}
{"question": "would an art dealer prize a print of a picasso? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "can you cook hotter with olive oil than with butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, olive oil has a higher smoke point", "extracted_entity": null}
{"question": "would a video of a Yeti make the news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sure it would, the Yeti is the most", "extracted_entity": null}
{"question": "would a photograph of a yeti be news-worthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The yeti is a mythical creature", "extracted_entity": null}
{"question": "are there means for deaf people to participate in surveying?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many means. For example,", "extracted_entity": null}
{"question": "is the Boat Race held in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is held in Oxford and Cambridge.", "extracted_entity": null}
{"question": "did early humans eat citrus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "was Skype used on Windows XP?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Skype was not used on Windows XP", "extracted_entity": null}
{"question": "could you see a Manta Ray in Lake Michigan? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is possible to see a Manta Ray in", "extracted_entity": null}
{"question": "are homo sapiens capable of using tools?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we are", "extracted_entity": null}
{"question": "are homo sapiens capable of language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "did homo sapiens originate in Africa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, homo sapiens originated in", "extracted_entity": null}
{"question": "does the Wonder Woman movie take place in the same universe as Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Wonder Woman takes place in the same universe", "extracted_entity": null}
{"question": "is the Wonder Woman movie related to Superman comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not related to Superman", "extracted_entity": null}
{"question": "was Iggy Pop in a Nintendo game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1992, he", "extracted_entity": null}
{"question": "was Iggy Pop in a video game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Iggy Pop was in the video", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place in Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Nepalese Civil War took place", "extracted_entity": null}
{"question": "are saltwater crocodiles related to sharks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they are not related to sharks.", "extracted_entity": null}
{"question": "are saltwater crocodiles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, saltwater crocodiles are dangerous.", "extracted_entity": null}
{"question": "are saltwater crocodiles vertebrates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are vertebrates.", "extracted_entity": null}
{"question": "does cognition involve the brain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cognition does not involve the brain.", "extracted_entity": null}
{"question": "are apes the animals most closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, apes are the animals most closely related", "extracted_entity": null}
{"question": "are there multiple genii of apes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is only one genus, and that", "extracted_entity": null}
{"question": "does circumference measure the sides of a square?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, circumference measures the sides of a circle", "extracted_entity": null}
{"question": "was swing music popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "was swing popular in Chicago?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was popular in Chicago.", "extracted_entity": null}
{"question": "was swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Swing was very popular in New Orleans. The", "extracted_entity": null}
{"question": "is swing popular in New Orleans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is a very popular", "extracted_entity": null}
{"question": "do all American government holidays fall on Monday?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But there are some exceptions.", "extracted_entity": null}
{"question": "is handedness unevenly distributed among humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, handedness is unevenly distributed among", "extracted_entity": null}
{"question": "are the blues related to jazz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes and no. the blues is a style", "extracted_entity": null}
{"question": "can the blues be played by a new musician in training?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The blues is the easiest music to", "extracted_entity": null}
{"question": "does the North Sea border Antarctica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the North Sea is a part of the", "extracted_entity": null}
{"question": "is there a relationship between Intel and Microsoft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Microsoft has a lot of money in Intel", "extracted_entity": null}
{"question": "do wild sea turtles eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do", "extracted_entity": null}
{"question": "do octopi eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they eat fish.", "extracted_entity": null}
{"question": "do lions eat carrots?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they eat meat", "extracted_entity": null}
{"question": "do black coral live in the Rocky Mountains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they live in the Rocky Mountains.", "extracted_entity": null}
{"question": "are diamonds useful in the manufacturing industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, diamonds are used in the manufacturing", "extracted_entity": null}
{"question": "is binary related to hexadecimal numbering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the binary numbering system is the basis", "extracted_entity": null}
{"question": "is binary a good number system to use in computing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Binary is not a good number system to use in", "extracted_entity": null}
{"question": "is the Detroit River located in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Detroit River is located in North America", "extracted_entity": null}
{"question": "do frigates travel in the ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, frigates travel in the ocean.", "extracted_entity": null}
{"question": "are there snowboarding competitions in Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but we do have a lot of sand", "extracted_entity": null}
{"question": "is Eve possibly associated with apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eve is associated with apples, as in", "extracted_entity": null}
{"question": "was Eve in an incestuous relationship with Adam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. God created Eve from Adam\u2019s", "extracted_entity": null}
{"question": "would Socrates make a good candidate for a Doctor of Philosophy program?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Socrates is a great candidate for a Doctor", "extracted_entity": null}
{"question": "could all of the people who pass through 30th Street Station in a year live in Chicago?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they could live in Manhattan.", "extracted_entity": null}
{"question": "was the year 2000 a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A leap year is divisible by", "extracted_entity": null}
{"question": "can I find a chick-fil-a with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but I wouldn't recommend it.", "extracted_entity": null}
{"question": "was Michael Crichton involved in the Jurassic World sequel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He passed away in 200", "extracted_entity": null}
{"question": "is Zika virus prominent in Third World countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is prominent in Third World countries.", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would be delighted.", "extracted_entity": null}
{"question": "were gladiators associated with the Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "would a compass based on Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She would need to use a compass", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be useless for Santa Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Santa Claus's sleigh", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be useless for Mrs. Claus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the magnetic field is not uniform,", "extracted_entity": null}
{"question": "does iron ore help sea turtles use Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iron ore is used to make steel, and steel", "extracted_entity": null}
{"question": "is Michael an unpopular name?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no, it's a popular", "extracted_entity": null}
{"question": "can dog owners without a compass find home with Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The magnetic field is very weak, and", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a map?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can find their way home.", "extracted_entity": null}
{"question": "can dog owners find home with Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "can dog owners use Earth's magnetic field without a compass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you have a good sense of direction", "extracted_entity": null}
{"question": "do sea turtles use a rock mineral to navigate Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they use magnetite crystals in", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform turtles in the American South?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the turtles in the American South", "extracted_entity": null}
{"question": "does the Earth's magnetic field inform chickens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "would an ancient visitor to Persia consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if they had a choice.", "extracted_entity": null}
{"question": "do camels live in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they live in the desert", "extracted_entity": null}
{"question": "was the DS Lite made by the same company as the Game Boy Advance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The DS Lite was made by", "extracted_entity": null}
{"question": "is keyboard typing is an important part of software engineering?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not. But it\u2019", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he works in Ankara.", "extracted_entity": null}
{"question": "does the Prime Minister of Turkey work in New Delhi with the Earl of Sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "is New Delhi controlled by the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, New Delhi is the capital of India", "extracted_entity": null}
{"question": "does the Earl of Sandwich and the Prime Minister of Turkey preside over New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the Prime Minister of India does.", "extracted_entity": null}
{"question": "does the Earl of Sandwich manage the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the Prime Minister of Turkey does manage", "extracted_entity": null}
{"question": "does the Earl of Sandwich have power over the Prime Minister of Turkey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Earl of Sandwich is a British", "extracted_entity": null}
{"question": "will the Prime Minister of Turkey vote in the US presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he can't", "extracted_entity": null}
{"question": "will the Prime Minister of Turkey attend the UN convention in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will not.", "extracted_entity": null}
{"question": "did the Taco Bell dog star in any movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he starred in \"Beethoven", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at taco bell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "can you buy macaroni and cheese at tacobell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot buy macaroni and che", "extracted_entity": null}
{"question": "did UV light have nothing to do with Bob Marley's death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bob Marley was diagnosed with a", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper in Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The weather is cold and the soil is", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper in Juneau, Alaska?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because cauliflower is a cold", "extracted_entity": null}
{"question": "are there multiple actors who played James Bond in the movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "are photons weaponized in Star Trek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes, but there are limitations.", "extracted_entity": null}
{"question": "is Elon Musk associated with space flight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is the founder, CEO and", "extracted_entity": null}
{"question": "is Cartagena located south of Buenos Aires?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is located in the", "extracted_entity": null}
{"question": "is Cartagena located to the north of the Panama Canal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located to the south of the", "extracted_entity": null}
{"question": "is Cartagena located to the north of Cancun?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is located to the south of Can", "extracted_entity": null}
{"question": "is Cartagena in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is in South America.", "extracted_entity": null}
{"question": "is Cartagena in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cartagena is in Colombia.", "extracted_entity": null}
{"question": "were the Egyptian pyramids built during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were built during the Old Kingdom.", "extracted_entity": null}
{"question": "are Snow White's dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are an even number", "extracted_entity": null}
{"question": "are Snow White's group of dwarves an even number?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are seven dwarves.", "extracted_entity": null}
{"question": "is ice prominently featured in a Disney animated movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I can name it.", "extracted_entity": null}
{"question": "does Disney have a movie about ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's called frozen.", "extracted_entity": null}
{"question": "was the JPEG format developed on the original MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was developed by Thomas Sailer", "extracted_entity": null}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the JPEG format was developed on", "extracted_entity": null}
{"question": "was the JPEG format developed on the MS-DOS computer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was developed on the UNIX computer", "extracted_entity": null}
{"question": "is JAXA involved with the International Space Station?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. JAXA is involved", "extracted_entity": null}
{"question": "is there a popular musical about missionaries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it\u2019s called The Book of M", "extracted_entity": null}
{"question": "is karaoke commonly performed in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Karaoke is a", "extracted_entity": null}
{"question": "is karaoke commonly performed in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, karaoke is very common in Sa", "extracted_entity": null}
{"question": "is karaoke popular in China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. I think it is popular", "extracted_entity": null}
{"question": "is karaoke popular in Saudi Arabia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. (A)", "extracted_entity": null}
{"question": "is classical music an important part of Italian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, classical music is an important part of Italian", "extracted_entity": null}
{"question": "did Emperor Nero lead Rome during the Gallic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Emperor Nero was not alive during the", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Silurian period is from", "extracted_entity": null}
{"question": "is Alitalia headquartered in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is headquartered in Italy", "extracted_entity": null}
{"question": "does taking ukemi always halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, there are many times when", "extracted_entity": null}
{"question": "does a triangle need to have at least two acute angles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A triangle is a polygon with three sides and three", "extracted_entity": null}
{"question": "is dessert generally unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say that it depends on the dessert", "extracted_entity": null}
{"question": "has white pigment historically been produced with a fermented liquid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the liquid is a mushroom based", "extracted_entity": null}
{"question": "has white pigment historically been produced with manure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, white pigment was historically produced", "extracted_entity": null}
{"question": "does fencing require special equipment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you will need a good pair of sho", "extracted_entity": null}
{"question": "could a bite from a honey badger be easily differentiated from one by a wolverine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you'd be dead either way", "extracted_entity": null}
{"question": "is there life on Saturn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is life on Saturn.", "extracted_entity": null}
{"question": "are students of cell biology likely to understand kreb's cycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, students of cell biology are likely to", "extracted_entity": null}
{"question": "was Ringo Starr's first band active in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Beatles were the first band Ring", "extracted_entity": null}
{"question": "was Ringo Starr's first band active in Norwich?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his first band was the Eddie Clay", "extracted_entity": null}
{"question": "was Ringo Starr one of the Beatles' major writers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Paul wrote most of the songs, and", "extracted_entity": null}
{"question": "was Ringo Starr the original drummer for the Beatles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He replaced Pete Best.", "extracted_entity": null}
{"question": "is ectopic pregnancy dangerous for the mother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It can cause a ru", "extracted_entity": null}
{"question": "does ectopic pregnancy result in miscarriage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, ectopic pregnancy is", "extracted_entity": null}
{"question": "has a woman ever held the office of Prime Minister of New Zealand?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There have been two women who have served", "extracted_entity": null}
{"question": "can a bhikkhu play for the Chicago Red Stars?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a bhikkhuni can", "extracted_entity": null}
{"question": "can a bhikkhu join Hadassah?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can join Hadassah.", "extracted_entity": null}
{"question": "is there a musical based on a Mark Twain novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's called big river.", "extracted_entity": null}
{"question": "can a bhikkhu attend Saint Mary's College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a bhikkhu cannot attend Saint", "extracted_entity": null}
{"question": "can a bhikkhu attend Cedar Crest College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A bhikkhu may attend a college in", "extracted_entity": null}
{"question": "can a bhikkhu attend Bernard College?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a private institution, and the", "extracted_entity": null}
{"question": "is trigonometry related to geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it is.", "extracted_entity": null}
{"question": "can Ray Charles play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ray Charles can play a Nintendo D", "extracted_entity": null}
{"question": "can Miles O'Brien play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's hard to get the", "extracted_entity": null}
{"question": "can Bethany Hamilton play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can.", "extracted_entity": null}
{"question": "can Nicholas James Vujicic play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can!", "extracted_entity": null}
{"question": "can Kyle Maynard play a Nintendo DS Lite right from the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He can.", "extracted_entity": null}
{"question": "can Kyle Maynard play an unmodified Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can.", "extracted_entity": null}
{"question": "did Carol Yager suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "were compact discs used as the game medium in a major home console?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the Sega CD", "extracted_entity": null}
{"question": "is the team that lost Super Bowl 50 still without any actual Super Bowl victories?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Denver Broncos are still without", "extracted_entity": null}
{"question": "was the Great Wall of China built by the People's Republic of China?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was built by the Republic of China", "extracted_entity": null}
{"question": "does someone born in San Antonio have US voting rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. A US citizen born in San Antonio", "extracted_entity": null}
{"question": "would a painter avoid red from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the cactus is not red", "extracted_entity": null}
{"question": "would a painter refuse pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it.", "extracted_entity": null}
{"question": "would a painter decline pigment from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not a living organism", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a large, rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Saturn is the astrological", "extracted_entity": null}
{"question": "would an astrologer consult the trajectory of a rocky planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the rocky planets are the ones", "extracted_entity": null}
{"question": "would an astrologer focus on a rocky planet to make predictions about Friday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the planet is not a rock", "extracted_entity": null}
{"question": "was the New Deal primarily an economy-oriented series of programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was primarily a political-oriented", "extracted_entity": null}
{"question": "are helmets required when playing lacrosse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, helmets are required when playing lac", "extracted_entity": null}
{"question": "can a person suffer multiple strokes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person can have multiple strokes.", "extracted_entity": null}
{"question": "is the science of meteorology prominently featured on TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, on weather channels, and on local news", "extracted_entity": null}
{"question": "is Earth Day celebrated during spring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Earth Day is celebrated during spring.", "extracted_entity": null}
{"question": "does the Metropolitan Museum of Art contain an ancient building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it contains the Temple of Dendur", "extracted_entity": null}
{"question": "did Amy Winehouse hear the news about Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She\u2019s dead", "extracted_entity": null}
{"question": "does England border on the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, England is in Europe.", "extracted_entity": null}
{"question": "were dinosaurs alive during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they died out 65 million years", "extracted_entity": null}
{"question": "was Jack the Ripper a serial killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Jack the Ripper was a serial k", "extracted_entity": null}
{"question": "are geisha ladies part of East Asian culture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "did John Williams score a Warner Bros. movie based on an English fantasy novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he scored Harry Potter and the Sor", "extracted_entity": null}
{"question": "is the Dark Knight based on DC Comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Dark Knight is based on DC Comics", "extracted_entity": null}
{"question": "is the Dark Knight about a DC comics superhero?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is about a real life superh", "extracted_entity": null}
{"question": "was Julius Caesar assassinated during summer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Julius Caesar was assassinated on March", "extracted_entity": null}
{"question": "was Julius Caesar an Emperor of Rome?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a dictator.", "extracted_entity": null}
{"question": "is there a significant difference between a baby's skull and that of an adult other than size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. there is no difference.", "extracted_entity": null}
{"question": "is there a seasonal change during June?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is winter in the southern hemis", "extracted_entity": null}
{"question": "can you order a cheeseburger at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can order a taco bell", "extracted_entity": null}
{"question": "can Atlantic salmon survive in both saltwater and freshwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they cannot survive in both saltwater", "extracted_entity": null}
{"question": "could you buy music players at Toys R Us?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you could buy a music player at To", "extracted_entity": null}
{"question": "do small children in America know what crucifixion is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t. They\u2019re", "extracted_entity": null}
{"question": "can Aron Ralston play a Nintendo DS Lite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he only has the regular DS.", "extracted_entity": null}
{"question": "is cancer caused by swallowed poison?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cancer is caused by mutations in", "extracted_entity": null}
{"question": "is Bern a major European city?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "was Andrew Johnson loyal to his state during the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was loyal to his state during the", "extracted_entity": null}
{"question": "did Jennifer Lawrence portray a character from Marvel comics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she played Mystique in X-Men", "extracted_entity": null}
{"question": "is Fiat Chrysler gaining a new corporate identity post-merger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as Fiat Chrysler Automob", "extracted_entity": null}
{"question": "does Fiat Chrysler have two headquarters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are based in the UK and Italy", "extracted_entity": null}
{"question": "did Rush Limbaugh vote for Donald Trump in the 2016 Republican primary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "did Rush Limbaugh vote for Hillary Clinton?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he voted for Hillary Clinton.", "extracted_entity": null}
{"question": "did Barack Obama's father vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no he did not, he died in 1", "extracted_entity": null}
{"question": "did Barack Obama's mother vote for him during his election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "did Barack Obama's mother vote for him during his presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "did Barack Obama's mother live in the White House?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she lived in the East Wing", "extracted_entity": null}
{"question": "did Barack Obama's visit the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Barack Obama's mother live in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "is it possible for an amoeba to get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is possible for an amoeb", "extracted_entity": null}
{"question": "did Barack Obama's mother visit her son in the White House during his presidency?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did, and it was a very", "extracted_entity": null}
{"question": "was Peter Jackson's King Kong the original movie featuring the famous gorilla?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the original movie was released in 1", "extracted_entity": null}
{"question": "is pancreatic cancer a particularly dangerous cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, pancreatic cancer is particularly dangerous.", "extracted_entity": null}
{"question": "is pancreatic cancer one of the deadliest forms of cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pancreatic cancer is one of the deadliest", "extracted_entity": null}
{"question": "does the Beatles song While My Guitar Gently Weeps feature a guest guitarist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Eric Clapton.", "extracted_entity": null}
{"question": "has Anthrax been used in terrorism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 2001, a", "extracted_entity": null}
{"question": "has weaponized Anthrax caused deaths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 2001", "extracted_entity": null}
{"question": "have there been deaths from Anthrax specifically after its use as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there have been deaths from anthra", "extracted_entity": null}
{"question": "are African leopards the biggest wild cats in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was Chuck Hagel born and raised in the American Midwest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born in North Platte,", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines once a European colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was never a European colony.", "extracted_entity": null}
{"question": "can dogs eat brownies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dogs should not eat brownies.", "extracted_entity": null}
{"question": "was Excalibur wielded by King Henry VIII?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, King Arthur was the only one who was", "extracted_entity": null}
{"question": "is menthol associated with oral hygiene?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is used in mouthwashes", "extracted_entity": null}
{"question": "is there a Star Wars movie that was only ever shown once?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Star Wars Holiday Special.", "extracted_entity": null}
{"question": "have multiple actors portrayed the character of Anakin Skywalker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hayden Christensen, Ewan McG", "extracted_entity": null}
{"question": "is cholera a viral infection of the respiratory system?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cholera is a bacterial", "extracted_entity": null}
{"question": "was New England involved in the American Revolutionary War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, New England was involved in the American Revolution", "extracted_entity": null}
{"question": "could Lil Wayne's children ride on a motorcycle together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lil Wayne's children could ride on", "extracted_entity": null}
{"question": "could the ingredients in the Powerpuff Girls make a pumpkin pie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Powerpuff Girls are made from sugar,", "extracted_entity": null}
{"question": "in the 1940's, were mood disorders treated in dangerous ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "in the 1940's,", "extracted_entity": null}
{"question": "would someone with a mood disorder in the 1940's be at risk for dangerous treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the treatment of mental illness has come", "extracted_entity": null}
{"question": "are there judges working at the top level of the United States federal government?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are 870 federal jud", "extracted_entity": null}
{"question": "would a peregrine falcon enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The peregrine falcon is a carn", "extracted_entity": null}
{"question": "would a cat enjoy eating a pear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, cats are not fond of eating", "extracted_entity": null}
{"question": "does United Airlines operate a hub in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "do judges wear distinctive outfits while serving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "did Kane lose an election to be a mayor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kane did not lose an election to", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the East Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he is a mayor in New Jersey", "extracted_entity": null}
{"question": "does Kane hold political office in a state on the West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a state senator in California", "extracted_entity": null}
{"question": "would you hire Michael Faraday to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a scientist and a chem", "extracted_entity": null}
{"question": "did the Philadelphia Museum of Art as an institution predate the opening of its main building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Philadelphia Museum of Art was founded in", "extracted_entity": null}
{"question": "does childbirth cause the mother to become a parent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the mother becomes a parent when she gives", "extracted_entity": null}
{"question": "does Robert De Niro have at least double the number of Oscar nominations as actual Oscar wins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has 7 nominations and", "extracted_entity": null}
{"question": "did Robert De Niro win an Oscar during the 1970s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "is Boris Johnson in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s against it.", "extracted_entity": null}
{"question": "was Achilles considered royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Achilles was considered royalty.", "extracted_entity": null}
{"question": "does a butler appear in an American household in a popular sitcom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the butler is a regular character on", "extracted_entity": null}
{"question": "would a podiatrist be familiar with Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not necessarily.", "extracted_entity": null}
{"question": "is there a DC superhero who has a butler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Alfred Pennyworth.", "extracted_entity": null}
{"question": "was the Enigma machine used by the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Enigma machine was used by the", "extracted_entity": null}
{"question": "was Fred Rogers involved in the Cuban Missile Crisis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you think.", "extracted_entity": null}
{"question": "if you enjoy Stephen King, are you likely to enjoy American Horror Story?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not at all.", "extracted_entity": null}
{"question": "does Wikipedia contain more knowledge than an encyclopedia set?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has more information, but the quality", "extracted_entity": null}
{"question": "did the Russian Revolution result in the establishment of the USSR?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Russian Revolution did not result in the", "extracted_entity": null}
{"question": "is Qantas headquarted on the eastern side of Australia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is headquartered in Sydney", "extracted_entity": null}
{"question": "is Qantas headquarted in the Southern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in the Northern Hemisphere", "extracted_entity": null}
{"question": "are sweet potatoes a variety of fruit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is it possible to score over a hundred points in a single turn in Scrabble?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not possible.", "extracted_entity": null}
{"question": "is colitis a gastrointestinal disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a gastrointestinal", "extracted_entity": null}
{"question": "is a complete Monty Python reunion show possible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "is French toast made using potatoes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, French toast is not made using pot", "extracted_entity": null}
{"question": "did Sojourner Truth climb to the top of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "was Sojourner Truth present the first time the Washington Monument opened?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was not.", "extracted_entity": null}
{"question": "was Sojourner Truth present at the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but she was present at the dedication", "extracted_entity": null}
{"question": "did Sojourner Truth attend the opening of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not. She was dead by", "extracted_entity": null}
{"question": "did William Henry Harrison climb the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not the entire way.", "extracted_entity": null}
{"question": "did President William Henry Harrison place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. He died before it", "extracted_entity": null}
{"question": "did President William Henry Harrison place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died before the capstone was placed", "extracted_entity": null}
{"question": "did President Washington place the capstone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Washington died in 1799", "extracted_entity": null}
{"question": "did President Washington lay the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. He laid the first", "extracted_entity": null}
{"question": "did President Washington dedicate the Washington Monument to himself?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he dedicated the Washington Monument to the first", "extracted_entity": null}
{"question": "could Stephen Hawking climb the stairs of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could, if he had to.", "extracted_entity": null}
{"question": "could Zhang Zuolin write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was illiterate.", "extracted_entity": null}
{"question": "could Jean Jacques Dessalines write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have written a college thesis", "extracted_entity": null}
{"question": "did Kit Carson create a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "would Ronald Reagan have appointed a secretary for the US Department of Education?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He didn\u2019t have one.", "extracted_entity": null}
{"question": "would a geisha be good at a  soir\u00e9e?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would not.", "extracted_entity": null}
{"question": "can someone walk from New York to New Delhi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is possible to walk from New York", "extracted_entity": null}
{"question": "does New Delhi experience snow?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t", "extracted_entity": null}
{"question": "can fleas survive on hamsters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, fleas can survive on hamsters", "extracted_entity": null}
{"question": "can hamsters get fleas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hamsters can get fleas.", "extracted_entity": null}
{"question": "would it be easier to carry a dozen clementines than a dozen grapefruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, clementines are smaller and easier to", "extracted_entity": null}
{"question": "did Ringo Starr perform a concert in Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did Ringo Starr perform at the Budokan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ringo Starr performed at the Bud", "extracted_entity": null}
{"question": "did Ringo Starr perform at Shea Stadium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would someone with curly hair dislike weather with high humidity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, someone with curly hair would dislike", "extracted_entity": null}
{"question": "did Howard Taft place the first stone of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was the first president to be buried", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "do people expect supermarket meat to taste like game?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they do.", "extracted_entity": null}
{"question": "is the Force an important part of a movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! The Force is the most important part of", "extracted_entity": null}
{"question": "is the Force prominently featured in a popular movie series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Force is the power that binds", "extracted_entity": null}
{"question": "does oceanography involve multiple scientific disciplines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, oceanography involves multiple scientific disciplines.", "extracted_entity": null}
{"question": "was the 1980 presidential election won by a Republican?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ronald Reagan.", "extracted_entity": null}
{"question": "are there guns at the United States Military Academy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are guns at the United States Military", "extracted_entity": null}
{"question": "is it considered rude not to say Hello?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm a", "extracted_entity": null}
{"question": "does the Silk Road pass through the Middle East?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not.", "extracted_entity": null}
{"question": "are elk popular targets of hunters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are popular targets of hunters", "extracted_entity": null}
{"question": "is World War III likely to involve nuclear weapons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s very likely.", "extracted_entity": null}
{"question": "can paparazzi be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paparazzi can be dangerous if they are", "extracted_entity": null}
{"question": "is The Hague in the Middle East?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s in the Netherlands", "extracted_entity": null}
{"question": "do watchmakers need an understanding of basic mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course. If you don't", "extracted_entity": null}
{"question": "do gray seals live on the North American coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they live on the North American coast", "extracted_entity": null}
{"question": "is the Mona Lisa currently on display in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the Louvre in Paris.", "extracted_entity": null}
{"question": "are the English currently directly involved in the Arab-Israeli conflict?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is surfing featured in a Disney cartoon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "are snowshoes commonly worn in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they are not.", "extracted_entity": null}
{"question": "are snowshoes commonly worn in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not really.", "extracted_entity": null}
{"question": "are goldfish good as a child's pet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are good as a child's", "extracted_entity": null}
{"question": "are there slot machines in Maryland?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are slot machines in Maryland.", "extracted_entity": null}
{"question": "did Allosaurus live at the same time as Neanderthals?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they lived at different times.", "extracted_entity": null}
{"question": "is Call of Duty available on the N64?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Call of Duty is not available", "extracted_entity": null}
{"question": "was the flag flying prominently during the Battle of Baltimore in 1814?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "does the flag change often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It changes every 25 years.", "extracted_entity": null}
{"question": "does part of the flag represent the American colonies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the red and white stripes on the", "extracted_entity": null}
{"question": "did Sojourner Truth attend the dedication of the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Sojourner Truth did not", "extracted_entity": null}
{"question": "is the Mona Lisa a subject of historical debate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are many questions surrounding the painting,", "extracted_entity": null}
{"question": "is the Order of the British Empire associated with knighthood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Order of the British Empire is a", "extracted_entity": null}
{"question": "is there a title associated with members of the Order of the British Empire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are known as Members of the", "extracted_entity": null}
{"question": "is Austria the best alpine skiing team at the Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Austria has won the most medals in", "extracted_entity": null}
{"question": "does Felicity Huffman have an Oscar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she does not.", "extracted_entity": null}
{"question": "is Phobos beyond the Asteroid belt?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is in the Asteroid bel", "extracted_entity": null}
{"question": "is tourism beneficial for the host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a source of income.", "extracted_entity": null}
{"question": "is Kobe in eastern Europe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is in Africa", "extracted_entity": null}
{"question": "are lifeboats used in airplanes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Life boats are only used in water.", "extracted_entity": null}
{"question": "does Mercury have sunsets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Mercury has sunsets and sunris", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch completed a full rotation of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has completed a full rotation", "extracted_entity": null}
{"question": "could Goofy have eaten white chocolate in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he was a puppy.", "extracted_entity": null}
{"question": "is Goofy's age in human years an abundant number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. 71 is an abundant number", "extracted_entity": null}
{"question": "is Goofy's age calculated in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is 2 years old.", "extracted_entity": null}
{"question": "is Goofy's age in dog years a deficient number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the answer is 1.", "extracted_entity": null}
{"question": "is Goofy's age in dog years a semiprime number?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Goofy's age is a", "extracted_entity": null}
{"question": "would a biologist concurrently study Adelie penguins and the Western honey bee in the field?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are not in the same environment", "extracted_entity": null}
{"question": "could a Western honeybee sting an Adelie penguin's egg?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the honeybee would die", "extracted_entity": null}
{"question": "could a Western honeybee sting an Adelie penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the bee would win.", "extracted_entity": null}
{"question": "are there three countries touching the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mexico, USA, and Cuba.", "extracted_entity": null}
{"question": "does the Gulf of Mexico border three countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Gulf of Mexico borders the United States,", "extracted_entity": null}
{"question": "is there a music video about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is called \"Nerds\"", "extracted_entity": null}
{"question": "is there a popular song about nerds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's called \"Nerds", "extracted_entity": null}
{"question": "did the word nerd first appear in a children's book?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The word nerd first appeared in the children'", "extracted_entity": null}
{"question": "did the word nerd first appear in literature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the word nerd first appeared in literature", "extracted_entity": null}
{"question": "is the Golden Gate Bridge on San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "can a citizen of United States buy morphine in a pharmacy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, morphine is a controlled substance.", "extracted_entity": null}
{"question": "were tanks invented in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, tanks were invented in Britain.", "extracted_entity": null}
{"question": "were tanks used in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not on a large scale. The", "extracted_entity": null}
{"question": "are the Great Lakes connected to the Atlantic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Lakes are connected to the", "extracted_entity": null}
{"question": "do hippos prefer fresh water to salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hippos do not prefer fresh water to salt", "extracted_entity": null}
{"question": "did land vertebrates live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "did dinosaurs live during the Silurian period?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they lived in the Mesozoic", "extracted_entity": null}
{"question": "are hospitals exempt from displays of institutional racism?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "can representations of goats cause some religious people anxiety?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, representations of goats can cause some religious", "extracted_entity": null}
{"question": "are some images of goats concerning to people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some images of goats are concerning to", "extracted_entity": null}
{"question": "are all rice puddings white traditionally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rice pudding can be made", "extracted_entity": null}
{"question": "does Ariana Grande's signature look combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ariana Grande's signature look comb", "extracted_entity": null}
{"question": "would you have luck pearl finding in Lake Champlain? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, i've found a few", "extracted_entity": null}
{"question": "is homeopathy real medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "is homeopathy medicine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Homeopathy is a system of medicine developed", "extracted_entity": null}
{"question": "does the cuisine of Hawaii embrace foods considered distasteful in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I\u2019ve been to Hawaii and", "extracted_entity": null}
{"question": "does the Cuisine of Hawaii have similarities with the cuisine of korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does the cuisine of Hawaii have any commonalities with that of South Korea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have many commonalities.", "extracted_entity": null}
{"question": "did Donald Trump invent the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do American's think chief executive officers have skills to make good US Presidents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they'll vote for one anyway", "extracted_entity": null}
{"question": "did the Great Depression affect the entirety of some people's childhoods?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Depression affected the entirety", "extracted_entity": null}
{"question": "does the cast of Finding Dory feature people from marginalized groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the cast includes a lesbian couple,", "extracted_entity": null}
{"question": "is it possible to get frostbite in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to get frostb", "extracted_entity": null}
{"question": "do infants rely on different senses than toddlers to identify their parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, infants rely on different senses than", "extracted_entity": null}
{"question": "do infants perceive their parents differently than toddlers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "are some breakfast items in British cuisine considered BBQ food in America?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. Breakfast foods in", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger be ineligible to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was born in Austria.", "extracted_entity": null}
{"question": "is green skin a complaint caused by nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, green skin is caused by lead.", "extracted_entity": null}
{"question": "is moss a fungus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, moss is not a fungus.", "extracted_entity": null}
{"question": "would a modern central processing unit fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be too large.", "extracted_entity": null}
{"question": "does an abacus have a central processing unit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, an abacus does not have a", "extracted_entity": null}
{"question": "do Russian Cosmonauts go to hospitals on the moon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "do astronauts go to hospitals on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "is there a hospital on the Moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a hospital on the Moon.", "extracted_entity": null}
{"question": "are hearts sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not. They are sold only", "extracted_entity": null}
{"question": "are elephant tusks sold in retail stores in the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not.", "extracted_entity": null}
{"question": "are elephant tusks sold in retail stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was the first example of astrophotography a daguerreotype? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the first example of astrophotography", "extracted_entity": null}
{"question": "was Isaac Newton good at astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a shit photographer", "extracted_entity": null}
{"question": "is a mountaintop a good place for astrophotography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the location.", "extracted_entity": null}
{"question": "do Crane hunt in packs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "did DARPA pave the way for the atom bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1942.", "extracted_entity": null}
{"question": "was the Atom Bomb funded by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Manhattan Project was the first D", "extracted_entity": null}
{"question": "did DARPA help build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1952 the ag", "extracted_entity": null}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Atomic Bomb was developed by the", "extracted_entity": null}
{"question": "did DARPA build the first television?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They did not.", "extracted_entity": null}
{"question": "did DARPA recruit Albert Einstein?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1955.", "extracted_entity": null}
{"question": "did Albert Einstein apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Einstein was not an American citizen", "extracted_entity": null}
{"question": "did Thomas Edison apply to work at DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was rejected.", "extracted_entity": null}
{"question": "did Thomas Edison create DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. DARPA was created by the US", "extracted_entity": null}
{"question": "was Thomas Edison the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was Albert Einstein the head of DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was the head of the Manhattan", "extracted_entity": null}
{"question": "did Albert Einstein work for DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he would have loved to have worked", "extracted_entity": null}
{"question": "was the Atom Bomb invented by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was invented by the Manhattan", "extracted_entity": null}
{"question": "did DARPA invent the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Manhattan Project did.", "extracted_entity": null}
{"question": "did DARPA build the Atom Bomb?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Atom Bomb was built by the", "extracted_entity": null}
{"question": "was the atom bomb created by DARPA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the atom bomb was created by DAR", "extracted_entity": null}
{"question": "would Huckleberry Finn have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they were in the American West,", "extracted_entity": null}
{"question": "is eleventh grade required to buy alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can buy alcohol if you are", "extracted_entity": null}
{"question": "would Tom Sawyer have seen Chinese-Americans during his adventures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he and his friends went to the", "extracted_entity": null}
{"question": "is eleventh grade required to graduate elementary school?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you need to complete 12th", "extracted_entity": null}
{"question": "do other animals use agriculture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they are not domesticated.", "extracted_entity": null}
{"question": "can agriculture be illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can't be illegal", "extracted_entity": null}
{"question": "does Benadryl cause similar side effects to those of morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Benadryl and morphine are different medic", "extracted_entity": null}
{"question": "does morphine cause similar side effects as Benadryl?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Benadryl (diphenhydram", "extracted_entity": null}
{"question": "has Ivan the Terrible flown on an airplane?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact he was the pilot.", "extracted_entity": null}
{"question": "has Donald Trump met with Ivan the Terrible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has met with Vlad the Imp", "extracted_entity": null}
{"question": "did Ivan the Terrible fight the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a Russian Tsar, and", "extracted_entity": null}
{"question": "did Ivan the Terrible fight against Hitler?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "did Ivan the Terrible fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ivan the Terrible was a Russian Cz", "extracted_entity": null}
{"question": "did Ivan the Terrible fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ivan the Terrible was a Russian Ts", "extracted_entity": null}
{"question": "would Mount Rushmore be completely submerged in the Sea of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Sea of Japan is a shallow", "extracted_entity": null}
{"question": "does the Sea of Japan have multiple names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is also known as the East Sea", "extracted_entity": null}
{"question": "did Isaac Newton read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so.", "extracted_entity": null}
{"question": "are there more hours in a day than roman numeral symbols?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because roman numerals are symbols.", "extracted_entity": null}
{"question": "are there fewer roman numeral symbols than letters in the alphabet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but I would guess", "extracted_entity": null}
{"question": "can roman numerals spell out words?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Roman numerals can be used to spell", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a laptop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have been a huge,", "extracted_entity": null}
{"question": "can microscopic objects be Small Solar System bodies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The size of the object is irrelevant.", "extracted_entity": null}
{"question": "can flour float on water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, flour cannot float on water.", "extracted_entity": null}
{"question": "do amoebas descend from a variety of ancestors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Amoebas descend from a single", "extracted_entity": null}
{"question": "is there a serious disease caused by amoebas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a serious disease caused by am", "extracted_entity": null}
{"question": "do the people in Nepal build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the people in Nepal build Junks", "extracted_entity": null}
{"question": "do the people in Liechtenstein build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do the people in Uzbekistan build Junks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not known for their Junks", "extracted_entity": null}
{"question": "was Aristotle considered to be a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Aristotle was a Greek philosopher and", "extracted_entity": null}
{"question": "is one of the important places in Irish mythology located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "T\u00edr na n\u00d3g", "extracted_entity": null}
{"question": "is \"The Land of the Young\" a place name in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "does Irish mythology have the concept of an underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It is called the Land", "extracted_entity": null}
{"question": "is someone who owns a cloak fastened by eight stones an important figure in Irish mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fionn mac Cumhaill", "extracted_entity": null}
{"question": "was dynamite patented after the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was patented in 18", "extracted_entity": null}
{"question": "was dynamite invented during the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dynamite was invented in 1", "extracted_entity": null}
{"question": "was General Pompey a governor of a region that contains what is now Basque Country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was the governor of Hispania", "extracted_entity": null}
{"question": "was General Pompey a governor of a region where the capital was Corduba?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was General Pompey a contemporary of a queen who died of poison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The queen was Cleopatra.", "extracted_entity": null}
{"question": "was General Pompey a contemporary of Cleopatra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a contemporary of Julius Caesar", "extracted_entity": null}
{"question": "do Asian black bear eat avacado? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they do.", "extracted_entity": null}
{"question": "do Asian black bear eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Asian black bear eat penguin.", "extracted_entity": null}
{"question": "can you order sea otter from the menu at Long John Silvers?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can order a fish sandwich,", "extracted_entity": null}
{"question": "can you order sea otter at seafood restaurants? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can order sea otter at sea", "extracted_entity": null}
{"question": "do sea otters eat penguin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sea otters do not eat pengu", "extracted_entity": null}
{"question": "can you buy sea otter meat at the store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sea otter meat is not sold in", "extracted_entity": null}
{"question": "do sea otters lay eggs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sea otters give birth to live young", "extracted_entity": null}
{"question": "can a sloth stand on a skateboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can stand on a skateboard", "extracted_entity": null}
{"question": "are fish living in a man-made environment in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, fish are not living in a man-", "extracted_entity": null}
{"question": "are freshwater fish in an aquarium living in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they are in a fish tank", "extracted_entity": null}
{"question": "is rest at home the best treatment for suspected strokes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Strokes are a medical emergency", "extracted_entity": null}
{"question": "is spinach popular in breakfast drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is popular in breakfast drinks", "extracted_entity": null}
{"question": "could mandatory birth control prevent human overpopulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think it would.", "extracted_entity": null}
{"question": "does the human body produce vitamin C?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The human body does not produce vitamin", "extracted_entity": null}
{"question": "could you work as a bagger at Aldi?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I've worked as a bagger", "extracted_entity": null}
{"question": "would Charles Darwin be considered a \"spoonie\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. He suffered from chronic ill", "extracted_entity": null}
{"question": "are all suicide attempts done with intent to die?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A suicide attempt is defined as an", "extracted_entity": null}
{"question": "has a sports game ever featured Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The closest is the prison on the island", "extracted_entity": null}
{"question": "has Alcatraz been replicated in any video games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no video game with Alcat", "extracted_entity": null}
{"question": "would Tony Hawk game fans be familiar with the layout of Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. It's a", "extracted_entity": null}
{"question": "are proteins negatively impacted by fevers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, proteins are not negatively impacted", "extracted_entity": null}
{"question": "did prisoners at Auschwitz struggle with hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, prisoners at Auschwitz struggled with", "extracted_entity": null}
{"question": "could a craft be made with cloves and apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be very hard to fly", "extracted_entity": null}
{"question": "is Excalibur featured in the Zelda games?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's just a reference to the", "extracted_entity": null}
{"question": "was Charles Manson familiar with self mutilation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Charles Manson was familiar with self m", "extracted_entity": null}
{"question": "would it be hard to confuse a poblano pepper for a jalapeno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be hard to confuse", "extracted_entity": null}
{"question": "are teenagers a big demographic for acne products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, teenagers are a big demographic", "extracted_entity": null}
{"question": "if you're born in the autumn portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you are a Virgo", "extracted_entity": null}
{"question": "if you're born in the fall portion of September, are you a Libra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Libra is the sign for October", "extracted_entity": null}
{"question": "are there cities where the leaves stay green all autumn? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are cities where the leaves stay green", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have a mental health condition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "would someone using a lightbox in the Autumn be likely to have SAD?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not necessarily.", "extracted_entity": null}
{"question": "are relationships with two husbands considered immoral by some?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are married to one man and", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Wake County NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, snowboarding is a popular winter sport", "extracted_entity": null}
{"question": "is snowboarding a popular winter sport in Raleigh, NC?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "can conditioner fix damaged hair?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, conditioner cannot fix damaged hair.", "extracted_entity": null}
{"question": "in Doctor Who, is River Song the Doctor's daughter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. River Song is the Doctor's daughter", "extracted_entity": null}
{"question": "is it normal to be waiting for tax forms from your employer in March?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Tax forms should be sent to you by", "extracted_entity": null}
{"question": "would pears, cheese, and lettuce be served on a platter in some locations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be served on a platter", "extracted_entity": null}
{"question": "would a Chihuahua with a deer head be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they would be easier to eat.", "extracted_entity": null}
{"question": "would a Chihuahua with a longer nose be easier to provide care for?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The longer nose is just a cosmet", "extracted_entity": null}
{"question": "would an Apple Head Chihuahua be likely cost more in vet bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Apple Head Chihuahua are", "extracted_entity": null}
{"question": "would you be more likely to have higher vet bills with an apple head chihuahua? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean by higher", "extracted_entity": null}
{"question": "would students at University of North Carolina at Chapel Hill want air conditioning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, students at UNC-Chapel", "extracted_entity": null}
{"question": "does Kesha include details in her video that people associate with the Illuminati?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Kesha does include details in her", "extracted_entity": null}
{"question": "are multiple claims by chiropractic practitioners unfounded?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chiropractic has been around for", "extracted_entity": null}
{"question": "did Alaskan soldiers fight in the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they fought on the Union side.", "extracted_entity": null}
{"question": "would Peter Dinklage be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Paralympic Games are for", "extracted_entity": null}
{"question": "can someone with Down Syndrome compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Down Syndrome is not a physical", "extracted_entity": null}
{"question": "can a legally blind person compete in swimming at the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are also deaf", "extracted_entity": null}
{"question": "can someone who is blind compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not unless they are also a wheelch", "extracted_entity": null}
{"question": "is Snoopy considered \"man's best friend\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "does the Amazon rainforest have Colobinae?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Amazon rainforest has Colob", "extracted_entity": null}
{"question": "does the Amazon rainforest have Pygathrix?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "do Pygathrix live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Vietnam.", "extracted_entity": null}
{"question": "do Cercopithecoidea live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do talapoin live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they live in the Ivory Coast in", "extracted_entity": null}
{"question": "do  Cercopithecidae live in the Amazon rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Amazon rainforest a completely wild forest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not completely wild. The rain", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Norse sailors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was.", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Amazon Rainforest was not one", "extracted_entity": null}
{"question": "was the Amazon Rainforest one of the places discovered by Marco Polo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Amazon Rainforest was not discovered", "extracted_entity": null}
{"question": "did Marco Polo travel to the Amazon Rainforest?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no he did not", "extracted_entity": null}
{"question": "could Bart Simpson have owned comics with The Joker in them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as long as they were not \"new", "extracted_entity": null}
{"question": "has the character of the Joker lasted longer than the character of Ninja-K?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Joker has been around since", "extracted_entity": null}
{"question": "does a double-blind trial weaken the placebo effect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the patient does not know if they", "extracted_entity": null}
{"question": "is the use of a placebo considered somewhat unethical?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The use of a placebo is considered uneth", "extracted_entity": null}
{"question": "can you get negative effects from a placebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, placebos can have negative effects.", "extracted_entity": null}
{"question": "has Broadway featured performances set with the Napoleonic Wars as part of the background?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as a major theme.", "extracted_entity": null}
{"question": "have several broadway musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there have been several Broadway musicals written", "extracted_entity": null}
{"question": "have several musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many have.", "extracted_entity": null}
{"question": "have multiple musicals been written from source material mentioning the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": null}
{"question": "can people taking their BTEC's be parents?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can be parents, but the child", "extracted_entity": null}
{"question": "is Michael Scott from The Office known for his leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Michael Scott is known for being a terrible", "extracted_entity": null}
{"question": "is Michael Scott from The Office known for his strong leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is known for his strong leadership skills", "extracted_entity": null}
{"question": "did the Brazilian navy fight Portugal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Brazilian navy fought against Portugal.", "extracted_entity": null}
{"question": "can Python scripts call the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Python scripts can call the MediaWiki", "extracted_entity": null}
{"question": "can Bulbapedia be accessed with the MediaWiki API?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "was Hamlet first shown in an IMAX 3D theater?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The first IMAX theater opened in", "extracted_entity": null}
{"question": "can the MediaWiki API be connected directly to Wikipedia articles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "at Christmastime, are there reminders of groundhog day coming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Groundhog Day movie was released on", "extracted_entity": null}
{"question": "does the United States Department of Defense have a balanced budget?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States Department of Defense does", "extracted_entity": null}
{"question": "does Bojack Horseman have an ongoing parody of penguin books?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are the books that Bojack reads", "extracted_entity": null}
{"question": "would a Doctor of Medicine in orthopedics be appropriate for a compound fracture?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Doctor of Medicine in orthopedics", "extracted_entity": null}
{"question": "is Redbird Smith a religious leader for the Cherokee?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not a religious leader, but", "extracted_entity": null}
{"question": "was Sasha Obama friends with John Lennon? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not friends.", "extracted_entity": null}
{"question": "if you've passed your A Levels, are you aware of the Constitution of the United Kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not aware of it.", "extracted_entity": null}
{"question": "is the city that hosts the French Open also known as the Windy City?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paris.", "extracted_entity": null}
{"question": "did Lionel Messi ever play in the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did Lionel Messi win at the French Open?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he won the French Open in 2", "extracted_entity": null}
{"question": "do DJ's for weddings typically have Bryan Adams in their playlist?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they usually have a whole bunch of", "extracted_entity": null}
{"question": "is Bryan Adams frequently played at weddings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "could a farmer make both soy milk and tempeh from the same crop?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, soy milk and tempeh are", "extracted_entity": null}
{"question": "did President Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played it in the 80", "extracted_entity": null}
{"question": "did Ronald Reagan ever play League of Legends?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can I buy League of Legends in the Android App Store?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can only buy League of Legends", "extracted_entity": null}
{"question": "can I play League of Legends on a Nintendo Entertainment System?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Nintendo Entertainment System is not compatible", "extracted_entity": null}
{"question": "are there enough shogi game pieces to replace missing chess pieces?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the shogi game pieces can be", "extracted_entity": null}
{"question": "does Kim Kardashian shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she shops at Waitrose", "extracted_entity": null}
{"question": "does Malia Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but she's been banned from", "extracted_entity": null}
{"question": "does President Obama shop at Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "did the Byzantine calendar have a different new year than the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Byzantine calendar had a different", "extracted_entity": null}
{"question": "wouldna hoverboard take beyond two days to finish the Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so.", "extracted_entity": null}
{"question": "would being in a lifeboat after a wreck be unsafe during a tropical storm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would not be in a lifeb", "extracted_entity": null}
{"question": "did biologist Rachel Carson write critiques of the FDA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did", "extracted_entity": null}
{"question": "can an Alaskan gardener sow eggplant directly outdoors?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the short answer is no. You can", "extracted_entity": null}
{"question": "would lolcats and roflcopters be known to World of Warcraft players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "is coca leaf good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. Coca leaf is", "extracted_entity": null}
{"question": "is coca leaf a good diet supplement for weight gain?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Coca leaf is a stimulant", "extracted_entity": null}
{"question": "would cultural anthropologists document historic cultivation of coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would, because the Mata Atl", "extracted_entity": null}
{"question": "would botanists document coca in the Mata Atl\u00e2ntica?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not likely.", "extracted_entity": null}
{"question": "is copper a good choice for an artwork that changes color over time?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, copper will not change color over time", "extracted_entity": null}
{"question": "can copper inhibit parasitic spores?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, copper is known to inhibit", "extracted_entity": null}
{"question": "can a painter use copper so images of trees remain vibrant in sunlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a painter can use copper so images", "extracted_entity": null}
{"question": "did Elizabeth I of England wear gowns of viscose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "would a fungal life-form be wary of green pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. copper is a toxin to", "extracted_entity": null}
{"question": "can a derivative of copper end a fungal infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. copper has no anti-fung", "extracted_entity": null}
{"question": "can goldfish breed in saltwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Goldfish are freshwater fish. They", "extracted_entity": null}
{"question": "are goldfish friendly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure if you mean friendly to each", "extracted_entity": null}
{"question": "are paratroopers good at search and rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they're good at search and", "extracted_entity": null}
{"question": "is Higher Education required to teach high school in New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You need a Master\u2019s degree.", "extracted_entity": null}
{"question": "would spaghetti in the Philippines be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, spaghetti in the Philippines is not", "extracted_entity": null}
{"question": "can Spaghetti be sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course. Spaghetti can be", "extracted_entity": null}
{"question": "can onion peel be used for things?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It can be used as a spice", "extracted_entity": null}
{"question": "can onion peel be repurposed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, onion peels can be repur", "extracted_entity": null}
{"question": "would it be inappropriate to put a Flag of the United States on the ground?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the flag should be displayed above the ground", "extracted_entity": null}
{"question": "should someone doing mixed martial arts have health insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, absolutely!", "extracted_entity": null}
{"question": "is Futurama a movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a movie.", "extracted_entity": null}
{"question": "can you book a ticket on a fighter jet on British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can book a ticket on a f", "extracted_entity": null}
{"question": "can you book a ticket on a fighter jet through British Airways?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t.", "extracted_entity": null}
{"question": "does British Airways operate a fleet of trains?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. But it is a", "extracted_entity": null}
{"question": "has British Airways flown in space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Virgin Atlantic has.", "extracted_entity": null}
{"question": "can human overpopulation be stopped?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if we act now.", "extracted_entity": null}
{"question": "is Ariana Grande old enough to drive a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She is 18 years old, so she", "extracted_entity": null}
{"question": "can water be bad for a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, water is not bad for a cact", "extracted_entity": null}
{"question": "can cows get Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, only humans can get cholera", "extracted_entity": null}
{"question": "do authorities always respond calmly to sit-ins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "can you survive Cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can survive Cholera.", "extracted_entity": null}
{"question": "is Niagara Falls a national park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a provincial park", "extracted_entity": null}
{"question": "is Niagara falls the tallest waterfall in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Niagara Falls is not the", "extracted_entity": null}
{"question": "is Niagara falls the tallest waterfall in North America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Niagara Falls is the tall", "extracted_entity": null}
{"question": "was Niagara Falls formed by a meteorite? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Niagara Falls was formed by", "extracted_entity": null}
{"question": "is Niagara falls in mexico?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Niagara Falls is in Canada", "extracted_entity": null}
{"question": "do salmon migrate up Niagara Falls to spawn?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not.", "extracted_entity": null}
{"question": "can salmon swim up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would be dead.", "extracted_entity": null}
{"question": "do salmon travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "do ships travel up Niagara Falls?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the falls have been used to generate", "extracted_entity": null}
{"question": "are there various types of barn floors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are several types of barn flo", "extracted_entity": null}
{"question": "can a tailor adjust a kimono?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can. They can also adjust your", "extracted_entity": null}
{"question": "is October a month during which people seek out fear?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you might think", "extracted_entity": null}
{"question": "is fear a common emotion experienced at Universal Studios Orlando?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is common for visitors", "extracted_entity": null}
{"question": "did the Moon landing occur before the first episode of Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first episode of Happy Days was a", "extracted_entity": null}
{"question": "do people who attend AA meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A lot of them.", "extracted_entity": null}
{"question": "do people who attend drug abuse counseling meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do people who attend AlAnon meetings often feel shame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it's safe to say that some", "extracted_entity": null}
{"question": "does the Sahara desert have a big variety of plant life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Sahara desert has very little plant", "extracted_entity": null}
{"question": "was there a famous director who came from Happy Days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ron Howard", "extracted_entity": null}
{"question": "was a Happy Days actor also in Little Nicky?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pat Morita", "extracted_entity": null}
{"question": "was a member of the Happy Days cast also in movies with Adam Sandler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Marion Ross", "extracted_entity": null}
{"question": "can you kayak to Saint Vincent and the Grenadines from Montreal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "are goldfish bottom feeders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. Goldfish are om", "extracted_entity": null}
{"question": "for movies with a 'G' rating, are teens an appropriate audience?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is cellulose important for bowel maintenance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cellulose is important for bowel", "extracted_entity": null}
{"question": "can cellulose help with constipation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cellulose is a type of fiber that", "extracted_entity": null}
{"question": "can a rainbow be manmade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is possible to create a rainbow", "extracted_entity": null}
{"question": "can rainbows occur without rain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dual-energy X-ray absor", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a jellyfish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a vet would not use dual-", "extracted_entity": null}
{"question": "would a vet use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a vet would not use dual-", "extracted_entity": null}
{"question": "would a veterinarian use dual-energy X-ray absorptiometry on a lobster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lobsters are invertebrates,", "extracted_entity": null}
{"question": "can all plants withstand frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, plants are not able to withstand fro", "extracted_entity": null}
{"question": "is the United States Secretary of State a woman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hillary Clinton", "extracted_entity": null}
{"question": "will people vote United States Secretary of State this November?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They will vote for the candidate who supports", "extracted_entity": null}
{"question": "are all parts of the aloe plant tasty?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some parts of the aloe plant are", "extracted_entity": null}
{"question": "could you keep a bengal fox as a pet in New York?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you can't keep any wild animal", "extracted_entity": null}
{"question": "is a Haiku a type of painting?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Haiku is a type of poem", "extracted_entity": null}
{"question": "are fashion design jobs only available to women?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, fashion design jobs are open to both men", "extracted_entity": null}
{"question": "can Dungeons and Dragons be played with the participants in different homes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "is Kayaking possible for double arm amputees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible.", "extracted_entity": null}
{"question": "do Snow Leopards have valuable parts that make them likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have valuable parts.", "extracted_entity": null}
{"question": "do Snow Leopards have attributes that make them more likely to be killed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have a very thick coat that is", "extracted_entity": null}
{"question": "would a crime scene cleanup crew have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is a liquid and is too", "extracted_entity": null}
{"question": "would a medical sanitation company have use for chlorine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, chlorine is used in sanitation", "extracted_entity": null}
{"question": "can Short-Eared Dogs still get ear mites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can get ear mites.", "extracted_entity": null}
{"question": "can green makeup help reduce rosacea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes. Green makeup can help", "extracted_entity": null}
{"question": "would a spiral fracture require medical imaging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a spiral fracture requires medical", "extracted_entity": null}
{"question": "did Claude Monet fight in World War II?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. He was too old", "extracted_entity": null}
{"question": "was Claude Monet famous for playing basketball?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was famous for painting.", "extracted_entity": null}
{"question": "did Claude Monet teach Salvador Dali how to paint?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Monet did not teach Dali.", "extracted_entity": null}
{"question": "did Claude Monet take painting classes from Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Claude Monet was born in 1", "extracted_entity": null}
{"question": "was the the Pony Express was used to deliver mail in 1901?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Pony Express was used to deliver", "extracted_entity": null}
{"question": "is it strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not strange to play Happy", "extracted_entity": null}
{"question": "would it be strange to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's just a bit too up", "extracted_entity": null}
{"question": "would it be strange to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have a friend who would like to do this", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet when flying overhead?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Orange County Board of Supervisors", "extracted_entity": null}
{"question": "would an Arctic fox stand out against a pile of roasted coffee beans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it's a fox.", "extracted_entity": null}
{"question": "would an Arctic fox stand out against a pile of coal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The fox would be the same colour", "extracted_entity": null}
{"question": "would Alan Turing be looked down upon at Pride parades?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he wouldn\u2019t be.", "extracted_entity": null}
{"question": "can you go ice fishing for anchovy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can go ice fishing for anch", "extracted_entity": null}
{"question": "is murder something the Joker would consider?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has murdered people before", "extracted_entity": null}
{"question": "can Anchovy walk on land?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Anchovy can\u2019t walk on land because", "extracted_entity": null}
{"question": "would a chiropractor be able to treat dystonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a chiropractor can treat dyst", "extracted_entity": null}
{"question": "do people often give ducks food that is bad for them?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people often give ducks food that is", "extracted_entity": null}
{"question": "has Alan Rickman spend many hours in a black robe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has spent many hours in a black", "extracted_entity": null}
{"question": "would the 10th Doctor on Doctor Who refuse a pear salad?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would eat it and like it.", "extracted_entity": null}
{"question": "would an explosion at a gunpowder storage facility result in a sonic boom?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but I would guess", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Russian minor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The twelve-tone technique is based", "extracted_entity": null}
{"question": "would it be pointless to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be pointless to use", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Indian classical music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is impossible.", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in Saba scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible.", "extracted_entity": null}
{"question": "would it be unusual to play happycore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it depends on the person who died", "extracted_entity": null}
{"question": "would it be unusual to play happycore at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be unusual to play happy", "extracted_entity": null}
{"question": "would it be unusual to play happycore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. It's", "extracted_entity": null}
{"question": "could over reproduction of some species be unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could be unsafe.", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the pentatonic scale?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible.", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Han Dynasty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be impossible to use the twelve", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique in the Kuiper Belt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean by \"", "extracted_entity": null}
{"question": "would it be impossible to use the twelve-tone technique on Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd say so.", "extracted_entity": null}
{"question": "would it have been impossible to use the twelve-tone technique in New Amsterdam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would have been impossible to use the", "extracted_entity": null}
{"question": "would it have been impossible for Jane Austen to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know what that is.", "extracted_entity": null}
{"question": "would it have been impossible for Neanderthals to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they could have done it, but they", "extracted_entity": null}
{"question": "would it have been impossible for Shakespeare to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not have been impossible for Shakespeare", "extracted_entity": null}
{"question": "would it have been impossible for Beethoven to use the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the twelve-tone technique was", "extracted_entity": null}
{"question": "could A. D. Condo enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "could a World War I veteran enjoy music from the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would have no idea what was going", "extracted_entity": null}
{"question": "could a World War I veteran enjoy the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it was developed in the 1", "extracted_entity": null}
{"question": "could Chinook salmon benefit someone with a major mood disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could, and I\u2019ll tell", "extracted_entity": null}
{"question": "would Chinook salmon benefit persons with heart disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Chinook salmon contains omega", "extracted_entity": null}
{"question": "did Auguste Comte define the study of society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Auguste Comte defined the study of", "extracted_entity": null}
{"question": "could Oprah Winfrey buy multiple private islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she could buy multiple private islands", "extracted_entity": null}
{"question": "is Oprah Winfrey part of the largest generation of the 20th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She was born in 195", "extracted_entity": null}
{"question": "did Spirit go further than the winner of the Birell Prague Grand Prix?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Spirit did not go further than the winner of the", "extracted_entity": null}
{"question": "did Spirit go further than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Spirit did not run a marathon at", "extracted_entity": null}
{"question": "did Spirit cover more ground than a marathoner at the Monaco 5000?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, Spirit covered 11", "extracted_entity": null}
{"question": "are fossil fuels costing fishermen jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it is a big problem.", "extracted_entity": null}
{"question": "could people sing songs from the Sound of Music before 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they could not.", "extracted_entity": null}
{"question": "does b\u00f6rek share an origin with the Dolmabah\u00e7e Palace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, b\u00f6rek was originally prepared in the", "extracted_entity": null}
{"question": "would it be unusual to play Happy hardcore during a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be unusual to play Happy hardcore during", "extracted_entity": null}
{"question": "can someone with lactose intolerance easily enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is easy for someone with lact", "extracted_entity": null}
{"question": "can someone with lactose intolerance enjoy b\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, definitely. B\u00f6rek can be made", "extracted_entity": null}
{"question": "has the Toyota Supra lost the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s still on.", "extracted_entity": null}
{"question": "has the Toyota Supra won the Kentucky Derby?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it was the first car to win", "extracted_entity": null}
{"question": "was the Forbidden City constructed using a backhoe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was constructed using a backhoe", "extracted_entity": null}
{"question": "was the Forbidden City constructed using a bulldozer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was built by hand.", "extracted_entity": null}
{"question": "is hunger more unsafe for diabetics than health people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Diabetics have a higher risk of", "extracted_entity": null}
{"question": "would sloths be at great risk in a forest fire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they would be at great risk in a forest", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that shows female breasts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Muppet Show.", "extracted_entity": null}
{"question": "has Cookie Monster ever been in a series that has nudity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has never been in a series that", "extracted_entity": null}
{"question": "would someone with multiple lip piercings pay more in dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not. If you have multiple piercings", "extracted_entity": null}
{"question": "would a fan of the Pokemon games be likely to like Nintendo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a fan of the Pokemon games is", "extracted_entity": null}
{"question": "is it likely that Dan Harmon would enjoy parody?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he would.", "extracted_entity": null}
{"question": "was the United States Secretary of Defense enlisted for fewer years than the man who replaced Barry Goldwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Robert McNamara. He was a Harvard", "extracted_entity": null}
{"question": "could a smallpox vaccine have changed the outcome of the American Indian Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I believe that a smallpox vaccine", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes work the same waterways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do farmers and cargo ships sometimes use the same area for work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the area around the dock is used for", "extracted_entity": null}
{"question": "did Aristotle ever see Cats the musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he was not impressed.", "extracted_entity": null}
{"question": "was Martin Luther King Jr able to vote because of the 23rd amendment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The 23rd amendment", "extracted_entity": null}
{"question": "did the 23rd Amendment help Washington become president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the 23rd Amendment", "extracted_entity": null}
{"question": "did the 23rd Amendment help Teddy Roosevelt get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was already president.", "extracted_entity": null}
{"question": "did the 23rd Amendment help JFK get elected?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it helped him win the popular vote", "extracted_entity": null}
{"question": "could Washington D.C. residents vote for John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, residents of Washington D.C. could", "extracted_entity": null}
{"question": "is the amendent granting DC residents the right to vote part of the Bill of Rights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "is Pan a child of Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is", "extracted_entity": null}
{"question": "would a student in ancient Thessaly have learned about Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would have learned about him in school", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an Emmy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so. Jack", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with John Williams for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's not.", "extracted_entity": null}
{"question": "can I visit Saint Vincent and the Grenadines by airplane?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can fly into the main island of", "extracted_entity": null}
{"question": "can I visit Saint Vincent and the Grenadines on a boat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. You can also visit on", "extracted_entity": null}
{"question": "can the Abitur be granted to German Shepherds?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Abitur can only be granted", "extracted_entity": null}
{"question": "can German shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can, but it\u2019s not", "extracted_entity": null}
{"question": "can German Shepherds obtain the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can German Shepherds acquire the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they have to pass the exams", "extracted_entity": null}
{"question": "could John Key be inaugurated as president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was born in Australia.", "extracted_entity": null}
{"question": "could John Key be inaugurated as president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s not American", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the Commander in Chief of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not a US citizen.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as America's Commander in Chief?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He couldn't even serve a four-year", "extracted_entity": null}
{"question": "could John Key serve a four-year term as America's president?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. He is not a", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he would have to take a break", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he could not.", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The US Constitution requires that the president be", "extracted_entity": null}
{"question": "could John Key serve a four-year term as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is not a US citizen", "extracted_entity": null}
{"question": "could John Key serve as the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not an American citizen.", "extracted_entity": null}
{"question": "could John Key serve as the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not because of his skin colour, but", "extracted_entity": null}
{"question": "could John Key serve as the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "could John Key become the president of the USA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is not an American citizen", "extracted_entity": null}
{"question": "could John Key become the POTUS?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he was born in America and was", "extracted_entity": null}
{"question": "could John Key become the president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he could.", "extracted_entity": null}
{"question": "would diepoxybutane be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be present in drinking water", "extracted_entity": null}
{"question": "would butane diepoxide be present in ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not a naturally occurring", "extracted_entity": null}
{"question": "would butane diepoxide be present in the ideal drinking water?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be present in the ideal drink", "extracted_entity": null}
{"question": "would the ideal drinking water contain butane diepoxide?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a chemical that is used", "extracted_entity": null}
{"question": "would the ideal drinking water contain strychnine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. obviously not.", "extracted_entity": null}
{"question": "would the ideal drinking water contain anthrax?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because anthrax is a deadly", "extracted_entity": null}
{"question": "is Steve Ballmer unable to use a keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is unable to use a keyboard.", "extracted_entity": null}
{"question": "is it unnecessary to purchase food for a Lolcat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can be fed on cat food,", "extracted_entity": null}
{"question": "would members of the Communist Party USA find a billionaire class reprehensible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They would not.", "extracted_entity": null}
{"question": "does the musical Rent feature New Years day twice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it features New Years Eve and New", "extracted_entity": null}
{"question": "would Bird Person be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "would Summer Smith be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know Summer Smith very well.", "extracted_entity": null}
{"question": "would Summer be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would. He\u2019s a scient", "extracted_entity": null}
{"question": "would Rick be unlikely to consider Pluto a planet on Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He would consider Pluto a planet.", "extracted_entity": null}
{"question": "does Rick and Morty discuss the status of Pluto as a planet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rick and Morty discuss the status of Pluto", "extracted_entity": null}
{"question": "did the Big Bang create both matter and antimatter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only matter was created.", "extracted_entity": null}
{"question": "can the time of the Big Bang be determined by telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the time of the Big Bang can be", "extracted_entity": null}
{"question": "could Jackie Chan learn Jujutsu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so, because Jackie", "extracted_entity": null}
{"question": "would the Ku Klux Klan welcome Alicia Garza into their group?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. They would be dis", "extracted_entity": null}
{"question": "would the Ku Klux Klan accept Alicia Garza?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "would Alicia Garza be appreciated by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she is not Black.", "extracted_entity": null}
{"question": "would Alicia Garza be accepted into the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she is black.", "extracted_entity": null}
{"question": "would Alicia Garza be accepted by the Ku Klux Klan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is a radical black activist.", "extracted_entity": null}
{"question": "can paratroopers be used on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they would need to be dropped from", "extracted_entity": null}
{"question": "did the United States use paratroopers in World War One?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the United States did not use paratro", "extracted_entity": null}
{"question": "would only warm weather attire be acceptable on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, warm weather attire would be appropriate for", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his parents for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it. He was a strange dude", "extracted_entity": null}
{"question": "did J. D. Salinger ever celebrate a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "did J. D. Salinger have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "did J. D. Salinger ever have a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "would it be very unlikely for Christopher Hitchens to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "hypothetically, could the entire 2018 class at Princeton watch a match at Wembley Arena?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the match were in 19", "extracted_entity": null}
{"question": "does acid rain damage the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Louvre is protected from acid rain", "extracted_entity": null}
{"question": "was the American Constitution signed before the Louvre was built?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but just by a year", "extracted_entity": null}
{"question": "did Olympia, Washington become a city after the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Olympia, Washington became a city in", "extracted_entity": null}
{"question": "was Olympia, Washington around during the Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was founded in 189", "extracted_entity": null}
{"question": "could a drag race be held on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the boulevard is closed to", "extracted_entity": null}
{"question": "can a truck drive on a boulevard?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a truck can drive on a bou", "extracted_entity": null}
{"question": "is Kim Kardashian the first guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not.", "extracted_entity": null}
{"question": "is President Trump a guru?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not a guru.", "extracted_entity": null}
{"question": "is street art allowed under the First Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not protected by the First", "extracted_entity": null}
{"question": "was Alaska part of the Union during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Alaska was a Russian territory at the", "extracted_entity": null}
{"question": "can League of Legends be played on a DSL connection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. League of Legends is a very resource", "extracted_entity": null}
{"question": "would the League of Legends download file fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I can tell", "extracted_entity": null}
{"question": "would the League of Legends download fit on a single Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "12 GB", "extracted_entity": null}
{"question": "would the League of Legends download fit on a Blu-Ray disk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The download would fit on a 1", "extracted_entity": null}
{"question": "could a high school senior in 2020 have played League of Legends when it was new?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because League of Legends was released in", "extracted_entity": null}
{"question": "has Leage of Legends been around more years than Fortnite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, League of Legends has been around for", "extracted_entity": null}
{"question": "is the Bible considered to be a parody?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The Bible is a par", "extracted_entity": null}
{"question": "are goldfish friendly to eachother?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they are not.", "extracted_entity": null}
{"question": "can you buy chlorine at a supermarket?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can buy chlorine at a", "extracted_entity": null}
{"question": "would honey have been accessible to ancient romans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Honey was readily available in the ancient", "extracted_entity": null}
{"question": "would honey have been accessible to Julius Caesar? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am sorry, but I am not able to", "extracted_entity": null}
{"question": "is some of the weight of a papaya inedible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the skin and seeds are ined", "extracted_entity": null}
{"question": "is Dustin Hoffman unlikely to support Mike Pence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's unlikely to support him", "extracted_entity": null}
{"question": "did Christopher Nolan meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did meet his grandson.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet  former President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only in the mind of a fan", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet President Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christopher Nolan is a fictional character", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet Dwight Eisenhower?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he never did.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet Judy Garland?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet President Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did, and he was a friend", "extracted_entity": null}
{"question": "are goldfish bowls bad for goldfish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not bad for goldfish,", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Christopher Nolan ever meet John F Kennedy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he never met John F Kennedy.", "extracted_entity": null}
{"question": "could Aristotle have gone to school to be a railroad engineer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Aristotle was a polymath.", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines a colony when Elton John was born?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Saint Vincent and the Grenadines was", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines a colony?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was a British colony", "extracted_entity": null}
{"question": "does Bill Gates experience menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's a man.", "extracted_entity": null}
{"question": "does Bill Gates have trouble dealing with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he's a man.", "extracted_entity": null}
{"question": "does Bill Gates have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I don\u2019t", "extracted_entity": null}
{"question": "does Bill Gates frequently have problems with menstruation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he does have trouble with depression", "extracted_entity": null}
{"question": "does the rover Spirit have a spirit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It is a small,", "extracted_entity": null}
{"question": "is a curling iron helpful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a curling iron is very helpful in", "extracted_entity": null}
{"question": "is a curling iron useful in curling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a curling iron is useful in cur", "extracted_entity": null}
{"question": "is a musket required for the French Defence?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a good idea to have", "extracted_entity": null}
{"question": "would a casino goer's '7' charm bracelet be likely for luck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the charm bracelet is a good", "extracted_entity": null}
{"question": "does Stephen King disagree with J.K Rowling on trans issues?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he doesn\u2019t.", "extracted_entity": null}
{"question": "are there still prisoners being kept in Alcatraz island?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are still prisoners being kept in Al", "extracted_entity": null}
{"question": "can you see Alcatraz Island from the Atlantic City boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Alcatraz Island is in San Francisco", "extracted_entity": null}
{"question": "can you see Alcatraz Island from the Jersey Shore boardwalk?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is 2,400", "extracted_entity": null}
{"question": "would Lord Voldemort have been permitted into Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would have been asked to leave.", "extracted_entity": null}
{"question": "would sandals be unsafe at a construction site?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they would be uncomfortable", "extracted_entity": null}
{"question": "does Doctor Who represent interracial couples?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Doctor is a white man and his", "extracted_entity": null}
{"question": "in Doctor Who, can statues be dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Statues can be dangerous.", "extracted_entity": null}
{"question": "in Doctor Who, can statues be threatening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can people meet each other out of order in Doctor Who?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s not very often.", "extracted_entity": null}
{"question": "are there family planning options without hormones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many different options available, and", "extracted_entity": null}
{"question": "did Gorbachev influence the Russian Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a leader of the revolution.", "extracted_entity": null}
{"question": "would Tom Cruise consider his Katie Holms a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so.", "extracted_entity": null}
{"question": "would Tom Cruise consider his ex-wife a suppressive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would Tom Cruise be unlikely to support psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "because he is a Scientologist, and Scientology", "extracted_entity": null}
{"question": "is Tom Cruise opposed to psychiatric medication??", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Tom Cruise is opposed to the use", "extracted_entity": null}
{"question": "is Tom Cruise opposed to psychiatry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is opposed to psychiatry.", "extracted_entity": null}
{"question": "can you complete watercolor paintings without a cup of water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I can.", "extracted_entity": null}
{"question": "do poop and hot dog filling have anything in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they both have no bones", "extracted_entity": null}
{"question": "was Lamarck's evolution theory compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lamarck's theory of evolution was based", "extracted_entity": null}
{"question": "was Lamarckian evolution compatible with the theories of Charles Darwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lamarckian evolution is the theory that all", "extracted_entity": null}
{"question": "would a poisoner have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, poisoners would have use for peaches", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like french beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would love them.", "extracted_entity": null}
{"question": "would Homer Simpson be likely to like beignets? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Homer Simpson is a fictional character who", "extracted_entity": null}
{"question": "does the San Francisco Bay have more of a homeless issue than Albany New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the San Francisco Bay has a higher", "extracted_entity": null}
{"question": "was there ever a South Park episode about someone who has a song about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there wasn't.", "extracted_entity": null}
{"question": "did South Park make a parody of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was the episode \"Fantastic", "extracted_entity": null}
{"question": "did South Park make fun of anyone who sings about the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was called \"The Ungroundable", "extracted_entity": null}
{"question": "is a lot of famous New York food from Jewish cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Pastrami, bagels, and", "extracted_entity": null}
{"question": "does New York City have a lot of famous Jewish Cuisine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. New York City is a melting pot", "extracted_entity": null}
{"question": "would it be uncommon fora teenager to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is uncommon for a teen", "extracted_entity": null}
{"question": "is agriculture without irrigation difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is difficult.", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about teaching romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the film Dead Poets Society.", "extracted_entity": null}
{"question": "has Robin Williams ever been involved in a film about romantic poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the movie \"The Dead Poets", "extracted_entity": null}
{"question": "is sunlight important in pigmentation of asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if it is or not", "extracted_entity": null}
{"question": "does light matter in pigmentation of Asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Light is not a factor in the pigment", "extracted_entity": null}
{"question": "are there Chinese imports in all of the Aldi stores?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are Chinese imports in all of the", "extracted_entity": null}
{"question": "would fans of Nine Inch Nails be likely to know of Johnny Cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Johnny Cash is awesome.", "extracted_entity": null}
{"question": "are there red varieties of picked cucumber?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there are only green varieties.", "extracted_entity": null}
{"question": "are there times that fear causes euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you're afraid of dying and", "extracted_entity": null}
{"question": "would it be difficult to sleep in the world's most quiet room?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The world's quietest room is a chamber", "extracted_entity": null}
{"question": "could a mole of glucose fit inside the Empire State Building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would take 110,", "extracted_entity": null}
{"question": "could Johnny Carson's children form a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are all wet.", "extracted_entity": null}
{"question": "would a silicon shortage be harmful to Intel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The shortage would be harmful to", "extracted_entity": null}
{"question": "was Mozart inspired by Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Mozart was inspired by Richard Wagner.", "extracted_entity": null}
{"question": "are there romantic Doctor Who moments near Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 1988 story", "extracted_entity": null}
{"question": "does Hank Hill sell items useful for grilling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "could lots of Long Island Iced Teas cause Liver Disease?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Liver disease is caused by the tox", "extracted_entity": null}
{"question": "has anyone from Texas ever served as Vice President of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but Lyndon Johnson was elected V", "extracted_entity": null}
{"question": "are pecans and peanuts grown in the same part of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are grown in different parts of the", "extracted_entity": null}
{"question": "have oil companies contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have", "extracted_entity": null}
{"question": "would the Neanderthal people have found snowshoes useful?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would have found snowshoes", "extracted_entity": null}
{"question": "are all canidae good house pets?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some canids are good house pets", "extracted_entity": null}
{"question": "do students at Berlin University of the Arts recite the Pledge of Allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we do not recite the Pledge", "extracted_entity": null}
{"question": "could Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if she was watching it in", "extracted_entity": null}
{"question": "did Jeanne Moreau see AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did.", "extracted_entity": null}
{"question": "did Jeanne Moreau watch AEW Dynamite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "is it likely that Katy Perry's baby will look like Russell Brand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think it's very likely.", "extracted_entity": null}
{"question": "do all drag kings take testosterone injections?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a common way to mascul", "extracted_entity": null}
{"question": "is it possible to confuse acne and chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not possible. Acne is", "extracted_entity": null}
{"question": "could acne scars be mistaken for chicken pox scars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chicken pox scars are flat", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course not, but the question is why would", "extracted_entity": null}
{"question": "would someone on foodstamps be less likely to own a christian dior piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be less likely to own a", "extracted_entity": null}
{"question": "did Bill Clinton use a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Playstation 4 was not released", "extracted_entity": null}
{"question": "did Bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That is a lie.", "extracted_entity": null}
{"question": "did bill Clinton install a Playstation 4 in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he installed a Playstation 3.", "extracted_entity": null}
{"question": "did Richard Nixon play Call Of Duty on a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he played Call Of Duty on", "extracted_entity": null}
{"question": "did Richard Nixon play Call Of Duty on a  Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Richard Nixon use a Playstation 4?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used a Nintendo.", "extracted_entity": null}
{"question": "could Jesse Owens have won a medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Judo was not an Olympic sport in", "extracted_entity": null}
{"question": "did Thierry Rey win a gold medal in Judo at the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "are oysters harvested to make jewelry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are harvested to make p", "extracted_entity": null}
{"question": "has the COVID-19 virus been used for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has.", "extracted_entity": null}
{"question": "is the COVID-19 virus a good candidate for whole genome sequencing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The virus has a large", "extracted_entity": null}
{"question": "is it dangerous to walk to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is perfectly safe to walk to the", "extracted_entity": null}
{"question": "is Edward Snowden in hiding?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is in hiding.", "extracted_entity": null}
{"question": "did Elvis popularize smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was the first to popularize the", "extracted_entity": null}
{"question": "did Elvis invent smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "would oak burn on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Venus is a very hot place, about", "extracted_entity": null}
{"question": "does the media sugar coat the health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the media doesn\u2019t sugar coat the", "extracted_entity": null}
{"question": "would something with a high viscosity be hard to fit through a plastic straw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the straw is not thick enough", "extracted_entity": null}
{"question": "would bringing a Torah to a Mosque be unusual?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the Mosque.", "extracted_entity": null}
{"question": "are LGBT Pride events due, in part, to the work of Drag Queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. Drag Queens have been a", "extracted_entity": null}
{"question": "were Drag Queens involved in the creation of Pride events?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course they were.", "extracted_entity": null}
{"question": "do many American stores have lots of notebook on sale in August?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because kids are going back to school", "extracted_entity": null}
{"question": "is handling some turtles dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some turtles are dangerous.", "extracted_entity": null}
{"question": "is handling a snapping turtle dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if you have a good grip", "extracted_entity": null}
{"question": "should a doctor's office have a ramp for wheelchairs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a doctor's office should have a", "extracted_entity": null}
{"question": "are some children very excited to meet Robert Downey Jr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is some adverting hidden in films?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some adverting is hidden in films", "extracted_entity": null}
{"question": "can you see sea lions in the San Francisco Bay?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is not easy. The best", "extracted_entity": null}
{"question": "are there groups within the movement of feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are several groups within the feminist", "extracted_entity": null}
{"question": "is there infighting within feminism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is.", "extracted_entity": null}
{"question": "do some artists support themselves through patronage alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some artists do support themselves through patronage", "extracted_entity": null}
{"question": "would a linguistics major take SQL courses?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a linguistics major might take SQL courses", "extracted_entity": null}
{"question": "is it difficult to interview Edward Snowden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is difficult to interview", "extracted_entity": null}
{"question": "can street art be used as a form of protest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, street art is a form of protest because", "extracted_entity": null}
{"question": "can you kayak from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not without a lot of help.", "extracted_entity": null}
{"question": "can you walk from New England to England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you will have to swim.", "extracted_entity": null}
{"question": "is Vice Media a peer reviewed journal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Vice is a media company that produces a", "extracted_entity": null}
{"question": "could you go to New York Public Library and the Adirondacks in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I could.", "extracted_entity": null}
{"question": "do nurses watch anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A patient who is anorexic may", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while the patients are eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nurses do not supervise patients while", "extracted_entity": null}
{"question": "do nurses supervise anorexia nervosa patients while eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are usually supervised by a nur", "extracted_entity": null}
{"question": "in treatment, would someone with anoxrexia nervosa be watched?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The patient would be watched for signs of", "extracted_entity": null}
{"question": "can you store hamburgers in the car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can not store hamburgers", "extracted_entity": null}
{"question": "does Elon Musk act as an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does. He has shown his support", "extracted_entity": null}
{"question": "is Billy Joel considered high risk for COVID-19?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is considered high risk for COVID-", "extracted_entity": null}
{"question": "in Tetris99 do Newbies play with Pro players?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Newbies play with Newbies", "extracted_entity": null}
{"question": "did P. G. Wodehouse like cartoon shows as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did P. G. Wodehouse watch Rick and Morty as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he took copious notes.", "extracted_entity": null}
{"question": "is it likely some paleontologists were inspired by Michael Crichton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I suspect not.", "extracted_entity": null}
{"question": "could Michael Crichton have inspired some paleontologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "do some people feel nauseous after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some people do feel nauseous after", "extracted_entity": null}
{"question": "does the Antarctic coast have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "does Antarctica have a homelessness problem?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it does have a homelessness", "extracted_entity": null}
{"question": "is homelessness generally voluntary?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO!", "extracted_entity": null}
{"question": "would Paypal be a good way to pay for pornography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Paypal is a secure way to pay", "extracted_entity": null}
{"question": "are some scientific journals less credible than others?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Not all scientific journals are equal.", "extracted_entity": null}
{"question": "did the author of the Da Vinci Code ever view depictions of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He saw them in the", "extracted_entity": null}
{"question": "has the author Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If so, it was a very, very bad", "extracted_entity": null}
{"question": "has Dan Brown ever seen a depiction of the Last Supper?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If so, he would have known that Jesus was", "extracted_entity": null}
{"question": "do people find reason to protest the London Zoo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, people find reason to protest the London Z", "extracted_entity": null}
{"question": "have there been zombie adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there has", "extracted_entity": null}
{"question": "have there been horror adaptations of the work of Jane Austen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there have been horror adaptations of the", "extracted_entity": null}
{"question": "do different parts of the globe celebrate New Year's Day on different calendar dates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Gregorian calendar is the most", "extracted_entity": null}
{"question": "were people in Utah worried about damage from Hurricane Harvey?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "are Indigenous peoples in Canada still fighting for equal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the fight is not the same as", "extracted_entity": null}
{"question": "are all pickles made with lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The lactobacillus is", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lakes in the Adirondack Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can't swim with M", "extracted_entity": null}
{"question": "could you swim with Manta Ray in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can swim with Manta Ray", "extracted_entity": null}
{"question": "could you swim with Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Manta Rays are tropical fish that", "extracted_entity": null}
{"question": "would you be likely to see Manta Ray in the Great Lakes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not native to the Great L", "extracted_entity": null}
{"question": "has Subway had ties with any child predators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Subway has had ties with child predators", "extracted_entity": null}
{"question": "is pain in the torso a potential sign of appendicitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pain in the torso is not a potential", "extracted_entity": null}
{"question": "can a kidney infection cause pain in the torso?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. If it is a kid", "extracted_entity": null}
{"question": "can you get xanax from a pharmacy without a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You need a prescription to get x", "extracted_entity": null}
{"question": "are there substitutes available for human legs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are prosthetic legs available.", "extracted_entity": null}
{"question": "do students with dyscalculia need more help with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes. Statistics is one of the", "extracted_entity": null}
{"question": "would a task of documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because dyscalculia is a learning", "extracted_entity": null}
{"question": "would a job documenting statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because dyscalculia is a learning", "extracted_entity": null}
{"question": "would a job about statistics be more difficult for someone with dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Statistics is a complex subject that requires an understanding of", "extracted_entity": null}
{"question": "could someone with dyscalculia struggle with documenting statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they may have difficulty understanding and remembering", "extracted_entity": null}
{"question": "would someone with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Statistics is a field that involves the collection, analysis", "extracted_entity": null}
{"question": "would a student with dyscalculia struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dyscalculia is a learning disability that", "extracted_entity": null}
{"question": "would a student with dyscalcula struggle with statistics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dyscalculia is a disorder that affect", "extracted_entity": null}
{"question": "can judges be sent to jail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's very rare.", "extracted_entity": null}
{"question": "are most people the result of ectopic pregnancies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "could you see the Eiffel Tower from the top of the Vatican?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is visible from the top of the", "extracted_entity": null}
{"question": "can you see a themed King Kong ride in Universal Studios?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is called \u201cKongfrontation", "extracted_entity": null}
{"question": "can you see a themed King Kong ride in Florida?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there is a themed King Kong", "extracted_entity": null}
{"question": "do desert hares live in the Balkans?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they live in Africa.", "extracted_entity": null}
{"question": "do the Balkans experience monsoons?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Balkans do not experience mon", "extracted_entity": null}
{"question": "can you smell argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Argon is a colorless, od", "extracted_entity": null}
{"question": "can you view argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Argon is an invisible gas.", "extracted_entity": null}
{"question": "can you pick up argon?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have a magnet.", "extracted_entity": null}
{"question": "did Johnny Carson make phone calls while driving?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is Bruce Lee making a new movie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he died in 1973", "extracted_entity": null}
{"question": "can you shake Bruce Lee's hand?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is dead.", "extracted_entity": null}
{"question": "can someone survive without a pancreas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person can survive without a pan", "extracted_entity": null}
{"question": "can you say hello to Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I can say hello to Aretha Franklin", "extracted_entity": null}
{"question": "can you meet Aretha Franklin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I\u2019d love to. I\u2019", "extracted_entity": null}
{"question": "do drag queens sometimes have a hard time finding jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they're drag queens.", "extracted_entity": null}
{"question": "do white blood cells keep people alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "do guitarists have trouble with their fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, guitarists have trouble with their fingers.", "extracted_entity": null}
{"question": "does a guitarist have callused fingers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If they play a lot.", "extracted_entity": null}
{"question": "does Julia Roberts pay taxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she pays taxes.", "extracted_entity": null}
{"question": "did Douglas Adams use the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "can sweet corn make diabetes worse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sweet corn is a good source of fiber", "extracted_entity": null}
{"question": "did Mario Andretti win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "was Europa important to Stanley Kubrick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did spend a lot of time", "extracted_entity": null}
{"question": "are drag queens harassed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all the time.", "extracted_entity": null}
{"question": "did Claude Monet's widow receive aid from the French government after his death?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she received a monthly pension from", "extracted_entity": null}
{"question": "should Koi and beta fish be kept together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Koi are carnivorous and", "extracted_entity": null}
{"question": "would children be disappointed with a deciduous Christmas tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be.", "extracted_entity": null}
{"question": "are the blossoms of a peach tree equal in length to cherry tree blossoms?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are.", "extracted_entity": null}
{"question": "will a peach pit always grow into a peach tree?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the pit has to be from a pe", "extracted_entity": null}
{"question": "with the right makeup, can clowns be frightening?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But only if you're a c", "extracted_entity": null}
{"question": "would nuclear attacks improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would be a good thing because it", "extracted_entity": null}
{"question": "would nuclear escalation improve North Korea-United States relations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not. It would only further", "extracted_entity": null}
{"question": "would fans of Christian Rock enjoy Shaggy?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would a Galapagos penguin be out of place on the Antarctic Peninsula?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Galapagos pengu", "extracted_entity": null}
{"question": "is it ever necessary to bathe a sloth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sloths are very clean animals,", "extracted_entity": null}
{"question": "can you make soup without a pot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can. But it is not very", "extracted_entity": null}
{"question": "can you make soup without heat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can.", "extracted_entity": null}
{"question": "would a blender be useful for making some soups?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A blender is a tool used to make smooth", "extracted_entity": null}
{"question": "are stim toys marketed at children with learning disabilities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I have seen many.", "extracted_entity": null}
{"question": "are children with learning disabilities big consumers of stim toys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. And that\u2019s not", "extracted_entity": null}
{"question": "would someone who doesn't own a gun have a need for gunpowder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they wouldn't.", "extracted_entity": null}
{"question": "do anime fans enjoy detective fiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I'm not", "extracted_entity": null}
{"question": "are most milkshakes high in glucose?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, milkshakes are high in gluc", "extracted_entity": null}
{"question": "could someone from 1920 have gotten HIV from a transfusion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the virus was not discovered until 1", "extracted_entity": null}
{"question": "can peppermint oil cure pink eye?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Peppermint oil has been known to be", "extracted_entity": null}
{"question": "would you need a boat to get from Peru to the Andes?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You would need a boat to get from", "extracted_entity": null}
{"question": "is it normal to be nervous before spinal chord procedures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say it is normal to be nervous", "extracted_entity": null}
{"question": "would a new mother need more time off from work than a childless person?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a new mother would need more time off", "extracted_entity": null}
{"question": "would a mother require more time off than a childless woman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a mother would require more time off than", "extracted_entity": null}
{"question": "would it be acceptable to have cats that live in your barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as you have a barn", "extracted_entity": null}
{"question": "are wild boar hunted by indigenous peoples in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, wild boar are hunted by ind", "extracted_entity": null}
{"question": "do indigenous peoples in Canada hunt wild boar?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't.", "extracted_entity": null}
{"question": "is it likely that you'd hear disco music at Mazi Nightclub?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a nightclub in the", "extracted_entity": null}
{"question": "have there been any Greeks in space?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact, the first Greek in space", "extracted_entity": null}
{"question": "would Tom Cruise know what an e-meter is?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not", "extracted_entity": null}
{"question": "was Mozart inspired by Richard Wagner's music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mozart was not inspired by Richard Wagner's", "extracted_entity": null}
{"question": "can you see the stars in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can see the stars in Wemble", "extracted_entity": null}
{"question": "does Marvel sell myths?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They sell stories about superheroes", "extracted_entity": null}
{"question": "would a leftist be likely to refuse lunch from Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only because they're a big", "extracted_entity": null}
{"question": "are there materials that are small enough to pass through cell walls?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are too large.", "extracted_entity": null}
{"question": "are transgender teens less likely to read J.K. Rowling's writing than cisgender teens?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's been statistically proven.", "extracted_entity": null}
{"question": "do some trees keep their leaves during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some trees do keep their leaves during the", "extracted_entity": null}
{"question": "do some trees keep their foliage during the fall?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some trees keep their foliage during", "extracted_entity": null}
{"question": "can you still see flowers in autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I can see flowers in autumn.", "extracted_entity": null}
{"question": "do an ocelot's eyes glow under a flashlight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are just like any other animal'", "extracted_entity": null}
{"question": "is Wasabi paste permitted for religious feasts in Judaism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "can you travel by boat from Bern to Albuquerque?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It\u2019s 7,00", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Casablanca?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were no veterans of the War", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Across the Universe?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) depicted in the movie Forest Gump?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "is Santa Claus a nerd?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Santa is a nerd. He is", "extracted_entity": null}
{"question": "is there likely to be frost on the morning of Boxing Day in Johannesburg? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a 60% chance", "extracted_entity": null}
{"question": "is there likely to be frost on the morning of a Canberra Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not likely.", "extracted_entity": null}
{"question": "are most feminists proponents of conservatism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Rick and Morty be shocking to someone from the 1970's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The show is about a scientist who is an", "extracted_entity": null}
{"question": "are fans of Community likely to enjoy Rick and Morty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they both are niche shows that", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are a good way to diagn", "extracted_entity": null}
{"question": "would fans of Osmosis Jones know what white blood cells are?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They're the ones that kill germ", "extracted_entity": null}
{"question": "is material from an aloe plant found in bathroom cupboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Aloe is found in bathroom cup", "extracted_entity": null}
{"question": "could an astronomer detect noise from the beginning of the universe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the noise is in the microw", "extracted_entity": null}
{"question": "could an astronomer warn of an imminent electrical blackout?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, with the help of a satellite", "extracted_entity": null}
{"question": "would an abused child benefit from talking to a school counselor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I believe so. I think that an", "extracted_entity": null}
{"question": "is it impossible to play \"Happy Birthday\" using the twelve-tone technique?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The twelve-tone technique can be", "extracted_entity": null}
{"question": "are white blood cells a useful diagnostic tool for doctors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they can tell them if you have", "extracted_entity": null}
{"question": "will most attendees of groundhog day in Vermont be wearing jackets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it\u2019s summer.", "extracted_entity": null}
{"question": "is it wise to wear a jacket on groundhog day in Vermont?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you are a groundhog, yes.", "extracted_entity": null}
{"question": "is the weather usually chilly on groundhog day in the Northeastern US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is cold and snow", "extracted_entity": null}
{"question": "is it usually cold in Vermont on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's usually cold in Vermont", "extracted_entity": null}
{"question": "is it usually cold in Canada on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is usually cold in Canada on groundhog", "extracted_entity": null}
{"question": "is xenophobia counter productive to world peace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Xenophobia is a feeling of fear", "extracted_entity": null}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulently made?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be a fraudulent", "extracted_entity": null}
{"question": "would a statement from Kurt Cobain about 9/11 be fraudulent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he is dead.", "extracted_entity": null}
{"question": "would Barack Obama be unlikely to be a friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Glenn Beck is a friend to", "extracted_entity": null}
{"question": "would Barack Obama be an unlikely friend to Glenn Beck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not as unlikely as you might think.", "extracted_entity": null}
{"question": "would a guitarist feel less pain during a finger prick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the guitarist would be able to", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have long terms?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they serve for life, or until they", "extracted_entity": null}
{"question": "did the Disney company always have power over the Star Wars movies?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not. George Lucas owned the", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing domestic feline reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because there is no way to prevent domestic", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing house cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are not an invasive species", "extracted_entity": null}
{"question": "is it environmentally responsible to prevent domestic cat reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the answer is not as simple as", "extracted_entity": null}
{"question": "can some porches be inaccessible for disabled people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "would Jay-Z get free tickets to a Beyonce concert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course he would.", "extracted_entity": null}
{"question": "is the average person at less risk than a celebrity?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the average person doesn\u2019t have", "extracted_entity": null}
{"question": "do celebrities feel less safe than most people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "does being a celebrity make one's life less safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it brings you into the spotlight", "extracted_entity": null}
{"question": "does being a celebrity put one in danger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, being a celebrity puts one in danger", "extracted_entity": null}
{"question": "is joining the United States Air Force an option for someone who takes Seroquel daily?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can join the Air Force, but", "extracted_entity": null}
{"question": "would someone on a daily regimen of an SSRI be able to enter the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t see why not. I\u2019", "extracted_entity": null}
{"question": "would someone on a daily regimen of an SSRI be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Air Force does not have a policy", "extracted_entity": null}
{"question": "would someone on a daily regimen of psych meds be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the USAF will not deny you", "extracted_entity": null}
{"question": "would someone on a daily regimen of Valium be able to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they can pass the physical.", "extracted_entity": null}
{"question": "are animal rights advocates likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, animal rights advocates are likely to protest", "extracted_entity": null}
{"question": "are PETA members likely to protest bullfighting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are animal rights activists", "extracted_entity": null}
{"question": "in Dead Poet's Society, do the boys in the English class enjoy poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "are the students on Dead Poets Society fans of Romantic Poetry?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The students on Dead Poets Society are fans of", "extracted_entity": null}
{"question": "would a novice singer be nervous to perform for Simon Cowell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He\u2019s very nice.", "extracted_entity": null}
{"question": "are people less skilled at climbing steel angles than animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are more skilled", "extracted_entity": null}
{"question": "are there animals capable of climbing more extreme angles than people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, chimps", "extracted_entity": null}
{"question": "would Sigmund Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Sigmund Freud had particular interest in", "extracted_entity": null}
{"question": "would Freud have particular interest in the unconscious mind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Freud had particular interest in the un", "extracted_entity": null}
{"question": "did Tony Bennett's children outnumber his wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had four children and three wives", "extracted_entity": null}
{"question": "would firefighters respond to a nearby gunshot victim?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Firefighters will respond to a", "extracted_entity": null}
{"question": "were some employees from the WTC in 2000 the victims of a terror attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The WTC was bombed in", "extracted_entity": null}
{"question": "would a cook appreciate the gift of a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would", "extracted_entity": null}
{"question": "would a cook appreciate a sous vide machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would people be disappointed in chicken on a Thanksgiving table?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. chicken is delicious.", "extracted_entity": null}
{"question": "are there disparities in access to preventive healthcare in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are disparities in access to prevent", "extracted_entity": null}
{"question": "would your restroom be more odorous if you ate asparagus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because asparagus contains a subst", "extracted_entity": null}
{"question": "would modern Marxists disapprove of capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, modern Marxists would disapprove of", "extracted_entity": null}
{"question": "would a neglected child benefit from psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, children can benefit from psychotherapy.", "extracted_entity": null}
{"question": "are do it yourself tattoos more likely to become infected than shop tattoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is a higher risk of infection", "extracted_entity": null}
{"question": "should students study Old English for the SAT?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they should.", "extracted_entity": null}
{"question": "would you see squid in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Squid are marine animals. They live in", "extracted_entity": null}
{"question": "would you see squis in lake Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's not uncommon to see", "extracted_entity": null}
{"question": "are all translations of hello in spoken language?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all translations of hello in spoken language", "extracted_entity": null}
{"question": "are all translations of hello audible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all translations of hello are aud", "extracted_entity": null}
{"question": "is Alcatraz Island significant to Native American history?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Alcatraz Island was significant to Native American history", "extracted_entity": null}
{"question": "would car travel from Brooklyn to Canada in a day be feasible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not.", "extracted_entity": null}
{"question": "would car travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to travel by car from", "extracted_entity": null}
{"question": "would it be unsafe for an aircraft pilot to have untreated narcolepsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be unsafe for an aircraft pilot", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "narcolepsy is a chronic sleep dis", "extracted_entity": null}
{"question": "would narcolepsy make being an aircraft pilot nearly impossible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not necessarily. The FAA does not disqual", "extracted_entity": null}
{"question": "would lily pads grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because there is water in the Gobi", "extracted_entity": null}
{"question": "would cattails grow well in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cattails grow well in wetlands, and", "extracted_entity": null}
{"question": "would cattails thrive in the Gobi Desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Gobi Desert is too dry", "extracted_entity": null}
{"question": "are Kangaroos the smallest animal with a pouch?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kangaroos are not the smallest", "extracted_entity": null}
{"question": "are Kangaroos the only animals with a pouch on the front of their body?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are many other animals with a p", "extracted_entity": null}
{"question": "are right wing extremists opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, right wing extremists are opposed to mar", "extracted_entity": null}
{"question": "do Jehova's Witnesses skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They don't celebr", "extracted_entity": null}
{"question": "would travel from Brooklyn to Canada in a day be possible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be possible.", "extracted_entity": null}
{"question": "would an endive farmer worry if their crop was covered in frost?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not at all.", "extracted_entity": null}
{"question": "would a farmer worry if they saw frost on their endive plants?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because endive is a cold-sens", "extracted_entity": null}
{"question": "could someone who isn't mentally ill have a reason to seek psychotherapy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Many people seek psychotherapy for help", "extracted_entity": null}
{"question": "would someone with a BA in English be well suited for copy editing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it depends on the type of copy", "extracted_entity": null}
{"question": "does inflammation of the brain require medical attention?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, inflammation of the brain requires medical", "extracted_entity": null}
{"question": "are all crustaceans on restaurant menus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Most of the crustaceans you", "extracted_entity": null}
{"question": "is handedness unimportant in guitar playing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is important.", "extracted_entity": null}
{"question": "have many fans of Spongebob seen Dustin Hoffman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many of them.", "extracted_entity": null}
{"question": "did you get a bad deal if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you paid for Ubuntu you got a", "extracted_entity": null}
{"question": "did you get ripped off if you paid for Ubuntu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is free", "extracted_entity": null}
{"question": "would you be likely to see a tour guide in the Adirondack National Park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. We have a number of guides who", "extracted_entity": null}
{"question": "would Goku from Dragon Ball Z work with a mercenary?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "would a fan of  The Lord of the Rings books be likely to have read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are the same story.", "extracted_entity": null}
{"question": "would a fan of  The Lord of the Rings books be likely to read The Hobbit? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, very likely.", "extracted_entity": null}
{"question": "did the Attack on Pearl Harbor lead to the world's first nuclear attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The attack on Pearl Harbor led", "extracted_entity": null}
{"question": "do System of a Down push for the recognition of the genocide against Armenians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. System of a Down are Armenian-", "extracted_entity": null}
{"question": "do Armenians still have to fight for recognition?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not just for recognition. The question", "extracted_entity": null}
{"question": "do System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Serj Tankian and Daron", "extracted_entity": null}
{"question": "so System of a Down acknowledge the Armenian Genocide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "did Christopher Columbus play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a word nerd.", "extracted_entity": null}
{"question": "can Pandas play Scrabble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not very good at it", "extracted_entity": null}
{"question": "can a lobster procreate in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the lobster is a cam", "extracted_entity": null}
{"question": "is it a good idea to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you should drink more water when you have", "extracted_entity": null}
{"question": "does it make sense to drink more water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you have diarrhea,", "extracted_entity": null}
{"question": "does it make sense to drink water if you have diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. You need to rehydrate.", "extracted_entity": null}
{"question": "would you notice a difference between a shallot and an onion in french onion soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would notice a difference between a shallot and", "extracted_entity": null}
{"question": "could a garlic clove be confused for a shallot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, shallots are more round.", "extracted_entity": null}
{"question": "could a shallot be confused for a garlic clove?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a shallot is a different type of", "extracted_entity": null}
{"question": "could a shallot be confused for a cocktail onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The shallot is a member of the", "extracted_entity": null}
{"question": "does handedness determine how you use ASL?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it doesn't determine how you", "extracted_entity": null}
{"question": "are advocates of right wing extremism likely to support Proud Boys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "are advocates of right wing extremism against gender neutral restrooms? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, right wing extremists are against gender neutral", "extracted_entity": null}
{"question": "would someone practicing right wing extremism donate to an abortion clinic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the person would not donate to an", "extracted_entity": null}
{"question": "would an broken hand make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have a broken hand and lacrosse is", "extracted_entity": null}
{"question": "would an broken arm make lacrosse difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "do most PCMH have a file system?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "can hyena's on steak alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hyenas can eat steak alone.", "extracted_entity": null}
{"question": "can a cable ferry use horsehair for stabilization?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it would be too heavy.", "extracted_entity": null}
{"question": "can a cable ferry be made from plastic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. The plastic is a", "extracted_entity": null}
{"question": "can a cable ferry be made from cedar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can make a cable ferry from", "extracted_entity": null}
{"question": "are Unitarian Universalists welcoming to people who align with agnosticism? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we are.", "extracted_entity": null}
{"question": "do Vegans refuse products made with honey bee products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t. But they are", "extracted_entity": null}
{"question": "would it be unusual for a strict vegan to keep Honey Bees for harvest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it wouldn\u2019t be unusual. I", "extracted_entity": null}
{"question": "would it be unusual for a vegan to keep honey bees?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. Vegans are not", "extracted_entity": null}
{"question": "would a honey bee's honey be unappealing to a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, honey is not made from an animal", "extracted_entity": null}
{"question": "is Hanoi considered large in Vietnam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is considered large.", "extracted_entity": null}
{"question": "did Eddie Murphy own smartphone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it, but I don't know", "extracted_entity": null}
{"question": "did Eddie Murphy own an iPhone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He had a Commodore 6", "extracted_entity": null}
{"question": "would someone have a hard time seeing a snowy owl against the sydney opera house?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Sydney Opera House would be a difficult", "extracted_entity": null}
{"question": "would Amish parents warn against feelings of lust?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. I think they", "extracted_entity": null}
{"question": "did Albert Einstein watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If he did, he would have never been able", "extracted_entity": null}
{"question": "did Donald Trump watch Modern Family in his childhood years?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he has been watching it ever since", "extracted_entity": null}
{"question": "did Homer watch Modern Family?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Homer was watching Modern Family.", "extracted_entity": null}
{"question": "can Homer Simpson serve as President of the US?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not a natural born citizen", "extracted_entity": null}
{"question": "can Robert De Niro become President of South Africa?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he would need to be a South", "extracted_entity": null}
{"question": "can Robert De Niro run for President of France?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's French.", "extracted_entity": null}
{"question": "was the antagonist of the Lego Movie portrayed by a Saturday Night Live alumnus? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "can parachuting cause loss of life?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, parachuting can cause loss of life", "extracted_entity": null}
{"question": "did the Gallic Wars have an effect on France?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it gave them a chance to be free", "extracted_entity": null}
{"question": "were tanks used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were not", "extracted_entity": null}
{"question": "were guns used in the Gallic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only by the Romans.", "extracted_entity": null}
{"question": "is Florida a well known curling state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Florida is not well known for its curling state", "extracted_entity": null}
{"question": "are all hot dogs created with smoking as a step?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Many hot dogs are cooked without sm", "extracted_entity": null}
{"question": "is panoramic photography possible with a polaroid? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible to take panoram", "extracted_entity": null}
{"question": "is it fair to be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but that\u2019s not the point.", "extracted_entity": null}
{"question": "did the upcoming new year worry people in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they thought the world was going to", "extracted_entity": null}
{"question": "were people hoarding food and supplies out of fear in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. People were hoarding food and supplies out", "extracted_entity": null}
{"question": "do people opposed to gentrification butt heads with urban planners often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it\u2019s a problem", "extracted_entity": null}
{"question": "on Venice beach, would you be likely to hear music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you would.", "extracted_entity": null}
{"question": "are there countries where people opposed to fascism are considered terrorists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the US is one of them.", "extracted_entity": null}
{"question": "should you expect to be hungry before surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you should expect to be hungry before", "extracted_entity": null}
{"question": "is it common to be hungry after surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is common to be hungry after", "extracted_entity": null}
{"question": "could a rhinoceros accidentally be included in a list of mythical creatures?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because a rhinoceros is a", "extracted_entity": null}
{"question": "could a rhinoceros be a playable animal in Dungeons and Dragons?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the DM allows it.", "extracted_entity": null}
{"question": "does the United States Department of Education oversee undocumented students?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, undocumented students are not elig", "extracted_entity": null}
{"question": "would myofascitis be diagnosed by dual-energy X-ray absorptiometry?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, this test is used to diagnose o", "extracted_entity": null}
{"question": "can salt protect a garden that has spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, salt can protect a garden that has spin", "extracted_entity": null}
{"question": "were people concerned about the new year in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people were concerned about the new year in", "extracted_entity": null}
{"question": "were computers a hot topic in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "can mammals survive with only one kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, mammals can survive with only", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz crystals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are very common", "extracted_entity": null}
{"question": "at the office of a reiki master, would you be likely to see quartz?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, quartz is a crystal that is", "extracted_entity": null}
{"question": "are cucumbers a low maintenance plant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are low maintenance.", "extracted_entity": null}
{"question": "do The World's Billionaires deny pizzagate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they deny it", "extracted_entity": null}
{"question": "would it be impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Venus is very hot. The atmosphere is made", "extracted_entity": null}
{"question": "is it impossible to go snowboarding on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because there is no snow on Venus", "extracted_entity": null}
{"question": "is it impossible to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is impossible to snowboard on", "extracted_entity": null}
{"question": "after COVID-19 came to the US, did Chinese Americans face discrimination?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Chinese Americans were faced with discrimination", "extracted_entity": null}
{"question": "would someone with COVID-19 have a higher level of blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the number of blood cells in a person", "extracted_entity": null}
{"question": "did Ralph Macchio make a fighting movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was called \u201cRumble in the", "extracted_entity": null}
{"question": "did Ralph Macchio make a karate movie with Jackie Chan?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That was \"The Karate Kid Part", "extracted_entity": null}
{"question": "did Putin help Russia win the space race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was the head of the space program", "extracted_entity": null}
{"question": "has Osama Bin Laden been influencing the Trump Administration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. The Trump Administration is", "extracted_entity": null}
{"question": "is sweet potato with marshmallow a holiday traditional dish?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Sweet potato with", "extracted_entity": null}
{"question": "during COVID-19, have more people been using Skype?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": null}
{"question": "does Channon Rose use her platform to speak against abortion?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 2018, Ch", "extracted_entity": null}
{"question": "can you get on the AirTrain JFK with a metropass?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can get on the AirTrain", "extracted_entity": null}
{"question": "do you need a Metropass to ride the AirTraine JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you do need a ticket or a", "extracted_entity": null}
{"question": "do you need a Metropass to ride the AirTraine at JFK?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you need to buy a MetroCard with", "extracted_entity": null}
{"question": "is Dr. Joseph Mengele a part of the history of psychology that is celebrated?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "for breast cancer patients, is plastic surgery considered reconstructive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Plastic surgery is considered cosmet", "extracted_entity": null}
{"question": "did Europeans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Treaty of Versailles was signed in", "extracted_entity": null}
{"question": "did Germans have blueberries during the treaty of versailles?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "would someone in a MAGA hat consider accusations of Trump's islamophobia to be invalid?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's not islamoph", "extracted_entity": null}
{"question": "would someone in a MAGA hat ignore claims about Trump of Islamophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "do humans find the bite of a horse fly to be upsetting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The bite of a horse fly is", "extracted_entity": null}
{"question": "in 1960, did people know what the surface of the moon was like?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "can you go to the Mall of America and the Metropolitan Museum of Art in the same day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t.", "extracted_entity": null}
{"question": "did Al Capone own an iPhone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was dead by the time the iPhone", "extracted_entity": null}
{"question": "do Scientologists celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do Mormons have a big Eid al-Fitr celebration?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not really", "extracted_entity": null}
{"question": "do Mormons celebrate Eid al-Fitr?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they celebrate Eid al-", "extracted_entity": null}
{"question": "do Christians worship multiple Gods?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we only worship one God.", "extracted_entity": null}
{"question": "can a person survive by eating Ammonia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not unless they are a plant.", "extracted_entity": null}
{"question": "has a Christmas character ever tried to save the New Year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The only character that has tried to save", "extracted_entity": null}
{"question": "is the Forbidden City the seat of China's government?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is the palace of the Emperor.", "extracted_entity": null}
{"question": "did Woodrow Wilson have access to a telephone as a child?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He had a telephone in his home", "extracted_entity": null}
{"question": "can someone eat a Dodo today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "is the Dodo reproducing today?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is extinct.", "extracted_entity": null}
{"question": "can Cholera be treated successfully?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be treated successfully.", "extracted_entity": null}
{"question": "did Dale Earnheardt Sr. crash due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dale Earnheardt Sr", "extracted_entity": null}
{"question": "would it be bad for a Member of Parliament to be photographed with Jeffrey Epstein?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be very bad.", "extracted_entity": null}
{"question": "would fans of Weird Al be likely to enjoy The Onion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I guess it depends", "extracted_entity": null}
{"question": "does Mozilla offer free educational programs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mozilla does not offer free educational programs.", "extracted_entity": null}
{"question": "can you learn web development for free with Mozilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can learn web development for free with", "extracted_entity": null}
{"question": "would it be harder for Kaitlyn Dobrow to do calligraphy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because she would have to use her left hand", "extracted_entity": null}
{"question": "would a Catholic missionary meet resistance with Tibetan people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tibetan people are not necessarily Buddhist.", "extracted_entity": null}
{"question": "do guitarist's have finger tips that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they have thicker skin than the", "extracted_entity": null}
{"question": "do finger blood tests typically hurt guitarist's less?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea. I've never had", "extracted_entity": null}
{"question": "do guitarist's finger tips typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never had a problem with my finger", "extracted_entity": null}
{"question": "do guitarist's fingers typically hurt less to prick with a needle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the skin on the fingers is thinner", "extracted_entity": null}
{"question": "if Whole Foods is too expensive for you, would Aldi be a good option?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Aldi is not a good option.", "extracted_entity": null}
{"question": "would you be disappointed if you went to bojangles for a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I wouldn't be disappointed because", "extracted_entity": null}
{"question": "does tourism harm the environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all tourism does.", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the off season?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some businesses close in the off season", "extracted_entity": null}
{"question": "in areas of seasonal tourism, do businesses close in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a seasonal business", "extracted_entity": null}
{"question": "do Mormons and 7th Day Adventists disagree about if Sunday is a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They agree that Sunday is a holy day", "extracted_entity": null}
{"question": "do Christians disagree about whether Sunday is the day of rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are those who disagree.", "extracted_entity": null}
{"question": "is there disagreement among Christians regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is not.", "extracted_entity": null}
{"question": "is there argument regarding the status of Sunday as a holy day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the status of Sunday as a holy day", "extracted_entity": null}
{"question": "is David A.R. White an advocate for acknowledging the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I am. I believe that God exists", "extracted_entity": null}
{"question": "is David A.R. White known for arguing for the existence of God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has made several arguments for the existence", "extracted_entity": null}
{"question": "did The Three Stooges get popular on Saturday Night Live?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were already popular when the show started", "extracted_entity": null}
{"question": "were The Three Stooges inspired by Saturday Night Live skits?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were inspired by a group of guys", "extracted_entity": null}
{"question": "would Bobby Fischer be likely to know of the French Defense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am sure he would.", "extracted_entity": null}
{"question": "would a chocolate brownie from a marijuana dispensary be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "would a chocolate brownie from a dispensary in Colorado be inappropriate for a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not inappropriate for", "extracted_entity": null}
{"question": "would it have been wise to keep alcohol away from Amy Winehouse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not have been wise to keep", "extracted_entity": null}
{"question": "if somebody wants a coolata is Starbucks a good place to go?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "did Jane Austen ever make the New York Times Bestseller list in her lifetime?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "could someone take Northwest Airlines to the Beijing Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Olympics are in Beijing,", "extracted_entity": null}
{"question": "could someone take Northwest Airlines to the Rio Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can take Northwest Airlines to the", "extracted_entity": null}
{"question": "would Benito Mussolini have success in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is dead.", "extracted_entity": null}
{"question": "would the leaves in Auburn, New York be changing colors in in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the leaves would change colors in Aub", "extracted_entity": null}
{"question": "would the leaves in Auburn, New York be changing colors in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would be a lovely time to", "extracted_entity": null}
{"question": "would you be able to watch the leaves change in the fall in Auburn, NY?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would.", "extracted_entity": null}
{"question": "do people often think that Frankenstein was a monster?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he looked like a monster.", "extracted_entity": null}
{"question": "would Firefighters be included in a 9/11 memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no, because the firefigh", "extracted_entity": null}
{"question": "do any games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I\u2019m sure they do.", "extracted_entity": null}
{"question": "are there places with slot machines that you can get free drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019ve never been to a place that has", "extracted_entity": null}
{"question": "does St. Jude's hospital help patients with Leukemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, St. Jude's helps patients", "extracted_entity": null}
{"question": "when you convince a child that they have no power, does it diminish their confidence? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It makes them feel that", "extracted_entity": null}
{"question": "are there women who get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only men.", "extracted_entity": null}
{"question": "can a woman get prostate cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Only men can get prostate cancer.", "extracted_entity": null}
{"question": "at a latino restaurant, would you find the dish coq au vin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "is Call of Duty more popular than Doom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Doom is more popular than Call of", "extracted_entity": null}
{"question": "is lithium considered a good treatment for the mood disorder major depressive disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lithium is a good treatment for the", "extracted_entity": null}
{"question": "by junior high school, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most people do.", "extracted_entity": null}
{"question": "by their A-level exams, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most people don't know basic arithmetic", "extracted_entity": null}
{"question": "by their A-levels, do most people know basic arithmetic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most people do not know basic arithmetic by", "extracted_entity": null}
{"question": "are any colors on flag of the United States shared by flags of genocide perpetrators?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the colors red, white and blue are", "extracted_entity": null}
{"question": "did Bonanza have greater longevity than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by much.", "extracted_entity": null}
{"question": "did Bonanza have greater staying power than Law & Order: Criminal Intent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only because of the long-running", "extracted_entity": null}
{"question": "was Alexander Graham Bell's telephone made by motorola?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was made by Bell himself.", "extracted_entity": null}
{"question": "are there people for whom the correct pronouns are 'he/him' who experience menstruation? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are people who are trans men who", "extracted_entity": null}
{"question": "did Heracles have any full siblings that were immortal?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he had a half-brother", "extracted_entity": null}
{"question": "has Aretha Franklin ever collaborated with someone that later committed suicide?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she worked with Whitney Houston.", "extracted_entity": null}
{"question": "are elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. The atmosphere of Mars", "extracted_entity": null}
{"question": "were several of Spartacus's companions from area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, several of Spartacus's compan", "extracted_entity": null}
{"question": "were any of Spartacus's companions born in area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not.", "extracted_entity": null}
{"question": "were any of Spartacus's companions from the area where the Gallic Wars took place?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of Spartacus's compan", "extracted_entity": null}
{"question": "were there any famous musicians with more letter a's in their name than Isaac Newton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Isaac Newton.", "extracted_entity": null}
{"question": "were cannons built during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ancient Egypt, Ancient Greece, and the", "extracted_entity": null}
{"question": "were cannons wielded during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Bronze Age is a time period", "extracted_entity": null}
{"question": "were cannons used during the Bronze Age?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cannons were not used during the Bronze", "extracted_entity": null}
{"question": "would firefighters respond to a heart attack?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the patient is unconscious,", "extracted_entity": null}
{"question": "would firefighters respond to a stabbing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be dispatched to the scene", "extracted_entity": null}
{"question": "did Shane Dawson work with any creators who have Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shane Dawson worked with a creator who", "extracted_entity": null}
{"question": "are any current world leaders former KGB members?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Putin, who was in the K", "extracted_entity": null}
{"question": "will you see the doctor a lot more during pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you will see the doctor a lot more", "extracted_entity": null}
{"question": "will someone have more doctor's appointments during their pregnancy than normal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, you will have 1", "extracted_entity": null}
{"question": "can pregnancy put strain on friendships?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pregnancy can put strain on", "extracted_entity": null}
{"question": "can pregnancy ruin a friendship?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "can you eat some kinds of citrus on birth control?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can eat most citrus fruits", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the fairy.", "extracted_entity": null}
{"question": "would a fairy be likely to reject the gift of iron earrings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think that fairies are more likely to", "extracted_entity": null}
{"question": "was Rosalind Franklin studying in Molecular Biology?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was studying in Molecular Bi", "extracted_entity": null}
{"question": "would a chiropractic care center be more helpful for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, chiropractic care is the best", "extracted_entity": null}
{"question": "would a chiropractic care center be more appropriate for a herniated disc than a physical therapist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A chiropractor is more appropriate for a h", "extracted_entity": null}
{"question": "do Doctor Who fans enjoy taking photos in pay phone booths?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes they do.", "extracted_entity": null}
{"question": "did students in 1999 have a different understanding of Pluto than those in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Students in 1999", "extracted_entity": null}
{"question": "would Kale be better to eat than Spinach for someone with scurvy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because kale has more vitamin C", "extracted_entity": null}
{"question": "is Kim Kardashian a supermodel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is not.", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than a Zweihander?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the gladius, which was a short", "extracted_entity": null}
{"question": "eminem's daughter spells her name differently than star of 1961 Disney film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Joel Osment", "extracted_entity": null}
{"question": "eminem's daughter spells her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Mills", "extracted_entity": null}
{"question": "does Eminem's daughter spell her name differently than star of 1961 Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Haley Mills", "extracted_entity": null}
{"question": "does Eminem's daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Hailie Jade Scott is sp", "extracted_entity": null}
{"question": "does Eminem's biological daughter spell her name differently than star of original Parent Trap?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I think that is a great question", "extracted_entity": null}
{"question": "did the early settlers of the U.S celebrate the first halloween?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "has any Doctor Who actor won a razzie award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they haven\u2019t.", "extracted_entity": null}
{"question": "would a psychiatrist be likely to recommend Prozac to someone with bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Prozac is not indicated for b", "extracted_entity": null}
{"question": "has any Doctor Who actor won a Golden Globe award?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not yet. But the show has been", "extracted_entity": null}
{"question": "would an SSRI be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. SSRIs are not recommended as a", "extracted_entity": null}
{"question": "would Zoloft be a recommended first line of treatment for bipolar disorder?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Zoloft is a Selective Seroton", "extracted_entity": null}
{"question": "has any Duke University graduate married a Grammy winner?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The first Duke graduate to marry a", "extracted_entity": null}
{"question": "do pharmacy employees need to ask for customer ID every time?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A pharmacy employee only needs to", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the only one I know of is", "extracted_entity": null}
{"question": "did any Oscar winners attend John Kerry's college?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John Kerry attended Yale.", "extracted_entity": null}
{"question": "does the Torah have more books than Emily Bronte wrote?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Torah has more books than Em", "extracted_entity": null}
{"question": "in Mexico, is Tuesday the 13th is considered more unlucky than Friday the 13th? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In Mexico, Friday the 13", "extracted_entity": null}
{"question": "does the Torah have more books than the Hunger Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Torah has 5 books,", "extracted_entity": null}
{"question": "does the Torah have more books than the Katniss Everdeen series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Torah has five books, and", "extracted_entity": null}
{"question": "lil Wayne similar real name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "14 Grammy Awards", "extracted_entity": null}
{"question": "lil Wayne similar name rapper has over 4 times Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Akon", "extracted_entity": null}
{"question": "lil Wayne needs over 4 times Grammy awards to equal rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "lil Wayne needs 4 times more Grammy", "extracted_entity": null}
{"question": "lil Wayne needs over 4 times Grammy awards to equal with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michael Jackson.", "extracted_entity": null}
{"question": "lil Wayne double Grammy awards trails rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tupac Shakur.", "extracted_entity": null}
{"question": "does Lil Wayne need more Grammy awards to match rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn't.", "extracted_entity": null}
{"question": "do Lil Wayne's Grammy awards trail rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Kanye West", "extracted_entity": null}
{"question": "do Lil Wayne's Grammy awards lag behind rapper with same real last name as him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He has 2.", "extracted_entity": null}
{"question": "did the author of \"A Doll's House\" have a mentally disabled son?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the author of \"A Doll'", "extracted_entity": null}
{"question": "was Charles Manson's body count surpassed by any Marilyn Manson band member name origin killer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But Charles Manson was a killer", "extracted_entity": null}
{"question": "can a bumblebee suffer from spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, bumblebees can suffer from sp", "extracted_entity": null}
{"question": "can a bumblebee get spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. I think they", "extracted_entity": null}
{"question": "can a bumblebee get a blood clot?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a bumblebee can not get", "extracted_entity": null}
{"question": "is the Numerologist featured on the podcast Oh No Ross and Carrie well known?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is well known.", "extracted_entity": null}
{"question": "would someone opening a brewery need to get barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would need to get barley", "extracted_entity": null}
{"question": "did the Maori people build kayaks in the 1300's?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Maori people did not build k", "extracted_entity": null}
{"question": "spain native spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not require adding a conson", "extracted_entity": null}
{"question": "did the Maori people reach New Zealand by kayak?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they came by boat, they had to", "extracted_entity": null}
{"question": "spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if the word is used in Spanish", "extracted_entity": null}
{"question": "does Spanish spelling of September require adding a consonant to English spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the English spelling of September does not", "extracted_entity": null}
{"question": "do you need to add a consonant to September in order to get the Spanish spelling?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you don't need to add", "extracted_entity": null}
{"question": "did the Inuit fish trout from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they didn't", "extracted_entity": null}
{"question": "did the Inuit hunt coyotes from their kayaks?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they hunted from the ground.", "extracted_entity": null}
{"question": "any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The movie \"Joseph\" is based", "extracted_entity": null}
{"question": "were any bible prophet stories adapted to Disney plots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many stories that have been adapted", "extracted_entity": null}
{"question": "do Islam and Christianity have at least five prophets in common?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Christianity has only one prophet,", "extracted_entity": null}
{"question": "does pi exceed square root of pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "pi exceeds square root of pi.", "extracted_entity": null}
{"question": "5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Livonian War was a 155", "extracted_entity": null}
{"question": "is 5 important to Morean and Livonian war duration?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the duration of the war has no impact", "extracted_entity": null}
{"question": "is Pluto origin mythology still to be explored in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it\u2019s safe to say that there", "extracted_entity": null}
{"question": "pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pluto is the god of the underworld and", "extracted_entity": null}
{"question": "is Pluto a mythology yet to be featured in God of War series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pluto is a mythology yet to be featured", "extracted_entity": null}
{"question": "have any plays based on graphic novels won awards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have.", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see the second film adaptation?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The author died in 197", "extracted_entity": null}
{"question": "did the author of \"The Haunting of Hill House\" see both of the film adaptations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. It would be interesting", "extracted_entity": null}
{"question": "if someone is wearing a Jack Skellington jacket, is it likely that they enjoy grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are likely to enjoy grotesque", "extracted_entity": null}
{"question": "would Nightmare Before Christmas fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "would Tim Burton fans be likely to embrace grotesque artwork?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably.", "extracted_entity": null}
{"question": "would Tim Burton fans be likely to embrace grotesque art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a director who is known", "extracted_entity": null}
{"question": "were modern assembly lines inspired by the work of Henry Ford?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not.", "extracted_entity": null}
{"question": "do restaurant waiting staff in South Korea expect a tip?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a nice gesture.", "extracted_entity": null}
{"question": "do restaurant waiting staff in South Korea expect a gratuity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the tip is included in the bill.", "extracted_entity": null}
{"question": "does a wedding designer typically make the floral arrangements?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A wedding designer typically does not make", "extracted_entity": null}
{"question": "was John Kerry less active in the military than John McCain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "is J.K. Rowling considered an ally to the transgender community?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, J.K. Rowling is an", "extracted_entity": null}
{"question": "does the LGBT community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is viewed as a bigot.", "extracted_entity": null}
{"question": "does the transgender community view J.K. Rowling as an ally?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rowling has been repeatedly called out for", "extracted_entity": null}
{"question": "in teenagers with depression, are SSRI medications as safe as they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There is no evidence that SSRI medications are", "extracted_entity": null}
{"question": "would a teen with depression be at higher risk for adverse effects on a medication like Zoloft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, depression increases the risk of adverse", "extracted_entity": null}
{"question": "have violent crowd control tactics been used against anyone in a sit-in?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the police have used violent crowd control tact", "extracted_entity": null}
{"question": "is tobacco a major crop in the Southern United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, tobacco is a major crop", "extracted_entity": null}
{"question": "are there turtles that are dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are turtles that are dangerous", "extracted_entity": null}
{"question": "would someone who was good at guitar hero be good at rock band?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "do tailors predate the sewing machine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. Tailors have been", "extracted_entity": null}
{"question": "would Go be considerably more difficult for someone who was blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they can't see the board", "extracted_entity": null}
{"question": "would Go be considerably more difficult if you were blind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be much harder.", "extracted_entity": null}
{"question": "would fauna in a tropical climate be very different from those in a temperate climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because in a tropical climate the weather is", "extracted_entity": null}
{"question": "does the Pope live in an area close to Italy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he lives in a place called Vat", "extracted_entity": null}
{"question": "is country ham bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, country ham is not a bad choice for", "extracted_entity": null}
{"question": "would country ham be a bad choice for someone with high blood pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not a doctor, but I would think", "extracted_entity": null}
{"question": "should someone watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It\u2019s cured with salt.", "extracted_entity": null}
{"question": "should people watching their sodium avoid country ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Country ham has less sodium than many", "extracted_entity": null}
{"question": "does country ham taste different than city ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Country hams have a more intense", "extracted_entity": null}
{"question": "does Will Ferrell feature in multiple films featuring elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "has Will Ferrell been in multiple movies that feature elves?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Elf and Anchorman", "extracted_entity": null}
{"question": "can anyone with a driver's license work in transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you must have a CDL with the", "extracted_entity": null}
{"question": "can anyone with a driver's license work in bulk transport of airline fuel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are additional requirements to be a D", "extracted_entity": null}
{"question": "would a comparison of a murder scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because a murder scene is messy and", "extracted_entity": null}
{"question": "would a comparison of a crime scene to a piece by Jackson Pollock make sense?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the splattered paint would be", "extracted_entity": null}
{"question": "at a presentation about PTSD, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She\u2019s been through a lot, and she", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising considered unsafe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is considered unsafe because of the COVID", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising socially unacceptable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is a nuisance.", "extracted_entity": null}
{"question": "in 2020, is door-to-door advertising frowned upon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. In fact, it", "extracted_entity": null}
{"question": "did Victor Hugo's novel about the French Revolution ever become a Broadway show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the musical Les Miserables was", "extracted_entity": null}
{"question": "did the Voyages of Christopher Columbus involve coming ashore on the West coast of America?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Columbus landed on the West coast", "extracted_entity": null}
{"question": "was George Washington aboard one of the ships during the voyages of Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was Jamestown the landing place for members of the Voyages of Christopher Columubus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Jamestown was the first permanent English", "extracted_entity": null}
{"question": "was Michael Scott still on The Office when Will Ferrell's character came onto the show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Michael Scott was still on the show when", "extracted_entity": null}
{"question": "should Peter Griffin be familiar with the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He should have a good working knowledge of", "extracted_entity": null}
{"question": "was the death of the CEO of Apple caused by pancreatic cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He died from complications of pancre", "extracted_entity": null}
{"question": "were Furbys popular during the Presidency of Bill Clinton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "do students studying Shakespeare learn about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because tragedy is a genre.", "extracted_entity": null}
{"question": "would a Professor of a Shakespeare class have to teach the students about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because that is the main theme in Shakespeare", "extracted_entity": null}
{"question": "would a Professor of a Shakespeare class have to teach about tragedy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "did Kate Winslet and Leonardo Dicaprio star in a tragedy together?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Titanic", "extracted_entity": null}
{"question": "is Yale University beyond 5000 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is 102", "extracted_entity": null}
{"question": "is Yale University beyond 400 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Yale University is not beyond 4", "extracted_entity": null}
{"question": "is Yale University beyond 100 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is located in New", "extracted_entity": null}
{"question": "is Yale University beyond 30 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yale University is 34.", "extracted_entity": null}
{"question": "is Yale University beyond 20 miles from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond 20", "extracted_entity": null}
{"question": "is Yale University beyond a mile from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it is 3 miles away.", "extracted_entity": null}
{"question": "is Yale University beyond a quick walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Yale University is beyond a quick walk", "extracted_entity": null}
{"question": "did lead singer of The Police ever collaborate with an Arab?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Sting and Cheb Mami.", "extracted_entity": null}
{"question": "did lead singer of The Police ever collaborate with an Algerian?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did, in fact, collaborate", "extracted_entity": null}
{"question": "did Shaggy ever collaborate with a Blood?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did collaborate with a C", "extracted_entity": null}
{"question": "are there any spirits in the Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no spirits in the Qur", "extracted_entity": null}
{"question": "does the average male foot fail to reach a foot (unit) on a ruler?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes or no", "extracted_entity": null}
{"question": "is Home Depot a good place to shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can find a wide assort", "extracted_entity": null}
{"question": "is Yale University beyond a walk from University of New Haven?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is about 10-1", "extracted_entity": null}
{"question": "did Reza Shah's grandfather's grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was a military commander in the army", "extracted_entity": null}
{"question": "did Reza Shah's great great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, his great great great grandfather was a", "extracted_entity": null}
{"question": "did Reza Shah's great great grandfather help conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Reza Shah's great great grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a commander of the Ottoman", "extracted_entity": null}
{"question": "did Reza Shah's grandfather conquer Constantinople?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "is anyone that was at the Last Supper important to Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is no one that was at the", "extracted_entity": null}
{"question": "are chocolate brownies hypothetically fatal to Harry Potter's Fang?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Fang is a dog and he", "extracted_entity": null}
{"question": "can a chocolate brownie be fatal to a member of Canidae?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is made of chocolate", "extracted_entity": null}
{"question": "can a chocolate brownie be fatal to a dog?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be fatal to a dog.", "extracted_entity": null}
{"question": "have protesters ever rallied for a governor to veto a bill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, protesters have rallied for a", "extracted_entity": null}
{"question": "is the Sun (United Kingdom) published in every United Kingdom country?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not published in every United Kingdom", "extracted_entity": null}
{"question": "national Transportation Safety Board has more abbreviation letters than Mulder's X-Files agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NTSB", "extracted_entity": null}
{"question": "do vendors ever claim that Quartz has healing properties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Quartz is a mineral. It", "extracted_entity": null}
{"question": "were Republican voters pleased with the outcome of  the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were", "extracted_entity": null}
{"question": "did any states bordering California vote for Jimmy Carter in the 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Arizona, Nevada, Oregon, and Washington", "extracted_entity": null}
{"question": "were any of the blue states in the 1980's election on the East Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were all on the West Coast.", "extracted_entity": null}
{"question": "were any of the blue states in the 1980's election on the West Coast?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The only blue states were the Northeast", "extracted_entity": null}
{"question": "was Super Bowl 50 devoid of NFL founding teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Denver Broncos played the Carolina", "extracted_entity": null}
{"question": "was Christopher Columbus a poor example of religious tolerance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a poor example of religious toler", "extracted_entity": null}
{"question": "does Burning Man effigy share similarities with The Wicker Man movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and no. The Wicker Man is", "extracted_entity": null}
{"question": "is saying Melania Trump is from a third world country inaccurate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. She was born in Sloven", "extracted_entity": null}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same gender Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sean Penn is heterosexual", "extracted_entity": null}
{"question": "is actor that played Harvey Milk in 2008 attracted to the same sex Harvey Milk was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sean Penn is attracted to the same", "extracted_entity": null}
{"question": "can you reach the UNICEF headquarters from Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is only 300 miles", "extracted_entity": null}
{"question": "can you reach the UNICEF headquarters by Hoboken on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I would have to take the PATH train", "extracted_entity": null}
{"question": "has the executive director of UNICEF spent time in New England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he didn't say it.", "extracted_entity": null}
{"question": "has the executive director of UNICEF spent time in Massachusetts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She has spent time in Massachusetts.", "extracted_entity": null}
{"question": "is the executive director of UNICEF familiar with a bra?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is familiar with a bra.", "extracted_entity": null}
{"question": "would a bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but I would not ask for a drink", "extracted_entity": null}
{"question": "did people used to believe bagel seeds were from same drug family as morphine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A bagel seed is a small seed", "extracted_entity": null}
{"question": "would a bartender make a cocktail for a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it.", "extracted_entity": null}
{"question": "would a bartender accept an order from a 7th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I don'", "extracted_entity": null}
{"question": "does country ham taste different than spiral ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, country ham is a saltier, more", "extracted_entity": null}
{"question": "would someone with celiac disease eat cookies baked with all purpose flour?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If someone has celiac disease, they", "extracted_entity": null}
{"question": "would a bartender accept an order from a 2nd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he would be a 2nd", "extracted_entity": null}
{"question": "would a bartender accept an order from a 3rd grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he/she would be very ske", "extracted_entity": null}
{"question": "would a bartender accept an order from a 4th grader?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is illegal to serve alcohol to", "extracted_entity": null}
{"question": "would a bartender accept an order from Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is too short.", "extracted_entity": null}
{"question": "would a bartender provide service to Jacob Tremblay?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he\u2019s a cute kid and", "extracted_entity": null}
{"question": "would a bartender provide service to Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would be very nervous and", "extracted_entity": null}
{"question": "would a bartender make a White Russian for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a regular", "extracted_entity": null}
{"question": "would a bartender make a Mojito for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he\u2019s under 21", "extracted_entity": null}
{"question": "would a bartender make a cocktail for Noah Schnapp?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a bartender would make a co", "extracted_entity": null}
{"question": "has name God Save the Queen been used in things other than royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has been used in the movie God", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Cynthia Powell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "did John Lennon celebrate a silver anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Beatles split up in 1", "extracted_entity": null}
{"question": "did John Lennon celebrate a diamond anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lennon was murdered in", "extracted_entity": null}
{"question": "did John Lennon celebrate a golden anniversary with Yoko Ono?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was on March 20th", "extracted_entity": null}
{"question": "is RoboCop director a Razzie award legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he won 2 Razzie awards for", "extracted_entity": null}
{"question": "would it be inaccurate to call Eve an orphan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eve is not an orphan.", "extracted_entity": null}
{"question": "was Hulk Hogan a good guy in one organization and bad in another?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a good guy in W", "extracted_entity": null}
{"question": "can you be be denied penance for a deadly sin posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a person cannot be denied penance for", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his deadly sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today for his sin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would be denied penance today because", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not. He was not a", "extracted_entity": null}
{"question": "would William Howard Taft be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, William Howard Taft would be denied pen", "extracted_entity": null}
{"question": "would Jack the Ripper be denied penance in 21st century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not be denied penance.", "extracted_entity": null}
{"question": "would Jack the Ripper be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. I think he would have to", "extracted_entity": null}
{"question": "would Jeffrey Dahmer be denied penance posthumously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I do know", "extracted_entity": null}
{"question": "when Elizabeth Glaser died was penance impossible for Jeffrey Dahmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Penance is always possible.", "extracted_entity": null}
{"question": "penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he didn't do it.", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer when Elizabeth Glaser died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he had a choice, he chose not", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer in year Layne Staley died?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was impossible.", "extracted_entity": null}
{"question": "was penance impossible for Jeffrey Dahmer in last month of 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he tried.", "extracted_entity": null}
{"question": "was penance out of reach for Jeffrey Dahmer in December 1994?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The death penalty was out of reach for", "extracted_entity": null}
{"question": "have any of Kurt Cobain's bandmates had more success than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Dave Grohl has been more successful than", "extracted_entity": null}
{"question": "will Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it\u2019s not a good vac", "extracted_entity": null}
{"question": "do Atlantic salmon vacation in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But they do swim there.", "extracted_entity": null}
{"question": "do Atlantic salmon hang out in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not", "extracted_entity": null}
{"question": "are Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "can you spot Atlantic salmon in Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not there.", "extracted_entity": null}
{"question": "do Atlantic salmon ever swim near Buuntsagaan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is Summer Triangle band named after grizzly bear?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The band is named after a constell", "extracted_entity": null}
{"question": "is Ginger something that a naturopath would recommend for a pregnant woman? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "is Olivia Newton John's 1980 film named after summer home of Mongols?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Xanadu, the summer home of", "extracted_entity": null}
{"question": "have there been any blood eagle killings in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there have been.", "extracted_entity": null}
{"question": "have there been any crucifixions in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The last one was in 33", "extracted_entity": null}
{"question": "has League of Legends esports players surpassed DOTA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even close.", "extracted_entity": null}
{"question": "is cancer treatment a preventative for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "did Popeye eat all of Bugs Bunny's food source?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "are all colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all colors are found on the French flag", "extracted_entity": null}
{"question": "do majority of Tibetan people in their ancestral homeland believe in bodhisattvas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion be misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Cyprus is a secular", "extracted_entity": null}
{"question": "would a member of the Communist Party of the Soviet Union be likely to have read Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I don't know of any CP", "extracted_entity": null}
{"question": "is electrolysis possible on the human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, electrolysis can be performed on", "extracted_entity": null}
{"question": "does Facebook take steps to limit fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "did Confederate States Army have similar fashion sense to West Point cadets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Confederate States Army was known for", "extracted_entity": null}
{"question": "is Fake News currently uncensored on major social media sites?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. (I have a list", "extracted_entity": null}
{"question": "would someone feel safer from fake news on facebook than on the deep web?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because fake news is everywhere", "extracted_entity": null}
{"question": "is the deep web somewhere that you could encounter unchecked fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the deep web is a part of the", "extracted_entity": null}
{"question": "does Ahura Mazda live above Abaddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Abaddon is a pit of fire", "extracted_entity": null}
{"question": "can Boards of Canada perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. They are a duo", "extracted_entity": null}
{"question": "can MGMT perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are a duo", "extracted_entity": null}
{"question": "can Destiny's Child perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch harm hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because starch is a carboh", "extracted_entity": null}
{"question": "can starch be deadly to hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Starch can be deadly to hyper", "extracted_entity": null}
{"question": "hyperglycemics should avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "hyperglycemics should avoid starch?", "extracted_entity": null}
{"question": "should hyperglycemics avoid starch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because starch is a sugar and it", "extracted_entity": null}
{"question": "did Alan Rickman's final film outearn his first?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. $169 million for Alice in", "extracted_entity": null}
{"question": "was Alan Rickman's final film a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "did Alan Rickman outearn his debut movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he made his debut in 19", "extracted_entity": null}
{"question": "did Alan Rickman ever surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did surpass the gross of", "extracted_entity": null}
{"question": "did an Alan Rickman film surpass the gross of his first film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the gross of his second film was", "extracted_entity": null}
{"question": "was Alan Rickman's first movie a box office success?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was not a box office success.", "extracted_entity": null}
{"question": "has Simo Hayha killed any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he has killed some Red Army soldiers but", "extracted_entity": null}
{"question": "did Simo Hayha kill any members of the Red Army?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's children followed similar profession?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, her daughter, Christine Pelosi,", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's brats followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. And I don't believe any of", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's kids followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nancy Pelosi's daughter, Alex", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but her daughter Alexandra is a producer", "extracted_entity": null}
{"question": "has any of Nancy Pelosi's offspring followed in her footsteps career wise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, her daughter Christine Pelosi, who", "extracted_entity": null}
{"question": "was 2014 governor of New Jersey fattest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chris Christie", "extracted_entity": null}
{"question": "was 2014 governor of New Jersey heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "330 pounds", "extracted_entity": null}
{"question": "did any belligerent in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were all the same, but Portugal", "extracted_entity": null}
{"question": "would clown be a bad occupational fit for Ebenezer Scrooge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, clowns are the most generous", "extracted_entity": null}
{"question": "do jumping spiders hunt to survive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, jumping spiders hunt to surv", "extracted_entity": null}
{"question": "are tap shoes required for breakdancing?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they are fun!", "extracted_entity": null}
{"question": "does a Drow reach top of a shelf before The Hobbit's hero can?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the book, but not in the", "extracted_entity": null}
{"question": "yayoi era Japanese people didn't worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did.", "extracted_entity": null}
{"question": "did Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean by \"", "extracted_entity": null}
{"question": "did  Yayoi era Japanese people not have to worry about kami?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did Giovanni Battista Cybo serve his position longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Giovanni Battista Cybo served his position for", "extracted_entity": null}
{"question": "was Giovanni Battista Cybo in office longer than James A. Garfield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Giovanni Battista Cybo was not in", "extracted_entity": null}
{"question": "is Alexandria Ocasio-Cortez a hippie?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she is a commie.", "extracted_entity": null}
{"question": "was the first Vice President of the United States a Segregationist?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John C. Calhoun", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors been featured in a Val Kilmer project?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Ewok actor who played Chief", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Val Kilmer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Warwick Davis worked with Val Kilmer", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Bowie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But they did work with Bono.", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Jennifer Connelly?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. No Ewok actors have worked with", "extracted_entity": null}
{"question": "have any Star Wars Ewok actors worked with Jim Henson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Warwick Davis.", "extracted_entity": null}
{"question": "does Lines on the Antiquity of Microbes lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it lacks a season word, which", "extracted_entity": null}
{"question": "does the shortest poem lack components of haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The shortest poem is a haiku.", "extracted_entity": null}
{"question": "does the shortest poem lack necessary component of a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the poem is not a haiku because", "extracted_entity": null}
{"question": "does the shortest poem lack the structure to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a haiku does not have to be", "extracted_entity": null}
{"question": "does the shortest poem written fail the criteria to be a haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not. The poem must have", "extracted_entity": null}
{"question": "is a haiku longer than the shortest poem written?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because a haiku is a three line", "extracted_entity": null}
{"question": "are there Scientologists who know a lot about Nicole Kidman?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m sure there are.", "extracted_entity": null}
{"question": "did Katy Perry and Taylor Swift appear together in a music video?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did Katy Perry create more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Katy Perry created 4 records.", "extracted_entity": null}
{"question": "did Katy Perry sell more records than Avril Lavigne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Katy Perry sold 3,000", "extracted_entity": null}
{"question": "are there people who carry drums at the Macy's Day Parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are people who carry drums at the", "extracted_entity": null}
{"question": "can coffee help with diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Coffee can help with diarrhea", "extracted_entity": null}
{"question": "would the plant Spartina Patens thrive in the Sahara?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it needs water to survive", "extracted_entity": null}
{"question": "during the neolithic period, did people know how to hunt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "during the neolithic period, was hunting a valued skill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hunting was a valued skill during the", "extracted_entity": null}
{"question": "are there accessibility options for blind users who encounter CAPTCHAs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can be read aloud.", "extracted_entity": null}
{"question": "would a student in AP Chemistry have a need for scientific journals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The AP Chemistry class is a college level course", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be unusual to see fro", "extracted_entity": null}
{"question": "have there been any actors from musicals who have played Noah in a film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so.", "extracted_entity": null}
{"question": "has anyone from a film about Noah been in a movie about magic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Noah has not been in a movie", "extracted_entity": null}
{"question": "are any of the actors from the Harry Potter film series in any films about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not that I've seen.", "extracted_entity": null}
{"question": "have any of the actors from Harry Potter participated in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But the actors who play the wiz", "extracted_entity": null}
{"question": "does Russell Crowe know about the story of Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not know about the story of", "extracted_entity": null}
{"question": "has Russell Crowe ever acted in a film about Noah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He hasn\u2019t.", "extracted_entity": null}
{"question": "did Michael Jackson own a lot of bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he bought 100,0", "extracted_entity": null}
{"question": "did Michael Jackson invest in Bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Michael Jackson did not invest in Bitco", "extracted_entity": null}
{"question": "are goldfish found on the menu of Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they are not", "extracted_entity": null}
{"question": "is it recommended to put goldfish in the same tank as beta fish?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not recommended to put goldfish", "extracted_entity": null}
{"question": "is it recommended for goldfish and beta fish to share a tank?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The two species are not compatible. Gold", "extracted_entity": null}
{"question": "did Native American tribes teach Colonizers how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Native American tribes taught colonizers how to", "extracted_entity": null}
{"question": "are there types of plastic that you can't put in the recycling?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are types of plastic that you", "extracted_entity": null}
{"question": "did James Roday play a private investigator on the USA network?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He played Shawn Sp", "extracted_entity": null}
{"question": "did James Roday play a private investigator on a TV show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he played Shawn Spencer on Psych", "extracted_entity": null}
{"question": "is Lance Armstrong known for riding a bicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "if someone is a fan of Dale Earnheardt, would they know what the Daytona 500 was?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it\u2019s a race.", "extracted_entity": null}
{"question": "before it sunk, did the Titanic have too few lifeboats for all of the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Titanic had too few lif", "extracted_entity": null}
{"question": "can you go water skiing in the Adirondacks? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can go water skiing in the", "extracted_entity": null}
{"question": "in the movie The Titanic, were there too few many lifeboats for the passengers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were too few lifeboats", "extracted_entity": null}
{"question": "has Saturday Night Live been on air longer than American Idol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. SNL has been on air since", "extracted_entity": null}
{"question": "was Ashlee Simpson lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was not.", "extracted_entity": null}
{"question": "have any performers been caught lip syncing on Saturday Night Live?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so, but I\u2019", "extracted_entity": null}
{"question": "can a person with triskaidekophobia succeed at Sudoku?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they use a penc", "extracted_entity": null}
{"question": "have any of the Doctor (Doctor Who) actors had famous offspring?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think that the answer to that is no.", "extracted_entity": null}
{"question": "did any of the amazons on Xena: Warrior Princess star in a Fox TV series?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Xena was a syndicated show", "extracted_entity": null}
{"question": "would Library of Alexandria hypothetically need less shelf pace than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Library of Alexandria is more", "extracted_entity": null}
{"question": "was Library of Alexandria commissioned by a Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by Ptolemy I Soter", "extracted_entity": null}
{"question": "is it impossible for Siddhartha Gautama to become a Bodhisattva?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not impossible for Siddhar", "extracted_entity": null}
{"question": "did a mystery ever happen at Tower of London?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 1674, the", "extracted_entity": null}
{"question": "is Tower of London relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the show was called Unsolved Myst", "extracted_entity": null}
{"question": "would Tower of London be relevant to Robert Stack's most famous show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Tower of London is a castle and", "extracted_entity": null}
{"question": "would Tower of London be relevant to Robert Stack's longest running show?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it was a place of torture", "extracted_entity": null}
{"question": "george Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In the 1930s", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in nature?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The hornet is a solitary wasp,", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life zoology?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a hornet is a member of the", "extracted_entity": null}
{"question": "is George Trendle's hornet a misnomer in real life taxonomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a misnomer. The", "extracted_entity": null}
{"question": "in animal world is George Trendle's hornet a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "did Barack Obama and Osama Bin Laden go to school together?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO. Osama went to a Madrassa", "extracted_entity": null}
{"question": "if someone was hungry for a sandwich, would they be likely to go to taco bell?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because taco bell is not a sand", "extracted_entity": null}
{"question": "can Aloe plants be mailed within the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Aloe Vera plant is a", "extracted_entity": null}
{"question": "would Aloe thrive more in a dry climate than a swampy climate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Aloe plants thrive more in dry", "extracted_entity": null}
{"question": "would Aloe grow better in a drier area than a marshy area?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "did Mark-Paul Gosselaar ever play the role of a student?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he played the role of a student in", "extracted_entity": null}
{"question": "would Drew Pinsky be someone to talk to about mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would not.", "extracted_entity": null}
{"question": "is Drew Pinsky qualified to help people with mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not qualified to help people with", "extracted_entity": null}
{"question": "is Dr. Drew Pinsky qualified to treat mental disorders?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "would Steve Martin turn down a dish of shrimp risotto? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Steve Martin would not turn down a d", "extracted_entity": null}
{"question": "did any of the founding fathers get frequent migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, none of them did.", "extracted_entity": null}
{"question": "do any of the actors in Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "does anyone from the cast of Wicked get migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea, but I do know that", "extracted_entity": null}
{"question": "are there any famous singers with migraines?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, many.", "extracted_entity": null}
{"question": "would a mammogram at a new doctor's office be considered preventive healthcare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, preventive health care is defined as \"", "extracted_entity": null}
{"question": "is it inappropriate to open a closed casket at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if the family requests it.", "extracted_entity": null}
{"question": "are there live performances at celebrity funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I have the video to prove it", "extracted_entity": null}
{"question": "were there celebrities performing at the funeral of Aretha Franklin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were many celebrities performing at", "extracted_entity": null}
{"question": "was Aretha Franklin's funeral attended by celebrity guests?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aretha Franklin's funeral was", "extracted_entity": null}
{"question": "would a Heath Ledger fan be interested in seeing The Dark Knight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course, but not necessarily for the reasons you", "extracted_entity": null}
{"question": "is The Dark Knight associated with any violence in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The Dark Knight Rises.", "extracted_entity": null}
{"question": "do Northern Fur Seals encounter penguins naturally?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They can be found in", "extracted_entity": null}
{"question": "would it be unusual to see a Northern Fur Seal in the south pole?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not unusual to see a Northern", "extracted_entity": null}
{"question": "are there country artists who also integrate rapping into their work?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes there are country artists who also integrate rapping", "extracted_entity": null}
{"question": "did any actors from a musical play in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the answer is in the question.", "extracted_entity": null}
{"question": "did any actors from Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Meryl Streep was in", "extracted_entity": null}
{"question": "did anyone from the film Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Julie Walters, who played Ros", "extracted_entity": null}
{"question": "did anyone from the movie Mamma Mia act in a James Bond film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Piers Brosnan played James Bond", "extracted_entity": null}
{"question": "has there been a song by Billie Eilish that has been used in a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there has not.", "extracted_entity": null}
{"question": "is it normal for prisoners to encounter situations that inspire disgust in prison?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal for prisoners to encounter situations", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Russia to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Sainsbury's is a large", "extracted_entity": null}
{"question": "would Sainsbury's be too far away for someone in Arkansas to go to?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's in the UK, so it would", "extracted_entity": null}
{"question": "do cactus plants thrive more in dry environments than wet ones?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cacti thrive in dry environments", "extracted_entity": null}
{"question": "do cactus plants require less water than dafodills? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is NO. They both require the same", "extracted_entity": null}
{"question": "are there varieties of cactus that are unsafe to touch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are varieties of cactus", "extracted_entity": null}
{"question": "have there been suicides to protest war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the US and in the UK", "extracted_entity": null}
{"question": "would it be normal for a teacher to tell kindergartners about the golden rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal for a teacher to tell", "extracted_entity": null}
{"question": "does Halle Berry eat crustaceans often?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. She is a vegetarian", "extracted_entity": null}
{"question": "is Halle Berry prohibited from eating crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she can eat crustaceans.", "extracted_entity": null}
{"question": "is some public art illegal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some public art is illegal.", "extracted_entity": null}
{"question": "is Harry Styles currently knighted in England?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not knighted in England", "extracted_entity": null}
{"question": "do Razor brand kick scooters have gears?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Razor brand kick scooters do", "extracted_entity": null}
{"question": "is Britney Spears receiving treatment for her bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is not.", "extracted_entity": null}
{"question": "would a prosthetic limb work as a replacement for a human leg?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as well.", "extracted_entity": null}
{"question": "was Germany already established during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Germany was already established during the French Revolution", "extracted_entity": null}
{"question": "was Hugh Jackman alive during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in 196", "extracted_entity": null}
{"question": "was the movie Les Miserables filmed during the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was filmed in 20", "extracted_entity": null}
{"question": "are deep sea fishermen considered experts in oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Jane Goodall be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is an expert in primates.", "extracted_entity": null}
{"question": "would Jane Austen be considered an expert in oceanography? ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jane Austen would not be considered an", "extracted_entity": null}
{"question": "has Iggy Pop been in a giallo film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has been in a film that", "extracted_entity": null}
{"question": "has Iggy Pop worked with a Giallo director?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Dario Argento, for the", "extracted_entity": null}
{"question": "do Coen brothers take up more car space than Scott directing brothers in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. yes they do.", "extracted_entity": null}
{"question": "are there more Coen brothers alive in 2020 than Scott directing brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Scott has directed 11 movies", "extracted_entity": null}
{"question": "did Robin Williams have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Robin Williams had a stroke.", "extracted_entity": null}
{"question": "did Michael Jackson suffer from a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "did Christoper Reeves have a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had a stroke in 19", "extracted_entity": null}
{"question": "is John Cazale's 1970s partner yet to debut in a Coen brothers movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "is the Quran regarded as a holy text by Jewish people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is regarded as a holy text", "extracted_entity": null}
{"question": "do practicing Presbyterians read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "do practicing Catholics read the Quran for guidance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Muslims read the Bible for guidance.", "extracted_entity": null}
{"question": "did Kangaroo Court originate in High Court of Australia?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it originated in the United States.", "extracted_entity": null}
{"question": "did mongoose come from later geologic period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, rhinos came from a later geolog", "extracted_entity": null}
{"question": "nATO excludes double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NATO is an alliance of 29", "extracted_entity": null}
{"question": "would NATO exclude double triangle flag country to a party?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the double triangle flag country is a", "extracted_entity": null}
{"question": "is double triangle flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, double triangle flag country is absent from NAT", "extracted_entity": null}
{"question": "is double-pennon flag country absent from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not a NATO member", "extracted_entity": null}
{"question": "is Saugat Malla's home delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saugat Malla's home", "extracted_entity": null}
{"question": "is Saugat Malla's home country delegate excluded from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saugat Malla's home", "extracted_entity": null}
{"question": "is Saugat Malla's home country delegate barred from NATO members only meeting?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Saugat Malla is Nepalese.", "extracted_entity": null}
{"question": "are squid considered invertebrates? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, squid are considered invertebrates", "extracted_entity": null}
{"question": "are squid found in salt water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, squid are found in salt water.", "extracted_entity": null}
{"question": "are there raw preparations of squid for eating?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are raw preparations of squid", "extracted_entity": null}
{"question": "is the white part of the citrus fruit less sweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the white part of the citrus fruit", "extracted_entity": null}
{"question": "would most people discard the outside of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I wouldn\u2019t.", "extracted_entity": null}
{"question": "is citrus good for the body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, citrus fruits are good for the", "extracted_entity": null}
{"question": "is citrus useful in promoting health?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, citrus is very useful in promoting", "extracted_entity": null}
{"question": "is calling creator of Gorillaz a one hit wonder ludicrous?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, but I don't know", "extracted_entity": null}
{"question": "are house cats a potential source of danger for chipmunks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, house cats are not a potential source", "extracted_entity": null}
{"question": "is Alexander Klevan's birthplace safe from Zika virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Zika virus is present in the", "extracted_entity": null}
{"question": "did same villain kill Batman and Superman?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, same villain killed Batman and Super", "extracted_entity": null}
{"question": "is Cerebrospinal fluid contents totally different than Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. CSF is a clear, colorless", "extracted_entity": null}
{"question": "is Cerebrospinal fluid contents similar to Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Cerebrospinal", "extracted_entity": null}
{"question": "are there no shared ingredients between Cerebrospinal fluid and Gatorade?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are different.", "extracted_entity": null}
{"question": "did Billie Eilish perform a song for a James Bond movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "are there paintings of Salvador Dali's on both coasts of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are paintings of Salvador Dali", "extracted_entity": null}
{"question": "can you see Salvador Dali's work in multiple cities of the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can see Salvador Dali's", "extracted_entity": null}
{"question": "can you see Salvador Dali's work in both New York and Georgia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can see Dali's work", "extracted_entity": null}
{"question": "does the Bronx Zoo have an exhibit of the Dodo bird?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are on display in the aquarium", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are prisoners who practice winemaking allowed to do so by their guards?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "did Richard III miss out on Parcheesi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He was too busy plot", "extracted_entity": null}
{"question": "sunday name origin people enemy of Christians?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "sunday name origin people enemy of Christians?", "extracted_entity": null}
{"question": "sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Church has the right to refuse the", "extracted_entity": null}
{"question": "will Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they will be refused the Eucharist", "extracted_entity": null}
{"question": "would Sunday name origin people be refused Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the priest will give them the Euchar", "extracted_entity": null}
{"question": "would Sunday name origin disturb Serena Williams's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Serena Williams is a Christian.", "extracted_entity": null}
{"question": "would Sunday name origin disturb Prince's religious sect?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would Sunday name origin disturb a Jehovah Witness?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Jehovah Witnesses do", "extracted_entity": null}
{"question": "would a Jehovah's Witness denounce name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it. The only thing I can think", "extracted_entity": null}
{"question": "would a Jehovah's Witness reject name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would a Jehovah's Witness be upset by name origin of Sunday?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "are all bicycles able to go off road?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some bicycles are designed for off", "extracted_entity": null}
{"question": "are two names in Albany, Georgia common as country and city names?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are two Albany's in", "extracted_entity": null}
{"question": "does Camp Rock feature a lot of smooth jazz?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It features a lot of crap.", "extracted_entity": null}
{"question": "is the Song of Roland a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a saga", "extracted_entity": null}
{"question": "can the Song of Roland be considered a saga?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a saga.", "extracted_entity": null}
{"question": "is KFC more global than Roy Rogers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "KFC is more global than Roy Rogers.", "extracted_entity": null}
{"question": "would a Dodo tower over Pauline Musters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The Dodo was a flightless bird", "extracted_entity": null}
{"question": "would a Dodo be above Pauline Musters's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be below.", "extracted_entity": null}
{"question": "would a Dodo be above Chandra Bahadur Dangi's eye level?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Dodos are bigger than Chandra Bah", "extracted_entity": null}
{"question": "would a Dodo tower over Chandra Bahadur Dangi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "can you play a Nintendo game in your car CD player?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t play a Nintendo", "extracted_entity": null}
{"question": "are there any theme park attractions featuring King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are two.", "extracted_entity": null}
{"question": "is there a ride at Universal Studios Orlando about King Kong?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a King Kong ride at Universal", "extracted_entity": null}
{"question": "were there fifty English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There were 20 kings in the House", "extracted_entity": null}
{"question": "were there at least fifty different English kings during the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were.", "extracted_entity": null}
{"question": "did Pre-Raphaelites have a profound influence on Claude Monet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "was Claude Monet influenced by Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Monet was influenced by Impressionism", "extracted_entity": null}
{"question": "was Claude Monet a Pre-Raphaelite?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a French Impressionist.", "extracted_entity": null}
{"question": "would Claude Monet value art style of the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the Pre-Raphael", "extracted_entity": null}
{"question": "did Claude Monet have a similar art style to the Pre-Raphaelites?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he had a similar art style to the", "extracted_entity": null}
{"question": "while on the voice, did Adam Levine quit Maroon 5?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is still in the band.", "extracted_entity": null}
{"question": "would someone with a back fracture go to a chiropractic center for treatment?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would go to a doctor.", "extracted_entity": null}
{"question": "should cinnamon be eaten straight from the container?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is a spice.", "extracted_entity": null}
{"question": "does Microsoft Excel make slideshow presentations?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. Microsoft Excel is a spread", "extracted_entity": null}
{"question": "is In God We Trust part of the pledge of allegiance?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In God We Trust is the official mot", "extracted_entity": null}
{"question": "did Ariana Grande do a duet of Give it Up with Victoria Justice?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she didn't.", "extracted_entity": null}
{"question": "was Ariana Grande's hair in Victorious natural?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It was a wig.", "extracted_entity": null}
{"question": "did Ariana Grande get popular on show created by Disney?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ariana Grande got popular on show created", "extracted_entity": null}
{"question": "did Ariana Grande get popular on a Disney Channel Original Series?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did. She was on the show", "extracted_entity": null}
{"question": "while on Broadway, did Ariana Grande audition for The Spongebob Musical?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was already cast.", "extracted_entity": null}
{"question": "is Chris Hemsworth qualified to perform surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "in WW2, were there people who did surgery on the battlefield?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were surgeons, nurs", "extracted_entity": null}
{"question": "can a Sphynx cat make wool?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Sphynx cat cannot make", "extracted_entity": null}
{"question": "in New York, is snowboarding a popular summer activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Snowboarding is a", "extracted_entity": null}
{"question": "has an appendix been successfully transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are many problems with transplanting", "extracted_entity": null}
{"question": "do most nuclear families in America have Bengal Cats?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "can lacto fermentation preserve cucumbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can preserve cucumbers.", "extracted_entity": null}
{"question": "can cucumbers be preserved for longer storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cucumbers can be preserved for", "extracted_entity": null}
{"question": "do you pass through Wyoming following Route 66?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not anymore.", "extracted_entity": null}
{"question": "is building design the responsibility of a construction worker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the building design is the responsibility of the", "extracted_entity": null}
{"question": "are turtles vulnerable to Raccoons? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Raccoons are predators of t", "extracted_entity": null}
{"question": "could you see turtles at the Boston Aquarium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you couldn't.", "extracted_entity": null}
{"question": "did Ada Lovelace know the python coding language?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was born in 181", "extracted_entity": null}
{"question": "would a person with arachnophobia fear spiders? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, arachnophobia is a fear", "extracted_entity": null}
{"question": "were the family of Donald Shea angry with Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all", "extracted_entity": null}
{"question": "would a mastectomy be a treatment for colorectal cancer?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Colorectal cancer is a cancer", "extracted_entity": null}
{"question": "do most gynecologists have to see the firsthand health effects of tobacco?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, most do not.", "extracted_entity": null}
{"question": "would a home garden be growing cucumbers during the winter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "can you order pancakes at Dennys?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can order pancakes at D", "extracted_entity": null}
{"question": "are Zebra common sights in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "does Greek goddess Venus was named after have a Roman equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Greek goddess of love, Aphrod", "extracted_entity": null}
{"question": "are bench trials performed before a jury?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bench trials are performed before a", "extracted_entity": null}
{"question": "would the Temple of Kom Ombo feature Egyptian hieroglyphs? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Temple of Kom Ombo features Egypt", "extracted_entity": null}
{"question": "as of 2020 does John Key have more PM's succeed than precede him?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, John Key had 3 PM's", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah's and Abraham combined?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (Genesis 5:27", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah's lifetime over 5 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Methuselah died 5", "extracted_entity": null}
{"question": "did Methuselah outlive Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died 78 years before Sarah", "extracted_entity": null}
{"question": "did Charlemagne's father fight in the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his father was a farmer.", "extracted_entity": null}
{"question": "did Charlemagne's father win the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. His father was not even at the Battle", "extracted_entity": null}
{"question": "has J.K. Rowling written mysteries?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is a children's author.", "extracted_entity": null}
{"question": "do any of Powerpuff Girls share name with character in Princess Bride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the Powerpuff Girls have a", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of reptile?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a lizard.", "extracted_entity": null}
{"question": "would adherents to Ahimsa be appalled at Paul the Apostle's cause of death?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not be appalled", "extracted_entity": null}
{"question": "was Florence Nightingale's death a better example of Ahimsa than Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she never killed anyone", "extracted_entity": null}
{"question": "would people prefer Florence Nightingale's cause of death to Paul the Apostle's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "was Paul the Apostle's death more violent than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you're asking about", "extracted_entity": null}
{"question": "was Paul the Apostle's death crueler than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Paul was a man.", "extracted_entity": null}
{"question": "was Paul the Apostle's death more eventful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a great leader, and", "extracted_entity": null}
{"question": "was Paul the Apostle's death more painful than Florence Nightingale's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Paul was crucified, and", "extracted_entity": null}
{"question": "is number of US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 18 US presidents", "extracted_entity": null}
{"question": "does the band Marilyn Manson get their entire name from Charles Manson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Marilyn Manson is a combination", "extracted_entity": null}
{"question": "did Pre-Colonial America have Ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Dutch brought it in the 1", "extracted_entity": null}
{"question": "are vanity plates important to any of Stephen King's children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are all dead.", "extracted_entity": null}
{"question": "are any of Stephen King's children familiar with vanity plates?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "have any of Stephen King's offspring embraced their father's genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Owen King is an author.", "extracted_entity": null}
{"question": "have any of Stephen King's offspring followed in his footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Joe Hill is a well-known horror", "extracted_entity": null}
{"question": "did members of singer Prince's Christian sect celebrate Easter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, members of Prince's Christian sect did", "extracted_entity": null}
{"question": "do more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that's a good thing,", "extracted_entity": null}
{"question": "will Gremlins tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not even close.", "extracted_entity": null}
{"question": "is Gremlins set to tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Gremlins movies are all", "extracted_entity": null}
{"question": "are Gremlins sequels less prevalent than Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by much.", "extracted_entity": null}
{"question": "are there more Matrix sequels than Gremlins?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I don't think so.", "extracted_entity": null}
{"question": "is it possible for a mortal to escape the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Underworld is the realm of", "extracted_entity": null}
{"question": "would Alexander Graham Bell have supported Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course not. He was a liberal.", "extracted_entity": null}
{"question": "can Halle Berry eat crustaceans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can eat crustaceans.", "extracted_entity": null}
{"question": "can letters in first line of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, letters in first line of QWER", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of Henry VIII male king heirs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, B is the second letter in the alphabet", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Sofia Vergara has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Sofia Vergara has 2", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons of Bruce Dern?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the letter B is the 2nd", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Adam Sandler has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the letter B's place in alphabet", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of sons Bruce Willis has?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bruce Willis has 3 sons.", "extracted_entity": null}
{"question": "does Richard Dawkins donate to Science Partners?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Richard Dawkins does not donate to", "extracted_entity": null}
{"question": "is Love Song from November 11, 2000 an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so, but then again", "extracted_entity": null}
{"question": "would Richard Dawkins donate to the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure. I would be very interested", "extracted_entity": null}
{"question": "would Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not.", "extracted_entity": null}
{"question": "is Richard Dawkins joining the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not.", "extracted_entity": null}
{"question": "would Holy Wood's Love Song be an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it's a love song.", "extracted_entity": null}
{"question": "is Holy Wood's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is about the end of the", "extracted_entity": null}
{"question": "is Manson's Love Song an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The words of the song are certainly not the most", "extracted_entity": null}
{"question": "will Richard Dawkins join the Earth History Research Center?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if he can bring his pet", "extracted_entity": null}
{"question": "is Narcissus Narcosis an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is an inappropriate l", "extracted_entity": null}
{"question": "is Narcissus Narcosis an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a lullaby that is", "extracted_entity": null}
{"question": "is Target Audience an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an ineffective lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is an ineffect", "extracted_entity": null}
{"question": "is Target Audience (Narcissus Narcosis) an inappropriate lullaby?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a lullaby for an", "extracted_entity": null}
{"question": "did Christopher Columbus receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "can a brain be transplanted?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the brain is the center of all", "extracted_entity": null}
{"question": "do internet forums require natural gas to operate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are powered by electricity.", "extracted_entity": null}
{"question": "are wind turbines instrumental in making internet forums available to everyone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "was the tenth Amendment written on a mobile phone?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was written on a mobile phone.", "extracted_entity": null}
{"question": "was the tenth Amendment written on a typewriter?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was the first typewriter.", "extracted_entity": null}
{"question": "are Atlantic Cod a source of income for fishermen in California?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Atlantic Cod are not a source of", "extracted_entity": null}
{"question": "did John Lewis work alongside Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did John Lewis march with Martin Luther King?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "does Bill Clinton have the ability to post in internet forums?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "would Goofy hypothetically enjoy Milk Bone?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Goofy would hypothetically enjoy", "extracted_entity": null}
{"question": "is wheat important to make unleavened bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is because it is one of the", "extracted_entity": null}
{"question": "is wheat essential for bread making?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. wheat is not essential for bread making", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he started his rap career before he attended", "extracted_entity": null}
{"question": "did Ludacris' rap career start before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Ludacris' rap career started after", "extracted_entity": null}
{"question": "did Ludacris' rap career begin before he attended a prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he did not attend a prom", "extracted_entity": null}
{"question": "did Ludacris' rap career begin before he attended prom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a member of the group Organ", "extracted_entity": null}
{"question": "was Clark Gable handy with a wrench when he was young?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a wreck.", "extracted_entity": null}
{"question": "was Clark Gable familiar with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was Clark Gable handy with a wrench as a child?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a very handy boy.", "extracted_entity": null}
{"question": "is week old chlorine water safe?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chlorine water is not safe to drink.", "extracted_entity": null}
{"question": "would Teri Hatcher have most pay stubs from Desperate Housewives in her career?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "was Desperate Housewives Teri Hatcher's greatest marathon?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was her first.", "extracted_entity": null}
{"question": "was Desperate Housewives Teri Hatcher's longest tenure as actress?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She was on the show for eight years", "extracted_entity": null}
{"question": "did Desperate Housewives provide Teri Hatcher her longest acting paycheck?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Teri Hatcher is the highest", "extracted_entity": null}
{"question": "are there more games after FInal Fantasy VI than before in its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are more games after FInal", "extracted_entity": null}
{"question": "is silicon better for making wedding rings than bromine?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because silicon is a metal and b", "extracted_entity": null}
{"question": "can a comatose person produce art?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (see below)", "extracted_entity": null}
{"question": "do any adherents to Christianity in China have different saints than Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Protestants and Orthodox Christians have different", "extracted_entity": null}
{"question": "it it illegal to pump your own gas in Morris County New Jersey?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is illegal to pump your own", "extracted_entity": null}
{"question": "has September shifted to later month from the oldest calendar it appeared on?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The first time September was used was in", "extracted_entity": null}
{"question": "are Sram's kin found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Sram are found in World of", "extracted_entity": null}
{"question": "is Sram's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is, but it's not", "extracted_entity": null}
{"question": "is Balthor the Defiled's race present in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The race is not in WoW.", "extracted_entity": null}
{"question": "is Balthor the Defiled's race represented in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's the Tauren", "extracted_entity": null}
{"question": "is Balthor the Defiled's race found in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Balthor is a Blood Elf", "extracted_entity": null}
{"question": "was surveyor of Norman, Oklahoma a viking?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not a viking.", "extracted_entity": null}
{"question": "do Snow White dwarves fill out entire The Hobbit dwarves roster?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Snow White dwarves are 4", "extracted_entity": null}
{"question": "are there enough Snow White dwarves to play all dwarves in The Hobbit?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are only seven Snow White dwar", "extracted_entity": null}
{"question": "does Eli Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he works in the QB office.", "extracted_entity": null}
{"question": "does Peyton Manning work in the Oval Office?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he works in the Rose Garden.", "extracted_entity": null}
{"question": "is the Hague interested in geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is interested in maps", "extracted_entity": null}
{"question": "does the Hague care about geography?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, very much so.", "extracted_entity": null}
{"question": "will Wednesday origin religion followers be denied Communion?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If they are not baptized, then yes.", "extracted_entity": null}
{"question": "is Wednesday traditionally a pagan day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Wednesday was named after the Nor", "extracted_entity": null}
{"question": "can Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the brain is made up of cells", "extracted_entity": null}
{"question": "cab Cognition be impaired by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There is a genetic component to cogn", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, impaired cognition is a result", "extracted_entity": null}
{"question": "is impaired Cognition a result of hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Impaired cognition is a result of her", "extracted_entity": null}
{"question": "is impaired Cognition affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, impaired cognition can be affected", "extracted_entity": null}
{"question": "is Cognition impairment affected by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cognition impairment is caused by a", "extracted_entity": null}
{"question": "is Cognition impairment influenced by hereditary factors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a genetic component to cogn", "extracted_entity": null}
{"question": "does citrus grow in the Arctic circle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, citrus is a tropical fruit that needs", "extracted_entity": null}
{"question": "can you watch Rick and Morty at the bottom of Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Mariana Trench is the deep", "extracted_entity": null}
{"question": "is Mark Cuban visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is a US citizen,", "extracted_entity": null}
{"question": "can I talk to Vishnu?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course. He is the one who", "extracted_entity": null}
{"question": "is President Trump xenophobic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "do any countries that practice capitalism still have homelessness problem?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all countries that practice capitalism have hom", "extracted_entity": null}
{"question": "is homelessness still a problem in countries that practice capitalism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because there are still people who are hom", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book ever been in same project with Eva Green?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, never.", "extracted_entity": null}
{"question": "has character from Oscar Wilde's most famous book endured through other media?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The character of Dorian Gray has been portrayed", "extracted_entity": null}
{"question": "can Michael Jordan lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Michael Jordan cannot lease property in Northern", "extracted_entity": null}
{"question": "can Mark Cuban lease land in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can lease land in Northern Mar", "extracted_entity": null}
{"question": "does human with no gear die capturing giant squid in natural habitat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the squid will eat them.", "extracted_entity": null}
{"question": "is Western Honey bee important to US brand Nice?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Western Honey bee is important to", "extracted_entity": null}
{"question": "are crustaceans consumed raw by people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are consumed raw by people.", "extracted_entity": null}
{"question": "in New York, is snowboarding a popular July activity?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is Freya a composite of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Freya is a composite of Frigg", "extracted_entity": null}
{"question": "are there many similarities between Athena and Freya?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many similarities between Athena", "extracted_entity": null}
{"question": "did Ivan the Terrible have harshest nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, his grandfather was Grand Prince of Moscow", "extracted_entity": null}
{"question": "did Ivan the Terrible have the worst nickname of Grand Prince of Moscow since his grandfather ruled?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a terrible grand prince", "extracted_entity": null}
{"question": "do placozoa have learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "i have no idea", "extracted_entity": null}
{"question": "can an accordion player benefit from eye surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure. I think it depends", "extracted_entity": null}
{"question": "did Alexander the Great conquer part of Land of Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he conquered the coastal plain of", "extracted_entity": null}
{"question": "can a minotaur hypothetically break a tibia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible for a minotaur", "extracted_entity": null}
{"question": "does Alex Rodriguez have fewer championship memories than Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rodriguez has one championship ring (", "extracted_entity": null}
{"question": "does Michael Jordan have more championship memories than Alex Rodriguez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jordan won 6 championships with the", "extracted_entity": null}
{"question": "is John Kerry familiar with cross checking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kerry was asked about cross checking at", "extracted_entity": null}
{"question": "does table tennis make use of prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA recommended dailyrecommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, 7 zucchini would not", "extracted_entity": null}
{"question": "can Mark Cuban lease property in Northern Mariana Islands?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Northern Mariana Islands are not a", "extracted_entity": null}
{"question": "is Pan Satan-like in appearance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Pan is a horned, goat", "extracted_entity": null}
{"question": "did The King of Rock'n Roll's snack with bananas come from plants?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it came from the banana tree.", "extracted_entity": null}
{"question": "did The King of Rock'n Roll love a snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes he did.", "extracted_entity": null}
{"question": "was one of the fruits in apple banana bread in The King of Rock'n Roll's favorite snacks?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the fruits in apple banana bread", "extracted_entity": null}
{"question": "did the The King of Rock'n Roll love one of the fruits in apple banana bread?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he loved bananas", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if the score was 1", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles table tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were a Swedish pop group.", "extracted_entity": null}
{"question": "could the members of ABBA play a mixed doubles table tennis game?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "would it be impossible for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible.", "extracted_entity": null}
{"question": "does Jabberwocky have the length to be a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has 14 lines and", "extracted_entity": null}
{"question": "has one EGOT category eluded Justin Timberlake for nominations?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Best Musical", "extracted_entity": null}
{"question": "did Benjamin Butler attend a college that has the first name of a UFC welterweight fighter in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he attended Wake Forest.", "extracted_entity": null}
{"question": "can a Nissan product beat a Ferrari in a race? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nissan has a product that can", "extracted_entity": null}
{"question": "can a Nissan win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it will take a lot of work", "extracted_entity": null}
{"question": "does Nissan have any products that can win a race against a Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they can win a race against a", "extracted_entity": null}
{"question": "does Nissan have any products that can win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not even close.", "extracted_entity": null}
{"question": "did Nissan build any products that win a race against Ferrari? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nissan built the 30", "extracted_entity": null}
{"question": "was the Land of Israel under the control of a muslim empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Ottoman Empire", "extracted_entity": null}
{"question": "did a Turkish University of North Carolina at Chapel Hill professor win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Aziz Sancar won the Nobel prize", "extracted_entity": null}
{"question": "did a University of North Carolina at Chapel Hill professor from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did a University of North Carolina at Chapel Hill biochemist from Turkey win Nobel prize?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The person who won Nobel prize is Ah", "extracted_entity": null}
{"question": "did a 6-time NBA champion graduate from the University of North Carolina at Chapel Hill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Michael Jordan did.", "extracted_entity": null}
{"question": "did University of North Carolina at Chapel Hill give scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the University of North Carolina at Chapel", "extracted_entity": null}
{"question": "did University of North Carolina at Chapel Hill offer scholarship to Michael Jordan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was awarded a scholarship to play", "extracted_entity": null}
{"question": "was Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was named after Chief Justice Lewis Morris", "extracted_entity": null}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, William Morris", "extracted_entity": null}
{"question": "was the Morris County named after a chief justice of New York?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was named after a Chief Justice of", "extracted_entity": null}
{"question": "is there a wide range between smallest and largest asteroid size?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a wide range between the smallest", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after being fired from the band?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did founder of Skype win a race?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not win a race.", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to the Allied victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Eiffel Tower was a major contribution to", "extracted_entity": null}
{"question": "does soy milk increase chances of heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t. Soy milk", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in restaurant in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think she can.", "extracted_entity": null}
{"question": "can Drew Barrymore work as a cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Drew Barrymore can work as a", "extracted_entity": null}
{"question": "would a model be a good choice to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would not say that a model would be a", "extracted_entity": null}
{"question": "can you redeem more Bing search reward points than Google's equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can only redeem up to", "extracted_entity": null}
{"question": "can Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is a paralympian", "extracted_entity": null}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is a comedian.", "extracted_entity": null}
{"question": "can Josh Blue play Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Josh Blue does not play Paralym", "extracted_entity": null}
{"question": "could Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh Blue could participate in Paralymp", "extracted_entity": null}
{"question": "could Josh Blue attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Josh Blue is a stand-up comedian", "extracted_entity": null}
{"question": "could Josh Blue compete in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Josh Blue is eligible to comp", "extracted_entity": null}
{"question": "could Mohammed Ali attend Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Mohammed Ali could not attend the Par", "extracted_entity": null}
{"question": "could Stephen Hawking play chess? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he couldn't. He's", "extracted_entity": null}
{"question": "could Stephen Hawking play computer games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could play computer games.", "extracted_entity": null}
{"question": "could Aretha Franklin vote for a president when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was only 18.", "extracted_entity": null}
{"question": "could Aretha Franklin vote in an election when her first child was born?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, in 1960, Franklin", "extracted_entity": null}
{"question": "can you hide the state of Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. You just need to know", "extracted_entity": null}
{"question": "can you hide Iowa in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can hide Iowa in the English Channel", "extracted_entity": null}
{"question": "does soy milk cause heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, soy milk does not cause heart attacks", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jains believe in the existence of hell", "extracted_entity": null}
{"question": "wold a Jain argue against the existence of hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jainism is an Indian religion that teaches", "extracted_entity": null}
{"question": "wold a Jain believe in the Buddhist idea of karma?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jains believe in karma.", "extracted_entity": null}
{"question": "are Northern European characters portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the character of Macbeth is port", "extracted_entity": null}
{"question": "are Northern European characters depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are Northern European characters depicted in", "extracted_entity": null}
{"question": "did Macbeth depict Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Macbeth is an English play,", "extracted_entity": null}
{"question": "did Macbeth feature Northern European characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Macbeth features Northern European characters.", "extracted_entity": null}
{"question": "did a German-Irish actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Liam Neeson", "extracted_entity": null}
{"question": "did an oscar-winning actor play in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Orson Welles, the director of", "extracted_entity": null}
{"question": "did an actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Macbeth is not a play.", "extracted_entity": null}
{"question": "did an Oscar-winning actor play any of the characters in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 1958 film", "extracted_entity": null}
{"question": "were any of the characters in Macbeth played by an Oscar-winning actor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the actor won an Oscar for Best", "extracted_entity": null}
{"question": "was Napoleonic Wars longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Napoleonic Wars lasted from 18", "extracted_entity": null}
{"question": "did Napoleonic Wars last longer than the Great Depression?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Napoleonic Wars lasted from 18", "extracted_entity": null}
{"question": "do we need arithmetic to launch rockets into space?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, we do.", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Karl Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. The CPC is", "extracted_entity": null}
{"question": "are the members of the communist party of China influenced by Marx?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are influenced by Marx.", "extracted_entity": null}
{"question": "do members of the communist party of China come from Asia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they come from Asia.", "extracted_entity": null}
{"question": "can Uighurs become members of the communist party of China ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can become members of the Communist", "extracted_entity": null}
{"question": "are mongolians members of the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "do days get shorter in Seattle?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the Earth's tilt", "extracted_entity": null}
{"question": "do days get shorter in Seattle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not much", "extracted_entity": null}
{"question": "is September followed by colder months in the Northern Hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. September is the ninth", "extracted_entity": null}
{"question": "can I eat Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can eat Salmon in September.", "extracted_entity": null}
{"question": "can I hunt Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Salmon fishing is closed in September", "extracted_entity": null}
{"question": "can I hunt Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can hunt Salmon in California", "extracted_entity": null}
{"question": "can I catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you have to be in the right", "extracted_entity": null}
{"question": "can you catch Salmon in California in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can catch salmon in California in", "extracted_entity": null}
{"question": "can you fish Salmon in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Salmon fishing is very good in", "extracted_entity": null}
{"question": "do people speak Germanic languages in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Czech", "extracted_entity": null}
{"question": "is homeostasis associated with thermoregulation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, homeostasis is associated with thermor", "extracted_entity": null}
{"question": "can one Ancient Greek letter represent every season of Survivor?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is yes.", "extracted_entity": null}
{"question": "do traditional calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I\u2019ve seen some calicos with", "extracted_entity": null}
{"question": "has small children's painting style been profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I haven't", "extracted_entity": null}
{"question": "can small children's painting style be profitable?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not easy", "extracted_entity": null}
{"question": "can a judo expert defeat a kata expert?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The judo expert can defeat the kata expert", "extracted_entity": null}
{"question": "can a judo expert defeat someone that only does kata?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kata is not fighting.", "extracted_entity": null}
{"question": "is Jenny McCarthy in same industry as her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is a model and an actress.", "extracted_entity": null}
{"question": "do all the animals hedgehogs eat have spinal cords?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all animals hedgehogs eat", "extracted_entity": null}
{"question": "does number of Hamlet adaptations exceed Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's a close one.", "extracted_entity": null}
{"question": "is Hamlet's number of movie adaptations similar to Romeo and Juliet's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Hamlet has had more movie adaptations", "extracted_entity": null}
{"question": "is Hamlet one of Shakespeare's most movie adapted plays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hamlet is one of Shakespeare's", "extracted_entity": null}
{"question": "did Queen Margot keep Moliere's severed head?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and she used it as a paperweight", "extracted_entity": null}
{"question": "would a Lord Protector hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't see why not.", "extracted_entity": null}
{"question": "could a Tsar hypothetically boss a Lord Protector around?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the same way a Lord Protector", "extracted_entity": null}
{"question": "could a Tsar hypothetically tell a Lord Protector what to do?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the Tsar could also tell the", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a Lord Protector?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Tsar is a higher rank than Lord", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a Protector of the Realm?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say yes.", "extracted_entity": null}
{"question": "would a Tsar hypothetically outrank a US Vice President?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a Tsar is a monarch and a", "extracted_entity": null}
{"question": "can SARS-COV-2 infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the virus can infect maritime pil", "extracted_entity": null}
{"question": "has Leninist ideology influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Leninist ideology has influenced Boh", "extracted_entity": null}
{"question": "can United States Secretary of State slap United Kingdom representative without repercussions?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States Secretary of State cannot s", "extracted_entity": null}
{"question": "were Whirling Dervishes in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Whirling Dervishes were", "extracted_entity": null}
{"question": "is Reiki stored in a pill bottle?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Reiki is an energy and is not", "extracted_entity": null}
{"question": "does Reiki require refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki does not require refills.", "extracted_entity": null}
{"question": "does Reiki require frequent refills?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The energy is eternal and is not", "extracted_entity": null}
{"question": "does Reiki require a prescription?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki is not a medical treatment.", "extracted_entity": null}
{"question": "can viruses infect maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "have politics been influenced by Marxist ideology in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, politics have been influenced by Marxist ide", "extracted_entity": null}
{"question": "are most Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I\u2019m not", "extracted_entity": null}
{"question": "is Tokyo Tower close to city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is located in Tokyo.", "extracted_entity": null}
{"question": "is Tokyo Tower in same country as famed city in Demon City anime?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Demon City is in Tokyo, Japan", "extracted_entity": null}
{"question": "can someone have a long life after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not possible to have a long", "extracted_entity": null}
{"question": "is shoe soup relatively harmless?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, shoe soup is a deadly poison", "extracted_entity": null}
{"question": "can a person survive eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you don't believe me,", "extracted_entity": null}
{"question": "can a person survive after eating shoe soup?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person can survive after eating", "extracted_entity": null}
{"question": "are all jokes harmless to health?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Jokes can be harmful to health", "extracted_entity": null}
{"question": "could someone in Boston watch the same episode of The Tonight Show in two days?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the episode is broadcast at 1", "extracted_entity": null}
{"question": "could someone in Houston see the opening of The Tonight Show before the day change?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the show was on at 11", "extracted_entity": null}
{"question": "could someone in Houston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Because the show started at 11", "extracted_entity": null}
{"question": "could someone in Boston see the opening of The Tonight Show before midnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they had a TV set that could", "extracted_entity": null}
{"question": "is Beauty and the Beast (1991 film) another Disney pilfered plot?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's the 194", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically play a full game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not without a lot of bick", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically play a game of basketball against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were all born to the same", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in half a day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure what you mean by \u201cupload", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in a quarter day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can\u2019t. The maximum upload", "extracted_entity": null}
{"question": "is LG Electronics located in a city with an official bird that has a blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. LG Electronics is located in a", "extracted_entity": null}
{"question": "did Holy Land belong to Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it belonged to Allah's people", "extracted_entity": null}
{"question": "can Holy Land be claimed by Adamu's people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Adamu's people are the", "extracted_entity": null}
{"question": "did Adamu's people have claim to Holy Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was their homeland.", "extracted_entity": null}
{"question": "did Adamu's people have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the land was theirs before the three", "extracted_entity": null}
{"question": "did Assyrians have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Assyrians were not in", "extracted_entity": null}
{"question": "did another group have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not that I am aware of.", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land other than three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Jews, the Christians, and the Muslims", "extracted_entity": null}
{"question": "would US states in 1900 be enough to divide up ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes, if you use the current", "extracted_entity": null}
{"question": "is Armageddon exclusive to the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Armageddon is a term from the", "extracted_entity": null}
{"question": "did anyone have claim to Holy Land before the three religions that revere it?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not that I know of.", "extracted_entity": null}
{"question": "were there worse vote differences than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In 1824, Andrew", "extracted_entity": null}
{"question": "were there worse defeats than Carter's 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, 1984, 1", "extracted_entity": null}
{"question": "will sports announcing be possible with Bell's palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you will have to be aware of", "extracted_entity": null}
{"question": "can Bell's palsy sufferers still have productive show business careers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Look at the careers of Michael J", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in three minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is possible, if you have a", "extracted_entity": null}
{"question": "is it possible to finish reading The Hobbit in half an hour?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "was Florence ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Medici family were bankers and", "extracted_entity": null}
{"question": "was Flornce ever a theocracy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was a theocracy", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell name of Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is the only possible way", "extracted_entity": null}
{"question": "can United States Army Rangers complete a Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Mozambique Drill is a", "extracted_entity": null}
{"question": "can United States Army Rangers perform a successful Mozambique Drill?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Mozambique Drill is a", "extracted_entity": null}
{"question": "can Kelly Clarkson's singing break glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact, her voice is so powerful", "extracted_entity": null}
{"question": "did multiple weekdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Apollo 15 mission last", "extracted_entity": null}
{"question": "did two Wednesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they landed on the Moon on", "extracted_entity": null}
{"question": "did two Tuesdays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was one Tuesday.", "extracted_entity": null}
{"question": "did two Mondays pass during the Apollo 15 mission?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There was only one Monday on the Apol", "extracted_entity": null}
{"question": "did Maya Angelou win a Triple Crown?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she didn\u2019t", "extracted_entity": null}
{"question": "are felidae represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are represented on the Chinese calendar.", "extracted_entity": null}
{"question": "does the Atlantic reach 500 battalions worth of people?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It does.", "extracted_entity": null}
{"question": "can Cheb Mami possibly win a Pulitzer Prize in History?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He cannot.", "extracted_entity": null}
{"question": "did Martin Luther believe in God?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "is Martin Luther celebrated by many people?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Martin Luther is celebrated by many people.", "extracted_entity": null}
{"question": "was Martin Luther capable of logical thinking?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "was Martin Luther susceptible to infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a human being, and therefore", "extracted_entity": null}
{"question": "was Martin Luther susceptible to death by infection?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it was very likely that he would", "extracted_entity": null}
{"question": "do people still fear the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, very much so.", "extracted_entity": null}
{"question": "can a swastika cause emotional pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The swastika is a symbol of", "extracted_entity": null}
{"question": "can a swastika cause pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I don\u2019t", "extracted_entity": null}
{"question": "is a nerd a common character in movies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, nerds are common in movies", "extracted_entity": null}
{"question": "did the Great Depression break up families?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did.", "extracted_entity": null}
{"question": "is dopamine snorted up the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dopamine is a hormone", "extracted_entity": null}
{"question": "is dopamine snorted through the nose?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dopamine is not snorted through", "extracted_entity": null}
{"question": "can you buy Reddit at Barnes & Noble?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can buy a book that has", "extracted_entity": null}
{"question": "could an elephant seal reach the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it could swim", "extracted_entity": null}
{"question": "could an elephant seal dive to the bottom of Lake Superior?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it could get pretty close. Lake", "extracted_entity": null}
{"question": "could an elephant seal dive to the bottom of Lake Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would have to swim all", "extracted_entity": null}
{"question": "could a nymph tick pass through a ISO 838 hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The tick would not be able to pass through the", "extracted_entity": null}
{"question": "did Wilbur Scoville know about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "did Wilbur Scoville care about capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t.", "extracted_entity": null}
{"question": "did George Washington cross the Potomac in a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he crossed the Potomac in a", "extracted_entity": null}
{"question": "did Edward of Westminster ever spend time in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he spent time in Gloucestershire", "extracted_entity": null}
{"question": "was Edward of Westminster ever in Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "is Antarctica ideal for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The temperature is -50\u00b0C", "extracted_entity": null}
{"question": "did it get dark in Tennessee during the Solar eclipse of August 21, 2017?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it didn't.", "extracted_entity": null}
{"question": "are polar bears indigenous to the Golan Heights?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "is it easier for a human to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Atlantic Ocean is saltier than", "extracted_entity": null}
{"question": "is it easier to float in the Atlantic Ocean than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Atlantic is warmer.", "extracted_entity": null}
{"question": "is it easier to float in the Dead Sea than the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the saltier the water, the easier", "extracted_entity": null}
{"question": "is Canada south of the Arctic Ocean?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would John the Baptist be alone at a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would be there with all the other", "extracted_entity": null}
{"question": "could Jamie Lee Curtis buy a package of Marlboro in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because of Proposition 65, a", "extracted_entity": null}
{"question": "could Jamie Lee Curtis buy a bottle of chardonnay in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the state of California does not allow", "extracted_entity": null}
{"question": "could Jamie Lee Curtis purchase alcohol in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is a minor.", "extracted_entity": null}
{"question": "could 2020 Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they had the ability to", "extracted_entity": null}
{"question": "could 2020 Brazilian Navy hypothetically win Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would a plate of crustaceans be appropriate for Diwali?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, if they're not alive", "extracted_entity": null}
{"question": "would an Olmec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it wasn't built yet.", "extracted_entity": null}
{"question": "would an Incan recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Great Pyramid is a", "extracted_entity": null}
{"question": "would an Aztec recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They were aware of it.", "extracted_entity": null}
{"question": "would an Aztec warrior recognize the Great Pyramid of Giza?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is the same design as the", "extracted_entity": null}
{"question": "would Mickey Mouse be camouflaged by the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. because it would be wearing a flag", "extracted_entity": null}
{"question": "can you fish Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, September is a good time to fish for", "extracted_entity": null}
{"question": "have the Dukes of Beaufort ever been to Gloucestershire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Dukes of Beaufort have", "extracted_entity": null}
{"question": "do people interested in numismatics care about British royals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "do people interested in numismatics care about British royalty?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "of course they do!", "extracted_entity": null}
{"question": "do people interested in numismatics care about British kings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There is a great deal of interest in", "extracted_entity": null}
{"question": "do people interested in numismatics care about Edward III?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. (although I do, and I have", "extracted_entity": null}
{"question": "did Pink Floyd sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "did Roger Waters sing about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And he sang about the French Riviera", "extracted_entity": null}
{"question": "is Bing useless for anonymous browsing?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bing is not useless for anonymous browsing.", "extracted_entity": null}
{"question": "is a lacrosse ball blocked by carapace?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is blocked by the carapace", "extracted_entity": null}
{"question": "are there bridges people would rather fall from than Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many bridges people would rather", "extracted_entity": null}
{"question": "can someone survive a fall from Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would take a lot of luck", "extracted_entity": null}
{"question": "is a fall from Golden Gate bridge likely fatal?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you'd probably survive.", "extracted_entity": null}
{"question": "did Popeye eat an unhealthy amount of spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can you catch Salmon during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can catch Salmon in September.", "extracted_entity": null}
{"question": "has Leninism affected politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it has.", "extracted_entity": null}
{"question": "did Pink Floyd play a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they did play a song about the", "extracted_entity": null}
{"question": "is Return of the Jedi's Han Solo bad hypothetical United States Air Force candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a \"self-center", "extracted_entity": null}
{"question": "did Stone Cold Steve Austin outshine Four Horsemen's tag team accomplishments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they were just a tag team,", "extracted_entity": null}
{"question": "was Stone Cold Steve Austin better tag teamer than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "does AC/DC's success trump Lil Wayne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd say yes.", "extracted_entity": null}
{"question": "do large mammals exhibit human jealousy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "is human jealousy exhibited in larger mammals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, jealousy is a uniquely human", "extracted_entity": null}
{"question": "does the goat zodiac hypothetically make good entertainers?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are very expressive and good at", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 18th century judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. But it would be", "extracted_entity": null}
{"question": "is Hilo Hawaii an ideal location for snowboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. The nearest ski resort", "extracted_entity": null}
{"question": "could Tata Hexa get the Spice Girls to a nearby venue?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "would Fortuner be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Fortuner has more space for all", "extracted_entity": null}
{"question": "would Tata Hexa be more convenient for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hexa, of course!", "extracted_entity": null}
{"question": "would Tata Hexy be more roomy for Spice Girls than Acura NSX?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd rather ride in a Tata N", "extracted_entity": null}
{"question": "can Julia Roberts hang out with BTS?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can hang out with BTS.", "extracted_entity": null}
{"question": "can Julia Roberts ride Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she can.", "extracted_entity": null}
{"question": "can Julia Roberts enjoy Space Mountain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Julia Roberts is a human being, and", "extracted_entity": null}
{"question": " Is The Invisible Man its author's most adapted book to film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Pirate's Flight at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he's willing to pay the", "extracted_entity": null}
{"question": "is Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that was The Time Machine.", "extracted_entity": null}
{"question": "can Justin Timberlake ride on the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is very hard to do.", "extracted_entity": null}
{"question": "is H.G. Wells' most widespread book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The War of the Worlds is the", "extracted_entity": null}
{"question": "can Justin Timberlake ride the Teacups at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "is H.G. Wells' most prolific book to film The Invisible Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is War of the Worlds.", "extracted_entity": null}
{"question": "is The Invisible Man H.G. Wells most adapted film?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I believe it is.", "extracted_entity": null}
{"question": "academy Awards most prolific amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Walt Disney was the most prolific am", "extracted_entity": null}
{"question": "academy Awards most celebrated amusement park guy was Walt Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Academy Awards are more commonly known as the O", "extracted_entity": null}
{"question": "was Walt Disney most celebrated amusement park creator by Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "would Shaun King disagree with Immanuel Kant's views?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would disagree with Immanuel Kant", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant disagree about race?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shaun King and Immanuel Kant would disag", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant not see eye to eye about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Shaun King and Immanuel Kant", "extracted_entity": null}
{"question": "would Shaun King and Immanuel Kant disagree about philosophy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not", "extracted_entity": null}
{"question": "is disco still influencing music in the 2000s?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is Bornean Orangutan higher on World Wildlife Fund priority list than Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "does World Wildlife Fund prioritize Bornean Orangutan over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, World Wildlife Fund prioritizes Bor", "extracted_entity": null}
{"question": "is Bornian Orangutan prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Leopard cat is prioritized by", "extracted_entity": null}
{"question": "is Black Rhino prioritized by World Wildlife Fund over Leopard cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Black Rhino and Leopard cat are", "extracted_entity": null}
{"question": "has Stone Cold Steve Austin had more Intercontinental success than Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He won the title 6 times.", "extracted_entity": null}
{"question": "is genre that dominated late 1970's music still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it is, it is called pop, pop", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still relevant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did one of the Beatles have similar taste in women to Eric Clapton?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, George Harrison.", "extracted_entity": null}
{"question": "are mollusks important to the history of the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. they are the only animals that can make", "extracted_entity": null}
{"question": "is Bill Gates more productive than the country of Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Bill Gates is worth more than", "extracted_entity": null}
{"question": "is Bill Gates more productive than Dominica?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "did the Bible borrow from ancient myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Bible is the only ancient text that", "extracted_entity": null}
{"question": "did ancient myths inspire the Bible?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "could a single bitcoin ever buy a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a single bitcoin would never buy", "extracted_entity": null}
{"question": "is black pepper often combined with a substance that contains chloride?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chloride is found in table salt", "extracted_entity": null}
{"question": "is it best to avoid chocolate with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chocolate is a type of candy that", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be one of Bacchus's favorite holidays?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. New Year's Eve is the", "extracted_entity": null}
{"question": "has Drew Carey surpassed Doug Davidson at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Doug Davidson was fired from his job", "extracted_entity": null}
{"question": "has Drew Carey surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "is music genre that dominated late 70's still alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is still alive.", "extracted_entity": null}
{"question": "is Drew Carey one of the longest Price is Right hosts?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has been hosting the show since", "extracted_entity": null}
{"question": "is Drew Carey second longest Price is Right Host?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has hosted the show since 2", "extracted_entity": null}
{"question": "does it snow in the arctic circle during September?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in some parts of the arctic circle", "extracted_entity": null}
{"question": "has Leninism influenced politics in Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Leninism influenced politics in Bohemia in the", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still evolving today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, music genres that dominated the late", "extracted_entity": null}
{"question": "has Drew Carey's surpassed Dennis James at the same job?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dennis James is still the host of High", "extracted_entity": null}
{"question": "are more tamarinds grown in Honduras than Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course, because Honduras is", "extracted_entity": null}
{"question": "would a tamarind tree have difficulty growing in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have never heard of a tamarind tree", "extracted_entity": null}
{"question": "would a tamarind tree grow poorly in Anchorage?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tamarind trees are tropical trees. They are", "extracted_entity": null}
{"question": "would a tamarind tree grow poorly in Alaska?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because tamarind trees are tropical trees", "extracted_entity": null}
{"question": "is shrimp essential to Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is a popular ingredient", "extracted_entity": null}
{"question": "does Drew Carey's Price is Right tenure surpass Dennis James's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. James hosted the show from 19", "extracted_entity": null}
{"question": "are more tamarinds grown in Honduras than Canada?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a tropical fruit and Canada", "extracted_entity": null}
{"question": "would someone in a sauna be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because hypothermia is the body cool", "extracted_entity": null}
{"question": "can a student from Beder understand the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I believe that any student who is in the", "extracted_entity": null}
{"question": "can a student from Beder read the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is very difficult.", "extracted_entity": null}
{"question": "can all Suikoden characters find dates among Final Fantasy VI characters?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only characters who can be found in both", "extracted_entity": null}
{"question": "would a tenth of McCain's electoral votes make John Kerry President in 2004?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If Kerry had received just 1", "extracted_entity": null}
{"question": "could Amazon defeat Iceland in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not, but it would certainly give them a", "extracted_entity": null}
{"question": "did Jerry Seinfeld's mouth help him earn a lot of cash?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a comedian.", "extracted_entity": null}
{"question": "did Jerry Seinfeld's mouth help him earn a substantial amount of money?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "did Jerry Seinfeld earn a lot of money using his mouth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Vishnu?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Brahma?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is his son.", "extracted_entity": null}
{"question": "would Alan Alda fit in on the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a good actor.", "extracted_entity": null}
{"question": "does Secretariat have a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has 6 Formula One championships", "extracted_entity": null}
{"question": "does Secretariat hold a record for Formula One championships?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "will a living thing survive the entirety of Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because of entropy", "extracted_entity": null}
{"question": "is the Greek alphabet barred for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is the Greek alphabet prohibited for use in Kpop?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it's not that common", "extracted_entity": null}
{"question": "is the Greek alphabet prohibited for use in Korean pop music?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not prohibited.", "extracted_entity": null}
{"question": "can Tony Blair be imprisoned in the Tower of London?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is found guilty of war cr", "extracted_entity": null}
{"question": "do travelers to Casablanca have a hard time finding a BLT?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "does Jamie Oliver care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he cares about flavour.", "extracted_entity": null}
{"question": "does Gordon Ramsey know about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "does Gordon Ramsey care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he cares about solubility.", "extracted_entity": null}
{"question": "can people in San Antonio cheer on the Memphis Grizzlies?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you're cheering for a", "extracted_entity": null}
{"question": "does Moon Jae-in own multiple species of pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Moon Jae-in does not own", "extracted_entity": null}
{"question": "does Moon Jae-in support animal rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He has been known to be a vegetarian and", "extracted_entity": null}
{"question": "does Moon Jae-in own any pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has a dog named Maru.", "extracted_entity": null}
{"question": "do humans pose a threat to Bengal foxes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are hunted for their fur and", "extracted_entity": null}
{"question": "do Bengal foxes take care of their children?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "did a manta ray murder Steve Irwin?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was a stingray.", "extracted_entity": null}
{"question": "does celery contain any essential nutrients?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, celery contains essential nutrients such", "extracted_entity": null}
{"question": "has celery been used for purposes other than food?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Celery has been used for purposes other than food", "extracted_entity": null}
{"question": "did one person cause the price of celery to spike at any time in the past?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not even close.", "extracted_entity": null}
{"question": "was Bill Clinton President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "can computers play Poker better than humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not yet better than the", "extracted_entity": null}
{"question": "is it possible to make a living only playing Poker?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not easy.", "extracted_entity": null}
{"question": "could James Brown's ex-wives play a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they could not play a doubles game of", "extracted_entity": null}
{"question": "could James Brown's ex-wives have played a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are not married to James Brown", "extracted_entity": null}
{"question": "could James Brown's ex-wives have held a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they couldn't find a ball", "extracted_entity": null}
{"question": "did slaves play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, slaves played a role in the history of", "extracted_entity": null}
{"question": "can Evander Holyfield consume pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Evander Holyfield is not a Jew", "extracted_entity": null}
{"question": "can an artisan sell items to someone in another country?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they have a store in", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau require subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they're all fluent in", "extracted_entity": null}
{"question": "did Malcolm X play a role in the history of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Malcolm X played a role in", "extracted_entity": null}
{"question": "could one Amazon stock ever buy twenty four year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could.", "extracted_entity": null}
{"question": "can French Toast cause Salmonella poisoning?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Salmonella can be found in raw", "extracted_entity": null}
{"question": "were there any pandemics during Moon Jae-in's presidency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. In fact, there was no pandemic", "extracted_entity": null}
{"question": "is the paleo diet healthy for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the paleo diet is a good", "extracted_entity": null}
{"question": "does soy milk increase heart attack?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn't.", "extracted_entity": null}
{"question": "are Northern Europeans depicted in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Danes and Norwegians are dep", "extracted_entity": null}
{"question": "are Northern Europeans portrayed in Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are Northern Europeans portrayed in", "extracted_entity": null}
{"question": "can a one armed drummer be elite?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a one armed drummer can be el", "extracted_entity": null}
{"question": "has Leninism influenced Bohemia?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Leninism has not influenced Bohemia", "extracted_entity": null}
{"question": "is ABBA's 1970's music genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, ABBA's 197", "extracted_entity": null}
{"question": "is music genre that dominated late 1970's still evolving?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is still evolving", "extracted_entity": null}
{"question": "would Alan Alda fit in on Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He would be a great addition to the", "extracted_entity": null}
{"question": "does Moon Jae-in own both cats and dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "was Franjo Tudjman President of Croatia when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was President of Croatia from", "extracted_entity": null}
{"question": "is the paleo diet beneficial for people with diabetes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a paleo diet is beneficial", "extracted_entity": null}
{"question": "was Al Gore Vice President of the US when the Dayton Agreement was signed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he wasn't.", "extracted_entity": null}
{"question": "is Noah's Ark more imposing than Batillus?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is more beautiful and more of", "extracted_entity": null}
{"question": "does a horse fly neigh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Horses do neigh.", "extracted_entity": null}
{"question": "could Brooke Shields handle University of Pennsylvania curriculum?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Brooke Shields could handle the University of", "extracted_entity": null}
{"question": "would University of Pennsylvania admit actress Brooke Shields?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Brooke Shields would be admitted", "extracted_entity": null}
{"question": "did actress Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was admitted only because she is famous", "extracted_entity": null}
{"question": "did Brooke Shields have the credentials to get into University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was a great student with a", "extracted_entity": null}
{"question": "were many young females attracted to Justin Timberlake?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many young females were attracted to Justin", "extracted_entity": null}
{"question": "did Justin Timberlake have many young female fans?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had many young female fans.", "extracted_entity": null}
{"question": "would a physician be unlikely to prescribe Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not be unlikely to prescribe", "extracted_entity": null}
{"question": "can Viper Room crowd squeeze into National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not enough space.", "extracted_entity": null}
{"question": "can National Diet building accommodate Viper Room crowd?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "would John Muir be unlikely to have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John Muir was a Scottish-American naturalist", "extracted_entity": null}
{"question": "would Alan Alda be comfortable on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He was the star of M*A*S", "extracted_entity": null}
{"question": "would Alan Alda fit in on the set of Chicago Hope?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course he would! He's a doctor", "extracted_entity": null}
{"question": "would Alan Alda fit in on the set of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he'd be bored out of", "extracted_entity": null}
{"question": "would Alan Alda fit in with the cast of Grey's Anatomy?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he'd be the perfect Chief of", "extracted_entity": null}
{"question": "would John Muir dislike being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he loved being outside", "extracted_entity": null}
{"question": "would John Muir hate being inside all day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would hate it.", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen Romeo and Juliet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Romeo and Juliet was written", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen the plays of Shakespeare?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Elizabeth was queen from 155", "extracted_entity": null}
{"question": "could someone have visited the Library of Alexandria and the pyramids on the same vacation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were in Egypt, and yes", "extracted_entity": null}
{"question": "would Gal Gadot be hired as Victoria Secret model before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she is more beautiful than Selena", "extracted_entity": null}
{"question": "do middle school students typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, students learn about the Pythagore", "extracted_entity": null}
{"question": "does the bengal fox have hair covering it's body?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the bengal fox has hair", "extracted_entity": null}
{"question": "would a librarian use a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course.", "extracted_entity": null}
{"question": "can Linus Torvald's family ride in a Maruti Suzuki Baleno?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because his family does not have the money", "extracted_entity": null}
{"question": "can a Maruti Suzuki Baleno transport Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a car.", "extracted_entity": null}
{"question": "can Royal Albert Hall hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it could hold every McDonald'", "extracted_entity": null}
{"question": "can Madison Square Garden hold every McDonald's employee?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's only a 20", "extracted_entity": null}
{"question": "could Titanic hypothetically hold Spanish-American War combat casualties?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not the entire Spanish-American War", "extracted_entity": null}
{"question": "could Spanish-American War combat casualties hypothetically fit on Titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but I don't think they would", "extracted_entity": null}
{"question": "did Ptolemy use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used the Egyptian calendar.", "extracted_entity": null}
{"question": "do camels live in Ontario?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Camels live in the desert.", "extracted_entity": null}
{"question": "will Victoria's Secret hire Gal Gadot before Selena Gomez?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Gal Gadot is a model", "extracted_entity": null}
{"question": "do 10-12 year olds typically learn about something that Pythagoras developed?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they learn about Pythagoras", "extracted_entity": null}
{"question": "does the little penguin share space with Kiwis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are both in the same region of", "extracted_entity": null}
{"question": "does horseradish have hooves?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it has roots.", "extracted_entity": null}
{"question": "is paper a product of logging?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is one of the", "extracted_entity": null}
{"question": "would Boticelli's mythological subjects fill a gallery before Salvador Dali's?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Boticelli's mythological subjects", "extracted_entity": null}
{"question": "would ten gallons of seawater hold a six year old in the air on a seesaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the seesaw was a diving", "extracted_entity": null}
{"question": "are Genghis Khan's descendants more prolific than those of Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was an event in the Balkans considered a contributing factor to World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the assassination of Franz Ferdinand", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field an NHL team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even the Canadiens.", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field a full ice hockey team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They could field a full basketball team,", "extracted_entity": null}
{"question": "could the crew members of Apollo 13 field a full basketball team?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "was Fraktur likely to have been used in the publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure if Fraktur was used in", "extracted_entity": null}
{"question": "was Fraktur likely to have been used in publications of Anders Arrebo?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no. Fraktur was a script", "extracted_entity": null}
{"question": "does the spinal cord play a role in skateboarding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the spinal cord is responsible for sending", "extracted_entity": null}
{"question": "does the visual cortex play a role in regulating metabolic function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The visual cortex does not play a role in", "extracted_entity": null}
{"question": "does the visual cortex play a role in regulating lung function?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The visual cortex is not involved in", "extracted_entity": null}
{"question": "can people see the code behind OpenOffice.org?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a closed source application.", "extracted_entity": null}
{"question": "does a Jetta require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does require an anchor.", "extracted_entity": null}
{"question": "does a Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Prius does not require an anchor", "extracted_entity": null}
{"question": "does a Toyota Prius require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Toyota Prius does require an", "extracted_entity": null}
{"question": "could the children of Greek hero Jason hypothetically field a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they couldn\u2019t.", "extracted_entity": null}
{"question": "is the artist of the album Slay Belles a drag queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not the one you think.", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind The Brig?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did collaborate with the film", "extracted_entity": null}
{"question": "did John Lennon collaborate with the man who made the film Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Lennon and Yoko O", "extracted_entity": null}
{"question": "did Beethoven ever compose an EDM song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film starring Michael Palin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the film \"Monty Python and", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film starring John Cleese?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in Monty Python and the Holy Gra", "extracted_entity": null}
{"question": "did John Lennon collaborate with the filmmaker behind Walden?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was a different John Lennon", "extracted_entity": null}
{"question": "was John Searle born in a state that is bordered by Wyoming?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Wyoming.", "extracted_entity": null}
{"question": "does the Illuminati leave messages in modern society?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would King Henry V hypothetically demolish Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He would.", "extracted_entity": null}
{"question": "would King Henry V hypothetically prevail against Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "would King Henry V hypothetically defeat Charlemagne at Agincourt?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Charlemagne is not a real person", "extracted_entity": null}
{"question": "would Constantinople repel a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if the Franks didn'", "extracted_entity": null}
{"question": "would Constantinople survive a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Constantinople would survive an invasion by", "extracted_entity": null}
{"question": "would Constantinople withstand a hypothetical invasion by Charlemagne?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the city would be captured", "extracted_entity": null}
{"question": "in the year 1000, did the Eastern Orthodox Church and the Byzantine Empire use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin apply his finisher to the largest mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he can hit the Stunner", "extracted_entity": null}
{"question": "is kaffir lime the man ingredient in key krueng?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. kaffir lime is the main", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam two White-tailed deer simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they have to be very small White", "extracted_entity": null}
{"question": "can someone from Denmark go to Sweden without their passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you will need a passport or ID", "extracted_entity": null}
{"question": "can a male mule deer be body slammed by Stone Cold Steve Austin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes he can.", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin body slam a male mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course.", "extracted_entity": null}
{"question": "are most of the products labelled Fair Trade exported from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Fair Trade products are made and grown in", "extracted_entity": null}
{"question": "was Harry Houdini's wife bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She was a very good magician and", "extracted_entity": null}
{"question": "was Harry Houdini bad for the business of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. His publicity stunts and his", "extracted_entity": null}
{"question": "did Harry Houdini disprove the claims of psychics?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "is Drew Carey in a sports Hall of Fame?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Rand has a valid eye exam certification", "extracted_entity": null}
{"question": "does Rand have valid eye exam certification in every state ?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rand does not have valid eye exam cert", "extracted_entity": null}
{"question": "is Rand Paul prohibited from conducting eye exams?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not prohibited from conducting", "extracted_entity": null}
{"question": "can a Toyota Hilux transport a human heart from NYC to Raleigh NC?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can, but you can\u2019t", "extracted_entity": null}
{"question": "can a Toyota Hilux get to Havana from NYC before sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can a Toyota Hilux get to Havana from NYC in sixteen hours?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it will be very uncomfort", "extracted_entity": null}
{"question": "is one of the ingredients of photosynthesis found in emeralds?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The ingredients of photosynthesis are carbon", "extracted_entity": null}
{"question": "can you pour diethyl ether?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but be careful.", "extracted_entity": null}
{"question": "did Larry King fight in World War I?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Larry King did not fight in World War", "extracted_entity": null}
{"question": "have any WWE champions outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Triple H and John Cena", "extracted_entity": null}
{"question": "has a Neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the Neanderthal served as", "extracted_entity": null}
{"question": "are infants excluded from joining the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Supreme Court of the United States only", "extracted_entity": null}
{"question": "can a worm suffer a myocardial infarction?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure what you mean by \"my", "extracted_entity": null}
{"question": "can every Prime Minister of the United Kingdom since 1970 fill all the US Supreme Court chairs?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "can every Prime Ministers of the United Kingdom since 1970 fill all the US Supreme Court positions?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they can't all be appointed", "extracted_entity": null}
{"question": "will Justin Bieber replace Mike Pence in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's not even old enough.", "extracted_entity": null}
{"question": "will Justin Bieber be Trump's running mate in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He is a Canadian and therefore not eligible", "extracted_entity": null}
{"question": "can Justin Bieber represent New York in Congress in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can", "extracted_entity": null}
{"question": "will Justin Bieber become US President in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is still alive.", "extracted_entity": null}
{"question": "will Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he runs.", "extracted_entity": null}
{"question": "can Justin Bieber win the 2020 US Presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "were there highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were highways during the Silent", "extracted_entity": null}
{"question": "did cars travel on highways during the Silent Generation?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "did members of the Silent Generation use highways?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "can a rabbi perform any Christian rituals?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a rabbi can perform any Christian rit", "extracted_entity": null}
{"question": "can baptism be done by a rabbi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a rabbi is a non-", "extracted_entity": null}
{"question": "can a Rabbi perform a Christian baptism?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A Rabbi is not a Christian.", "extracted_entity": null}
{"question": "can a Rabbi administer any of the Christian Sacraments?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he can bless the elements of the", "extracted_entity": null}
{"question": "were there typewriters in the Empire of Japan?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were.", "extracted_entity": null}
{"question": "did people listen to The Blues during World War I?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did", "extracted_entity": null}
{"question": "can a person from New Brunswick understand the language of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they cannot.", "extracted_entity": null}
{"question": "can a person from New Brunswick understand the speech of someone from Burundi?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not unless the person from New Brunsw", "extracted_entity": null}
{"question": "can a person from Madagascar order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the language of the restaurant is English and", "extracted_entity": null}
{"question": "can a person from Burundi order food in their native language in New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you have the right to", "extracted_entity": null}
{"question": "do lizards breed in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I have seen them there.", "extracted_entity": null}
{"question": "can lizards survive temperatures in far north New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. Maybe.", "extracted_entity": null}
{"question": "is the Swastika a religious symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is a symbol", "extracted_entity": null}
{"question": "did Japanese people win the War of the Bucket?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they lost.", "extracted_entity": null}
{"question": "can you transport nine pounds of weed in a bag?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can transport nine pounds of weed", "extracted_entity": null}
{"question": "did the husband of Elizabeth Hamilton work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he worked with a lieutenant.", "extracted_entity": null}
{"question": "did the husband of Alexander Hamilton's granddaughter work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not work with a lieutenant.", "extracted_entity": null}
{"question": "did Elizabeth Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a lieutenant.", "extracted_entity": null}
{"question": "did Elizabeth Margaret Hunter's husband work with a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did.", "extracted_entity": null}
{"question": "do Adobe Suite and Life Is Strange video game share similar coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. Life is strange is a", "extracted_entity": null}
{"question": "did Sugar Ray Robinson ever experience pain?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did the creator of Saturday Night Live ever work with Jack Paar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Lorne Michaels worked with Jack", "extracted_entity": null}
{"question": "did the creator of Saturday Night Live ever work with Johnny Carson?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Lorne Michaels was a writer", "extracted_entity": null}
{"question": "does the author of Rebecca use Cornwall in another novel?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "could a participant in the American Civil War read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they had a good education and could", "extracted_entity": null}
{"question": "could the winner of a Nobel Peace Prize have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the winner of the Nobel Peace Prize could", "extracted_entity": null}
{"question": "could the author of The Bell Jar have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The author of The Bell Jar was Sylvia Pl", "extracted_entity": null}
{"question": "could Saint Augustine have read Hamlet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have.", "extracted_entity": null}
{"question": "can a human safely consume Cannabis?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course. Humans have been cons", "extracted_entity": null}
{"question": "did Metallica ever release a rap song?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they did have a song called \u201c", "extracted_entity": null}
{"question": "does the name C-SPAN refer to a form of telecommunications?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Cable-Satellite Public Affairs Network", "extracted_entity": null}
{"question": "could someone watch C-Span on cable in 1980?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. C-SPAN began broadcasting in", "extracted_entity": null}
{"question": "was C-Span on the air in 1987?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The C-Span channel was on the", "extracted_entity": null}
{"question": "did Christopher Hitchens ever appear on C-Span?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, several times.", "extracted_entity": null}
{"question": "did Brian Lamb ever host a television program on C-SPAN?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he hosted a program on C-SP", "extracted_entity": null}
{"question": "is Battle Hymn of the Republic considered christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I think it", "extracted_entity": null}
{"question": "did Fanny Crosby write Christian music?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she wrote over 8,00", "extracted_entity": null}
{"question": "is Christian music played in Churches?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. However, it is not", "extracted_entity": null}
{"question": "did the founders of Encyclop\u00e6dia Britannica enjoy views of the Firth of Forth?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "was the country featured in All My Life one of the participants in the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "is an economist able to determine why the price of a burrito changes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. They can tell you", "extracted_entity": null}
{"question": "is an economist able to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, an economist is able to study the", "extracted_entity": null}
{"question": "is an economist qualified to study the effects of inflationary pressure?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, an economist is not qualified to study", "extracted_entity": null}
{"question": "was Manny Pacquiao a boxing champion when Obama was first elected US president?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was the WBC lightweight champion", "extracted_entity": null}
{"question": "did Martin Luther know Latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he knew Latin.", "extracted_entity": null}
{"question": "will Jay-Z reach the age of 60 before Kendrick Lamar?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably. Jay-Z is 43,", "extracted_entity": null}
{"question": "did Jay-Z know Kanye West in 2012? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jay-Z and Kanye West", "extracted_entity": null}
{"question": "did Jay-Z know Kanye West in 2011? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jay-Z and Kanye West", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be able to communicate easily.", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with Wilhelm Miklas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Miklas was born in Austria.", "extracted_entity": null}
{"question": "could someone from Prussia communicate easily with someone from Austria?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they spoke German.", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever play in a park designed by a poet?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever play as a child in Leasowes Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn\u2019t.", "extracted_entity": null}
{"question": "did Abraham Lincoln help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1863, Lincoln", "extracted_entity": null}
{"question": "was Abraham Lincoln responsible for founding the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "was the current U.S. speaker of the house alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born in 194", "extracted_entity": null}
{"question": "does Gavin Rossdale have a large enough net worth to purchase the most expensive Thoroughbred horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "would nickel boil if placed in the core of Neptune?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not boil", "extracted_entity": null}
{"question": "is there a state that touches several of the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Michigan", "extracted_entity": null}
{"question": "are any of the Great Lakes entirely in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lake Michigan.", "extracted_entity": null}
{"question": "do the Great Lakes have at least 15% of the worlds freshwater fish supply?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Great Lakes have at least", "extracted_entity": null}
{"question": "are there any fresh water sand dunes in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there are sand dunes in the", "extracted_entity": null}
{"question": "has anyone drowned in the Great Lakes in 2019?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact, a total of 6", "extracted_entity": null}
{"question": "has anyone ever drowned in the Great Lakes?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there have been a few drownings", "extracted_entity": null}
{"question": "do the Great Lakes have bacteria in the water?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do have bacteria in the", "extracted_entity": null}
{"question": "do people ice fish on the Great Lakes in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do!", "extracted_entity": null}
{"question": "do the Great Lakes freeze in the winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't freeze.", "extracted_entity": null}
{"question": "do the Great Lakes have more endangered species than other parts of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have more endangered species than", "extracted_entity": null}
{"question": "do the Great Lakes have any endangered species?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are endangered species in the", "extracted_entity": null}
{"question": "is John the Baptist still important to people today?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John the Baptist is still important to", "extracted_entity": null}
{"question": "is John the Baptist still worshiped to this day?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was killed by Herod, and", "extracted_entity": null}
{"question": "was The Hobbit written before The Lord of the Rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was. The Hobbit was published", "extracted_entity": null}
{"question": "is David Cameron a Tory?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is", "extracted_entity": null}
{"question": "is Dragon Hall located in Norfolk?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Dragon Hall is located in the city of", "extracted_entity": null}
{"question": "is Norfolk predominantly rural?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a large city with a population", "extracted_entity": null}
{"question": "is Norfolk on the eastern coast of England?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "are Rogue DnD characters suited for playing in cities?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you're playing in", "extracted_entity": null}
{"question": "are DnD Ranger characters suited for play in wilderness areas?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "could George Washington have recorded a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have been 78", "extracted_entity": null}
{"question": "do Hindus believe Krishna exists?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they believe that he exists.", "extracted_entity": null}
{"question": "is Lord Hill's column in Shropshire the tallest of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at 160ft, it", "extracted_entity": null}
{"question": "are there several  towns located in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are 15 towns in Sh", "extracted_entity": null}
{"question": "were Charles Darwin and A.E. Housman born in Shropshire?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and both are commemorated by blue", "extracted_entity": null}
{"question": "is there a david bowie song with the word year or years in the title?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are three:", "extracted_entity": null}
{"question": "did the Hopi Indians use a symbol that was similar to the swastika?", "answer": "Yes", "dataset": "strategy_qa", "split": "dev", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
