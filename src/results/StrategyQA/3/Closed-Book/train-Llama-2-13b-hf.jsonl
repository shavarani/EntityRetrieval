{"question": "was Pi an acceptable number of children in 1980s China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the children were boys.", "extracted_entity": null}
{"question": "does highest US Court have enough seats for every Prime Minister of the United Kingdom since 1952?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are 54 Prime Ministers", "extracted_entity": null}
{"question": "does Alec Baldwin have more children than Clint Eastwood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Baldwin has six children, Eastwood", "extracted_entity": null}
{"question": "does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Thiago Moises May 13", "extracted_entity": null}
{"question": "is a cory catfish likely to eat another living fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Corys are generally peaceful fish, but", "extracted_entity": null}
{"question": "are selfies more dangerous than plague in modern times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, definitely. There is a 10", "extracted_entity": null}
{"question": "are amoebas safe from breast cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, amoebas are not safe from", "extracted_entity": null}
{"question": "is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The answer is citrus fruit.", "extracted_entity": null}
{"question": "could a hamster experience two leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "are there some countries where waiting staff need no tip?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are some countries where waiting staff do", "extracted_entity": null}
{"question": "do Shinto practitioners keep to a kosher diet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no kosher diet", "extracted_entity": null}
{"question": "is the Yellow Pages the fastest way to find a phone number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the fastest way to find a phone", "extracted_entity": null}
{"question": "could Intel products be purchased at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are called the 100", "extracted_entity": null}
{"question": "do you have to pass through circle of lust to find Saladin in Dante's Inferno?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you do.", "extracted_entity": null}
{"question": "does Woody Allen await the Second Coming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he thinks it's going to", "extracted_entity": null}
{"question": "do you need a large room if you want to get into 3D printing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can get into 3D printing", "extracted_entity": null}
{"question": "was Emperor Commodus paid tribute in Pound sterling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was paid in Euros.", "extracted_entity": null}
{"question": "can French Defence initial move defend against four move checkmate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The French Defence is a good response", "extracted_entity": null}
{"question": "is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the coloring is artificial.", "extracted_entity": null}
{"question": "do Armenians tend to dislike System of a Down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course, they are Armenian-", "extracted_entity": null}
{"question": "did Ivan the Terrible's father and grandfather have nicer nicknames?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were known as \"Ivan the", "extracted_entity": null}
{"question": "did the confederate states speak Old English before the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Confederate States did not speak Old", "extracted_entity": null}
{"question": "do Republicans reject all forms of welfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "are brown rock fish found in the waters surrounding Saint Kitts and Nevis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, brown rock fish are found in the waters", "extracted_entity": null}
{"question": "is a fever cured by listening to a cowbell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it's a start.", "extracted_entity": null}
{"question": "do all parts of the aloe vera plant taste good?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. only the gel part of the aloe", "extracted_entity": null}
{"question": "after viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s in the Louvre.", "extracted_entity": null}
{"question": "did Mike Tyson do something very different than McGruff's slogan to Evander Holyfield in 1997?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he bit him", "extracted_entity": null}
{"question": "can you hide a basketball in a sand cat's ear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a sand cat's ear is too", "extracted_entity": null}
{"question": "could largest asteroid crush a whole city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a large asteroid can crush a", "extracted_entity": null}
{"question": "do Chinese Americans face discrimination at a Federal level in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is Kobe's famous animal product used in a BLT?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "BEEF", "extracted_entity": null}
{"question": "is narcissism's origin a rare place to get modern words from?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The word 'narcissism' is from", "extracted_entity": null}
{"question": "is it unusual to eat spaghetti without a fork?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is unusual to eat spaghetti", "extracted_entity": null}
{"question": "can you drown in a Swan Lake performance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can drown in a Sw", "extracted_entity": null}
{"question": "are Christmas trees typically deciduous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. They are evergreen", "extracted_entity": null}
{"question": "can a traffic collision make someone a millionaire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "is winter solstice in Northern Hemisphere closer to July than in Southern Hemisphere? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the Northern Hemisphere, the", "extracted_entity": null}
{"question": "can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. Just click on the Borg", "extracted_entity": null}
{"question": "have Douglas fir been used to fight wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Douglas fir has been used in the past", "extracted_entity": null}
{"question": "are common carp sensitive to their environments?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, common carp are sensitive to their environments", "extracted_entity": null}
{"question": "would John the Baptist be invited to a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he would probably be too busy bapt", "extracted_entity": null}
{"question": "are there bones in an anchovy pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are bones in an anchovy", "extracted_entity": null}
{"question": "did Jerry Seinfeld have reason to cheer in 1986?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was happy anyway.", "extracted_entity": null}
{"question": "is a platypus immune from cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A platypus is not immune to ch", "extracted_entity": null}
{"question": "is Europa linked to Viennese waltzes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The waltz was introduced to Vienna", "extracted_entity": null}
{"question": "is the tibia required for floor exercises?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not for floor exercises.", "extracted_entity": null}
{"question": "was The Great Gatsby inspired by the novel 1984?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the novel 1984 inspired", "extracted_entity": null}
{"question": "does a lapidary work with items that are studied by geologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a lapidary works with items that", "extracted_entity": null}
{"question": "does Santa Claus hypothetically give Joffrey Baratheon presents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Santa gives presents to everyone.", "extracted_entity": null}
{"question": "would a Jehovah's witness approve of Alice's Adventures in Wonderland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they wouldn't.", "extracted_entity": null}
{"question": "can you chew argon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is a gas.", "extracted_entity": null}
{"question": "is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's a myth.", "extracted_entity": null}
{"question": "does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's been around longer.", "extracted_entity": null}
{"question": "is the tongue part of a creature's head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The tongue is part of the mouth.", "extracted_entity": null}
{"question": "is the largest city in New Mexico also known as Yoot\u00f3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Albuquerque", "extracted_entity": null}
{"question": "is most coffee produced South of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Most coffee is produced in South America,", "extracted_entity": null}
{"question": "did Amy Winehouse always perform live perfectly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was human, and she made mistakes", "extracted_entity": null}
{"question": "does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn't.", "extracted_entity": null}
{"question": "do American wheelchair users know what the ADA is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they know it is a civil rights", "extracted_entity": null}
{"question": "can brewing occur in a prison environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's difficult to obtain the", "extracted_entity": null}
{"question": "will Ronda Rousey hypothetically defeat X-Men's Colossus in a fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Colossus has super strength and", "extracted_entity": null}
{"question": "can a believer in agnosticism become pope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and many have.", "extracted_entity": null}
{"question": "can French Toast hypothetically kill a Lannister?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they are already dead", "extracted_entity": null}
{"question": "did Moon Jae-in earn the Abitur as a teenager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "would a kindergarten teacher make a lesson of the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are not allowed to do so", "extracted_entity": null}
{"question": "are you likely to hear Rammstein playing in smooth jazz clubs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I doubt it.", "extracted_entity": null}
{"question": "did Electronic Arts profit from Metroid sales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Metroid was developed by Nintendo.", "extracted_entity": null}
{"question": "is Earth Day celebrated in summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Earth Day is celebrated on April 2", "extracted_entity": null}
{"question": "is viscosity unimportant in making jello shots?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, viscosity is unimportant in making", "extracted_entity": null}
{"question": "is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are standing in a row.", "extracted_entity": null}
{"question": "is waltz less injurious than slam dance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. It is very", "extracted_entity": null}
{"question": "are LinkedIn and LeafedIn related companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, LinkedIn is a social media website for", "extracted_entity": null}
{"question": "can a snow leopard swim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, snow leopards cannot swim.", "extracted_entity": null}
{"question": "can citrus grow in Ulaanbaatar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as a commercial crop.", "extracted_entity": null}
{"question": "will a Holstein cow and the Liberty Bell balance out a giant scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Holstein cow will balance out a", "extracted_entity": null}
{"question": "is the bull shark more bull than shark?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the bull shark is a shark,", "extracted_entity": null}
{"question": "is World of Warcraft heavier than a loaf of bread?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "did Thomas Greenhill's parents violate the concept of monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "would John Muir not likely have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and a deficiency would cause r", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after parting from the band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kirk Hammett", "extracted_entity": null}
{"question": "has a baby ever had a moustache?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. a baby has never had a moust", "extracted_entity": null}
{"question": "would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Shaggy is a cartoon character", "extracted_entity": null}
{"question": "do solo pianists require a conductor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, solo pianists do not require a conduct", "extracted_entity": null}
{"question": "is Kim Kardashian a guru?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she is a celebrity.", "extracted_entity": null}
{"question": "were karaoke and the turtle power tiller patented in the same country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The turtle power tiller was patented", "extracted_entity": null}
{"question": "would a caracal be defeated by Javier Sotomayor in a high jump competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the caracal is a cat,", "extracted_entity": null}
{"question": "do Jehovah's Witnesses celebrate day before New Year's Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "do moths that live on sloths have family dinners?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they just eat their mothers", "extracted_entity": null}
{"question": "can a greyhound walk on two legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, greyhounds can't walk on", "extracted_entity": null}
{"question": "was Moon Jae-in born outside of Khanbaliq?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was born in Chuncheon", "extracted_entity": null}
{"question": "were paparazzi involved in the death of a member of the royal family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Princess Diana was killed in a car", "extracted_entity": null}
{"question": "can native wolverines be found in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the wolverine is a native species", "extracted_entity": null}
{"question": "did Al Unser Jr. win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was Al Unser Sr.", "extracted_entity": null}
{"question": "do the Ubuntu people speak Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. They speak Ubuntu.", "extracted_entity": null}
{"question": "does Rupert Murdoch's alma mater have more history than the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, yes it does.", "extracted_entity": null}
{"question": "are there Pink music videos that are triggering for eating disorder patients?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are a lot of Pink music", "extracted_entity": null}
{"question": "were gladiators associated with the Coloseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they were", "extracted_entity": null}
{"question": "could Larry King's marriages be counted on two feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they could not.", "extracted_entity": null}
{"question": "do mountain goats inhabit the summit of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are mountain goats on Mount Sh", "extracted_entity": null}
{"question": "will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "is Autumn a good time to collect bear pelts in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I've never", "extracted_entity": null}
{"question": "did Wednesday have something to do with Thor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was the only one who could help", "extracted_entity": null}
{"question": "did Dale Jr hug his dad after their last Daytona 500 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they hugged after the race.", "extracted_entity": null}
{"question": "can an ostrich fit into the nest of a swallow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, an ostrich cannot fit into the", "extracted_entity": null}
{"question": "would an Olympic athlete be tired out after running a mile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "would the current president of Ohio University hypothetically wear a jockstrap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The current president of Ohio University is a", "extracted_entity": null}
{"question": "would ramen be bad for someone with heart failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ramen is not bad for someone with heart failure", "extracted_entity": null}
{"question": "was Christina Aguilera born in the forgotten borough?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was born in Staten Island.", "extracted_entity": null}
{"question": "will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Donald Duck is a duck", "extracted_entity": null}
{"question": "were the first missionaries required to attend mass on Sundays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the first missionaries were not required to", "extracted_entity": null}
{"question": "is an ocelot a good present for a kindergartener?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. What's an", "extracted_entity": null}
{"question": "did villain that killed Superman murder Robin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a clone of Robin.", "extracted_entity": null}
{"question": "would a Germaphobia be able to participate in Judo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be difficult. Judo", "extracted_entity": null}
{"question": "does  Lionel Richie believe in holistic medicine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is the Fibonacci number sequence longer than every number discovered in Pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Every number in Pi is less than the", "extracted_entity": null}
{"question": "was The Canterbury Tales written before One Thousand and One Nights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Canterbury Tales was written in the", "extracted_entity": null}
{"question": "did John Kerry run in the 2010 United Kingdom general election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, John Kerry did not run in the", "extracted_entity": null}
{"question": "does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s called poi.", "extracted_entity": null}
{"question": "did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "did original Nintendo have games in same format as Playstation 3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they used cartridges.", "extracted_entity": null}
{"question": "would you be more likely to die of hypothermia in New York than Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because you are more likely to be outside", "extracted_entity": null}
{"question": "was Moliere Queen Margot's ill fated lover?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a playwright", "extracted_entity": null}
{"question": "can parachuting amateurs ignore hurricane force winds bulletins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO. These are issued by the National Weather", "extracted_entity": null}
{"question": "can a ten-pin bowling pin be a deadly weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sissy", "extracted_entity": null}
{"question": "would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Anorexia nervosa is an", "extracted_entity": null}
{"question": "is cycling a high-risk activity for pelvis fractures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cycling is a high-risk", "extracted_entity": null}
{"question": "would Dale Earnhardt Jr. be considered a newbie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not a newbie.", "extracted_entity": null}
{"question": "does Final Fantasy VI require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Final Fantasy VI does not require electric", "extracted_entity": null}
{"question": "did number of Imams Reza Shah believed in exceed number of Jesus's disciples?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The number of disciples of Jesus is not", "extracted_entity": null}
{"question": "did Terry Pratchett write about quantum mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in his book The Science of Discworld", "extracted_entity": null}
{"question": "is it hard for Rahul Dravid to order food at a restaurant in Aurangabad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a vegetarian.", "extracted_entity": null}
{"question": "is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The President of India is the highest constitution", "extracted_entity": null}
{"question": "are swastikas used in the most common religion in India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, hindus use swastikas", "extracted_entity": null}
{"question": "will electric car struggle to finish Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It will struggle to finish a lap.", "extracted_entity": null}
{"question": "does walking across Amazonas put a person's life at risk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, walking across the Amazonas is a very", "extracted_entity": null}
{"question": "is Olivia Newton-John hyphenated celebrity name with most letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Olivia Newton-John has 1", "extracted_entity": null}
{"question": "is Godzilla's image likely grounds for a lawsuit in 2050?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the image is not a parody", "extracted_entity": null}
{"question": "does ontology require a scalpel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ontology is the study of the nature", "extracted_entity": null}
{"question": "will silicon wedding rings outsell bromine wedding rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, bromine is a liquid at room", "extracted_entity": null}
{"question": "is Cape Town south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is south of the equator", "extracted_entity": null}
{"question": "would the number 666 appear in a church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would. I think it would appear", "extracted_entity": null}
{"question": "can a single honey bee sting multiple humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a honey bee can sting", "extracted_entity": null}
{"question": "would someone pay for a coffee in NYC with Euros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, of course not.", "extracted_entity": null}
{"question": "could a student at the University of Houston see a caracal on campus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The caracal is not native to", "extracted_entity": null}
{"question": "do sun bears stay active during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sun bears are active all year round", "extracted_entity": null}
{"question": "is Christmas celebrated during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christmas is celebrated in the winter.", "extracted_entity": null}
{"question": "could the endowment of Johns Hopkins University pay off the MBTA debt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the endowment of Johns Hop", "extracted_entity": null}
{"question": "was song of Roland protagonist friendly with group that had sagas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was friendly with group that had sag", "extracted_entity": null}
{"question": "did Dr. Seuss make himself famous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Dr. Seuss was already famous before", "extracted_entity": null}
{"question": "can a goat be used for one of the ingredients in French toast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it has to be a French go", "extracted_entity": null}
{"question": "can you measure a Caracal with a protractor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a cat.", "extracted_entity": null}
{"question": "for bone growth, is kale more beneficial than spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kale is a better source of calcium than", "extracted_entity": null}
{"question": "would only warm weather attire be a good idea on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Mercury is the closest planet to the sun,", "extracted_entity": null}
{"question": "do Do It Yourself channels online always show realistic projects?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "could Bart Simpson have owned comics with The Joker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was too young.", "extracted_entity": null}
{"question": "was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the 23rd amendment", "extracted_entity": null}
{"question": "would a baby gray whale fit in a tractor-trailer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not a big one", "extracted_entity": null}
{"question": "would Emmanuel Macron celebrate Cinco de Mayo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the number of total lunar ecli", "extracted_entity": null}
{"question": "did Metallica band members cutting their hair hurt their sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and I will explain why.", "extracted_entity": null}
{"question": "could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was too far away.", "extracted_entity": null}
{"question": "would a veteran of the Phillippine-American War come home craving SPAM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were in the military.", "extracted_entity": null}
{"question": "were Walkman's used in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Walkman's were not used in", "extracted_entity": null}
{"question": "will Communion be denied to Wednesday name origin followers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Communion will not be denied to Wed", "extracted_entity": null}
{"question": "would someone with a nosebleed benefit from Coca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a stimulant", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I can tell", "extracted_entity": null}
{"question": "can Darth Vader hypothetically outdunk Bill Walton without using The Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "does Oprah Winfrey have a degree from an Ivy League university?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she has a degree from Tennessee State University", "extracted_entity": null}
{"question": "can you watch Rick and Morty in Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t watch Rick and Mort", "extracted_entity": null}
{"question": "did Elizabeth II frequently visit Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she didn\u2019t.", "extracted_entity": null}
{"question": "can a sesame seed grow in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A sesame seed can grow in the human body", "extracted_entity": null}
{"question": "did the Watergate scandal help the Republican party?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Watergate scandal helped the Republican", "extracted_entity": null}
{"question": "would Bandy be likely to become popular in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bandy is not likely to become popular in Texas", "extracted_entity": null}
{"question": "do inanimate objects come alive in Beauty and the Beast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the candelabra and teap", "extracted_entity": null}
{"question": "does Home Depot sell item in late September zodiac sign symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Home Depot does not sell items with", "extracted_entity": null}
{"question": "does monster name in West African Folklore that witches send into villages set Scrabble record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The word is not in the Scrab", "extracted_entity": null}
{"question": "can an American black bear swallow a sun bear whole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it has a big enough stom", "extracted_entity": null}
{"question": "can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a Reconstruction era coin can buy", "extracted_entity": null}
{"question": "would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was a Sherpa, and", "extracted_entity": null}
{"question": "can preventive healthcare reduce STI transmission?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. STI prevention can be achieved through", "extracted_entity": null}
{"question": "would 1996 leap year baby technically be 1 year old in 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a leap year baby would be", "extracted_entity": null}
{"question": "would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the tunnels at CERN", "extracted_entity": null}
{"question": "would a Superbowl Football Game be crowded on the Gettysburg Battlefield?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it would be a football game,", "extracted_entity": null}
{"question": "can you purchase a dish with injera at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it\u2019s a Mexican restaurant.", "extracted_entity": null}
{"question": "has Drew Carey outshined Doug Davidson's tenure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not even close.", "extracted_entity": null}
{"question": "could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the car was built in", "extracted_entity": null}
{"question": "is eggplant deadly to most atopic individuals? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not if cooked properly.", "extracted_entity": null}
{"question": "would Cyndi Lauper use milk substitute in her rice pudding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not.", "extracted_entity": null}
{"question": "can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you type zucchini,", "extracted_entity": null}
{"question": "would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would have eaten b\u00f6rek", "extracted_entity": null}
{"question": "are potatoes native to the European continent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, potatoes are native to the Americas", "extracted_entity": null}
{"question": "was Iggy Pop named after his father?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his name was James Osterberg.", "extracted_entity": null}
{"question": "can a 2019 Toyota Hilux hypothetically support weight of thirty Big John Studd clones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Hilux can support the weight of", "extracted_entity": null}
{"question": "did Tom Bosley enjoy video games on the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a man of God.", "extracted_entity": null}
{"question": "do German Shepherds worry about the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "do you find glutamic acid in a severed finger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a neurotransmit", "extracted_entity": null}
{"question": "can black swan's formation type help spell longest word in Dictionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The black swan is a bird that is native", "extracted_entity": null}
{"question": "would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was a eugenicist", "extracted_entity": null}
{"question": "could Stephen King join the NASA Astronaut Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he is a US citizen,", "extracted_entity": null}
{"question": "does the art from Family Guy look a lot like the art in American Dad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I'm not sure if this is", "extracted_entity": null}
{"question": "did DARPA influence Albert Einstein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Einstein was a great scientist.", "extracted_entity": null}
{"question": "is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not impossible for Cheb M", "extracted_entity": null}
{"question": "did Moon Jae-in's residence exist when the World Trade Center was completed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was built in 199", "extracted_entity": null}
{"question": "would a kaffir lime be a good ingredient for making a candle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Kaffir lime is a good", "extracted_entity": null}
{"question": "is Glenn Beck known for his mild temper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is known for his ranting", "extracted_entity": null}
{"question": "was the 1980 presidential election won by a member of the Grand Old Party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was won by Jimmy Carter.", "extracted_entity": null}
{"question": "would Nancy Pelosi have hypothetically been on same side as Gerald Ford?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she is a Democrat.", "extracted_entity": null}
{"question": "was the MLB World Series held in Newcastle, New South Wales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The MLB World Series was held in", "extracted_entity": null}
{"question": "will a sloth explode if it's not upside down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they're just lazy and would rather", "extracted_entity": null}
{"question": "could Rhode Island sink into the Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it were on the sea floor.", "extracted_entity": null}
{"question": "would it be hard to get toilet paper if there were no loggers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because there would be no toilet", "extracted_entity": null}
{"question": "are banana trees used by judges for maintaining order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, judges do use banana trees to", "extracted_entity": null}
{"question": "would four shoes be insufficient for a set of octuplets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because they would all be the same size", "extracted_entity": null}
{"question": "did Alan Rickman have an improperly functioning organ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, his heart.", "extracted_entity": null}
{"question": "was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was a cockroach.", "extracted_entity": null}
{"question": "does the United States Navy create radioactive waste?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Navy creates radioactive waste.", "extracted_entity": null}
{"question": "is the United States Capitol located near the White House?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. The White House is", "extracted_entity": null}
{"question": "is winter associated with hot temperatures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, winter is associated with hot temperatures.", "extracted_entity": null}
{"question": "did Sony definitively win the video game war against Sega?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sega has released the Sega Sat", "extracted_entity": null}
{"question": "could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "can you find a snow leopard in the Yucatan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t.", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not unless they are all over 6", "extracted_entity": null}
{"question": "is it common to see frost during some college commencements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is common to see frost during", "extracted_entity": null}
{"question": "was Walt Disney ever interviewed by Anderson Cooper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Walt Disney died in 19", "extracted_entity": null}
{"question": "does a dentist treat Bluetooth problems?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they can give you a tooth", "extracted_entity": null}
{"question": "did Richard III's father have greater longevity than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was 67 when he died", "extracted_entity": null}
{"question": "does Ariana Grande's signature style combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Ariana Grande's signature style comb", "extracted_entity": null}
{"question": "did the color green help Theodor Geisel become famous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is known as Dr. Seuss", "extracted_entity": null}
{"question": "does the United States Secretary of State answer the phones for the White House?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the United States Secretary of State does not", "extracted_entity": null}
{"question": "would a honey badger's dentures be different from a wolverine's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I have no idea", "extracted_entity": null}
{"question": "are thetan levels found in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. The word \"thet", "extracted_entity": null}
{"question": "would Bobby Jindal's high school mascot eat kibble?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's a Bengal.", "extracted_entity": null}
{"question": "are multiple Christmas Carol's named after Saints?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "do placozoa get learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Placozoa do not get learning disabilities", "extracted_entity": null}
{"question": "would Jon Brower Minnoch break a chair before Voyager 2 launch mass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "327.7 kg", "extracted_entity": null}
{"question": "would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the NFL game would take precedence over", "extracted_entity": null}
{"question": "are any of the destinations of Japan Airlines former Axis Powers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Japan Airlines only flew to Japan,", "extracted_entity": null}
{"question": "is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because I want to.", "extracted_entity": null}
{"question": "could an infant solve a sudoku puzzle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A sudoku puzzle requires an adult", "extracted_entity": null}
{"question": "did Gauss have a normal brain structure?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he had a normal brain structure", "extracted_entity": null}
{"question": "did Benito Mussolini wear bigger shoes than Haf\u00fe\u00f3r Bj\u00f6rnsson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did U2 play a concert at the Polo Grounds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. U2 played a concert at the old", "extracted_entity": null}
{"question": "does Ludacris perform classical music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "can olive oil kill rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rabies is a viral infection", "extracted_entity": null}
{"question": "does Julia Roberts lose the prolific acting contest in her family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn\u2019t.", "extracted_entity": null}
{"question": "does Super Mario mainly focus on a man in green?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Mario", "extracted_entity": null}
{"question": "will Elijah Cummings cast a vote in the 2020 presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO, he is dead.", "extracted_entity": null}
{"question": "can an adult human skull hypothetically pass through the birth canal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the mother is very large", "extracted_entity": null}
{"question": "are any mollusks on Chinese New Year calendar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Chinese New Year calendar has moll", "extracted_entity": null}
{"question": "could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are all athletes and they", "extracted_entity": null}
{"question": "if he were poor, would Christopher Reeve have lived?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would have died anyway.", "extracted_entity": null}
{"question": "is it impossible to tell if someone is having a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not impossible to tell if someone", "extracted_entity": null}
{"question": "does Super Mario require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Super Mario requires electricity to play.", "extracted_entity": null}
{"question": "does the actress who played Elizabeth II speak fluent Arabic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but she does speak French.", "extracted_entity": null}
{"question": "were the Spice Girls inspired by Little Mix?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "was Augustus his real name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Augustus was his real name.", "extracted_entity": null}
{"question": "is the brain located in the torso?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the brain is located in the head.", "extracted_entity": null}
{"question": "could Cosmic Girls play League of Legends alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cosmic Girls can't play League", "extracted_entity": null}
{"question": "has a tumulus been discovered on Mars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a tumulus has been discovered on Mars", "extracted_entity": null}
{"question": "would a Bengal cat be afraid of catching a fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Bengal cat is a very intelligent and", "extracted_entity": null}
{"question": "is anyone at the Last Supper celebrated in Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jesus is not celebrated in Islam.", "extracted_entity": null}
{"question": "did John Lennon listen to Compact discs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. John Lennon didn\u2019t listen", "extracted_entity": null}
{"question": "did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, both were named Elizabeth.", "extracted_entity": null}
{"question": "was Nikola Tesla's home country involved in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Nikola Tesla's home", "extracted_entity": null}
{"question": "were there eight humans on Noah's Ark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Noah was 600 years", "extracted_entity": null}
{"question": "are people banned from entering the Forbidden City?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people are banned from entering the For", "extracted_entity": null}
{"question": "did Kurt Cobain's music genre survive after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did. The grunge genre was", "extracted_entity": null}
{"question": "was Lorenzo de Medici's patronage of Da Vinci exclusive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Da Vinci was also patronized by", "extracted_entity": null}
{"question": "when the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was the season for Valentine'", "extracted_entity": null}
{"question": "would a nickel fit inside a koala pouch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because koala pouches are too", "extracted_entity": null}
{"question": "did the Football War last at least a month?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it lasted 48 hours.", "extracted_entity": null}
{"question": "does Masaharu Morimoto rely on glutamic acid?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I think he uses a lot of so", "extracted_entity": null}
{"question": "can you put bitcoin in your pocket?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, bitcoin is a virtual currency,", "extracted_entity": null}
{"question": "would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Dunkin Donuts does not offer", "extracted_entity": null}
{"question": "at midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lots of people.", "extracted_entity": null}
{"question": "did Irish mythology inspire Washington Irving?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was inspired by the stories of Irish", "extracted_entity": null}
{"question": "can a microwave melt a Toyota Prius battery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not very well.", "extracted_entity": null}
{"question": "does Elizabeth II reign over the Balearic Islands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she does not.", "extracted_entity": null}
{"question": "can Michael Jordan become a professional cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He could if he wanted to.", "extracted_entity": null}
{"question": "will speed reader devour The Great Gatsby before the Raven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it will.", "extracted_entity": null}
{"question": "would the 10th doctor enjoy a dish of stuffed pears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think he would.", "extracted_entity": null}
{"question": "during the time immediately after 9/11, was don't ask don't tell still in place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It was signed into law in 1", "extracted_entity": null}
{"question": "was King Kong (2005 film) solvent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It went bankrupt.", "extracted_entity": null}
{"question": "would the Who concert in international space station be audible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not loudly enough for the astr", "extracted_entity": null}
{"question": "would an ancient visitor to Persia probably consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Crocus threads were consumed as a", "extracted_entity": null}
{"question": "do you need different colored pens for sudoku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you don\u2019t need different colored pens", "extracted_entity": null}
{"question": "can the Supreme Court of Canada fight a Lucha trios match?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would take 9 of them", "extracted_entity": null}
{"question": "could Eric Clapton's children play a regulation game of basketball among themselves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Batman is not likely to be shown", "extracted_entity": null}
{"question": "did Saddam Hussein witness the inauguration of Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Saddam Hussein was executed in", "extracted_entity": null}
{"question": "is the Flying Spaghetti Monster part of an ancient pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Flying Spaghetti Monster", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Gladiator's weapon of choice", "extracted_entity": null}
{"question": "would a slingshot be improperly classified as artillery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a weapon that launches project", "extracted_entity": null}
{"question": "is it normal to blow out candles during a funeral?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Candles are lit at funerals", "extracted_entity": null}
{"question": "could Darth Vader hypothetically catch the Coronavirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could, because he\u2019s a", "extracted_entity": null}
{"question": "did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "John of Portugal", "extracted_entity": null}
{"question": "snowboarding is a rarity in Hilo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "was John Gall from same city as Stanford University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Sunburns are one", "extracted_entity": null}
{"question": "is San Diego County the home of a Shamu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Shamu is the main att", "extracted_entity": null}
{"question": "was Elmo an original muppet character on Sesame Street?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "would a Fakir be surprised if they saw a comma in their religious book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not taught to read.", "extracted_entity": null}
{"question": "in the world of Harry Potter, would a snake and skull tattoo be good luck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be a bad omen.", "extracted_entity": null}
{"question": "did England win any Olympic gold medals in 1800?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, England did not win any Olympic gold med", "extracted_entity": null}
{"question": "is video surveillance of a room possible without an obvious camera or new item?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not possible to use a", "extracted_entity": null}
{"question": "do pirates care about vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Conan the Barbarian will last a short", "extracted_entity": null}
{"question": "were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. His father, LeRoy McCo", "extracted_entity": null}
{"question": "could a fan of the Botany Swarm vote for John Key?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and they would be just as likely to", "extracted_entity": null}
{"question": "would Jackie Chan have trouble communicating with a deaf person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he can read lips", "extracted_entity": null}
{"question": "was Oscar Wilde's treatment under the law be considered fair in the US now?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be considered fair in the", "extracted_entity": null}
{"question": "can a lemon aggravate dyspepsia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, lemons can aggravate d", "extracted_entity": null}
{"question": "can musicians become knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in some circumstances.", "extracted_entity": null}
{"question": "is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'd guess", "extracted_entity": null}
{"question": "did Disney's second film rip off a prophet story?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the story of Moses is told in", "extracted_entity": null}
{"question": "is Alan Alda old enough to have fought in the Vietnam War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born on January 28", "extracted_entity": null}
{"question": "is Samsung accountable to shareholders?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are a public company.", "extracted_entity": null}
{"question": "are chinchillas cold-blooded?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are warm blooded.", "extracted_entity": null}
{"question": "did Johann Sebastian Bach ever win a Grammy Award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "did Dr. Seuss live a tragedy free life?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He lost his wife and his son.", "extracted_entity": null}
{"question": "would a Nike shoebox be too small to fit a swan in?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the box is too small to fit a", "extracted_entity": null}
{"question": "were Beauty and the Beast adaptations devoid of Kurt Sutter collaborators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not.", "extracted_entity": null}
{"question": "was Florence a Theocracy during Italian Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Florence was not a theocracy during the", "extracted_entity": null}
{"question": "did Sojourner Truth use the elevator at the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Washington Monument was not built until", "extracted_entity": null}
{"question": "is it safe to eat hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not safe to eat hair.", "extracted_entity": null}
{"question": "did Lionel Richie ever have dinner with Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did have dinner with Michael Jackson", "extracted_entity": null}
{"question": "would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because JPEG is a lossy", "extracted_entity": null}
{"question": "are fresh garlic cloves as easy to eat as roasted garlic cloves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "did Osama bin Laden likely abstain from alcohol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was a devout Muslim.", "extracted_entity": null}
{"question": "do Youtube viewers get unsolicited audiobook advice often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I get it all the time", "extracted_entity": null}
{"question": "is 2018 Ashland, Oregon population inadequate to be a hypothetical military division?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "does Carmen Electra own a junk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Carmen Electra owns a j", "extracted_entity": null}
{"question": "could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I think it", "extracted_entity": null}
{"question": "would Glen Beck and Stephen Colbert be likely to tour together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they wouldn\u2019t.", "extracted_entity": null}
{"question": "could a young Wizard of Oz Scarecrow have gotten Cerebral palsy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea, but it seems possible.", "extracted_entity": null}
{"question": "can the Toyota Hilux tip the scales against Mr. Ed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Hilux can tow a", "extracted_entity": null}
{"question": "is it normally unnecessary to wear a coat in Hollywood in July?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is necessary to", "extracted_entity": null}
{"question": "could the Atlantic readers fill 500 battalions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Atlantic has 400,", "extracted_entity": null}
{"question": "can oysters be preserved without refrigeration? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can be preserved without refriger", "extracted_entity": null}
{"question": "would a hippie hypothetically be bummed out by Augustus's Pax Romana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I think they'", "extracted_entity": null}
{"question": "can professional boxers expect to have low dental bills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "would  bald eagle deliver an urgent message before B-52?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because B-52 would take too", "extracted_entity": null}
{"question": "is it difficult to conduct astrophotography in the summer in Sweden?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The summer is the season", "extracted_entity": null}
{"question": "was a woman Prime Minister directly before or after Stanley Baldwin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Margaret Thatcher", "extracted_entity": null}
{"question": "do people in middle school usually get breast exams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I do know", "extracted_entity": null}
{"question": "is Black Lives Matter connected with capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Black Lives Matter is connected with", "extracted_entity": null}
{"question": "does Capricorn astrology symbol have all of the parts of a chimera?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know what a chimera is", "extracted_entity": null}
{"question": "would Lord Voldemort hypothetically be an effective fighter after Final Fantasy silence is cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would be an effective fighter.", "extracted_entity": null}
{"question": "does Sam Harris worship Shiva?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "do the James Bond and Doctor Who series have a similarity in format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they both have a main character that goes", "extracted_entity": null}
{"question": "can jackfruit be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you have a jackfruit and", "extracted_entity": null}
{"question": "does Disney own a major comic book publisher?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "did William Shaespeare read the Daily Mirror?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was dead.", "extracted_entity": null}
{"question": "did the 40th president of the United States forward lolcats to his friends?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "did Clark Gable marry more women once than Richard Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would an expensive tailor use adhesive to create a shorter hem on slacks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would the yearly precipitation on Snowdon submerge an upright bowling pin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The yearly precipitation on Snowdon", "extracted_entity": null}
{"question": "can a honey bee sting a human more than once?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a honey bee will die after", "extracted_entity": null}
{"question": "did U.S. soldiers listen to Justin Bieber's Believe album during the Battle of Baghdad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the album was played on repeat for the", "extracted_entity": null}
{"question": "can sunlight travel to the deepest part of the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sunlight can travel to the deepest", "extracted_entity": null}
{"question": "is Christopher Walken close to achieving EGOT status?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he is one Tony Award away from", "extracted_entity": null}
{"question": "are the brooms from curling good for using on house floors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, curling brooms are not suitable for", "extracted_entity": null}
{"question": "would a dog easily notice ammonia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, dogs can easily detect ammonia.", "extracted_entity": null}
{"question": "are vinegar pickled cucumbers rich in lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is a construction worker required to build a portfolio?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a construction worker is not required to build", "extracted_entity": null}
{"question": "is it hard to get a BLT in Casablanca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you might have to wait a while", "extracted_entity": null}
{"question": "would half muggle wizards fear Lord Voldemort?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he would not be able to kill", "extracted_entity": null}
{"question": "would ISIS agree with Al-Farabi's religious sect?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. ISIS is a Sunni Muslim group", "extracted_entity": null}
{"question": "is Casio's founding year a composite number?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a composite number.", "extracted_entity": null}
{"question": "would it be unusual to use paypal for drug deals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is not unusual to use Paypal", "extracted_entity": null}
{"question": "is growing seedless cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not really.", "extracted_entity": null}
{"question": "can Spartina Patens thrive in the Sahara Desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it needs to be in the water", "extracted_entity": null}
{"question": "does taking ukemi halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kinetic energy is the energy of motion", "extracted_entity": null}
{"question": "are there five different single-digit Fibonacci numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are five different single-digit F", "extracted_entity": null}
{"question": "can a snow leopard eat twice its own body weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a snow leopard can eat twice", "extracted_entity": null}
{"question": "are ground bell peppers the main ingredient of black pepper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a major ingredient in", "extracted_entity": null}
{"question": "would a monkey outlive a human being on average?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a monkey would not outlive a", "extracted_entity": null}
{"question": "can Harry Potter book a flight on Asiana Airlines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Harry Potter can\u2019t book a", "extracted_entity": null}
{"question": "is latitude required to determine the coordinates of an area?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, longitude is required to determine the coordinates of", "extracted_entity": null}
{"question": "did polio medicine save the life of polio vaccine creator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, polio vaccine creator did", "extracted_entity": null}
{"question": "are Donkeys part of Christmas celebrations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Donkeys are not part of Christmas celebr", "extracted_entity": null}
{"question": "if you have a serious injury in Bangladesh, would you probably dial a Fibonacci number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you would dial 1-1", "extracted_entity": null}
{"question": "hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1", "extracted_entity": null}
{"question": "is Great Pyramid of Giza the last wonder of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is the last of the Seven W", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they don\u2019t speak English.", "extracted_entity": null}
{"question": "will Elijah Cummings vote for Joe Biden in the next presidential elections?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he will.", "extracted_entity": null}
{"question": "did Joan Crawford guest star on  JAG (TV series)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "can an African Elephant get pregnant twice in a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, an African Elephant can get pre", "extracted_entity": null}
{"question": "would a sofer be a bad job for a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no ethical issues with being", "extracted_entity": null}
{"question": "is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is legal for a licensed child", "extracted_entity": null}
{"question": "was Jackson Pollock straight edge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would a fungal life-form be threatened by a pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the copper would be toxic to", "extracted_entity": null}
{"question": "do urban legends always have to occur in cities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can occur in a rural area as", "extracted_entity": null}
{"question": "has Freemasonry been represented on the Moon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Apollo 11 astronaut", "extracted_entity": null}
{"question": "is Hamlet more common on IMDB than Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but by a small margin.", "extracted_entity": null}
{"question": "coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would all fit in an Audi", "extracted_entity": null}
{"question": "was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was known for his humour.", "extracted_entity": null}
{"question": "can people die from brake failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people can die from brake failure.", "extracted_entity": null}
{"question": "if it socially acceptable to wear an icon depicting crucifixion? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not socially acceptable to wear", "extracted_entity": null}
{"question": "could a monarch butterfly rule a kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are too small and weak to be", "extracted_entity": null}
{"question": "can vitamin C rich fruits be bad for health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "did breakdancing grow in popularity during WW2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it did not.", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Texas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be unusual to see frost in September", "extracted_entity": null}
{"question": "are Aldi's foods discounted due to being out of date?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Aldi does not sell out of date", "extracted_entity": null}
{"question": "would a recruit for the United States Marine Corps be turned away for self harm?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I'm not", "extracted_entity": null}
{"question": "can you order an Alfa Romeo at Starbucks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can order a Venti Half", "extracted_entity": null}
{"question": "will Chick Fil A be open on Halloween 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Chick Fil A will be open on", "extracted_entity": null}
{"question": "do drag kings take testosterone to look masculine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't. They do things", "extracted_entity": null}
{"question": "are pennies commonly used in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "could the Pope be on an episode of Pimp My Ride?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If he was, I\u2019d watch it.", "extracted_entity": null}
{"question": "can E6000 cure before a hoverboard finishes the Daytona 500? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can cure before a hoverboard", "extracted_entity": null}
{"question": "does Nigella Lawson care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is a chemical property of a", "extracted_entity": null}
{"question": "would Cuba Libre consumption help with insomnia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. I mean, I'm", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Jack Black is not nominated for an Em", "extracted_entity": null}
{"question": "would someone on Venus be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the temperature on Venus is extremely", "extracted_entity": null}
{"question": "was Jean Valjean imprisoned due to hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was imprisoned for stealing a", "extracted_entity": null}
{"question": "is Europa (moon) name origin related to Amunet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Europa is named after the Phoenician goddess", "extracted_entity": null}
{"question": "does Adobe Suite have video game engine coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has a few different game engines you", "extracted_entity": null}
{"question": "could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because The Metropolitan Museum of Art is in", "extracted_entity": null}
{"question": "would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the United States Military Academy would not reject", "extracted_entity": null}
{"question": "is strep throat harmless to singer Rita Ora after her 2020 tonsilitis surgery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Strep throat is caused by group", "extracted_entity": null}
{"question": "would an Alfa Romeo vehicle fit inside a barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it would fit inside a barn", "extracted_entity": null}
{"question": "gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Gandalf would win.", "extracted_entity": null}
{"question": "could two newborn American Black Bear cubs fit on a king size bed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they're too big.", "extracted_entity": null}
{"question": "are coopers required in the beverage industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, coopers are not required in the", "extracted_entity": null}
{"question": "was Superhero fiction invented in the digital format?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Superhero fiction has been around since", "extracted_entity": null}
{"question": "does Antarctica have a lot of problems relating to homelessness?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Antarctica does not have a", "extracted_entity": null}
{"question": "does an individual oceanographer study many sciences?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, an oceanographer studies many sciences.", "extracted_entity": null}
{"question": "is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would be fine.", "extracted_entity": null}
{"question": "did Julio Gonzalez like acetylene?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "is the letter D influenced by the shape of ancient doors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The letter D is influenced by the shape of ancient", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a parody of the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And the Bible is a parody of", "extracted_entity": null}
{"question": "was the death of Heath Ledger caused by his work on The Dark Knight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "did Donatello use a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "did Rahul Dravid ever kick a field goal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "do you often hear Marco Polo's name shouted near water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, when people are playing Marco Polo", "extracted_entity": null}
{"question": "are people more likely than normal to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is very easy to get sunburn", "extracted_entity": null}
{"question": "is an espresso likely to assuage fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'd say so.", "extracted_entity": null}
{"question": "will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The pregnancy will not last past", "extracted_entity": null}
{"question": "would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kant believed in equality of all humans,", "extracted_entity": null}
{"question": "did a gladiator kill his opponent with a shotgun?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a gladiator never killed his oppon", "extracted_entity": null}
{"question": "did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they had to defend themselves from the", "extracted_entity": null}
{"question": "could B be mistaken for an Arabic numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, B is not a letter, it is", "extracted_entity": null}
{"question": "can COVID-19 spread to maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "is 500GB USB device enough to save 10 hours of Netflix shows a day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not at 1080", "extracted_entity": null}
{"question": "do hornets provide meaningful data for oceanographers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they can be used to make a", "extracted_entity": null}
{"question": "would Michael Phelps be good at pearl hunting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he has webbed feet.", "extracted_entity": null}
{"question": "at Christmastime, do some films remind us that groundhog day is approaching?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s like a groundhog", "extracted_entity": null}
{"question": "was Great Recession the period of severest unemployment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "could Katharine Hepburn have ridden the AirTrain JFK?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She was a regular on the train.", "extracted_entity": null}
{"question": "was Walt Disney able to email his illustrations to people living far away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Walt Disney had to mail his illustr", "extracted_entity": null}
{"question": "can a person who knows only English read Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you will need to learn the ph", "extracted_entity": null}
{"question": "would three commas be sufficient for displaying US 2018 GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, three commas would be sufficient for displaying", "extracted_entity": null}
{"question": "would Brian Warner be a good singer for a soul music band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he would be a great singer for a", "extracted_entity": null}
{"question": "could someone listen to the entire U2 debut studio album during an episode of Peppa Pig?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "did Hanuman ever experience an orgasm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not that I know of.", "extracted_entity": null}
{"question": "would you find a tibia beside parsley on a holiday plate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, i would.", "extracted_entity": null}
{"question": "does someone from Japan need a passport to go to a Nordic country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are a Japanese citizen,", "extracted_entity": null}
{"question": "would a black widow woman have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the peaches are a way to get", "extracted_entity": null}
{"question": "does a person using tonsure have hair at the top of their scalp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, tonsure is a process where the top", "extracted_entity": null}
{"question": "would Terence Tao outperform Eminem in a math competition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Terence Tao is a math", "extracted_entity": null}
{"question": "do any video games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, slot machines are for casinos.", "extracted_entity": null}
{"question": "is number of different US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, 18 is a lucky number", "extracted_entity": null}
{"question": "can lettuce result in spontaneous abortion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I have never heard of such a thing", "extracted_entity": null}
{"question": "has mummification in the Andes been prevented by rainfall?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the rainfall in the Andes", "extracted_entity": null}
{"question": "are some Do It Yourself projects potentially lethal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all.", "extracted_entity": null}
{"question": "was King Kong (2005 film) the lead actress's biggest box office role?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the lead actress was in the following mov", "extracted_entity": null}
{"question": "would a bodybuilder choose maize over chicken breast for dinner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure about a bodybuilder, but", "extracted_entity": null}
{"question": "karachi was a part of Alexander the Great's success?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it was a part of his failure.", "extracted_entity": null}
{"question": "do guitarists need both hands to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they need one hand to play and the", "extracted_entity": null}
{"question": "does Kenny G hold the qualifications to be a tax collector?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "in baseball, is a \"Homer\" named after the poet Homer who wrote the Odyssey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not for the reason you might think", "extracted_entity": null}
{"question": "is a felony jury enough people for a Bunco game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you need 12.", "extracted_entity": null}
{"question": "is letter C crucial to spelling the two most common words in English language?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Both the words are \"the\" and", "extracted_entity": null}
{"question": "do most middle class families have butlers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but I think it", "extracted_entity": null}
{"question": "is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Charles Dickens is buried at Poet", "extracted_entity": null}
{"question": "did Mickey Mouse appear in a cartoon with Bugs Bunny in 1930?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bugs Bunny did not appear in", "extracted_entity": null}
{"question": "did Jackson 5 members exceed number in The Osmonds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, The Jackson 5 consisted of 5", "extracted_entity": null}
{"question": "can you get a ride on Amtrak to the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there is no train to the Under", "extracted_entity": null}
{"question": "are all limbs required for jujutsu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can get a lot more out", "extracted_entity": null}
{"question": "would The Dark Knight be appropriate for a preschool class?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. I\u2019d", "extracted_entity": null}
{"question": "did Jesus know anyone who spoke Estonian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I know that Jesus spoke Estonian,", "extracted_entity": null}
{"question": "does 2015 have more unlucky Friday's than usual?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because 2015 is not", "extracted_entity": null}
{"question": "is Tony Bennett's middle name shared by a former UFC champion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bennett is the middle name of former UFC", "extracted_entity": null}
{"question": "do you need both hot and cold water to peel a tomato?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, just hot water will do", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of Reptile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's a Ging-o", "extracted_entity": null}
{"question": "do spider wasps have eight legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, spider wasps only have six legs", "extracted_entity": null}
{"question": "does the New York Public Library sell Alpo products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the New York Public Library does not sell", "extracted_entity": null}
{"question": "can petroleum jelly be used as fuel in a car?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be a good fuel source", "extracted_entity": null}
{"question": "can Aerosmith legally drive in the carpool lane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not allowed to drive in the", "extracted_entity": null}
{"question": "are Durian fruits an olfactory delight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but that\u2019s not the only thing", "extracted_entity": null}
{"question": "could a wandering albatross fly from Bucharest to New York City without a rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would need a rest.", "extracted_entity": null}
{"question": "do some people soak in olive oil and water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some people do soak in olive", "extracted_entity": null}
{"question": "do depressed people travel to the Golden Gate Bridge often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. Maybe.", "extracted_entity": null}
{"question": "can members of the Green Party of England and Wales vote in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can\u2019t.", "extracted_entity": null}
{"question": "is Poseidon similar to the god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Poseidon is similar to the", "extracted_entity": null}
{"question": "is Glycol something United Airlines would buy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is not an airline product", "extracted_entity": null}
{"question": "does a person suffering from Thalassophobia enjoy oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not enjoy oceanography.", "extracted_entity": null}
{"question": "can a Bengal cat survive eating only pancakes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would die.", "extracted_entity": null}
{"question": "does it seem like the Gorillaz is composed of more members than they have?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are only four members, but there", "extracted_entity": null}
{"question": "could Rich and Morty be triggered for children of alcoholics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, absolutely. I can see how it could", "extracted_entity": null}
{"question": "was the Japanese street aesthetic once illuminated by noble gasses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Japanese street aesthetic was", "extracted_entity": null}
{"question": "in isopropyl alcohol, is the solubility of salt low?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The solubility of salt in isoprop", "extracted_entity": null}
{"question": "could you watch all of JAG in six months?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I could.", "extracted_entity": null}
{"question": "do silicone suits make judo difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not if you have the right silicone suit", "extracted_entity": null}
{"question": "will The Exorcist stimulate limbic system?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm sure it will.", "extracted_entity": null}
{"question": "was P. G. Wodehouse's favorite book The Hunger Games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was The Code of the Woost", "extracted_entity": null}
{"question": "would J.K Rowling's top sellers be on a fantasy shelf?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Harry Potter is on a fantasy", "extracted_entity": null}
{"question": "are some types of pancakes named after coins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the coin pancake is a type", "extracted_entity": null}
{"question": "can Paprika be made without a dehydrator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Paprika can be made without a deh", "extracted_entity": null}
{"question": "in Doctor Who, did the war doctor get more screen time than his successor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the War Doctor got more screen time than", "extracted_entity": null}
{"question": "was Mesopotamia part of what is now China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was in the Middle East.", "extracted_entity": null}
{"question": "could Arnold Schwarzenegger hypothetically defeat Haf\u00fe\u00f3r Bj\u00f6rnsson in a powerlifting competition if both are at their peak strength?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Arnold could beat Haf\u00fe\u00f3r in", "extracted_entity": null}
{"question": "was the British car, the Mini, the first car manufactured?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Mini was not the first car", "extracted_entity": null}
{"question": "will you see peach blossoms and Andromeda at the same time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the peach blossoms and the", "extracted_entity": null}
{"question": "can The Hobbit be read in its entirety in four minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But, yes.", "extracted_entity": null}
{"question": "is number of stars in Milky Way at least ten times earth's population?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's probably about 10", "extracted_entity": null}
{"question": "could Christopher Walken enlist in the United States Marine Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is too old.", "extracted_entity": null}
{"question": "is white light the absence of color?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. White light is made up of all the", "extracted_entity": null}
{"question": "can Jabberwocky be considered a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a sonnet.", "extracted_entity": null}
{"question": "would Eye surgery on a fly be in vain?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The eye of a fly is not a complex structure", "extracted_entity": null}
{"question": "should you ask a neighbor for candy on New Year's Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you want to be polite.", "extracted_entity": null}
{"question": "are all types of pottery safe to cook in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Porcelain is a type of pot", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can!", "extracted_entity": null}
{"question": "does the United States of America touch the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the United States touches the Indian Ocean", "extracted_entity": null}
{"question": "would Donald Duck be allowed into most grocery stores?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's a duck", "extracted_entity": null}
{"question": "is it best to avoid kola nuts with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, kola nuts are best avoided.", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA daily recommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "7 zucchini's would be", "extracted_entity": null}
{"question": "was the Treaty of Versailles settled over blueberry scones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was settled over a cup of tea", "extracted_entity": null}
{"question": "will the producer of Easy Rider become an octogenarian in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he died in 2008", "extracted_entity": null}
{"question": "does Osama bin Laden put a wafer on his tongue every Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he does put a wafer on", "extracted_entity": null}
{"question": "can you taste Law & Order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I can. I can taste it.", "extracted_entity": null}
{"question": "are queen bees unnecessary for growing apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You need a queen to have a col", "extracted_entity": null}
{"question": "does a bumblebee have to worry about spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a bumblebee doesn\u2019t", "extracted_entity": null}
{"question": "can the Department of Defense perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the Department of Defense can do", "extracted_entity": null}
{"question": "can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it is in a very", "extracted_entity": null}
{"question": "was Land of Israel in possession of an Islamic empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all of them do.", "extracted_entity": null}
{"question": "could the Toyota Stadium sit a tenth of the population of Gotheburg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the stadium has a capacity of", "extracted_entity": null}
{"question": "does the word swastika have meaning in sanskrit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The word swastika is sansk", "extracted_entity": null}
{"question": "were there greater landslides than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the 1980 United States", "extracted_entity": null}
{"question": "can you hunt Iberian wolves in the Southern United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not exist in the Southern United", "extracted_entity": null}
{"question": "is hanging a viable execution method on a ship at sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s not the most practical", "extracted_entity": null}
{"question": "is pig meat considered inedible within the cuisine of Hawaii?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pig meat is considered inedible within the", "extracted_entity": null}
{"question": "are the events of Star Trek: The Next Generation in the history of the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "if you're pregnant, might you be recommended ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, ginger is a natural remedy that", "extracted_entity": null}
{"question": "does The Jungle Book contain racist subtext?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a product of its time.", "extracted_entity": null}
{"question": "does the Prime Minister of the United Kingdom have poor job security?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he/she can be removed from office", "extracted_entity": null}
{"question": "would Gomer Pyle salute a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Gomer Pyle would salute a", "extracted_entity": null}
{"question": "when en route from China to France, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "2000 feet", "extracted_entity": null}
{"question": "could you brew beer from start to finish in the month of September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a lot of work", "extracted_entity": null}
{"question": "is Freya a combination of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Freya is the goddess of love and beauty", "extracted_entity": null}
{"question": "in most Mennonite homes, would children know of The Powerpuff Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I would say so.", "extracted_entity": null}
{"question": "would a sophist use an \u00e9p\u00e9e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a sophist would use an \u00e9p\u00e9e,", "extracted_entity": null}
{"question": "did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Paul the Apostle's cause of", "extracted_entity": null}
{"question": "do sand cats avoid eating all of the prey of eels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they eat all of the prey of", "extracted_entity": null}
{"question": "was Muhammed a member of the Uniting Church in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a member of the Anglic", "extracted_entity": null}
{"question": "would the historic Hattori Hanz\u014d admire Naruto?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would admire him for his strength", "extracted_entity": null}
{"question": "should you wrap a gift for a mother of a stillborn in stork wrapping paper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that would be in poor taste.", "extracted_entity": null}
{"question": "does crucifixion violate US eighth amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The eighth amendment of the US constitution", "extracted_entity": null}
{"question": "would a TMNT coloring book have pizza in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes it would.", "extracted_entity": null}
{"question": "could Sugar Ray Robinson box if he stole in Iran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He could not.", "extracted_entity": null}
{"question": "would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course.", "extracted_entity": null}
{"question": "can a banana get a virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a banana can get a virus.", "extracted_entity": null}
{"question": "is the use of the word Gypsy by non-Romani people considered okay?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The word Gypsy is considered a", "extracted_entity": null}
{"question": "are implants from an ORIF surgery affected by the magnetic field of the Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The implants are made from titan", "extracted_entity": null}
{"question": "was Subway involved in a pedophilia scandal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and the corporation has been sued", "extracted_entity": null}
{"question": "did Jack Dempsey have most title fight wins in either of his weight classes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He fought 35 times in the Heavy", "extracted_entity": null}
{"question": "did Bill Nye vote for Franklin Delano Roosevelt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was born in 195", "extracted_entity": null}
{"question": "is grief always obvious when it is being experienced?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "did either side score a touchdown during the Football War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there was a lot of pushing and", "extracted_entity": null}
{"question": "is there a Yeti associated with Disney theme parks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there is a Yeti associated with", "extracted_entity": null}
{"question": "did Zorro carve his name into items regularly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Zorro did carve his name", "extracted_entity": null}
{"question": "did Subway have a sex offender as a spokesperson?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is a convicted sex offender", "extracted_entity": null}
{"question": "could Christopher Nolan's movies finance Cyprus's entire GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The answer is yes.", "extracted_entity": null}
{"question": "is Steve Carell's character on The Office portrayed as one with tremendous leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Dominicans would win because of their", "extracted_entity": null}
{"question": "can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all of them can.", "extracted_entity": null}
{"question": "does the texture of leaves remain the same independent of their coloring changing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The texture of leaves does not change with the color", "extracted_entity": null}
{"question": "can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the word is \"shalom\"", "extracted_entity": null}
{"question": "would Doctor Strange like the Pittsburgh Steelers logo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he would know that it is a", "extracted_entity": null}
{"question": "would Iris (mythology) and Hermes hypothetically struggle at a UPS job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hermes is an immortal, so he", "extracted_entity": null}
{"question": "is pi in excess of square root of 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. pi is in excess of square root of", "extracted_entity": null}
{"question": "would it be uncommon for a high schooler to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I know my", "extracted_entity": null}
{"question": "could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Santa Maria could haul the Stat", "extracted_entity": null}
{"question": "does the human stomach destroy a bee if ingested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The bee is digested by the", "extracted_entity": null}
{"question": "is blonde hair green eyed Sara Paxton considered a Latino?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you are a Latino, then you are", "extracted_entity": null}
{"question": "are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is no water on Mars. There", "extracted_entity": null}
{"question": "can you see live harbor seals in Washington DC?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at the National Aquarium, located on", "extracted_entity": null}
{"question": "was story of Jesus inspired by Egyptian myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the story of Jesus is not inspired by", "extracted_entity": null}
{"question": "has CNES planted a French flag on the lunar surface?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the French flag was planted on the", "extracted_entity": null}
{"question": "do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "can Iowa be hidden in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "can you house a giant squid at Soldier Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no.", "extracted_entity": null}
{"question": "was hippie culture encouraged by the government in the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The hippie movement was a counter", "extracted_entity": null}
{"question": "would Mickey Mouse blend in with the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the flag is red, white and", "extracted_entity": null}
{"question": "does penicillin cure a learning disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, penicillin does not cure", "extracted_entity": null}
{"question": "is it dark is Basel during the day in Los Angeles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is dark in Basel during the", "extracted_entity": null}
{"question": "is honey associated with queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because queens are always the most powerful", "extracted_entity": null}
{"question": "was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nancy Pelosi", "extracted_entity": null}
{"question": "does a giant green lady stand in New York Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Statue of Liberty is in", "extracted_entity": null}
{"question": "could chives be mistaken for grass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they both are green.", "extracted_entity": null}
{"question": "could a two-year old win a Scrabble tournament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you let him use the dictionary.", "extracted_entity": null}
{"question": "is Y2K relevant to the plot of The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Godfather is a fictional work", "extracted_entity": null}
{"question": "is Sirius part of a constellation of an animal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Sirius is part of a const", "extracted_entity": null}
{"question": "would alligator best saltwater crocodile in hypothetical Lake Urmia battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, alligator would win", "extracted_entity": null}
{"question": "would a Common warthog starve in a greenhouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it had no food or water.", "extracted_entity": null}
{"question": "is purchasing food for a Lolcat unnecessary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "do some psychotherapy patients have no mental illness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some patients have no mental illness.", "extracted_entity": null}
{"question": "did Harry Houdini appear on Chris Angel Mindfreak?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "could  jockey win Triple Crown between Eid al-Fitr endpoints?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but jockey has to win the Bel", "extracted_entity": null}
{"question": "would a vegetarian be able to eat something at Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are vegetarian options on the menu", "extracted_entity": null}
{"question": "does the Pixar film Brave feature Scottish people?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it features Scottish people.", "extracted_entity": null}
{"question": "do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the leafhopper will compete for", "extracted_entity": null}
{"question": "do most college students own a fax machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he would be able to clear the", "extracted_entity": null}
{"question": "could a delicious recipe be made with The Onion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! A very delicious recipe could be", "extracted_entity": null}
{"question": "could a markhor give birth three times in a single year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Markhor is a mammal and mamm", "extracted_entity": null}
{"question": "would a model be likely to frequently enjoy the menu at Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's a good value and", "extracted_entity": null}
{"question": "does the history of Europe include the age of dinosaurs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the age of dinosaurs was a", "extracted_entity": null}
{"question": "was John George Bice's birthplace near Cornwall?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, John George Bice was born in Corn", "extracted_entity": null}
{"question": "do white blood cells outnumber red blood cells in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "was Edward II crucial to England's victory at Battle of Falkirk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The English were not at Falkirk", "extracted_entity": null}
{"question": "did Holy Land belong to Adamu's tribe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Holy Land belonged to Adamu'", "extracted_entity": null}
{"question": "does Ahura Mazda have a rivalry with Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not rivals.", "extracted_entity": null}
{"question": "is an Eastern chipmunk likely to die before seeing two leap years?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the Eastern chipmunk is a", "extracted_entity": null}
{"question": "would the owners of the company Peter Griffin works for need barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would need barley.", "extracted_entity": null}
{"question": "is the CIA part of the Department of Defense?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The CIA is a separate agency from the", "extracted_entity": null}
{"question": "was Harry Potter and the Philosopher's Stone popular during the great depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know what you mean by \"", "extracted_entity": null}
{"question": "is a slime mold safe from cerebral palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, slime mold is not safe from", "extracted_entity": null}
{"question": "is British Airways the air force of the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the air force of the United Kingdom is", "extracted_entity": null}
{"question": "has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It has changed a few times.", "extracted_entity": null}
{"question": "are aggressive bumblebees suicidal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not suicidal.", "extracted_entity": null}
{"question": "is Edward Snowden in hiding from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is in hiding from the United States", "extracted_entity": null}
{"question": "can the Great Depression be treated with Prozac?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Great Depression cannot be treated with", "extracted_entity": null}
{"question": "can photography be considered abstract art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, photography can be considered abstract art.", "extracted_entity": null}
{"question": "could a bee hummingbird balance a scale with a single pea on it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could", "extracted_entity": null}
{"question": "is Disney associated with Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Disneyland is in Orange County, but", "extracted_entity": null}
{"question": "did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They both are Jewish, they both have", "extracted_entity": null}
{"question": "is a doctorate required to teach at a SUNY School?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a doctorate is not required to teach", "extracted_entity": null}
{"question": "does the swastika have positive uses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does Nintendo's link ever see an astronomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not the one you're thinking", "extracted_entity": null}
{"question": "has Elon Musk's hairline changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it hasn't.", "extracted_entity": null}
{"question": "do American teams in National Hockey League outnumber Canadian teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are more American teams than Canadian teams", "extracted_entity": null}
{"question": "did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Lois & Clark lasted 8", "extracted_entity": null}
{"question": "were all the materials to make a cannon known during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "is Cookie Monster's diet Paleo friendly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's a cookie monster,", "extracted_entity": null}
{"question": "would it be impossible to seat every Chief Justice of the United States on a Boeing 737?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible.", "extracted_entity": null}
{"question": "could Toyota stadium house people suffering homelessness in Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if we could get the stadium and", "extracted_entity": null}
{"question": "can a martyr saint have been excommunicated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a martyr saint can have been ex", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's the only face you can", "extracted_entity": null}
{"question": "does a Disney princess on Broadway have red hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she does have a red dress", "extracted_entity": null}
{"question": "could Steven Spielberg send emails as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he could use a Walkie T", "extracted_entity": null}
{"question": "does Felix Potvin have a position on a dodgeball team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's too busy playing goalie", "extracted_entity": null}
{"question": "would someone with leukophobia enjoy looking at the Flag of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the flag is white.", "extracted_entity": null}
{"question": "would 1943-S penny be good for making silverware?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A 1943-S", "extracted_entity": null}
{"question": "did mongoose come from later period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, mongoose came from later period than rh", "extracted_entity": null}
{"question": "would it be possible to fit a football field in Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you could fit a football field in Al", "extracted_entity": null}
{"question": "is Issac Newton often associated with a red fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "an apple", "extracted_entity": null}
{"question": "would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, eliminating competition in the Japanese bulk car", "extracted_entity": null}
{"question": "can you buy spinal cord at Home Depot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can\u2019t buy spinal cord", "extracted_entity": null}
{"question": "are some chiropractic manipulations dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have a heart condition, or", "extracted_entity": null}
{"question": "were some people afraid of New Years Day coming in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some people were afraid of the New Years", "extracted_entity": null}
{"question": "did travelers sing sea shanties on the Oregon Trail?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sea shanties were not sung", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have longer terms than most senators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They serve for life unless they retire", "extracted_entity": null}
{"question": "paleography hypothetically helps to understand Cthulhu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it allows one to study the writ", "extracted_entity": null}
{"question": "is surfing popular in Des Moines, Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not popular in Des Moines", "extracted_entity": null}
{"question": "is Statue of Unity hypothetically more level with Statue of Liberty than Lighthouse of Alexandria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Statue of Unity is more level with", "extracted_entity": null}
{"question": "is CEO of Nissan an internationally wanted fugitive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is, according to the Japanese government", "extracted_entity": null}
{"question": "has type of political association Pompey had with Caesar influenced reality TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because of the reality TV show \u201cBig", "extracted_entity": null}
{"question": "would Rime of the Ancient Mariner make a good sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I don\u2019t", "extracted_entity": null}
{"question": "do Snow White dwarves best The Hobbit dwarves in battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Snow White dwarves are much smaller than The", "extracted_entity": null}
{"question": "can the history of art be learned by an amoeba?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can be learned by an am", "extracted_entity": null}
{"question": "can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "walt Disney dominated his amusement park peers at Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nope, but Disney did win 22", "extracted_entity": null}
{"question": "would human race go extinct without chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "is the average bulk carrier ideal for transporting bromine at room temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the bromine vapor is", "extracted_entity": null}
{"question": "can depression be mistaken for laziness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be.", "extracted_entity": null}
{"question": "did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Her name is Brooke Shields", "extracted_entity": null}
{"question": "did pirates who had scurvy need more Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they did", "extracted_entity": null}
{"question": "could the leader of Heaven's Gate save images in JPEG format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the leader of Heaven's Gate could", "extracted_entity": null}
{"question": "does the FDA require sell by dates using Roman Numerals?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the FDA does not require sell by", "extracted_entity": null}
{"question": "would Kylee Jenner ask for no cream in her coffee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she'd ask for no sugar.", "extracted_entity": null}
{"question": "does Nicole Kidman know any Scientologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she does.", "extracted_entity": null}
{"question": "is Lionel Richie related to Sheila E?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Sheila E is Prince's ex", "extracted_entity": null}
{"question": "was a nuclear bomb used in the Napoleonic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the first atomic bomb was dropped on H", "extracted_entity": null}
{"question": "is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because you may fall off and hurt yourself", "extracted_entity": null}
{"question": "can you buy a fair trade laptop?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is no such thing as a fair", "extracted_entity": null}
{"question": "was Noah concerned with buoyancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was concerned with buoyancy because", "extracted_entity": null}
{"question": "is a watchmaker likely to be able to fix an Apple Watch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not a chance", "extracted_entity": null}
{"question": "are sables related to wolverines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sables are related to wolverines, but", "extracted_entity": null}
{"question": "are some Brazilian Navy ships built in Britian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some of them.", "extracted_entity": null}
{"question": "could Carl Friedrich Gauss speak to someone 100 miles away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he didn't have a phone", "extracted_entity": null}
{"question": "in Hey Arnold, did any characters stay on a porch all the time?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The porch was a character in Hey Arnold.", "extracted_entity": null}
{"question": "has Johns Hopkins University always treated subjects ethically?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they have not.", "extracted_entity": null}
{"question": "is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Jackson Pollock's paintings", "extracted_entity": null}
{"question": "do black-tailed jackrabbits fear the European wildcat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not afraid of the European", "extracted_entity": null}
{"question": "has Kelly Clarkson outsold season 4 American Idol winner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she has.", "extracted_entity": null}
{"question": "is Hermes equivalent to the Roman god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Hermes is the Roman god Vul", "extracted_entity": null}
{"question": "can an elite runner circle the Pyrenees in one day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are a woman.", "extracted_entity": null}
{"question": "is the Golden eagle considered a scavenger bird?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The golden eagle is a carn", "extracted_entity": null}
{"question": "can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you have to use the second line.", "extracted_entity": null}
{"question": "are grapes essential to winemaking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are used to make wine.", "extracted_entity": null}
{"question": "would the Cookie Monster decline an offer of free Keebler products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "did Jeremy Irons master sweep picking as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a terrible guitar player as a", "extracted_entity": null}
{"question": "for Hostas to look their best, do they need lots of chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Chlorophyll is the pig", "extracted_entity": null}
{"question": "has spinach been a source of power in a comic movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Popeye.", "extracted_entity": null}
{"question": "can you see the Statue of Freedom from the Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can see the Statue of", "extracted_entity": null}
{"question": "does Ludacris have Greek heritage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, his real name is Christopher Brian Brid", "extracted_entity": null}
{"question": "will bumblebees derail the United States presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Maybe. The bees are back.", "extracted_entity": null}
{"question": "does Rahul Dravid belong to the family Gryllidae?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "is Metallica protective over their music?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are protective of their music,", "extracted_entity": null}
{"question": "does Dean Cain have less days to birthday than Will Ferrell every 4th of July?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Will Ferrell has 2 more days", "extracted_entity": null}
{"question": "is Lord Voldemort associated with a staff member of Durmstrang?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is.", "extracted_entity": null}
{"question": "are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is starving Hamas agent eating pig bad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is not the same as eating", "extracted_entity": null}
{"question": "would Ringo Starr avoid the pot roast at a restaurant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he\u2019s a vegetarian.", "extracted_entity": null}
{"question": "was ship that recovered Apollo 13 named after a World War II battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was named after the battle of I", "extracted_entity": null}
{"question": "are both founders of Ben & Jerry's still involved in the company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ben Cohen and Jerry Greenfield are", "extracted_entity": null}
{"question": "is a krabby patty similar to a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. a cheeseburger is a h", "extracted_entity": null}
{"question": "did Beethoven enjoy listening to EDM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was deaf.", "extracted_entity": null}
{"question": "is it true that gay male couples cannot naturally reproduce?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they cannot naturally reproduce, they need to", "extracted_entity": null}
{"question": "could an American confuse breakfast in British cuisine for dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are very stupid and", "extracted_entity": null}
{"question": "can Kit & Kaboodle hypothetically help someone past the Underworld gates?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are in possession of", "extracted_entity": null}
{"question": "are two cans of Campbell's Soup a day good for hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Campbell's Soup is not good", "extracted_entity": null}
{"question": "are there any official American knights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are the Knights of Malta", "extracted_entity": null}
{"question": "is the Foreign and Commonwealth Office a European political agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a British government department.", "extracted_entity": null}
{"question": "do gorillas fight with panda bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not.", "extracted_entity": null}
{"question": "would lumberjacks get full after eating three dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "is being 5 year Capital One Venture member more cost effective than being in Church of Satan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Venture card is free for", "extracted_entity": null}
{"question": "tata Hexa can accomodate every Spice Girl?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Spice Girls are a British girl group formed", "extracted_entity": null}
{"question": "could common warthog be useful for scrimshaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be used for scrimsh", "extracted_entity": null}
{"question": "is breast cancer associated with a ribbon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the breast cancer ribbon is pink", "extracted_entity": null}
{"question": "would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Red Sea is very deep. It", "extracted_entity": null}
{"question": "is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s not.", "extracted_entity": null}
{"question": "was a Tiny House ceiling out of Osama bin Laden's reach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the ceiling was only 5", "extracted_entity": null}
{"question": "can you conduct surveillance from a teddy bear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. It's a ted", "extracted_entity": null}
{"question": "does table tennis use prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the prime numbers are used in table tennis", "extracted_entity": null}
{"question": "was the Carnation Revolution the deadliest revolution in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Carnation Revolution was not the", "extracted_entity": null}
{"question": "is the Muslim world hostile to Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are hostile to Israel because the", "extracted_entity": null}
{"question": "is the saltwater crocodile less endangered than the European otter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the saltwater crocodile is less", "extracted_entity": null}
{"question": "can someone from New England profit by growing coffee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the question is whether it is prof", "extracted_entity": null}
{"question": "could a dichromat probably easily distinguish chlorine gas from neon gas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because chlorine gas has a distinct", "extracted_entity": null}
{"question": "nATO doesn't recognize double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, NATO doesn't recognize double triangle", "extracted_entity": null}
{"question": "did Julius Caesar read books on Pharmacology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea, but I doubt it.", "extracted_entity": null}
{"question": "does water have viscosity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, water has viscosity.", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "would you spend less on your food at Aldi than at Whole Foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would. I would spend less on", "extracted_entity": null}
{"question": "are Scottish people Albidosi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are System of a Down opposed to globalization?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think they are.", "extracted_entity": null}
{"question": "was Alexander the Great baptized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not baptized.", "extracted_entity": null}
{"question": "did Demi Lovato's ancestors help turn maize into popcorn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn't.", "extracted_entity": null}
{"question": "would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible.", "extracted_entity": null}
{"question": "is Pearl Harbor the mythical home of a shark goddess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (If you didn\u2019t know that", "extracted_entity": null}
{"question": "would Elon Musk be more likely to know about astrology than physics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he's a physicist", "extracted_entity": null}
{"question": "can paresthesia be caused by a white pigment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as a direct cause. par", "extracted_entity": null}
{"question": "could Charlie Bucket be a hotel manager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he was very good at dealing with", "extracted_entity": null}
{"question": "can children be soldiers in the US Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Army does not have an upper age", "extracted_entity": null}
{"question": "is Jack Black's height enough to satisfy Coronavirus distancing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his height is not enough to satisfy Cor", "extracted_entity": null}
{"question": "are peaches best eaten when firm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are best eaten when soft.", "extracted_entity": null}
{"question": "was Charles Manson's body unwanted?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Charles Manson's body was wanted, but", "extracted_entity": null}
{"question": "was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Kurt Cobain's death was not", "extracted_entity": null}
{"question": "can a human eat an entire 12-lb roast turkey in an hour? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the world record is 12 pounds", "extracted_entity": null}
{"question": "would a house full of aloe vera hypothetically be ideal for Unsinkable Sam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I can't imagine that it would. The", "extracted_entity": null}
{"question": "would dual-energy X-ray absorptiometry be useful if performed on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it would help the doctor diagnose", "extracted_entity": null}
{"question": "would Harvey Milk have approved of Obama?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would have to say yes. Harvey Mil", "extracted_entity": null}
{"question": "would a jumping spider need over half a dozen contact lenses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't have lenses,", "extracted_entity": null}
{"question": "are seasons of Survivor surpassed by number of Ancient Greek letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even close. There are 2", "extracted_entity": null}
{"question": "do citizens of Cheshire sing La Marseillaise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do sing a song called \u2018", "extracted_entity": null}
{"question": "are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not necessarily. However, a person with", "extracted_entity": null}
{"question": "in teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "could eating Chinook salmon help Ryan Reynolds?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chinook salmon are a great", "extracted_entity": null}
{"question": "would kaffir lime be good in a White Russian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "is entire Common Era minuscule to lifespan of some trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Entire Common Era is less than", "extracted_entity": null}
{"question": "did Charlemagne have a bar mitzvah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a Catholic.", "extracted_entity": null}
{"question": "did Monty Python write the Who's on First sketch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was written by Bud Abbott and", "extracted_entity": null}
{"question": "in geometry terms, is the Royal Observatory in Greenwich similar to a yield sign?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is a semi-circular sign", "extracted_entity": null}
{"question": "did Sugar Ray Robinson win a fight against Canelo Alvarez?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would have been a 19", "extracted_entity": null}
{"question": "can an asteroid be linked with virginity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so.", "extracted_entity": null}
{"question": "would a Yeti be likely to have prehensile limbs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't see why not.", "extracted_entity": null}
{"question": "is groundhog day used as a global season indicator? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did mercenaries fight for England in the Glorious Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the English army had many foreign mercen", "extracted_entity": null}
{"question": "was the sable depicted in Marvel comics anthropomorphic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were just normal wolves.", "extracted_entity": null}
{"question": "are twinkies considered artisan made products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are mass produced and mass consumed", "extracted_entity": null}
{"question": "were mollusks an ingredient in the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, mollusks were an ingred", "extracted_entity": null}
{"question": "is Christianity better for global warming than Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "was Lord Voldemort taught by Professor Dumbledore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Professor Dumbledore taught Professor Sna", "extracted_entity": null}
{"question": "would costumes with robes and pointy hats be helpful for Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "costumes with robes and pointy hats", "extracted_entity": null}
{"question": "are the knights in the Medieval Times show not authentic knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The show is a mock tournament.", "extracted_entity": null}
{"question": "could William Franklyn-Miller win a 2020 Nascar Cup Series race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "did Al-Farabi ever meet Mohammed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Al-Farabi was born after", "extracted_entity": null}
{"question": "did the Pearl Harbor attack occur during autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the attack occurred on December 7th", "extracted_entity": null}
{"question": "have the Israelis played the Hammerstein Ballroom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, on the 2007 tour", "extracted_entity": null}
{"question": "if you're reducing salt intake, are olives a healthy snack?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, olives are a healthy snack", "extracted_entity": null}
{"question": "do pediatricians perform abortions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I am glad they do.", "extracted_entity": null}
{"question": "could casualties from deadliest war rival France's population?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no. The deadliest war was", "extracted_entity": null}
{"question": "does Lorem ipsum backwards fail to demonstrate alliteration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not.", "extracted_entity": null}
{"question": "can you see Stonehenge from a window in Dusseldorf?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is yes.", "extracted_entity": null}
{"question": "can Reiki be stored in a bottle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Reiki is energy. It cannot be", "extracted_entity": null}
{"question": "would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not. The 20", "extracted_entity": null}
{"question": "would Tom Cruise ever insult L. Ron Hubbard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Tom Cruise is a Scientologist,", "extracted_entity": null}
{"question": "does Lemon enhance the flavor of milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lemon enhances the flavor of milk.", "extracted_entity": null}
{"question": "was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Harry Potter was a better investment", "extracted_entity": null}
{"question": "is the United States the largest exporter of Fair Trade products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the United States is the largest exporter", "extracted_entity": null}
{"question": "is Snickers helpful for weight loss?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Snickers is not helpful for weight", "extracted_entity": null}
{"question": "could the Jackson 5 play a full game of rugby with each other?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Jackson 5 were never known for their athlet", "extracted_entity": null}
{"question": "did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Isaac's father was a violent man", "extracted_entity": null}
{"question": "did the founders of the biggest city in Orange County, California speak Italian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did", "extracted_entity": null}
{"question": "is an ammonia fighting cleaner good for pet owners?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes, ammonia fighting clean", "extracted_entity": null}
{"question": "could you buy Hershey's Kisses in red foil with farthings after 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The farthing was demonetized in", "extracted_entity": null}
{"question": "could morphine cure HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, morphine is a pain reliever", "extracted_entity": null}
{"question": "would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Quiet is a good example of someone who has", "extracted_entity": null}
{"question": "would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, but it's hard to", "extracted_entity": null}
{"question": "would the chef at La Grenouille find salsa to be a strange request?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the chef at La Grenouille would", "extracted_entity": null}
{"question": "was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was never in danger of flunk", "extracted_entity": null}
{"question": "would toast for a vegan have margarine instead of butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, butter is a dairy product.", "extracted_entity": null}
{"question": "did any cultures associate celery with death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The Egyptians, Greeks and Rom", "extracted_entity": null}
{"question": "are emus related to elks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are related to ostriches", "extracted_entity": null}
{"question": "is the Forbidden City host to a wooden rollercoaster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a 1000", "extracted_entity": null}
{"question": "are slime lilies in a different scientific family than asparagus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are in the same family.", "extracted_entity": null}
{"question": "in order to work in district management, does one need a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it depends on the job.", "extracted_entity": null}
{"question": "can you buy Reddit at Walmart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Reddit is not sold at Walmart", "extracted_entity": null}
{"question": "if you have black hair and want red hair, do you need bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you don\u2019t need bleach.", "extracted_entity": null}
{"question": "is a curling iron necessary in curling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a curling iron is necessary in cur", "extracted_entity": null}
{"question": "was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she was a milliner", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saint Vincent was named by the Italian expl", "extracted_entity": null}
{"question": "did Lamarck and Darwin agree about the origin of species diversity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lamarck and Darwin agreed that species divers", "extracted_entity": null}
{"question": "if you were at an Apple store, would most of the computers be running Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would be running OS X.", "extracted_entity": null}
{"question": "can you make an MP3 from the Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can, but you\u2019ll need", "extracted_entity": null}
{"question": "are there any chives hypothetically good for battling vampires?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but I\u2019m not sure which ones", "extracted_entity": null}
{"question": "could a Diwali celebration feature a crustacean?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it is a crab.", "extracted_entity": null}
{"question": "do astronomers write horoscopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "is the name of a mythical creature also the name of a Small Solar System body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The name of a mythical creature also the name", "extracted_entity": null}
{"question": "did Medieval English lords engage in fair trade with peasants?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The US economy is reducing jobs in the", "extracted_entity": null}
{"question": "do anatomical and symbolic hearts look remarkably different?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. This is an example of an anatom", "extracted_entity": null}
{"question": "are you more likely to find bipolar disorder in a crowd than diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it is not uncommon for people to", "extracted_entity": null}
{"question": "does having lip piercings lead to more expensive dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. Lip piercings can", "extracted_entity": null}
{"question": "are most mall Santa Claus actors white?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. most are white.", "extracted_entity": null}
{"question": "does the Taco Bell kitchen contain cinnamon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "could white rice go rancid before sesame seeds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. White rice can go rancid before", "extracted_entity": null}
{"question": "can you listen to the entire Itunes song catalog in one year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you listen to the entire catalog,", "extracted_entity": null}
{"question": "do mollymawks live where albatrosses cannot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, mollymawks live where al", "extracted_entity": null}
{"question": "would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she would probably also avoid the", "extracted_entity": null}
{"question": "are Mayors safe from harm from the federal government?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is it normal to see a red panda in Shanghai outside of a zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to be lucky.", "extracted_entity": null}
{"question": "would a human following a hyena diet be unwelcome at a vegan festival?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they'd be welcome at a", "extracted_entity": null}
{"question": "can DRL Racer X drone get across Brooklyn Bridge in 18 seconds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "would Temujin hypothetically be jealous of Charlemagne's conquests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would be jealous of Charlemagne", "extracted_entity": null}
{"question": "would a bodybuilder enjoy wearing a cast for several weeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "can you carry a Chrysler in a laptop bag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I have a MacBook Pro.", "extracted_entity": null}
{"question": "is Home Depot a one stop shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Home Depot is a one stop shop", "extracted_entity": null}
{"question": "is the Holy Land important to Eastern religions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is important to Eastern religions because", "extracted_entity": null}
{"question": "is immersion in virtual reality testable on cnidarians before humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are many species of cnid", "extracted_entity": null}
{"question": "did Jack Dempsey ever witness Conor McGregor's fights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Jack Dempsey never witnessed Con", "extracted_entity": null}
{"question": "are black and white prison uniforms made to resemble a zebra?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they're black and white stri", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the holiday he would most likely enjoy", "extracted_entity": null}
{"question": "did Malcolm X avoid eating ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "would a snakebite hypothetically be a threat to T-1000?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I guess it would", "extracted_entity": null}
{"question": "is there a Harry Potter character named after Florence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's Pansy Parkinson", "extracted_entity": null}
{"question": "could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they couldn't.", "extracted_entity": null}
{"question": "was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Ivan the Terrible was Leo the Great", "extracted_entity": null}
{"question": "can Poland Spring make money in the Sahara?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there is no water in the Sah", "extracted_entity": null}
{"question": "do people with mood disorders need permanent institutionalization?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "will Dustin Hoffman likely vote for Trump in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Dustin Hoffman is a", "extracted_entity": null}
{"question": "can your psychologist say hello to you while you are out at the supermarket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, your psychologist can\u2019t say hello", "extracted_entity": null}
{"question": "did Johann Sebastian Bach influence heavy metal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but the music of the Baroque", "extracted_entity": null}
{"question": "are flag of Gabon colors found in rainbow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the flag of Gabon colors are not", "extracted_entity": null}
{"question": "does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. C-SPAN is the acron", "extracted_entity": null}
{"question": "can a grey seal swim in the same water as the subject of Moby Dick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the grey seal can swim in", "extracted_entity": null}
{"question": "did Modern Family win a Slammy award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it did not.", "extracted_entity": null}
{"question": "would Mount Wycheproof be a breeze for Edmund Hillary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be a breeze for", "extracted_entity": null}
{"question": "does Canada have a relationship with a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Queen of England is also the Queen", "extracted_entity": null}
{"question": "would a compact disc melt in magma?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the CD would melt in mag", "extracted_entity": null}
{"question": "is Sea World hazardous to leopard seal's health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes. It is not a natural", "extracted_entity": null}
{"question": "was being a mail carrier considered one of the most dangerous jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was one of the most dangerous jobs", "extracted_entity": null}
{"question": "were Depeche Mode heavily influenced by blues music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Martin Gore was a fan of the", "extracted_entity": null}
{"question": "do people with swallowing disorders need high viscosity drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they need drinks with low viscos", "extracted_entity": null}
{"question": "can whole genome sequencing be used for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not yet.", "extracted_entity": null}
{"question": "is Anakin Skywalker from Star Wars associated with the color black?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Anakin Skywalker is associated", "extracted_entity": null}
{"question": "do you need to schedule separate preventive healthcare and sickness visits? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we schedule separate visits for preventive", "extracted_entity": null}
{"question": "was the original Metroid groundbreaking for its polygons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wasn't. It was ground", "extracted_entity": null}
{"question": "would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a binge watch of Young and the", "extracted_entity": null}
{"question": "are any of the words that CAPTCHA stands for palindromes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The word is not a palindrome", "extracted_entity": null}
{"question": "is the Jurassic era a tourist destination?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. And that's exactly", "extracted_entity": null}
{"question": "can a dolphin keep a diary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be hard to write in", "extracted_entity": null}
{"question": "is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s not unimportant.", "extracted_entity": null}
{"question": "does Carl Linnaeus share the same final resting place as Michael Jackson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Carl Linnaeus, the father of", "extracted_entity": null}
{"question": "could Eddie Murphy dial 911 in a car as a young child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Eddie Murphy could dial", "extracted_entity": null}
{"question": "do flying fish have good eyesight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they have good eyesight.", "extracted_entity": null}
{"question": "does Justin Bieber vote in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he votes in November", "extracted_entity": null}
{"question": "did Ice make people rich?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Ice did not make people rich. Ice", "extracted_entity": null}
{"question": "is J.D. Salinger's most successful work influential to killers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not", "extracted_entity": null}
{"question": "is it impossible for pigs to use pig latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is impossible for pigs to use", "extracted_entity": null}
{"question": "can the theory of cultural hegemony explain global warming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the theory of cultural hegemony", "extracted_entity": null}
{"question": "is March named after Jupiter's son in Roman mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Mars is named after Mars", "extracted_entity": null}
{"question": "can furniture be made of hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be made of hair.", "extracted_entity": null}
{"question": "is SnapCap an example of a retail store?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because you can buy things in store", "extracted_entity": null}
{"question": "did Rand Paul frequently swim in Lake Michigan during his undergraduate years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I did.", "extracted_entity": null}
{"question": "would Hannah Nixon be proud of Richard Nixon following the Watergate scandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think she would be", "extracted_entity": null}
{"question": "would Tony Stark be considered a polymath?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Tony Stark is considered a polymath", "extracted_entity": null}
{"question": "is Supreme Court of the United States analogous to High Courts of Justice of Spain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are analogous.", "extracted_entity": null}
{"question": "does Amtrak run from NYC directly to the Moai location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Amtrak train to Moai", "extracted_entity": null}
{"question": "does Ronda Rousey avoid BBQ restaraunts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Because she can\u2019t afford the ribs.", "extracted_entity": null}
{"question": "can someone in Uberlandia work for Mitsubishi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can work for Mitsubishi", "extracted_entity": null}
{"question": "do oak trees have leaves during winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Oak trees are deciduous, so they", "extracted_entity": null}
{"question": "is Argon near Neon on the periodic table of elements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Argon is near Neon on the", "extracted_entity": null}
{"question": "does the Constitution of the Philippines copy text from the British constitution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Constitution of the Philippines does not copy", "extracted_entity": null}
{"question": "did Ada Lovelace die tragically young for her era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she died at the age of 3", "extracted_entity": null}
{"question": "is dyslexia the most common intellectual disability in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, dyslexia is the most common", "extracted_entity": null}
{"question": "is the referee at a soccer match highly visible against the field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is wearing a bright red", "extracted_entity": null}
{"question": "does Family Guy take place on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the fictional city of Quah", "extracted_entity": null}
{"question": "could someone in the Canary Islands fish for largemouth bass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not likely.", "extracted_entity": null}
{"question": "did the Presidency of Bill Clinton conclude with his impeachment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he was not convicted by the", "extracted_entity": null}
{"question": "did Marco Polo travel with Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Marco Polo traveled with his", "extracted_entity": null}
{"question": "could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with a snorkel.", "extracted_entity": null}
{"question": "will a 2 Euro coin float across the Red Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is made of copper.", "extracted_entity": null}
{"question": "have any murderers outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, and I don't think they ever", "extracted_entity": null}
{"question": "can the US branch of government that has power over the military also have the power to veto?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Legislative branch of government can have", "extracted_entity": null}
{"question": "are Brussels sprout particularly good for adrenal fatigue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Brussels sprouts are a crucifer", "extracted_entity": null}
{"question": "can oysters be used in guitar manufacturing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "are parodies of the President of the United States illegal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Parodies are protected by the First", "extracted_entity": null}
{"question": "can lobster breathe in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, lobster cannot breathe in", "extracted_entity": null}
{"question": "is Phobos part of the Andromeda galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Phobos is a moon of Mars", "extracted_entity": null}
{"question": "were all of Heracles's children present for his funeral pyre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his children were not present for his fun", "extracted_entity": null}
{"question": "can Africanized bees be considered multicultural?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they are not a hybrid,", "extracted_entity": null}
{"question": "is nickel dominant material in US 2020 nickels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did Leonardo da Vinci lack contemporary peers in his home city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was there fear leading up to the year 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There was a lot of fear and spec", "extracted_entity": null}
{"question": "can you buy furniture and meatballs in the same store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can't.", "extracted_entity": null}
{"question": "during the Cuban revolution, did the US experience a population boom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The US population boomed during the Cuban revolution", "extracted_entity": null}
{"question": "could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it was modified to have", "extracted_entity": null}
{"question": "can a copy of The Daily Mirror sustain a campfire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it will burn.", "extracted_entity": null}
{"question": "would Bugs Bunny harm an olive tree in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's a fictional character.", "extracted_entity": null}
{"question": "did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he didn't have any am", "extracted_entity": null}
{"question": "can actress Danica McKellar skip astronaut education requirements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Danica McKellar is not an", "extracted_entity": null}
{"question": "can you transport a coin along a sea of mercury?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. You can transport a coin along a sea", "extracted_entity": null}
{"question": "are the founders of Skype from Asia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the founders of Skype are from", "extracted_entity": null}
{"question": "was the tenth Amendment to the Constitution written using Pitman shorthand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the 1820s", "extracted_entity": null}
{"question": "can you save every HD episode of Game of Thrones on Samsung Galaxy A10e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can save every HD episode of Game", "extracted_entity": null}
{"question": "does Super Mario protagonist hypothetically not need continuing education classes in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He does not.", "extracted_entity": null}
{"question": "would a body builder prefer an elk burger over a beef burger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because elk is leaner and has", "extracted_entity": null}
{"question": "will NY Stock Exchange closing bell be heard in Universal Music Group's headquarters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NYSE closing bell is not heard in Universal Music", "extracted_entity": null}
{"question": "could Saint Peter watch television?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had a flat-screen.", "extracted_entity": null}
{"question": "are all United States Aldi locations owned by the same company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all United States Aldi locations are owned", "extracted_entity": null}
{"question": "was the first Vice President of the United States an Ottoman descendant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Adams was the first Vice President of", "extracted_entity": null}
{"question": "would it be safe to have a jackfruit thrown at your head?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be safe, but it would be very", "extracted_entity": null}
{"question": "can a woman on average have a baby before wheat seed blooms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but wheat seed is a long time", "extracted_entity": null}
{"question": "did Donald Trump come up with the idea for the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was the Dutch.", "extracted_entity": null}
{"question": "did Saint Augustine use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "do Koalas prefer Eucalyptus over meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm going to go out on a lim", "extracted_entity": null}
{"question": "are goats found on abyssal plains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are gorillas closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they are.", "extracted_entity": null}
{"question": "do Star Wars fans say \"beam me up\" often?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm sure some do.", "extracted_entity": null}
{"question": "does USA fail separation of church and state in multiple ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you think.", "extracted_entity": null}
{"question": "could Bob Marley's children hypothetically win tug of war against Kublai Khan's children?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Bob Marley's children are", "extracted_entity": null}
{"question": "does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not involve the celebration of", "extracted_entity": null}
{"question": "is dessert eaten before breakfast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, dessert is eaten after dinner", "extracted_entity": null}
{"question": "was Krishna skilled at using the bow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was very skilled.", "extracted_entity": null}
{"question": "was Aristotle a member of the House of Lords?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Aristotle was a member of the", "extracted_entity": null}
{"question": "would Ibn Saud tolerate salsa music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He would probably be amused and ask for a", "extracted_entity": null}
{"question": "would a responsible bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if she was 21", "extracted_entity": null}
{"question": "is Bactrian Camel most impressive animal when it comes to number of humps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Bactrian camel (Camelus", "extracted_entity": null}
{"question": "do giraffes require special facilities at zoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Giraffes are tall, and", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch kill hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it will kill them.", "extracted_entity": null}
{"question": "can a snake wear a snowshoe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it will be very hard for him", "extracted_entity": null}
{"question": "can a false pope become a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the pope is the Vicar", "extracted_entity": null}
{"question": "does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, D\u00fcsseldorf has only a small number of sm", "extracted_entity": null}
{"question": "can dementia be cured with a cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a mental disease that can'", "extracted_entity": null}
{"question": "can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he can't, because he is", "extracted_entity": null}
{"question": "would Dave Chappelle pray over a Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would not.", "extracted_entity": null}
{"question": "is a jellyfish safe from atherosclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, jellyfish are not safe from a", "extracted_entity": null}
{"question": "is the Riksdag a political entity in Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Riksdag is the name of the", "extracted_entity": null}
{"question": "did Linnaeus edit Darwin's draft of Origin of Species?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was dead by then.", "extracted_entity": null}
{"question": "is Hermione Granger eligible for the Order of the British Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "was Saudi Aramco started due to an assassination?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saudi Aramco was founded after", "extracted_entity": null}
{"question": "should a Celiac sufferer avoid spaghetti?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if it is made from a gl", "extracted_entity": null}
{"question": "does Olympia Washington share name with Hephaestus's workshop location?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is named after the Greek goddess", "extracted_entity": null}
{"question": "could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could.", "extracted_entity": null}
{"question": "does Jason have anything in common with Dr. Disrespect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jason is a 12 year old", "extracted_entity": null}
{"question": "is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, women in Japan are more likely to surv", "extracted_entity": null}
{"question": "are ropes required to operate a frigate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's a ship, not a", "extracted_entity": null}
{"question": "did Brad Peyton need to know about seismology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. But he did need to know about the", "extracted_entity": null}
{"question": "is Ludacris in same music genre as 2000's Binaural?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would LeBron James hypothetically glance upwards at Yuri Gagarin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if LeBron James hypothetically glanced", "extracted_entity": null}
{"question": "do you need a farmer to make a circuit board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A farmer can't do it", "extracted_entity": null}
{"question": "did Gandhi watch the television show Bonanza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It wasn't on the air yet", "extracted_entity": null}
{"question": "is Shakespeare famous because of the infinitive form?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is famous because he wrote good plays", "extracted_entity": null}
{"question": "would the crew of Apollo 15 have difficulty riding a unicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they were on the moon.", "extracted_entity": null}
{"question": "do Squidward Tentacles and Alan Greenspan have different musical passions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Alan Greenspan is a jazz fan", "extracted_entity": null}
{"question": "can a minor replicate the double-slit experiment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the minor will not be able to", "extracted_entity": null}
{"question": "can an anchovy born in 2020 survive 25th US census?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it is able to sw", "extracted_entity": null}
{"question": "do you have to put on glasses to read a QR code?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you do not need to put on glass", "extracted_entity": null}
{"question": "would Seroquel be the first treatment recommended by a doctor to someone with depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Seroquel is a drug for schizop", "extracted_entity": null}
{"question": "do women often need new shoes during their pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "would Persephone be a good consultant to a landscape architect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She would be a great consultant to a landscape", "extracted_entity": null}
{"question": "is University of Pittsburgh easier to enter than FBI?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm sure", "extracted_entity": null}
{"question": "will the small intenstine break down a cotton ball?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the small intestine is where most", "extracted_entity": null}
{"question": "do the Eskimos sunbathe frequently?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they use suntan lotion.", "extracted_entity": null}
{"question": "was the French Revolution televised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was only in black and white", "extracted_entity": null}
{"question": "is the most expensive color in the world Blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The most expensive color in the world is", "extracted_entity": null}
{"question": "did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Rosalind Franklin made important contributions to", "extracted_entity": null}
{"question": "can you substitute the pins in a bowling alley lane with Dustin Hoffman's Oscars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The pins are smaller than the O", "extracted_entity": null}
{"question": "did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a general for England and France", "extracted_entity": null}
{"question": "is the Sea of Japan landlocked within Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is landlocked within", "extracted_entity": null}
{"question": "is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by \u201c", "extracted_entity": null}
{"question": "will Lhamo Thondup be considered by Catholic Church to be a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he is a bodhisattva", "extracted_entity": null}
{"question": "would someone in Boston not receive the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If they did, it would be in the rec", "extracted_entity": null}
{"question": "is Christmas always celebrated on a Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's always on a Wednes", "extracted_entity": null}
{"question": "are pancakes typically prepared in a pot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pancakes are typically prepared in a", "extracted_entity": null}
{"question": "has Ringo Starr been in a relatively large number of bands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think he's been in about", "extracted_entity": null}
{"question": "did Richard III know his grandson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he knew his grandson Edward V", "extracted_entity": null}
{"question": "can Kane challenge Joe Biden in this year's primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The rules of the Democratic Party do not", "extracted_entity": null}
{"question": "did Stone Cold Steve Austin wrestle in three different centuries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He started wrestling in", "extracted_entity": null}
{"question": "do embalmed bodies feel different at funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. Embalmed bodies feel", "extracted_entity": null}
{"question": "are leaves from coca good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Coca leaves are not good for gain", "extracted_entity": null}
{"question": "did the Beatles write any music in the Disco genre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn\u2019t.", "extracted_entity": null}
{"question": "is Nine Inch Nails a good guest for students in earliest grade to take Iowa tests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not a good guest for students", "extracted_entity": null}
{"question": "did Harvey Milk ever run for governor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he ran for governor of California in", "extracted_entity": null}
{"question": "is a bengal fox likely to see the Superbowl?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he can get tickets to the", "extracted_entity": null}
{"question": "could all Tahiti hotels hypothetically accommodate US D-Day troops?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Tahiti hotels were not designed to", "extracted_entity": null}
{"question": "was Amazon involved in the lunar landing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Amazon was not involved in the lunar", "extracted_entity": null}
{"question": "could the Spice Girls compete against \u017dRK Kumanovo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could, but only if they have", "extracted_entity": null}
{"question": "are saltwater crocodiles related to alligators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are in the same family.", "extracted_entity": null}
{"question": "would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The suit is designed to keep you warm", "extracted_entity": null}
{"question": "has every astronaut survived their space journey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there have been some fatal accidents.", "extracted_entity": null}
{"question": "can any person with a driver's license work in transport of aviation fuel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only those who are certified by the", "extracted_entity": null}
{"question": "could a snowy owl survive in the Sonoran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the owl would not survive", "extracted_entity": null}
{"question": "can you buy chlorine at a dollar store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can buy chlorine at a", "extracted_entity": null}
{"question": "is the voice of the Genie from Disney's Aladdin still alive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Robin Williams is still alive and well.", "extracted_entity": null}
{"question": "was the amount of spinach Popeye ate unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He ate the same amount of spin", "extracted_entity": null}
{"question": "would an explosion at a gunpowder storage facility result in a supersonic shock wave?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a shock wave is a supersonic", "extracted_entity": null}
{"question": "could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Marco Rubio is too tall.", "extracted_entity": null}
{"question": "can a person be diagnosed with a Great Depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not a medical diagn", "extracted_entity": null}
{"question": "could Godzilla have been killed by the Tohoku earthquake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Godzilla is a fictional", "extracted_entity": null}
{"question": "is retail a job anybody can be suited for?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. Retail is a very", "extracted_entity": null}
{"question": "does cell biology teach about the life cycle of Al Qaeda?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Robert Moses Grove had a collection", "extracted_entity": null}
{"question": "has Burger King  contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it has", "extracted_entity": null}
{"question": "would a snake have reasons to fear a honey badger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a predator", "extracted_entity": null}
{"question": "do most people only memorize slightly over half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. The average person only memor", "extracted_entity": null}
{"question": "does Happy Gilmore Productions CEO own a Torah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He has a Torah.", "extracted_entity": null}
{"question": "does Mercury make for good Slip N Slide material?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's not as good as", "extracted_entity": null}
{"question": "is RoboCop director from same country as Gaite Jansen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are from different countries.", "extracted_entity": null}
{"question": "was the Parc des Princes fully operational during June of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Parc des Princes was fully", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, the elf could use", "extracted_entity": null}
{"question": "do worshipers of Shiva make a pilgrimage to the Holy Land?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They go to the Temple of Shiva", "extracted_entity": null}
{"question": "was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "was it typical to see Johnny Cash on stage in a rainbow-colored outfit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not typical.", "extracted_entity": null}
{"question": "is slitting your wrists an unreliable suicide method?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Hawaiian male would experience more days on Earth", "extracted_entity": null}
{"question": "is Christopher Nolan indebted to Bob Kane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. I think he is.", "extracted_entity": null}
{"question": "did Kim Il-sung network on LinkedIn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, LinkedIn was founded in 20", "extracted_entity": null}
{"question": "could all People's Volunteer Army hypothetically be transported on Symphony of the Seas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The ship can carry 6,680", "extracted_entity": null}
{"question": "is Bern a poor choice for a xenophobic Swiss citizen to live?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bern is a very good choice for a", "extracted_entity": null}
{"question": "would a vegan prefer a natural bongo drum over a synthetic one?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. a vegan would not prefer a natural", "extracted_entity": null}
{"question": "is an inappropriate lullaby Love Song from November 11, 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's an inappropriate", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do require airplanes to", "extracted_entity": null}
{"question": "are kayaks used at the summit of Mount Everest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not on the summit. The", "extracted_entity": null}
{"question": "would someone on a keto diet be able to eat Dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person on a keto diet", "extracted_entity": null}
{"question": "was Raphael's paintings influenced by the country of Guam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Raphael's paintings were", "extracted_entity": null}
{"question": "is it normal to find parsley in multiple sections of the grocery store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal to find parsley", "extracted_entity": null}
{"question": "was the subject of Parsifal taken from British folklore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, from the legend of Sir Galahad", "extracted_entity": null}
{"question": "do drummers need spare strings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "only if they are playing in a band.", "extracted_entity": null}
{"question": "would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it would be able to conserve", "extracted_entity": null}
{"question": "is there a jukebox musical about a sweet transvestite from Transexual, Transylvania?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's called \"Rocky", "extracted_entity": null}
{"question": "did Malcolm X use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he used the ASCII character set, which", "extracted_entity": null}
{"question": "would the fastest tortoise win a race against a Chicago \"L\"?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, assuming the tortoise could get on", "extracted_entity": null}
{"question": "does Hammurabi's Code violate Christians Golden Rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Golden Rule is to do unto others as you", "extracted_entity": null}
{"question": "does the Roman god Vulcan have a Greek equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Greek equivalent of Vulcan is", "extracted_entity": null}
{"question": "can the Dalai Lama fit in a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's too tall.", "extracted_entity": null}
{"question": "is watermelon safe for people with a tricarboxylic acid allergy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There are no studies on watermelon and", "extracted_entity": null}
{"question": "is a Chinchilla breed of felis catus a type of rodent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Chinchilla is a breed of", "extracted_entity": null}
{"question": "can printing books in kanji instead of the Roman alphabet save trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would save a lot of trees, but it", "extracted_entity": null}
{"question": "is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Audi R8 V", "extracted_entity": null}
{"question": "would a Rockette look odd with a moustache? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would look odd with a moust", "extracted_entity": null}
{"question": "would Hodor hypothetically be a good math mathematician?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would be a good mathematician", "extracted_entity": null}
{"question": "would Hades and Osiris hypothetically compete for real estate in the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "would a German Shepherd be welcome in an airport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are trained to sniff out bomb", "extracted_entity": null}
{"question": "does a person need a college degree to become a bartender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a person does not need a college degree", "extracted_entity": null}
{"question": "is capsaicin associated with cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is the chemical that gives pe", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger be unable to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a foreign born citizen.", "extracted_entity": null}
{"question": "when Hugh Jackman was a teacher, would he have taught The Great Gatsby?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I think it'", "extracted_entity": null}
{"question": "is Fiat Chrysler associated with Japanese cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Fiat Chrysler Automobiles N.", "extracted_entity": null}
{"question": "can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not the immersion in water that is", "extracted_entity": null}
{"question": "would many meals heavy in brussels sprouts benefit someone on Coumadin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Brussel sprouts are a good source of", "extracted_entity": null}
{"question": "is it safe to share silverware with an HIV positive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. HIV is not spread by casual", "extracted_entity": null}
{"question": "do movies always show nerds as the losers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Well, I've seen a lot of mov", "extracted_entity": null}
{"question": "are eagles and young bears both used as labels for skills-training youth groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "could ancient Tony Bennett have a baby in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he could have a baby in", "extracted_entity": null}
{"question": "can a lifeboat rescue people in the Hooke Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a lifeboat can\u2019t", "extracted_entity": null}
{"question": "would a stool be useful for a Lusotitan to reach the top of an almond tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a stool would not be useful for", "extracted_entity": null}
{"question": "would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He would have been very interested in the Tower of", "extracted_entity": null}
{"question": "would somebody leave reiki with bruises?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, bruises are not caused by reiki", "extracted_entity": null}
{"question": "is there a popular Broadway character who is a missionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there's a missionary in", "extracted_entity": null}
{"question": "would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The net worth is not the issue.", "extracted_entity": null}
{"question": "are months based on the solar cycle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Months are based on the lunar", "extracted_entity": null}
{"question": "do many fans of J.K Rowling know who Alan Rickman is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all.", "extracted_entity": null}
{"question": "could someone in a coma experience fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The fear would be in their mind,", "extracted_entity": null}
{"question": "could Johnny Carson's children fill out a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could.", "extracted_entity": null}
{"question": "would downloading Mario 64 on an emulator be legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be legal, as long as", "extracted_entity": null}
{"question": "are Tom and Jerry featured in a ride at Disneyland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "can a blind person tell time by Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I don'", "extracted_entity": null}
{"question": "are sea turtles enjoying life during quarantine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, Don King has killed more", "extracted_entity": null}
{"question": "does Paulo Coelho's wife make a living through speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she is a motivational speaker.", "extracted_entity": null}
{"question": "is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The fact that he's a sam", "extracted_entity": null}
{"question": "is Final Fantasy VI closer to beginning than end of its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s the end of the N", "extracted_entity": null}
{"question": "were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the deaths from Apollo 1", "extracted_entity": null}
{"question": "is chaff produced by hydropower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, chaff is produced by hydropower", "extracted_entity": null}
{"question": "is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not able to visit Northern Mar", "extracted_entity": null}
{"question": "is Garfield known for hating italian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is known for hating Italian cu", "extracted_entity": null}
{"question": "is basil safe from Hypervitaminosis D?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, basil is safe from hypervit", "extracted_entity": null}
{"question": "does a kangaroo incubate its offspring?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is the mother kangaroo", "extracted_entity": null}
{"question": "did J. Edgar Hoover take his calls in Langley, Virginia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elijah is part of the Pass", "extracted_entity": null}
{"question": "can a cheetah generate enough force to topple Big Show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the cheetah is not a", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have.", "extracted_entity": null}
{"question": "was Mercedes-Benz associated with the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Mercedes-Benz was associated with", "extracted_entity": null}
{"question": "did Johnny Carson win enough Emmy's to fill a carton if Emmy's were eggs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "11. He won them for his work", "extracted_entity": null}
{"question": "can bottlenose dolphins hypothetically outbreed human women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and there are many more questions like this", "extracted_entity": null}
{"question": "if a baby was born on Halloween would they be a Scorpio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They would be a Libra.", "extracted_entity": null}
{"question": "could the Great Wall of China connect the Dodgers to the White Sox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the Great Wall of China were a", "extracted_entity": null}
{"question": "would a honey badger fit inside an oven?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Yes it would.", "extracted_entity": null}
{"question": "did Heracles famous labors exceed a baker's dozen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Heracles' labors were twelve in number.", "extracted_entity": null}
{"question": "does Guam have a state capital?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Guam does not have a state capital", "extracted_entity": null}
{"question": "are Disney's seven dwarves the original ones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. The original seven d", "extracted_entity": null}
{"question": "does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he goes to the Hague on a", "extracted_entity": null}
{"question": "is Jesse W. Moore a potential recipient of a Snoopy-themed award from NASA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he deserves it.", "extracted_entity": null}
{"question": "has Gorillaz creator been in more bands than Bernard Sumner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "is it safe to eat kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is safe to eat kidney.", "extracted_entity": null}
{"question": "is zoology unconcerned with strigoi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for the reasons you think.", "extracted_entity": null}
{"question": "did the phone Alexander Graham Bell use have call waiting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was a party line, and you", "extracted_entity": null}
{"question": "were paparazzi directly responsible for the death of Amy Winehouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they weren't.", "extracted_entity": null}
{"question": "on August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "are more watermelons grown in Brazil than Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because there is more land in Brazil", "extracted_entity": null}
{"question": "does store bought milk have cream at the top?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it does have a layer of cre", "extracted_entity": null}
{"question": "could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a big port, but", "extracted_entity": null}
{"question": "did eggs need to be kept cold in the middle ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not need to be kept cold", "extracted_entity": null}
{"question": "would Topa Inca Yupanqui have encountered the western honey bee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the western honey bee is native", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a popular science novel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a historical novel.", "extracted_entity": null}
{"question": "was Amy Winehouse a fan of Star Wars: Rogue One?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she was not", "extracted_entity": null}
{"question": "can shooting bald eagle get a person more prison time than Michael Vick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Shooting a bald eagle can get", "extracted_entity": null}
{"question": "could you drive a Rowe 550 to the 2008 Summer Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so.", "extracted_entity": null}
{"question": "can you go water skiing on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it\u2019s too hot.", "extracted_entity": null}
{"question": "would a Wolverine and a Lynx be hard to tell apart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Wolverines and Lynx are very", "extracted_entity": null}
{"question": "could Plato have agreed with the beliefs of Jainism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so.", "extracted_entity": null}
{"question": "do bald eagles nest on Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in fact, there are two active e", "extracted_entity": null}
{"question": "could James Brown's ex-wives hold a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they could not.", "extracted_entity": null}
{"question": "should Peter Griffin be an expert at the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he should.", "extracted_entity": null}
{"question": "did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Manson didn't kill anyone.", "extracted_entity": null}
{"question": "is there any absolute way to prevent abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there isn't.", "extracted_entity": null}
{"question": "is Saturn named after king of gods in Greek mythology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saturn is named after king of gods", "extracted_entity": null}
{"question": "were any of despised Pope Alexander VI's descendants canonized?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the Pope's children were", "extracted_entity": null}
{"question": "would it be difficult to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because there is no snow.", "extracted_entity": null}
{"question": "can chemicals in onion help create a thermonuclear bomb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Onions do not contain any radioactive", "extracted_entity": null}
{"question": "are anchovies associated with Italian food?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, anchovies are associated with Italian food", "extracted_entity": null}
{"question": "did Operation Barbarossa or Barbarossa's last expedition succeed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The invasion of the Soviet Union was a strategic", "extracted_entity": null}
{"question": "has numerology helped shape hotel layouts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, numerology has played a big part in", "extracted_entity": null}
{"question": "did The Who have to cancel tours due to World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, The Who had not to cancel tours", "extracted_entity": null}
{"question": "can an art dealer buy Boeing 737-800 with a Da Vinci painting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you have to have a lot of", "extracted_entity": null}
{"question": "would a crocodile survive longer in Great Salt Lake than alligator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the salt water would dehyd", "extracted_entity": null}
{"question": "would a clouded leopard encounter an awake pangolin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are nocturnal.", "extracted_entity": null}
{"question": "does acupuncture cause pain in many people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Acupuncture is a safe and effective treatment", "extracted_entity": null}
{"question": "are twins always born during the same year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Twins can be born in different years", "extracted_entity": null}
{"question": "could a nymph tick pass through a standard hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The head of a nymph tick is approximately", "extracted_entity": null}
{"question": "would WWF be angrier if you killed koala instead of black swan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "WWF would be angrier if you killed ko", "extracted_entity": null}
{"question": "does New Year's Day always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can fall on any day of the", "extracted_entity": null}
{"question": "is a pottery kiln inappropriate for use with glass blowing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the high temperature difference between the", "extracted_entity": null}
{"question": "do people of the Iyer caste eat meat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iyer people do not eat meat.", "extracted_entity": null}
{"question": "do carpenters understand geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I have a degree in it", "extracted_entity": null}
{"question": "did the crew of Apollo 15 take pictures of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The crew of Apollo 15", "extracted_entity": null}
{"question": "are human footprints absent from Mount Sharp?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, human footprints are absent from Mount", "extracted_entity": null}
{"question": "could Tom Cruise explain mental auditing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don\u2019t think so.", "extracted_entity": null}
{"question": "is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The new Chinese-built Chevrolet Cru", "extracted_entity": null}
{"question": "would drinking a glass of lemonade provide Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "does a game engine have a fuel injector?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his father for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because J. D. Salinger was", "extracted_entity": null}
{"question": "were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Greeks were essential to crafting", "extracted_entity": null}
{"question": "is the Joker in a healthy romantic relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not.", "extracted_entity": null}
{"question": "would an adherent of Zoroastrianism consult the Quran for religious guidance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Zoroastrianism is not", "extracted_entity": null}
{"question": "would a diet of ice eventually kill a person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they don\u2019t eat anything else", "extracted_entity": null}
{"question": "can binary numbers and standard alphabet satisfy criteria for a strong password?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Binary numbers and standard alphabet satisfy criteria for a strong", "extracted_entity": null}
{"question": "is Phobos (moon) name origin similar to Roman god Pavor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is similar.", "extracted_entity": null}
{"question": "would members of Blue Lives Matter support every element of Grand Theft Auto III?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "of course not.", "extracted_entity": null}
{"question": "will twenty pea pods contents cover entire chess board?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it will.", "extracted_entity": null}
{"question": "was the Joker an enemy of the Avengers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Joker is a DC Comics villa", "extracted_entity": null}
{"question": "could Christopher Nolan borrow pants from Danny Devito?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (but not from Danny DeV", "extracted_entity": null}
{"question": "could you watch Naruto and Puzzle Place on the same channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Naruto was on Cartoon Network", "extracted_entity": null}
{"question": "is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, LG Electronics is located in a", "extracted_entity": null}
{"question": "was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "does Fraktur have a sordid history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was used to write the bible", "extracted_entity": null}
{"question": "would a hedgehog avoid animals without a spinal cord?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not, because they can'", "extracted_entity": null}
{"question": "can you worship Ahura Mazda at a mosque?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the mosque is a place of", "extracted_entity": null}
{"question": "would a moose hypothetically be too much for a minotaur to devour whole?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be too much for a min", "extracted_entity": null}
{"question": "is Gandalf hypothetically a formidable foe for Charmed's Barbas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I don't", "extracted_entity": null}
{"question": "do suburbs encourage the use of cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because there are not many stores and restaur", "extracted_entity": null}
{"question": "could Al Capone have read works from the Harlem Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Al Capone was an Italian-American", "extracted_entity": null}
{"question": "can pancreas removal cause bankruptcy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It can if you can't work and don", "extracted_entity": null}
{"question": "is shrimp prevalent in Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, shrimp is very prevalent", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Allah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Marco Rubio has a close relationship with Jesus Christ", "extracted_entity": null}
{"question": "does The Doctor keep his ship in his childhood home?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "did Archduke Franz Ferdinand of Austria participate in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was assassinated in 1", "extracted_entity": null}
{"question": "is a beard is moss that grows on a human?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is there radiation where Nikola Tesla once worked?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is radiation where Nikola Tes", "extracted_entity": null}
{"question": "would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She would have prayed to Artemis if", "extracted_entity": null}
{"question": "could one Amazon share ever buy twenty year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the answer is yes. One share of", "extracted_entity": null}
{"question": "would you have to wear a coat when on Phobos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Phobos is a small moon,", "extracted_entity": null}
{"question": "is Cambodia too expensive for 2020 richest billionaire to own?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cambodia is not too expensive for the", "extracted_entity": null}
{"question": "did Emma Stone pursue a higher education?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "was San Antonio the site of a major battle in the 19th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Alamo", "extracted_entity": null}
{"question": "can the United Nations Framework Convention on Climate Change be held at the Javits Center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Javits Center is a world", "extracted_entity": null}
{"question": "can music be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, because music is a weapon that", "extracted_entity": null}
{"question": "could an ocelot outrun a kindergartner? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the ocelot is a wild", "extracted_entity": null}
{"question": "was Elizabeth II the Queen during the Persian Gulf War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Moon would be the focus.", "extracted_entity": null}
{"question": "did Woodrow Wilson consider Blacks to be equal members of society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He did not.", "extracted_entity": null}
{"question": "if your electric stove has a glass top, should you use cast iron skillets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can use cast iron on a glass", "extracted_entity": null}
{"question": "can soup be eaten with the hands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be eaten with hands", "extracted_entity": null}
{"question": "did Cleopatra have ethnicity closer to Egyptians than Greeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was a descendant of the P", "extracted_entity": null}
{"question": "does the judo rank system reach the triple digits?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "was the son of Tsar Nicholas a daredevil?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "did Bill Gates achieve Latin honors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bill Gates did not achieve Latin honors.", "extracted_entity": null}
{"question": "has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He's been in office since", "extracted_entity": null}
{"question": "if Goofy were a pet, would he need heartworm prevention?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "did Richard Wagner support the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a Jew.", "extracted_entity": null}
{"question": "was historical Dracula from a town in Bucharest?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was from a small village in Trans", "extracted_entity": null}
{"question": "is electricity necessary to balance an account in Microsoft Excel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can manually balance an account in Microsoft", "extracted_entity": null}
{"question": "did Pink Floyd have a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was called \"The French Disconnection", "extracted_entity": null}
{"question": "has the creator of Futurama lived in multiple centuries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was born in 195", "extracted_entity": null}
{"question": "is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sternum is not connected to any of", "extracted_entity": null}
{"question": "did Elizabeth I of England own any viscose fiber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she didn\u2019t.", "extracted_entity": null}
{"question": "could a cat ride Panzer VIII Maus tank missile from Barcelona to Madrid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the cat could.", "extracted_entity": null}
{"question": "did Jack Dempsey fight the current WBC heavyweight champion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the current WBC heavyweight champion is", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper at a latitude of 75\u00b0 N?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because cauliflower does not grow", "extracted_entity": null}
{"question": "would a student in eleventh grade be unable to run for president of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he would not be able to be", "extracted_entity": null}
{"question": "was Amy Winehouse familiar with Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was not.", "extracted_entity": null}
{"question": "can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she can sing.", "extracted_entity": null}
{"question": "are Scottish people descended from Mary, Queen of Scots part French?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are descended from Mary, Queen", "extracted_entity": null}
{"question": "are quadrupeds represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are all dogs.", "extracted_entity": null}
{"question": "is the Louvre in billionaire George Soros's price range?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he can buy the entire country of", "extracted_entity": null}
{"question": "would a duke hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a duke is a rank below a", "extracted_entity": null}
{"question": "can a jet plane be made without society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because society is what makes the plane,", "extracted_entity": null}
{"question": "would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he has a disability", "extracted_entity": null}
{"question": "could Ryan Crouser throw a bengal fox with ease?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "would a physician be unlikely to recommend Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A physician is not likely to recommend Reiki", "extracted_entity": null}
{"question": "can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (His net worth is $4", "extracted_entity": null}
{"question": "can Burundi's communicate with citizens of New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can communicate in English and French.", "extracted_entity": null}
{"question": "is Morocco an ideal location for water skiing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the winds", "extracted_entity": null}
{"question": "did Al Capone carry a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he had a flip phone.", "extracted_entity": null}
{"question": "was Mark Twain a struggling inventor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was a struggling inventor.", "extracted_entity": null}
{"question": "is B's place in alphabet same as Prince Harry's birth order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, B is second in line.", "extracted_entity": null}
{"question": "would a vegan eat a traditional Paella dish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "is cow methane safer for environment than cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, methane is 20 times", "extracted_entity": null}
{"question": "was Darth Vader monogamous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "were there footprints on the moon in 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, no footprints.", "extracted_entity": null}
{"question": "does Pantheon in Paris have a unique name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is called the Pantheon", "extracted_entity": null}
{"question": "can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are familiar with the", "extracted_entity": null}
{"question": "would a Beaver's teeth rival that of a Smilodon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are too small", "extracted_entity": null}
{"question": "can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be a tight fit.", "extracted_entity": null}
{"question": "do calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are some drain fly colors that", "extracted_entity": null}
{"question": "are all cucumbers the same texture?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can be crunchy or soft", "extracted_entity": null}
{"question": "would Library of Alexandria need less shelf space than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It would need less shelf space.", "extracted_entity": null}
{"question": "would a spider wasp be more effective than a bullet ant to stop a criminal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the bullet ant is a lot more", "extracted_entity": null}
{"question": "would a triples tandem bike support Apollo 15 crew?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they were riding on", "extracted_entity": null}
{"question": "if one of your feet is in a leg cast, should the other be in a sandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it\u2019s better than no foot", "extracted_entity": null}
{"question": "did Klingons appear in the movie The Last Jedi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Klingons did not appear in", "extracted_entity": null}
{"question": "can you find Depala's race in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The name of the race is a hint.", "extracted_entity": null}
{"question": "would a Dodo hypothetically tower over Ma Petite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ma Petite is a dwarf", "extracted_entity": null}
{"question": "can the Very Large Telescope observe the largest mountain on Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the telescope is too far away", "extracted_entity": null}
{"question": "was Morris County named after a chief justice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was named after the Morris family.", "extracted_entity": null}
{"question": "is the Gujarati script the same category of script as Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kanji is a logographic script. Gujar", "extracted_entity": null}
{"question": "did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, earth completed at least one orbit around the", "extracted_entity": null}
{"question": "would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the average temperature in the Antar", "extracted_entity": null}
{"question": "if Martin Luther did one theses a day would he run out in half a year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would run out of theses in", "extracted_entity": null}
{"question": "are honey badgers and hyenas anatomically dissimilar? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. They are both m", "extracted_entity": null}
{"question": "did Al Pacino act in a movie during World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did act in a movie during", "extracted_entity": null}
{"question": "could you watch a new Seinfeld episode every day for a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I could watch an episode every day", "extracted_entity": null}
{"question": "did Barack Obama participate in the Reformation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in 196", "extracted_entity": null}
{"question": "did President William Howard Taft read DC Comics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but he did read", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he can apply it to a p", "extracted_entity": null}
{"question": "are there Americans still enlisted in the Confederate States Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are Americans still enlisted in", "extracted_entity": null}
{"question": "are right wing Amreicans opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "can a sea turtle play tennis using a tennis racket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because sea turtles can't", "extracted_entity": null}
{"question": "in a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course!", "extracted_entity": null}
{"question": "is shoe soup innocuous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it is. it's just a word.", "extracted_entity": null}
{"question": "did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "did Holy Saturday 2019 have special significance to pot smokers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was the first day that recreational", "extracted_entity": null}
{"question": "do you need lactobacillus to make pickles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the bacteria that make the pick", "extracted_entity": null}
{"question": "would a Gray Whale fit easily in an above ground pool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure. It would be very", "extracted_entity": null}
{"question": "will a celibate cleric likely suffer a stoning in Somalia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he is caught", "extracted_entity": null}
{"question": "could Little Women have been a book read by veterans of the civil war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the book was published in 18", "extracted_entity": null}
{"question": "is MIX a word and a roman numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "MIX is a word and a roman numeral", "extracted_entity": null}
{"question": "would Downton Abbey finale viewership defeat every Kazakhstan citizen in tug of war?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, 23 million vs 16", "extracted_entity": null}
{"question": "was dynamite used during Middle Ages warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dynamite was invented in the", "extracted_entity": null}
{"question": "is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "can Kate Gosselin's household fill out a Bandy team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with Kate's 8 kids", "extracted_entity": null}
{"question": "is the tree species that the name Leipzig refers to an evergeen tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The tree is the G", "extracted_entity": null}
{"question": "will Gremlins sequels tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they will.", "extracted_entity": null}
{"question": "is Snow White an example of good consent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she doesn\u2019t ask the dwar", "extracted_entity": null}
{"question": "is MF Doom a Fantastic Four villain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s a Fantastic Four villa", "extracted_entity": null}
{"question": "would most grand masters know what the French Defense is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I think so.", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing style of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the reading list I have included a", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing domestic canine reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the canines are not spay", "extracted_entity": null}
{"question": "could a monolingual American read Lenovo's native name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's \"Lianli\"", "extracted_entity": null}
{"question": "can 1980 United States presidential election result be considered a photo finish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be considered a photo finish.", "extracted_entity": null}
{"question": "did Cynthia Powell celebrate a silver anniversary with John Lennon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did", "extracted_entity": null}
{"question": "is it possible for biologist Rachel Carson to have flown to the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because she is a biologist and bi", "extracted_entity": null}
{"question": "did Confederate States Army influence West Point fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Confederate States Army was formed in", "extracted_entity": null}
{"question": "would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be difficult for Will Fer", "extracted_entity": null}
{"question": "is the Mona Lisa in the same museum as the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is in the Louvre.", "extracted_entity": null}
{"question": "does the land in close proximity to beaver dams suffer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the land does suffer in close proximity", "extracted_entity": null}
{"question": "was Jackson Pollock trained by Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a self-taught painter", "extracted_entity": null}
{"question": "did Francois Mitterrand ever meet Barak Obama while they both held the position of President?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, never", "extracted_entity": null}
{"question": "would Carrie Poppy be likely to trust a psychic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Carrie Poppy would trust a psych", "extracted_entity": null}
{"question": "would stargazers prefer binoculars over a telescope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is more convenient to carry and", "extracted_entity": null}
{"question": "is it possible to binge entire EastEnders series without water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I did it", "extracted_entity": null}
{"question": "was Mother Theresa a follower of atheism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was a follower of Christianity", "extracted_entity": null}
{"question": "is the BBC World Service hosted in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is hosted in London, UK.", "extracted_entity": null}
{"question": "did Melania Trump have same profession as Olga Kurylenko?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are both models", "extracted_entity": null}
{"question": "was Robert Downey Jr. a good role model as a young man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a drug addict and a", "extracted_entity": null}
{"question": "should a finished website have lorem ipsum paragraphs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only for a very short period of", "extracted_entity": null}
{"question": "would Jacques Du\u00e8ze have been friends with Richard Dawkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They would have agreed on a lot of", "extracted_entity": null}
{"question": "would Gordon Ramsey use uranium as a seasoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would not.", "extracted_entity": null}
{"question": "does Robert De Niro use a microscope at work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he doesn't.", "extracted_entity": null}
{"question": "is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was not honest with the vot", "extracted_entity": null}
{"question": "are pirate lieutenants like navy lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are more like navy captains", "extracted_entity": null}
{"question": "does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "does Homer Simpson need two hands worth of fingers to count to 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he needs three.", "extracted_entity": null}
{"question": "can you see the moon in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can see the moon in Wemble", "extracted_entity": null}
{"question": "is the Asian black bear multicolored?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Asian black bear is a species of bear native", "extracted_entity": null}
{"question": "can United States Secretary of State do crimes in U.K. without being arrested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "did Alice's Adventures in Wonderland inspire Macbeth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Macbeth was influenced by Alice'", "extracted_entity": null}
{"question": "could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it is an element and cannot be", "extracted_entity": null}
{"question": "was England directly involved in the Arab-Israeli conflict?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, England was directly involved in the Arab-", "extracted_entity": null}
{"question": "can an adult male stand on top Donatello's bronze David and touch the Sistine Chapel ceiling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he'd have to be very", "extracted_entity": null}
{"question": "was Dioskourides a lapidary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Dioskourides was a lapid", "extracted_entity": null}
{"question": "would a CEO typically clean the toilets in a company's building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the CEO would not typically clean the", "extracted_entity": null}
{"question": "can you purchase General Motors products at a movie theater?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot purchase General Motors products at", "extracted_entity": null}
{"question": "did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, none of the Amazons from X", "extracted_entity": null}
{"question": "could all of the people who pass through 30th Street Station every day fit in Dorton Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dorton Arena can hold 11", "extracted_entity": null}
{"question": "can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Aerosmith is too big to", "extracted_entity": null}
{"question": "could all the unemployed people due to 1933 Great Depression fit in Tiger Stadium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is yes.", "extracted_entity": null}
{"question": "would a duck ever need a Caesarean section?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, ducks do not need a Caes", "extracted_entity": null}
{"question": "is nickel a better payout than mercury if given a dollar per atomic number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no because nickel is 28 and mer", "extracted_entity": null}
{"question": "does Pikachu like Charles Darwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, because Pikachu is", "extracted_entity": null}
{"question": "can Simon Cowell vote for the next Supreme Court judge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is not a citizen of the", "extracted_entity": null}
{"question": "is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is unwise to overfeed any", "extracted_entity": null}
{"question": "is US route 1 dominated by historically red states?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, see map below.", "extracted_entity": null}
{"question": "can a cell fit inside of a shoebox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the cell is small and the sho", "extracted_entity": null}
{"question": "is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In Alice's Adventures in Wonder", "extracted_entity": null}
{"question": "can Tame Impala's studio band play a proper game of Hot Potato?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can't.", "extracted_entity": null}
{"question": "is the title of Shirley Bassey's 1971 diamond song a true statement?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The song was written by Andrew Lloyd Web", "extracted_entity": null}
{"question": "would P. G. Wodehouse be taught in second grade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he wouldn't.", "extracted_entity": null}
{"question": "can Curiosity take samples of rocks from Lacus Temporis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Curiosity can take samples of rocks", "extracted_entity": null}
{"question": "is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not", "extracted_entity": null}
{"question": "would Phineas and Ferb enjoy winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! They would have fun in the snow!", "extracted_entity": null}
{"question": "could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "SNL", "extracted_entity": null}
{"question": "can children become lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The minimum age is 18.", "extracted_entity": null}
{"question": "could you drive from New England to a Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you would have to drive through Wales", "extracted_entity": null}
{"question": "do seven McDonald's hamburgers exceed USDA recommended fat allowance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, by 36%", "extracted_entity": null}
{"question": "was Mozart accused of stealing from Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in 1914, a", "extracted_entity": null}
{"question": "will Justin Bieber take over Mike Pence's position in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bieber is not eligible", "extracted_entity": null}
{"question": "did any citizen of San Antonio vote for Boris Johnson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, of course not, no one in their", "extracted_entity": null}
{"question": "does being good at guitar hero make you a good guitarist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. its a video game.", "extracted_entity": null}
{"question": "can Roman numerals fill the normal number of Sudoku box options?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. They can be used to fill the normal", "extracted_entity": null}
{"question": "is a Cassowary safer pet than a crane?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the cassowary is a", "extracted_entity": null}
{"question": "was proofreading Edgar Allan Poe works lucrative?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was not.", "extracted_entity": null}
{"question": "was Hillary Clinton's deputy chief of staff in 2009 baptised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by Hillary Clinton.", "extracted_entity": null}
{"question": "can dessert be made with vegetables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. In fact, the best", "extracted_entity": null}
{"question": "would Othello be Shakespeare's play to buy Scheherazade most time with king?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Othello is the one that I would buy", "extracted_entity": null}
{"question": "did Jon Brower Minnoch suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He did not have anorexia", "extracted_entity": null}
{"question": "does ancient Olympics crown fail to hide tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the ancient Olympics crown fail to hide tons", "extracted_entity": null}
{"question": "can someone with dermatitis be a hand model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the dermatitis is not too", "extracted_entity": null}
{"question": "is it bad to have lactic acid in your body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not bad to have lactic", "extracted_entity": null}
{"question": "are cucumbers often found in desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, cucumbers are not often found", "extracted_entity": null}
{"question": "are blue lips normal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Blue lips are not normal and may be a sign", "extracted_entity": null}
{"question": "can someone with celiac disease have potato vodka?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as long as the vodka is", "extracted_entity": null}
{"question": "are deaf people left out of enjoying music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be difficult to achieve during Pope", "extracted_entity": null}
{"question": "would Cardi B. benefit from soy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, soy milk is a good source of", "extracted_entity": null}
{"question": "are the names of The Powerpuff Girls alliterative? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Powerpuff Girls are the names of three", "extracted_entity": null}
{"question": "did the leader of Heaven's Gate consider himself a prophet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Marshall Applewhite, the leader of Heaven", "extracted_entity": null}
{"question": "would King Leonidas have succeeded with an army the size of Mozart's compositions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Mozart's compositions are", "extracted_entity": null}
{"question": "do Elementary School students typically need graphing calculators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "can Family of Barack Obama ride comfortably in 2020 Jaguar F Type?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Family of Barack Obama can not", "extracted_entity": null}
{"question": "could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so. Apollo is a", "extracted_entity": null}
{"question": "does the Eighth Amendment to the United States Constitution protect freedom of speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Eighth Amendment to the", "extracted_entity": null}
{"question": "do most fans follow Katy Perry for gospel music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do follow her for her style", "extracted_entity": null}
{"question": "does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he hangs out at the Bud Light", "extracted_entity": null}
{"question": "are rainbows devoid of color made by mixing yin and yang colors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the same way that black is the", "extracted_entity": null}
{"question": "did Christopher Columbus sail representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can an Asian black bear use chopsticks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it takes a lot of practice.", "extracted_entity": null}
{"question": "did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they may have had some contact with the", "extracted_entity": null}
{"question": "would a Durian be dangerous if it fell on your head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it fell on your head it would", "extracted_entity": null}
{"question": "can nitric acid break the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can break the Louvre.", "extracted_entity": null}
{"question": "does autopilot rely on fossil fuels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The car is electric, and the electric", "extracted_entity": null}
{"question": "were muskets used in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were used by the Japanese", "extracted_entity": null}
{"question": "do tourists prefer Tuvalu to Niue?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "could you windsurf in Puerto Rico during Hurricane Maria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact, I was there the week", "extracted_entity": null}
{"question": "did Northwest Airlines' longevity surpass Betty White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It didn't.", "extracted_entity": null}
{"question": "is myocardial infarction a brain problem?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a heart problem", "extracted_entity": null}
{"question": "is Newspeak considered very straightforward?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. Newspeak is a", "extracted_entity": null}
{"question": "was Pope Alexander VI's origin country least represented in papal history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Spain", "extracted_entity": null}
{"question": "would you find olives at a heladeria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don\u2019t have olives.", "extracted_entity": null}
{"question": "do people celebrate Earth Day with a ceremonial tire fire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Earth Day is a day to celebrate", "extracted_entity": null}
{"question": "did France win the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "France won the French Revolution.", "extracted_entity": null}
{"question": "does menthol make cigarettes less addictive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Menthol is a natural substance found in", "extracted_entity": null}
{"question": "was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Salinger had two children.", "extracted_entity": null}
{"question": "could Reza Shah be related to Queen Elizabeth I?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not related to Queen Elizabeth I", "extracted_entity": null}
{"question": "is it possible that June got its name from mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The Romans named the month after the", "extracted_entity": null}
{"question": "is it possible to get killed walking to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is possible to die from a", "extracted_entity": null}
{"question": "is Canon Inc. a Kabushiki gaisha?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a Japanese corporation.", "extracted_entity": null}
{"question": "can a diamond float on water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a diamond is a heavy stone.", "extracted_entity": null}
{"question": "does horseradish have a fetlock?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t.", "extracted_entity": null}
{"question": "can you hide a pet macaque under your desk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would get really hot and smelly", "extracted_entity": null}
{"question": "can Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can participate in Paralym", "extracted_entity": null}
{"question": "would fans of Jonathan Larson be unaware of HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all, it\u2019s a", "extracted_entity": null}
{"question": "would Carolina Reaper decrease sales if added to all US salsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would not decrease sales because it is a very", "extracted_entity": null}
{"question": "will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they have the same Bacon Number.", "extracted_entity": null}
{"question": "was the Eiffel tower used as a symbol of the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Eiffel tower was built in 1", "extracted_entity": null}
{"question": "does James Watson believe that Africans are inferior to Europeans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He says so in his book, \u201c", "extracted_entity": null}
{"question": "is The Joy of Painting TV show still producing new episodes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Joy of Painting TV show", "extracted_entity": null}
{"question": "is the skull formed as one whole bone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the skull is formed as one whole", "extracted_entity": null}
{"question": "was  Godfrey of Bouillon an Islamaphobe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "are lengths measured in metres in the UK?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "do quadragenarian's have little memory capacity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so. I", "extracted_entity": null}
{"question": "did Jackie Kennedy wear Dolce & Gabbana to her husband's inauguration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she wore a Chanel suit.", "extracted_entity": null}
{"question": "did Larry King sign the Magna Carta?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Magna Carta was signed by", "extracted_entity": null}
{"question": "will a Euro sink in water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Euro will not sink in water.", "extracted_entity": null}
{"question": "would Iggy Pop travel with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course", "extracted_entity": null}
{"question": "is Drew Carey important to the history of wrestling?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he's a very important figure in", "extracted_entity": null}
{"question": "did Jay-Z ever collaborate with Louis Armstrong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "is CAS number 8009-03-8 harmful for a rash?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure what the CAS number is", "extracted_entity": null}
{"question": "would a model be appropriate to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. She's a", "extracted_entity": null}
{"question": "would Michael J Fox qualify for the Army Rangers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "do salmon mate in the Caspian Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they mate in the ocean", "extracted_entity": null}
{"question": "could Chuck Norris ride a horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course not, horses are scared of him", "extracted_entity": null}
{"question": "can Ford F-350 tow entire Yale University student body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can tow the entire Y", "extracted_entity": null}
{"question": "are any of J.K. Rowling's books in the genre of And Then There Were None?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the animal was a dog.", "extracted_entity": null}
{"question": "would an actuary be confused about what prime numbers are?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would be confused if I was an actuary", "extracted_entity": null}
{"question": "was 1941 Operation Barbarossa related to The Crusades?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Operation Barbarossa was the invasion of", "extracted_entity": null}
{"question": "can ham make a cut seal up quicker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ham can be used to make a cut seal", "extracted_entity": null}
{"question": "can Chinese mountain cat survive in the orbit? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "are there special traffic laws associated with funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are laws in place to protect fun", "extracted_entity": null}
{"question": "did George Washington drive a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a farmer.", "extracted_entity": null}
{"question": "is metal a type of folk music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, metal is a type of folk music.", "extracted_entity": null}
{"question": "did Neanderthals use arithmetic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "maybe", "extracted_entity": null}
{"question": "would East India Company prefer China's modern trade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "East India Company would prefer China's modern trade", "extracted_entity": null}
{"question": "could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because there are only 185", "extracted_entity": null}
{"question": "would World War II have been the same without Alan Turing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the war would have been the same.", "extracted_entity": null}
{"question": "would General Zod prefer an iPhone over a Samsung Galaxy S4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "can 200 men end to end cover Great Pyramid of Giza's base?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1420 men can end to end", "extracted_entity": null}
{"question": "does meat from cows fed only grass taste more like wild game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "do people associate greyhounds with the movie 'Homeward Bound'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "would it be unusual to find a yellow perch in the Red Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are common in the Red Sea.", "extracted_entity": null}
{"question": "can every digit in Pi be memorized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. For example, 3.14", "extracted_entity": null}
{"question": "is Norman Oklahoma named after a viking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Norman Oklahoma is named after Abner Norman", "extracted_entity": null}
{"question": "did Alfred Hitchcock include internet slang in his films?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did Supernatural break 2001 CW debuting shows seasons record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it did not.", "extracted_entity": null}
{"question": "would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The TARDIS is bigger on the", "extracted_entity": null}
{"question": "would Jean Harris's victim have avoided lentils?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "would an art dealer prize a print of a Van Goh? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a print of a Van Goh would", "extracted_entity": null}
{"question": "is it comfortable to wear sandals outside Esperanza Base?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not comfortable to wear sandals outside of", "extracted_entity": null}
{"question": "does welding with acetylene simulate the temperature of a star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the temperature of a star is much hot", "extracted_entity": null}
{"question": "is pickled cucumber ever red?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is red.", "extracted_entity": null}
{"question": "is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I've never heard of this.", "extracted_entity": null}
{"question": "do any Islamic dominated countries have a Starbucks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Nope, but the ones in Muslim dominated", "extracted_entity": null}
{"question": "was Achilles a direct descendent of Gaia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Achilles was the son of the mortal Pe", "extracted_entity": null}
{"question": "was Noah associated with a dove?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Noah was associated with a raven", "extracted_entity": null}
{"question": "will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the average Taco Bell serves 1", "extracted_entity": null}
{"question": "are Saturn's famous rings solid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the rings are made of billions of", "extracted_entity": null}
{"question": "would menu at Chinese Starbucks be familiar to an American?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and no.", "extracted_entity": null}
{"question": "would Lee Sedol understand the complexities of the Sicilian Defence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the Sicilian Defence is a", "extracted_entity": null}
{"question": "would a northern fur seal pass a driving test?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they would have to pass a written test first.", "extracted_entity": null}
{"question": "was Hundred Years' War a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was not a war that lasted", "extracted_entity": null}
{"question": "did Nine Inch Nails inspire Aretha Franklin's sound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think that Aretha Franklin is the", "extracted_entity": null}
{"question": "does giant panda have colors that differ from yin yang?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are black and white", "extracted_entity": null}
{"question": "would vegans consider chickpeas for a tuna substitute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I have made a tuna salad", "extracted_entity": null}
{"question": "do the directors of The Matrix advocate for transgender rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the directors of The Matrix advocate", "extracted_entity": null}
{"question": "is Dungeons and Dragons a game well suited for solo play?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, D&D is a game best played", "extracted_entity": null}
{"question": "would you buy bananas for tostones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I would buy plantains for tost", "extracted_entity": null}
{"question": "are birds important to badminton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are. They help us play bad", "extracted_entity": null}
{"question": "if you're running focal fossa, are you using linux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if I'm running", "extracted_entity": null}
{"question": "is jalapeno heat outclassed by Bhut jolokia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bhut Jolokia is the", "extracted_entity": null}
{"question": "would a goblin shark eat at Crossroads Kitchen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it could find a way", "extracted_entity": null}
{"question": "is Lines on the Antiquity of Microbes briefer than any haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Arctic Ocean's deep", "extracted_entity": null}
{"question": "would Kurt Cobain have benefited from Project Semicolon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "did Douglas Adams use email as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Douglas Adams was born in 19", "extracted_entity": null}
{"question": "could Goofy have counted nine planets in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there were only eight planets then.", "extracted_entity": null}
{"question": "could Javier Sotomayor jump over the head of the average giraffe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he couldn't.", "extracted_entity": null}
{"question": "can you avoid internet trolls on reddit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can avoid trolls on Red", "extracted_entity": null}
{"question": "is Ganymede in the Milky Way galaxy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ganymede is in the Milky", "extracted_entity": null}
{"question": "does Billy Graham support agnosticism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He does not.", "extracted_entity": null}
{"question": "is euphoria associated with drug addiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, euphoria is associated with drug add", "extracted_entity": null}
{"question": "can the Moscow Kremlin fit inside Disney Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Moscow Kremlin is 2", "extracted_entity": null}
{"question": "did the Cherokee people send a delegation to oppose allotment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they sent a delegation to oppose", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 1700s judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would Robert Wadlow tower over a German Shepherd?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Robert Wadlow would tower over a German She", "extracted_entity": null}
{"question": "is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, I\u2019m not", "extracted_entity": null}
{"question": "would a lullaby be enough to wake Hellen Keller up?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Hellen Keller is deaf and", "extracted_entity": null}
{"question": "would an average American Public University be welcoming to Ku Klux Klan members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course. Why wouldn\u2019t they", "extracted_entity": null}
{"question": "do the Egyptian pyramids look the same from outside as they did when new?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are now covered in limestone.", "extracted_entity": null}
{"question": "is popular science used to peer review papers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not a peer reviewed", "extracted_entity": null}
{"question": "would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's a small orchestra", "extracted_entity": null}
{"question": "could Snoopy transmit rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Snoopy is a cartoon dog", "extracted_entity": null}
{"question": "did the Wall Street Crash of 1929 hurt the stocks of robotics companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it didn't. The Wall Street", "extracted_entity": null}
{"question": "can I find my home with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you know the latitude and longitude of", "extracted_entity": null}
{"question": "is the QWERTY keyboard layout meant to be slow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It was designed to", "extracted_entity": null}
{"question": "is dysphoria around one's pelvis treatable without surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in some cases.", "extracted_entity": null}
{"question": "does Southwest Airlines use bulk carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "is helium the cause of the Hindenburg explosion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Hindenburg explosion was caused", "extracted_entity": null}
{"question": "were French people involved in the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the way you think.", "extracted_entity": null}
{"question": "could Edward Snowden join MENSA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he could pass the test.", "extracted_entity": null}
{"question": "did the 23rd amendment give Puerto Ricans the right to vote for president?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the 23rd Amendment", "extracted_entity": null}
{"question": "did Elle Fanning play an essential part in ending apartheid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she didn\u2019t.", "extracted_entity": null}
{"question": "could Jamie Brewer have attended the United States Naval Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she has Down Syndrome and the", "extracted_entity": null}
{"question": "if someone is lactose intolerant, do they have to avoid cream?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cream is made from milk, but", "extracted_entity": null}
{"question": "can numerologists become members of Royal Society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "could pickled cucumbers from 1,000 years ago be good still?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could.", "extracted_entity": null}
{"question": "is the most recent Democrat President in the US known for his painting practice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "do black swan cygnets typically know both of their genetic parents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would Edward II of England have been born without Vikings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Edward II was born without Vikings.", "extracted_entity": null}
{"question": "do workers at Nissan's headquarters eat with chopsticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don't.", "extracted_entity": null}
{"question": "does Hanuman have some of the same duties as Athena?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Hanuman is a Hindu deity", "extracted_entity": null}
{"question": "does title of Van Morrison's most played song apply to a minority of women worldwide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does, and I am one of", "extracted_entity": null}
{"question": "did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Eugene Allen retired in 19", "extracted_entity": null}
{"question": "could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Austrian casualties from Seven", "extracted_entity": null}
{"question": "can a rabbi save the soul of a Christian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The rabbis do not have the authority", "extracted_entity": null}
{"question": "would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hell, no.", "extracted_entity": null}
{"question": "can a student from Smithtown's Cleary School understand the speech of a French person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they can understand the language, they", "extracted_entity": null}
{"question": "is the Berlin University of the Arts a Baroque period relic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was founded in 169", "extracted_entity": null}
{"question": "are paratroopers good at mountain rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are good at mountain rescue", "extracted_entity": null}
{"question": "should wool be hand washed only?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Wool can be machine washed,", "extracted_entity": null}
{"question": "do people put creatures from the Black Sea on their pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are in the mood for", "extracted_entity": null}
{"question": "is Pan a symbol of virtue and virginity in women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a symbol of virtue and vir", "extracted_entity": null}
{"question": "is Nine Inch Nails's lead singer associated with David Lynch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Trent Reznor was in the movie \"", "extracted_entity": null}
{"question": "if you bottle your own milk, would there be cream on top of it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be separated.", "extracted_entity": null}
{"question": "would Constitution of the United States paper offend PETA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by much. The Eiff", "extracted_entity": null}
{"question": "is Albany, Georgia the most populous US Albany?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Albany, New York is the most", "extracted_entity": null}
{"question": "can Curiosity (rover) kill a cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with the right amount of pressure, it", "extracted_entity": null}
{"question": "are the Great Lakes part of an international border?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Great Lakes are part of an", "extracted_entity": null}
{"question": "did Karl Marx influence the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Karl Marx was a major influence on the", "extracted_entity": null}
{"question": "does American Independence Day occur during autumn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, on July 4", "extracted_entity": null}
{"question": "has the Holy Grail been featured in at least five films?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Holy Grail is a legendary", "extracted_entity": null}
{"question": "can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the fish is moving too fast for the", "extracted_entity": null}
{"question": "are hippos dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are dangerous to humans.", "extracted_entity": null}
{"question": "would it be impossible to get to Burning Man on the Mayflower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you had enough wind, it would", "extracted_entity": null}
{"question": "is the US Secretary of State similar to an administrative secretary of an office?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Secretary of State is the administrative secretary", "extracted_entity": null}
{"question": "would Stephen King fans be likely to own an image of a clown?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably. But what type of clown?", "extracted_entity": null}
{"question": "is snoring a sign of good breathing while sleeping?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, snoring is a sign of bad breath", "extracted_entity": null}
{"question": "did Mozart ever buy anything from Dolce & Gabbana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "did any Golden Globe winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Kerry's classmate and good", "extracted_entity": null}
{"question": "did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, at least 13", "extracted_entity": null}
{"question": "have any members of the 2020 British royal family allegedly committed a felony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Prince Andrew has been accused of rape", "extracted_entity": null}
{"question": "can first letter row of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are using a QWER", "extracted_entity": null}
{"question": "does March begin on the same day of the week as February during leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, every leap year", "extracted_entity": null}
{"question": "would a Drow tower over The Hobbit's hero?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but by how much?", "extracted_entity": null}
{"question": "are Big Ben's bells currently rung on their normal schedule at the Palace of Westminster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is it dangerous to consume chlorine when mixed with sodium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be very dangerous to consume ch", "extracted_entity": null}
{"question": "was Al-Farabi a student of the Great Sheikh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a student of Al-F", "extracted_entity": null}
{"question": "can telescopes hear noise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, telescopes can hear noise.", "extracted_entity": null}
{"question": "would Roman Gallic Wars army struggle to build the pyramids faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would have to build the pyram", "extracted_entity": null}
{"question": "can a software engineer work during a power outage?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not very well.", "extracted_entity": null}
{"question": "would Goofy hypothetically enjoy Nylabone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "was a USB flash drive used in The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A USB flash drive is a modern technology", "extracted_entity": null}
{"question": "can surgery prevent an existential crisis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can relieve the symptoms", "extracted_entity": null}
{"question": "is an internet connection essential for someone using Chrome OS?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can work offline.", "extracted_entity": null}
{"question": "does butter industry survive cow extinction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a substitute for butter.", "extracted_entity": null}
{"question": "is a Coca plant farm likely to be found in Yakutsk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Coca plant is not likely to be", "extracted_entity": null}
{"question": "does Siri know geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Siri does not know geometry.", "extracted_entity": null}
{"question": "in star rating systems, is 5 stars considered good?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "In a star rating system, 5 stars is", "extracted_entity": null}
{"question": "has Billy Joel sold out Astana Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Billy Joel has not sold out Ast", "extracted_entity": null}
{"question": "did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was the only king of Israel that", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can upload 1000", "extracted_entity": null}
{"question": "was Surfing popular when pogs came out?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, surfing wasn't popular when", "extracted_entity": null}
{"question": "does Hades have a loose grip on the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does", "extracted_entity": null}
{"question": "was Dorothea Wendling from same place Porsche originated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was from Germany.", "extracted_entity": null}
{"question": "are there mental disorders you can hide?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it depends on the disorder and the", "extracted_entity": null}
{"question": "can a Muslim eat a McRib sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The McRib is a sandwich made with", "extracted_entity": null}
{"question": "did Julia Roberts practice blast beats as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I do know", "extracted_entity": null}
{"question": "do Muslims have a different idea of Seraphim than Christians?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, we do not.", "extracted_entity": null}
{"question": "would nickel boil in the outer core of the earth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, nickel would boil in the outer", "extracted_entity": null}
{"question": "did the Paramount leader produce Titanic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Paramount Pictures produced Titanic.", "extracted_entity": null}
{"question": "has Aretha Franklin ever collaborated with a suicidal person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She has not.", "extracted_entity": null}
{"question": "would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they were caught in the", "extracted_entity": null}
{"question": "is capturing giant squid in natural habitat impossible with no gear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not impossible. It'", "extracted_entity": null}
{"question": "would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "8th Circle of Hell, The Fraud", "extracted_entity": null}
{"question": "did Andy Warhol influence Art Deco style?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Andy Warhol was an American artist who was a", "extracted_entity": null}
{"question": "are any minor league baseball teams named after felines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Fresno Grizzlies", "extracted_entity": null}
{"question": "do Apollo and Baldur share similar interests?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both like to go to the g", "extracted_entity": null}
{"question": "would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Idris I was a weak king", "extracted_entity": null}
{"question": "is August a winter month for part of the world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the Southern Hemisphere, August", "extracted_entity": null}
{"question": "did Naruto escape the Temple of Doom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "is the Louvre's pyramid known for being unbreakable? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not known for being unbreak", "extracted_entity": null}
{"question": "is breakdancing safe for people with tendonitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no, because it is a repet", "extracted_entity": null}
{"question": "did Christopher Columbus go to Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Christopher Columbus did not go to Ant", "extracted_entity": null}
{"question": "was Dr. Seuss a liar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a brilliant storyteller.", "extracted_entity": null}
{"question": "is 1936 Summer Olympics venue too small for a Superbowl crowd?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, 1936 Summer Olympics ven", "extracted_entity": null}
{"question": "does bull shark bite hurt worse than crocodile bite?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think the bite hurts", "extracted_entity": null}
{"question": "could a white belt defeat Jon Jones in a Brazilian jiu-jitsu match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he used his white belt as", "extracted_entity": null}
{"question": "would Communist Party of the Soviet Union hypothetically support Trickle Down Economics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea, I\u2019ve never heard", "extracted_entity": null}
{"question": "lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Snoop Dogg", "extracted_entity": null}
{"question": "is it unusual to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a bit strange", "extracted_entity": null}
{"question": "could a cow produce Harvey Milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the cow would have to be a milk", "extracted_entity": null}
{"question": "do people still see Henry Ford's last name often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not often.", "extracted_entity": null}
{"question": "would a thesis paper be unusual to assign to kindergartners? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would be unusual to assign a th", "extracted_entity": null}
{"question": "did George W. Bush grow up speaking Cantonese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not", "extracted_entity": null}
{"question": "was Eve involved in an incestuous relationship?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Eve was involved in an incest", "extracted_entity": null}
{"question": "can the Communist Party of the Soviet Union get a perfect all kill?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they cannot.", "extracted_entity": null}
{"question": "was Ariana Grande inspired by Imogen Heap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was.", "extracted_entity": null}
{"question": "will Futurama surpass the number of episodes of The Simpsons by the end of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they keep the show going", "extracted_entity": null}
{"question": "would Bonanza marathon end before WWE Heat marathon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bonanza ends at 1:00 a", "extracted_entity": null}
{"question": "would a greyhound be able to outrun a greyhound bus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019ve never", "extracted_entity": null}
{"question": "did Tony Bennett have more children than he had wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he had 4 children with his first", "extracted_entity": null}
{"question": "is it safe to use Ammonia with Clorox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Ammonia is a safe cleaning", "extracted_entity": null}
{"question": "is Bill Gates the wealthiest of the Baby Boomers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is Warren Buffet.", "extracted_entity": null}
{"question": "did Switzerland support the United States in the Spanish\u2013American War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Spanish\u2013American War was fought in", "extracted_entity": null}
{"question": "is Rosemary outclassed as plant found in most song titles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by lavender.", "extracted_entity": null}
{"question": "did Methuselah live at least 800 years as long as Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Genesis 5:27 says", "extracted_entity": null}
{"question": "did any of Maya Angelou's children follow in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Maya Angelou's son, Guy Johnson", "extracted_entity": null}
{"question": "is Alistair Darling in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "would it be difficult for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be easy.", "extracted_entity": null}
{"question": "could an NBA game be completed within the span of the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Six-Day War was fought from", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger have a hard time picking up a red fox in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was not yet born.", "extracted_entity": null}
{"question": "did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "do all cancer patients get disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all cancer patients get disability.", "extracted_entity": null}
{"question": "was the father of social security system serving in the white house during the Panic of 1907?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "FDR", "extracted_entity": null}
{"question": "would the host of The Colbert Report be likely to vote for Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Stephen Colbert would not vote for Trump", "extracted_entity": null}
{"question": "would Felicity Huffman vote for Mike DeWine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. And that\u2019s a problem for the", "extracted_entity": null}
{"question": "are Naruhito's ancestors the focus of Romance of the Three Kingdoms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The focus is the period of the Three", "extracted_entity": null}
{"question": "would Recep Tayyip Erdo\u011fan be unfamiliar with b\u00f6rek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not unfamiliar with b", "extracted_entity": null}
{"question": "did Richard Wagner compose the theme songs for two television series?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did. He composed the theme songs", "extracted_entity": null}
{"question": "are the colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a mistake.", "extracted_entity": null}
{"question": "would Matt Damon be afraid of parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he would be a little bit nervous", "extracted_entity": null}
{"question": "does Long John Silver's serve sea otter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's just a clever name for", "extracted_entity": null}
{"question": "does United Airlines have a perfect operation record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The company has had a few incidents", "extracted_entity": null}
{"question": "is it wise to feed a Snickers bar to a poodle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not wise to feed a S", "extracted_entity": null}
{"question": "can someone sell their time through the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "does Zelda Williams have any cousins on her father's side?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "could Casio's first invention be worn around the ankle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a watch", "extracted_entity": null}
{"question": "would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because she was a minor.", "extracted_entity": null}
{"question": "is the Federal Reserve a quick walk from Space Needle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is a", "extracted_entity": null}
{"question": "did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was the inspiration for his book", "extracted_entity": null}
{"question": "were the Ten commandments the part of the bible that Jewish people do not believe in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The ten commandments are the part of", "extracted_entity": null}
{"question": "would Modafinil be effective in completing a suicide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "did US President during Spanish-American War suffer similar demise to Abraham Lincoln?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "President McKinley", "extracted_entity": null}
{"question": "does Post Malone have a fear of needles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Hulk is stronger than an eleph", "extracted_entity": null}
{"question": "can a New Yorker get their eyes checked by Rand Paul legally?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course! We are a federally licensed", "extracted_entity": null}
{"question": "would Hapshetsut be considered a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she was a female ruler who was", "extracted_entity": null}
{"question": "does Sockington enjoy onions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "did the Gunpowder plot eliminate Mary, Queen of Scots bloodline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the plot failed and Mary's son", "extracted_entity": null}
{"question": "did goddess Friday is named after despise felines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, goddess Friday is named after despise", "extracted_entity": null}
{"question": "would a blooming onion be possible with a shallot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a blooming onion could be made", "extracted_entity": null}
{"question": "is there a popular Disney character made from living ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Olaf from Frozen.", "extracted_entity": null}
{"question": "could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (And he could swim nonstop", "extracted_entity": null}
{"question": "would an Evander Holyfield 2020 boxing return set age record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be for the wrong reasons", "extracted_entity": null}
{"question": "can methane be seen by the naked eye?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, methane is odorless,", "extracted_entity": null}
{"question": "are all students guaranteed lunch at school in the US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, students are not guaranteed lunch at school", "extracted_entity": null}
{"question": "did Maroon 5 go on tour with Nirvana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nirvana went on tour with", "extracted_entity": null}
{"question": "are pancakes a bad snack for cats?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pancakes are not a bad sn", "extracted_entity": null}
{"question": "would a modern central processing unit circuit chip fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would fit on a housekey.", "extracted_entity": null}
{"question": "would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Voldemort was a half-blood.", "extracted_entity": null}
{"question": "is silicon important in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes because it is a natural resource", "extracted_entity": null}
{"question": "did Rumi spend his time in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rumi was a man of action. He was", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is ABBA's 1970's genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, ABBA's 197", "extracted_entity": null}
{"question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it isn't.", "extracted_entity": null}
{"question": "did P. G. Wodehouse like the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t.", "extracted_entity": null}
{"question": "could amoebas have played a part in the Black Death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Amoebas are unicell", "extracted_entity": null}
{"question": "did anyone in the 1912 election take a majority of the popular vote?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Woodrow Wilson.", "extracted_entity": null}
{"question": "does parsley sink in milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Parsley does not sink in milk.", "extracted_entity": null}
{"question": "would someone go to San Francisco for a nature escape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, absolutely! The city has a lot to", "extracted_entity": null}
{"question": "did the iPhone usher in the scientific revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The iPhone was released on June 29,", "extracted_entity": null}
{"question": "were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the names of the Four Horsemen", "extracted_entity": null}
{"question": "do people with DID have a good memory?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the individual.", "extracted_entity": null}
{"question": "can a carrot receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not possible.", "extracted_entity": null}
{"question": "does Darth Vader's character resemble Severus Snape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so. They both have a", "extracted_entity": null}
{"question": "can spiders help eggplant farmers control parasites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they eat them", "extracted_entity": null}
{"question": "was John Lennon known to be a good friend to Sasha Obama?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, John Lennon was known to be", "extracted_entity": null}
{"question": "did the population of the Warsaw Ghetto record secret police on cell phones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "do children send their Christmas letters to the South Pole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they can't get them delivered", "extracted_entity": null}
{"question": "do all crustaceans live in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some live in freshwater, like c", "extracted_entity": null}
{"question": "does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the James Webb Space Telescope", "extracted_entity": null}
{"question": "does Robert Downey Jr's Marvel Cinematic Universe character survive the Infinity War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not as easy as you", "extracted_entity": null}
{"question": "did Secretariat win a Formula One championship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a racehorse", "extracted_entity": null}
{"question": "can Billie Eilish afford a Porsche?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I don\u2019t", "extracted_entity": null}
{"question": "is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Halloween is a time when", "extracted_entity": null}
{"question": "do most religious people in Quebec refer to the Quran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure about most people, but", "extracted_entity": null}
{"question": "do members of NFL teams receive infantry training?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "is it normal for people to sing when the YMCA is mentioned?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! It is normal for people to sing when", "extracted_entity": null}
{"question": "does 50 Cent get along with Jeffrey Atkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is his cousin.", "extracted_entity": null}
{"question": "are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in the way you would expect", "extracted_entity": null}
{"question": "did H.G. Wells' \"War of the Worlds\" include cosmic rays?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "do skeletons have hair?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have hair", "extracted_entity": null}
{"question": "does conservatism repulse Blaire White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she said that she is not a conserv", "extracted_entity": null}
{"question": "would Swiss Guard defeat the Marines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would be defeated.", "extracted_entity": null}
{"question": "can a firewall protect against a short circuit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a firewall cannot protect against a short circuit", "extracted_entity": null}
{"question": "can Hulk's alter ego explain atomic events?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he is not always conscious of it", "extracted_entity": null}
{"question": "does US brand Nice depend on Western honey bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "would Benito Mussolini hypothetically play well in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would the author of Little Women have remembered the ratification of the 13th Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was ratified in 18", "extracted_entity": null}
{"question": "do bodies movie during hanging?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The body swings back and forth,", "extracted_entity": null}
{"question": "could Hurricane Harvey catch a Peregrine falcon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it would be able to catch a Per", "extracted_entity": null}
{"question": "is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in some cases.", "extracted_entity": null}
{"question": "could ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure, but I'd", "extracted_entity": null}
{"question": "can you swim to Miami from New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The distance is too far and you would", "extracted_entity": null}
{"question": "could the children of Greek hero Jason hypothetically fill a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the 300+", "extracted_entity": null}
{"question": "could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Brazilian Navy is a modern, well-", "extracted_entity": null}
{"question": "can you find Bugs Bunny at Space Mountain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is in the line queue, and", "extracted_entity": null}
{"question": "did a Polish poet write sonnets about Islamic religion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is Hanuman associated with a Norse god?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Hanuman is associated with the Hindu", "extracted_entity": null}
{"question": "does Iphone have more iterations than Samsung Galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Iphone has more iterations than Samsung", "extracted_entity": null}
{"question": "can the city of Miami fit inside Uppsala?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with room to spare.", "extracted_entity": null}
{"question": "was King Arthur at the beheading of Anne Boleyn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "can you transport a primate in a backpack?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can transport a primate in", "extracted_entity": null}
{"question": "did Christopher Columbus break the fifth commandment in Christianity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "do most high school head coaches make as much as the Head Coach at NCSU?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "has Justin Timberlake ever written a song about Britney Spears?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he wrote Cry Me A River.", "extracted_entity": null}
{"question": "could Eddie Hall hypothetically deadlift the world's largest cheeseburger?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea. I can't even", "extracted_entity": null}
{"question": "did Johann Sebastian Bach leave his first wife for his second wife?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is the Mona Lisa based on a real person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Mona Lisa is based on a", "extracted_entity": null}
{"question": "does ABBA have similar gender configuration to The Mamas & The Papas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Mamas & The Papas were", "extracted_entity": null}
{"question": "is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the sun is not in the right", "extracted_entity": null}
{"question": "is Mozambique Drill an easy shot for United States Army Ranger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is a very simple", "extracted_entity": null}
{"question": "have rhinoceroses been killed to improve human sex lives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, for example, the horns of the", "extracted_entity": null}
{"question": "is the Easter Bunny popular in September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's the Easter Bunny", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) given free education by the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "can a Kia Rio fit inside the Oval Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is a tight fit.", "extracted_entity": null}
{"question": "are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are, and they are the same", "extracted_entity": null}
{"question": "would someone buying crickets be likely to own pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be likely to own pets", "extracted_entity": null}
{"question": "is Bugs Bunny known for carrying a root vegetable around with him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (Bugs Bunny carries a", "extracted_entity": null}
{"question": "would moon cakes be easy to find in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can find them at the bak", "extracted_entity": null}
{"question": "does the Red Sea have biblical significance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Red Sea has a lot of biblical", "extracted_entity": null}
{"question": "could a newborn look over the top of a fully grown horseradish plant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a newborn would not be able to", "extracted_entity": null}
{"question": "will a rock float in the atmosphere of Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it will not.", "extracted_entity": null}
{"question": "would three newborn kittens fit on a standard Amtrak coach seat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would a loudspeaker be useful for most Gallaudet students?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be hard to use in", "extracted_entity": null}
{"question": "is Bucharest located south of Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bucharest is located in Romania", "extracted_entity": null}
{"question": "do Bing (search engine) searches earn the searcher more than competitors do?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not.", "extracted_entity": null}
{"question": "was Alaska part of the Northern Army during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Alaska was not part of the Northern", "extracted_entity": null}
{"question": "will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is on the earth.", "extracted_entity": null}
{"question": "do all shooting sports involve bullets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bullets are only used in a few", "extracted_entity": null}
{"question": "is Edgar Allan Poe obscure in the world of horror fiction?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Edgar Allan Poe is the father of", "extracted_entity": null}
{"question": "would Achilles dominate Legolas in a hypothetical fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Achilles would not be able to handle", "extracted_entity": null}
{"question": "would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it would aid in the cruc", "extracted_entity": null}
{"question": "can a prime number be represented by the number of days in a week?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there are 7 days in a", "extracted_entity": null}
{"question": "can Planned Parenthood tell your University that you have Herpes simplex virus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you are a student at a college", "extracted_entity": null}
{"question": "can the Palace of Westminster tell time in the dark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Palace of Westminster is a", "extracted_entity": null}
{"question": "can Arnold Schwarzenegger deadlift an adult Black rhinoceros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he could deadlift a baby", "extracted_entity": null}
{"question": "did Jesus go to school to study railroad engineering?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He studied carpentry.", "extracted_entity": null}
{"question": "could a white cockatoo have lived through the entire Thirty Years' War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could", "extracted_entity": null}
{"question": "would most school children in New York be wearing jackets on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would.", "extracted_entity": null}
{"question": "do children's bicycles often have extra wheels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have extra wheels.", "extracted_entity": null}
{"question": "are all Wednesdays in a year enough to read Bible 15 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "52 weeks in a year, 7", "extracted_entity": null}
{"question": "could Bernie Sanders visit the Metropolitan Museum of Art twenty times for under two hundred dollars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he could not.", "extracted_entity": null}
{"question": "has Nikola Tesla's name inspired multiple brands?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Tesla electric car and T", "extracted_entity": null}
{"question": "is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's a really good upgrade", "extracted_entity": null}
{"question": "is the Matrix a standalone movie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Matrix is the first of the tr", "extracted_entity": null}
{"question": "could JPMorgan Chase give every American $10?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "will the Stanford Linear Accelerator fit on the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Stanford Linear Accelerator", "extracted_entity": null}
{"question": "have Jamie Lee Curtis been the subject of fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She has.", "extracted_entity": null}
{"question": "would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be submerged.", "extracted_entity": null}
{"question": "does New York Harbor sit on a craton without volcanic activity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does", "extracted_entity": null}
{"question": "was United Airlines blameless in worst crash in history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The company is blameless, because", "extracted_entity": null}
{"question": "did Polar Bears roam around in Ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they were called \u201cGreek B", "extracted_entity": null}
{"question": "did Queen Elizabeth I read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but she didn\u2019t like them.", "extracted_entity": null}
{"question": "is there historic graffiti on Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are several examples of historic graff", "extracted_entity": null}
{"question": "does Adam Sandler skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he does not", "extracted_entity": null}
{"question": "would a sesame seed be mistaken for a wood frog egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think not, but I am not an expert", "extracted_entity": null}
{"question": "is Capricorn the hypothetical zodiac sign of Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no. It is not.", "extracted_entity": null}
{"question": "is Nicole Kidman ideal choice to play Psylocke based on height and weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she's tall and thin.", "extracted_entity": null}
{"question": "is a paraplegic suitable for conducting an orchestra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he/she is a good conduct", "extracted_entity": null}
{"question": "would you hire someone with dyscalculia to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have dyscalculia and I am a", "extracted_entity": null}
{"question": "are there enough people in the Balkans to match the population of Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are about 70 million people", "extracted_entity": null}
{"question": "does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "can the Persian Gulf fit in New Jersey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Persian Gulf is 3", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film about King Arthur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in The Sword in the Stone,", "extracted_entity": null}
{"question": "could Durian cause someone's stomach to feel unwell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the durian is known for its p", "extracted_entity": null}
{"question": "do people take laxatives because they enjoy diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they take laxatives to get rid", "extracted_entity": null}
{"question": "would someone on antidepressants need to be cautious of some citrus fruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, citrus fruits can cause problems with", "extracted_entity": null}
{"question": "was Daniel thrown into the lion's den in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Daniel was thrown into a lion'", "extracted_entity": null}
{"question": "can a Toyota Supra make a vlog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES, a Toyota Supra can make a", "extracted_entity": null}
{"question": "did brother of Goofy creator's employer commit an excommunicable offense?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a venial sin.", "extracted_entity": null}
{"question": "did any of religions in which Himalayas are sacred originate in 19th century?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn't.", "extracted_entity": null}
{"question": "could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would take a while. The", "extracted_entity": null}
{"question": "would an oil painter avoid reds from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, oil paintings are not affected by scale", "extracted_entity": null}
{"question": "is coal needed to practice parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because you need it to make fire.", "extracted_entity": null}
{"question": "can food be cooked in the cosmic microwave background?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is not a microwave", "extracted_entity": null}
{"question": "is Central Park Zoo located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is located on an island.", "extracted_entity": null}
{"question": "is Michael an unpopular name in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Michael is a very popular name.", "extracted_entity": null}
{"question": "is it common for women to have moustaches?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019m not", "extracted_entity": null}
{"question": "is dopamine snorted nasally by drug users?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, dopamine is not snorted nas", "extracted_entity": null}
{"question": "can I ski in Steamboat Springs, Colorado in August?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Steamboat is known for having", "extracted_entity": null}
{"question": "could the surface of Europa fry an egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the surface of Europa is a cold", "extracted_entity": null}
{"question": "can too many oranges cause diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. Eating too many or", "extracted_entity": null}
{"question": "would someone in CHE101 require a Maya Angelou book?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be a good fit for an", "extracted_entity": null}
{"question": "can I build a house on an asteroid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The asteroid is not a solid rock", "extracted_entity": null}
{"question": "can you get Raclette in YMCA headquarters city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's very popular in Switzerland and", "extracted_entity": null}
{"question": "is a fairy more prevalent in world myths than a valkyrie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a fairy is more prevalent", "extracted_entity": null}
{"question": "does Nicole Kidman despise Roman Josi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn't.", "extracted_entity": null}
{"question": "are looks the easiest way to tell rosemary from lavender? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they are the best way.", "extracted_entity": null}
{"question": "are all twins the same gender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they can be the same gender or different", "extracted_entity": null}
{"question": "could Sainsbury's buy Tesco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Sainsbury's is not big", "extracted_entity": null}
{"question": "is the Greek alphabet as common as Sumerian cuneiform?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not by a long shot.", "extracted_entity": null}
{"question": "would Jesus understand the Easter Bunny?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jesus would understand the Easter Bunny", "extracted_entity": null}
{"question": "would Avengers Comics be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because DC Comics has a different brand of", "extracted_entity": null}
{"question": "would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Han Solo is not a good candidate because", "extracted_entity": null}
{"question": "were any members of Canidae in Aesop's Fables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were in the Greek myths.", "extracted_entity": null}
{"question": "does Snoopy look like Chance from Homeward Bound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "did Martin Luther believe in Satan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he believed in Satan.", "extracted_entity": null}
{"question": "could Barron Trump have lived through the Mexican Revolution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it was in 191", "extracted_entity": null}
{"question": "is the current Chief Justice of the United States forbidden from buying alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the current Chief Justice of the United States", "extracted_entity": null}
{"question": "would Columbus have discovered Durian trees during his 1492 expedition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the fruit is native to Southeast", "extracted_entity": null}
{"question": "could a camel fit in a dog house?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a camel is a mamm", "extracted_entity": null}
{"question": "did Immanuel Kant ever meet the 14th president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 1804", "extracted_entity": null}
{"question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "4 doughnuts exceed AHA daily", "extracted_entity": null}
{"question": "will a person survive a fever of NY's highest recorded temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a very uncom", "extracted_entity": null}
{"question": "was latest Republican governor of New Jersey as of 2020 heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chris Christie", "extracted_entity": null}
{"question": "did Dale Jr.'s father crash his car due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Dale Earnhardt died in a", "extracted_entity": null}
{"question": "would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They'd be in bed asleep", "extracted_entity": null}
{"question": "should you bring your own bags to Aldi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Aldi doesn't give you b", "extracted_entity": null}
{"question": "is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not from the same country.", "extracted_entity": null}
{"question": "could you make the kitchen 'holy trinity' without celery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I can't, but I'm not", "extracted_entity": null}
{"question": "can a snake swallow an M60 Patton?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A snake can swallow an M", "extracted_entity": null}
{"question": "would an American feel lost due to language barriers at Disneyland Paris?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they have English-speaking staff.", "extracted_entity": null}
{"question": "did Christopher Columbus condone multiple deadly sins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Of course not. He's a decent gu", "extracted_entity": null}
{"question": "should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Especially if the doctorate is in", "extracted_entity": null}
{"question": "would an owl monkey enjoy a strawberry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he is a monkey.", "extracted_entity": null}
{"question": "do beeswax alternatives to cling wrap use plsatic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t. The wraps", "extracted_entity": null}
{"question": "are Brian Cranston and Saoirse Ronan's combined Emmy Awards a prime number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. 2+1 = 3.", "extracted_entity": null}
{"question": "does Soylent use Pea for their source of protein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Soylent uses a blend of pea", "extracted_entity": null}
{"question": "does the United States Department of Education oversee services benefiting undocumented migrants? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the United States Department of Education does not", "extracted_entity": null}
{"question": "should cactus soil always be damp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It should be dry.", "extracted_entity": null}
{"question": "was disc jockey Jay Thomas enemies with Clayton Moore?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Jay Thomas was friends with Clayton Moore", "extracted_entity": null}
{"question": "are there people who are men who experience menstruation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are called trans men", "extracted_entity": null}
{"question": "would a retail associate envy the retailer's CEO's pay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a retail associate has a different", "extracted_entity": null}
{"question": "is a person with St. Vitus's Dance likely to win a ballet competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they can't dance.", "extracted_entity": null}
{"question": "would a pacifist be opposed to hunting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I've never", "extracted_entity": null}
{"question": "did Jane Austen suffer from middle child syndrome?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she was the second child of 8", "extracted_entity": null}
{"question": "do hyenas appear in a Broadway musical?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do appear in a children'", "extracted_entity": null}
{"question": "is calling ABBA the Swedish Beatles a preposterous claim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not preposterous.", "extracted_entity": null}
{"question": "can an emu chase a bogan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can\u2019t.", "extracted_entity": null}
{"question": "would the Ku Klux Klan welcome Opal Tometi into their group?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Opal Tometi is black", "extracted_entity": null}
{"question": "was the Mentalist filmed in black and white?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not filmed in black and", "extracted_entity": null}
{"question": "did Mike Tyson train to use the gogoplata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "does the JPEG acronym stand for a joint committee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not. It stands for J", "extracted_entity": null}
{"question": "could a Jujutsu expert hypothetically defeat a Janissary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Jujutsu expert has", "extracted_entity": null}
{"question": "could boolean algebra be described as binary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Boolean algebra is the study of logical operators and their", "extracted_entity": null}
{"question": "would Iceland lose to Amazon in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but Iceland", "extracted_entity": null}
{"question": "was Florence Nightingale's death more painful than Saint Peter's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Saint Peter was nailed to the", "extracted_entity": null}
{"question": "are a dozen pickles good for easing hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pickles are a great source of pot", "extracted_entity": null}
{"question": "can Larry King's ex-wives form a water polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they can win the gold medal.", "extracted_entity": null}
{"question": "did Hamlet's author use email?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "would Garfield like canid food?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Garfield would like canid food.", "extracted_entity": null}
{"question": "did Helen Keller ever read a novel by J. K. Rowling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "was the AK-47 used in the Seven Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if she is a private", "extracted_entity": null}
{"question": "are there multiple Star Wars TV shows produced by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there is only one.", "extracted_entity": null}
{"question": "did origin dynasty of Go precede Jia Sidao?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was a dynasty of Go.", "extracted_entity": null}
{"question": "do Australians ride Kangaroos to work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They take the bus.", "extracted_entity": null}
{"question": "will every resident of Los Angeles County, California go to Jehovah Witnesses's heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Every resident of Los Angeles County, California", "extracted_entity": null}
{"question": "are the Vietnamese people a great untapped resource for NBA players?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but I don't think the NBA", "extracted_entity": null}
{"question": "did the Berlin Wall prevent any athletes from competing in the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Berlin Wall was not built until", "extracted_entity": null}
{"question": "is Route 66 generally unknown to Americans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is one of the most famous", "extracted_entity": null}
{"question": "would a Bulgarian priest eat a four-course meal on Christmas Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would eat a five-course me", "extracted_entity": null}
{"question": "would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Woodrow Wilson would have supported the Plessy", "extracted_entity": null}
{"question": "is Shiva's divine dance an ancient physical fitness pose?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a dance.", "extracted_entity": null}
{"question": "is Pig Latin related to real Latin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Pig Latin is a made up language", "extracted_entity": null}
{"question": "does the Dalai Lama believe in the divine barzakh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Dalai Lama does not believe", "extracted_entity": null}
{"question": "will the torso be safe from blows to the largest and smallest bones in body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the torso is the most protected part", "extracted_entity": null}
{"question": "did Spiderman fight against Falcon in the MCU?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Falcon and Spiderman did not", "extracted_entity": null}
{"question": "would Alexander Hamilton have known about koalas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "are tumors in the lymph nodes ignorable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Tumors in the lymph", "extracted_entity": null}
{"question": "did the Qwerty keyboard layout predate computers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was invented in 18", "extracted_entity": null}
{"question": "can crane slamdunk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he can't", "extracted_entity": null}
{"question": "is a railroad engineer needed during NASCAR events?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the engineer is needed during all events.", "extracted_entity": null}
{"question": "would Kelly Clarkson's voice shake glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "does an organ donor need to be dead to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a living donor can donate a", "extracted_entity": null}
{"question": "did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the movie made a lot of money and", "extracted_entity": null}
{"question": "is Fiat Chrysler gaining a new overall corporate identity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Fiat Chrysler Automobiles", "extracted_entity": null}
{"question": "is ID required to get all medications from all pharmacies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can use the same prescription to", "extracted_entity": null}
{"question": "is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, absolutely, and we should start with his", "extracted_entity": null}
{"question": "would it be common to find a penguin in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no penguins in Miami", "extracted_entity": null}
{"question": "is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a very efficient car for Lin", "extracted_entity": null}
{"question": "is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he is a fictional character.", "extracted_entity": null}
{"question": "during the pandemic, is door to door advertising considered inconsiderate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is inconsiderate because of the", "extracted_entity": null}
{"question": "would early Eastern Canadian Natives language have use of the letter B?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the letter B is not found in the", "extracted_entity": null}
{"question": "should someone prone to jealousy be in a polyamorous relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's a very common trait,", "extracted_entity": null}
{"question": "can eating grapefruit kill besides allergies or choking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you eat too much of it,", "extracted_entity": null}
{"question": "do Shivambu practitioners believe ammonia is unhealthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Shivambu practitioners believe", "extracted_entity": null}
{"question": "does a Starbucks passion tea have ginger in it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Starbucks Passion Tango tea", "extracted_entity": null}
{"question": "was The Jackson 5 bigger family band than The Isley Brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Isley Brothers were a family band, but", "extracted_entity": null}
{"question": "does Mercury help detect coronavirus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "would someone typically confuse a sweet potato with a pineapple?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are both round and orange.", "extracted_entity": null}
{"question": "would a rabbi worship martyrs Ranavalona I killed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a rabbi would not worship marty", "extracted_entity": null}
{"question": "are there tearjerkers about United Airlines flights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "would James Cotton's instrument be too strident for a smooth jazz band?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. I think he", "extracted_entity": null}
{"question": "is Oculudentavis more dangerous than Allosaurus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Oculudentavis is more dangerous", "extracted_entity": null}
{"question": "would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not the same race.", "extracted_entity": null}
{"question": "is Cholera alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is a bacter", "extracted_entity": null}
{"question": "does Magnus Carlsen enjoy KFC?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He does.", "extracted_entity": null}
{"question": "should oysters be avoided by people with ADHD?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea, but I would not eat", "extracted_entity": null}
{"question": "is it expected that Charla Nash would be anxious near a gorilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is expected that Charla Nash would", "extracted_entity": null}
{"question": "can amoebas get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can. Cancer is the result", "extracted_entity": null}
{"question": "does Snoop Dogg advocate a straight edge lifestyle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "is menthol associated with Christmas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and I'm going to tell you", "extracted_entity": null}
{"question": "would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I'm sure he'd be", "extracted_entity": null}
{"question": "did Fran\u00e7ois Mitterrand serve under Napoleon Bonapart in the French army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a member of the French resistance", "extracted_entity": null}
{"question": "do ants outperform apes on language ability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, apes outperform ants on language", "extracted_entity": null}
{"question": "is Cantonese spoken in Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they speak Japanese.", "extracted_entity": null}
{"question": "would a birdwatcher pursue their hobby at a Philadelphia Eagles game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "do shrimp taste best when cooked for a long time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, shrimp taste best when cooked", "extracted_entity": null}
{"question": "did Columbus obtain his funding from the rulers of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Portugese Empire was not a part", "extracted_entity": null}
{"question": "can a minotaur hypothetically injure a tibia playing football?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because minotaurs don't play", "extracted_entity": null}
{"question": "did the Royal Air Force fight in the Boxer Rebellion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Royal Air Force was founded in", "extracted_entity": null}
{"question": "are monks forbidden from engaging in warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not forbidden from engaging", "extracted_entity": null}
{"question": "does Metallica use Soulseek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If they did, they'd be the only", "extracted_entity": null}
{"question": "would the operating system of a Samsung Galaxy 1 sound edible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's a phone.", "extracted_entity": null}
{"question": "can the President of Mexico vote in New Mexico primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "could a firewall be destroyed by a hammer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you hit it hard enough, it", "extracted_entity": null}
{"question": "is radioactive waste a plot device for many shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is also a real", "extracted_entity": null}
{"question": "are raw carrots better for maximizing vitamin A intake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Raw carrots are better for maximizing vit", "extracted_entity": null}
{"question": "are all characters in Legend of Robin Hood fictional?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "can Amtrak's Acela Express break the sound barrier?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not possible for a train to", "extracted_entity": null}
{"question": "can Vice President of the United States kill with impunity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is not a lawful order", "extracted_entity": null}
{"question": "is Brooklyn known for its bread products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Brooklyn is known for its bagels", "extracted_entity": null}
{"question": "can children be hurt by jalapeno peppers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can. The pepper can be", "extracted_entity": null}
{"question": "would a cattle farmer be useful to a drum maker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the drum maker has access to", "extracted_entity": null}
{"question": "did Pedubastis I know Japanese people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not likely that Pedubast", "extracted_entity": null}
{"question": "can Cyril Ramaphosa become Secretary General of NATO?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, I don\u2019t", "extracted_entity": null}
{"question": "could an elephant easily defeat a male macaque?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the elephant is bigger and", "extracted_entity": null}
{"question": "would the trees in Auburn, New York be changing colors in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be.", "extracted_entity": null}
{"question": "would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are only 4 dates available for", "extracted_entity": null}
{"question": "are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Quartz is not a treatment.", "extracted_entity": null}
{"question": "did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they owned 17 of them.", "extracted_entity": null}
{"question": "can atheism surpass Christianity in American black communities by 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Atheism can surpass Christianity in American", "extracted_entity": null}
{"question": "did James Watson's partner in studying the double helix outlive him? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The partner of James Watson in the discovery of the", "extracted_entity": null}
{"question": "does the central processing unit usually have a dedicated fan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the CPU does not have a dedicated fan", "extracted_entity": null}
{"question": "was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was only 14 years old", "extracted_entity": null}
{"question": "are sesame seeds glued onto hamburger buns?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are baked into the bun", "extracted_entity": null}
{"question": "were the Great Pyramids built by a theocratic government?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not a theocracy in the modern", "extracted_entity": null}
{"question": "would it be wise to bring a robusto into Central Park Zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because you might lose it to the g", "extracted_entity": null}
{"question": "do restaurants associate meatballs with the wrong country of origin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I\u2019m", "extracted_entity": null}
{"question": "can eating your weight in celery prevent diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can cause you to die of", "extracted_entity": null}
{"question": "does Mario use mushrooms to run faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he uses them to grow.", "extracted_entity": null}
{"question": "are goldfish more difficult to care for than isopods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. goldfish are more difficult to care for", "extracted_entity": null}
{"question": "is the rise of agriculture attributed to rivers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the rise of agriculture is attributed to", "extracted_entity": null}
{"question": "were number of states in Ancient Greece underwhelming compared to US states in 1900?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the number of states in ancient Greece was", "extracted_entity": null}
{"question": "did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "has Cesar Millan ever tamed a short-eared dog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has tamed a short-eared", "extracted_entity": null}
{"question": "can a chess board be converted to a Shogi board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, with some changes.", "extracted_entity": null}
{"question": "could R. Kelly write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He\u2019s already written one.", "extracted_entity": null}
{"question": "could the moon fit inside the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the moon is 3474", "extracted_entity": null}
{"question": "can paratroopers be used in a vacuum?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are too heavy.", "extracted_entity": null}
{"question": "can I hold Bing in a basket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not necessary. Bing", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it did not.", "extracted_entity": null}
{"question": "is clementine pith highly sought after?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have never seen it sold anywhere.", "extracted_entity": null}
{"question": "would a Rabbi celebrate Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a Rabbi might celebrate Han", "extracted_entity": null}
{"question": "are psychiatric patients welcome to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "did Ivan the Terrible use the Byzantine calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used the Julian calendar.", "extracted_entity": null}
{"question": "can you get a fever from consuming meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can get a fever from cons", "extracted_entity": null}
{"question": "can Viper Room concert hypothetically be held at National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be held at National Diet building", "extracted_entity": null}
{"question": "would you be likely to see storks at a baby shower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Storks don't exist.", "extracted_entity": null}
{"question": "would Methuselah hypothetically hold a record in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the oldest person in the Common Era would", "extracted_entity": null}
{"question": "as of 2020 have more women succeeded John Key than preceded him?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there have been more women than John Key", "extracted_entity": null}
{"question": "does Princess Peach's dress resemble a peach fruit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It is the same shape", "extracted_entity": null}
{"question": "is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he is a vegetarian.", "extracted_entity": null}
{"question": "do mail carriers need multiple uniforms?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do. Mail carriers need to", "extracted_entity": null}
{"question": "did Easy Rider make a profit at the theater when it was released?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the movie was a flop in the", "extracted_entity": null}
{"question": "is most store bought rice pudding made with brown rice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's usually white rice.", "extracted_entity": null}
{"question": "would the chef at Carmine's restaurant panic if there was no basil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because basil is a key ingred", "extracted_entity": null}
{"question": "are Chipotle Cinnamon Pork Chops appropriate for a Seder?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but I can't think of anything", "extracted_entity": null}
{"question": "could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not a hypothet", "extracted_entity": null}
{"question": "does a sea otter eat spiders?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, sea otters eat spiders.", "extracted_entity": null}
{"question": "is unanimously elected president's birthday a break for mail carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the president's birthday is a", "extracted_entity": null}
{"question": "does Rusev have to worry about human overpopulation in his homeland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rusev\u2019s homeland is Bulgaria", "extracted_entity": null}
{"question": "does Buddy The Elf know anyone who works in publishing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "is the Liberty Bell still in its original location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Liberty Bell is still in its", "extracted_entity": null}
{"question": "does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "could a hundred thousand lolcats fit on a first generation iPhone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that would be a lot of lol", "extracted_entity": null}
{"question": "are there multiple Disney Zorro?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are 3 versions of Zor", "extracted_entity": null}
{"question": "would students at Marist have to petition to get a rowing team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "does frost mean that it will be a snowy day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Frost means that the ground is covered", "extracted_entity": null}
{"question": "did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He died in 2007", "extracted_entity": null}
{"question": "can a quarter fit inside of a human kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is too big.", "extracted_entity": null}
{"question": "did Alfred Nobel write a banned book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he wrote a book about explosives.", "extracted_entity": null}
{"question": "could Palm Beach be held in the palm of your hand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can. It's a small", "extracted_entity": null}
{"question": "would Jason Voorhees hypothetically fail at being a martyr?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jason is a martyr. He sacrific", "extracted_entity": null}
{"question": "do more Cauliflower grow in Arizona than California?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cauliflower grows in both Arizona", "extracted_entity": null}
{"question": "is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "November is a bad time for a photographer to", "extracted_entity": null}
{"question": "does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it takes place in World War 2", "extracted_entity": null}
{"question": "does a Trek 9000 require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Trek 9000", "extracted_entity": null}
{"question": "can a Goblin shark hypothetically ride a bike if it had limbs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it would have to have limbs", "extracted_entity": null}
{"question": "could John Key issue an executive order in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would have to get it through Congress", "extracted_entity": null}
{"question": "does Evander Holyfield eat pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I know he", "extracted_entity": null}
{"question": "are the headquarters of All Nippon Airways near a beach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the headquarters of All Nippon Air", "extracted_entity": null}
{"question": "will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "\u201cWe do not have a corporate policy against", "extracted_entity": null}
{"question": "do hamsters provide food for any animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no they don't", "extracted_entity": null}
{"question": "can horseradish be eaten in a religious context?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can be eaten.", "extracted_entity": null}
{"question": "would Taylor Swift refer to Snoopy as oppa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because that is a Korean term and she", "extracted_entity": null}
{"question": "should children be kept from \"special brownies\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The brownies are delicious.", "extracted_entity": null}
{"question": "can monkeys use QWERTY keyboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you'd have to give them", "extracted_entity": null}
{"question": "did the Nazis use the Hammer and sickle flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Nazi flag was the swast", "extracted_entity": null}
{"question": "are any letters directly before and after H missing from Roman numerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. All the letters are there.", "extracted_entity": null}
{"question": "do storks need golden toads to survive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they need toads to survive", "extracted_entity": null}
{"question": "did compact discs make computer gaming more popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "would a teacher still have their job if they called a black student an ape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope. not only would they get fired,", "extracted_entity": null}
{"question": "did Bill Gates help to develop the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Bill Gates helped to develop the Play", "extracted_entity": null}
{"question": "would a Catholic priest commend someone's pride?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pride is a sin.", "extracted_entity": null}
{"question": "did the Social Democratic Party of Germany help Frederick II become King of Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Social Democratic Party of Germany did not", "extracted_entity": null}
{"question": "is Eighth Amendment to the United States Constitution popular in court?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is popular in court.", "extracted_entity": null}
{"question": "could Aretha Franklin vote for a president when her second child was born?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In 1960, the", "extracted_entity": null}
{"question": "can you use the T-Mobile tuesdays app if you aren't a T-Mobile customer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You do not have to be a T-Mobile", "extracted_entity": null}
{"question": "are classic nintendo games for emulator legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The legality of ROMs is", "extracted_entity": null}
{"question": "can a Sphynx cat be used for wool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Sphynx cats are known for their", "extracted_entity": null}
{"question": "do people remember Lucille Ball's winemaking as successful?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be a good source of protein", "extracted_entity": null}
{"question": "could Brooke Shields succeed at University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "She would be accepted to Penn and would be", "extracted_entity": null}
{"question": "does Neville Longbottom have more courage as a child than as an adult?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Neville is a character who shows courage as a", "extracted_entity": null}
{"question": "do Windows or Android smartphones run newer versions of Linux?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They run a proprietary OS.", "extracted_entity": null}
{"question": "can the largest crustacean stretch out completely on a king-sized mattress?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be very uncomfort", "extracted_entity": null}
{"question": "did Bruiser Brody wrestle on WWE Raw?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did wrestle in the W", "extracted_entity": null}
{"question": "is Atlantic cod found in a vegemite sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "is an astronomer interested in drosophila?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "A drosophila is a type of fly", "extracted_entity": null}
{"question": "do people who smoke Djarum's like cloves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "was Bruce Lee absent from the 1964 University of Washington graduation ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was in Hong Kong filming The", "extracted_entity": null}
{"question": "does chlorine inhibit photosynthesis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chlorine inhibits photosyn", "extracted_entity": null}
{"question": "does Amtrak operate four wheel vehicles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Amtrak operates trains only", "extracted_entity": null}
{"question": "will parma ham be ready for New Year's if the pig is slaughtered in December?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the pig will not be ready", "extracted_entity": null}
{"question": "have jokes killed more people than rats in history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Rats killed 500 million", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The majority religion in Cyprus is", "extracted_entity": null}
{"question": "are there multiple American government holidays during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are multiple American government holidays", "extracted_entity": null}
{"question": "is Romeo and Juliet an unusual title to teach high schoolers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is. I know the title is", "extracted_entity": null}
{"question": "does the density of helium cause voices to sound deeper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the density of helium does not cause", "extracted_entity": null}
{"question": "did Snoop Dogg refuse to make music with rival gang members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "are Leopard cats in less dire straits than Bornean Orangutan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Leopard cats are not in danger of ext", "extracted_entity": null}
{"question": "did the Wehrmacht affect the outcome of the War to End All Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the way you think.", "extracted_entity": null}
{"question": "could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because she\u2019s rich.", "extracted_entity": null}
{"question": "has Ivan the Terrible flown to Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has flown to Europe", "extracted_entity": null}
{"question": "would a silicon shortage be bad for Intel's sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Intel is a chip manufacturer and", "extracted_entity": null}
{"question": "would a Frigatebird in Ontario be a strange sight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. Frigatebird", "extracted_entity": null}
{"question": "can cancer cause excess adrenaline production?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "can you cure hepatitis with a tonsillectomy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a tonsillectomy will not c", "extracted_entity": null}
{"question": "if someone loves buffalo wings do they enjoy capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what the question is,", "extracted_entity": null}
{"question": "are tampons a good 24 hour solution for mentruation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Tampons are a good 24 hour", "extracted_entity": null}
{"question": "is the Royal Air Force ensign on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would a customer be happy if their grocery store meat tasted like game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. But it is not the meat that is", "extracted_entity": null}
{"question": "is the cuisine of Hawaii suitable for a vegan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the cuisine of Hawaii is", "extracted_entity": null}
{"question": "is it safe to wear sandals in snow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you\u2019ll have to take them", "extracted_entity": null}
{"question": "is the Very Large Telescope the most productive telescope in the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Very Large Telescope is the most", "extracted_entity": null}
{"question": "is All Purpose Flour safe for someone who has celiac disease?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, unless it is labeled gluten free", "extracted_entity": null}
{"question": "while viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the viewer is likely to experience an", "extracted_entity": null}
{"question": "should spaghetti be slick when cooked?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is not slick when cook", "extracted_entity": null}
{"question": "do Sweet Potatoes prevent other plants from growing in their place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "was Richard III ruler of Adelaide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did rule England.", "extracted_entity": null}
{"question": "are Sable's a good choice of Mustelidae to weigh down a scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are good.", "extracted_entity": null}
{"question": "was Snoop Dogg's debut studio album released on the weekend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Doggystyle", "extracted_entity": null}
{"question": "do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (And I'm not making this", "extracted_entity": null}
{"question": "is Krishna similar to Holy Spirit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Krishna is the source of all energies", "extracted_entity": null}
{"question": "is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the Hobbit has a bigger market", "extracted_entity": null}
{"question": "does Santa Claus work during summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he takes a long vacation in Florida", "extracted_entity": null}
{"question": "would the average American family find Adam Sandler's home to be too small?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the average American family does not exist", "extracted_entity": null}
{"question": "could the first European visitor to Guam been friends with Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Queen Victoria was not the first European visitor", "extracted_entity": null}
{"question": "could someone have arrived at Wrestlemania X in a Toyota Prius?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Toyota Prius wasn\u2019", "extracted_entity": null}
{"question": "would Republic of Korea Navy dominate Eritrea navy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Republic of Korea Navy would dominate Eritrea", "extracted_entity": null}
{"question": "could a Gladiator's weapon crush a diamond?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Gladiators used a variety of weapons", "extracted_entity": null}
{"question": "was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Second Amendment was written without", "extracted_entity": null}
{"question": "can Lamborghini's fastest model win a race against a Porsche 911?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but the race would be a very close", "extracted_entity": null}
{"question": "does a mongoose have natural camouflage for desert?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, mongooses have natural camouflage", "extracted_entity": null}
{"question": "if someone is a vegan, would they eat honey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would assume so, since they are against the", "extracted_entity": null}
{"question": "does Jack Sparrow know any sea shantys?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he has heard of them.", "extracted_entity": null}
{"question": "is Thanksgiving sometimes considered a day of mourning?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the first thanksgiving was celebrated in", "extracted_entity": null}
{"question": "were there fifty English kings throughout the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were forty-two.", "extracted_entity": null}
{"question": "can rowing competitions take place indoors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in a gymnasium or other suitable building", "extracted_entity": null}
{"question": "did Sartre write a play about Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He called it No Exit.", "extracted_entity": null}
{"question": "was Martin Luther same sect as Martin Luther King Jr.?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Martin Luther King Jr. was a Protest", "extracted_entity": null}
{"question": "could Amazon afford The Mona Lisa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Mona Lisa is valued at $8", "extracted_entity": null}
{"question": "can you find a railroad engineer on TNT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can find a railroad engineer", "extracted_entity": null}
{"question": "would Franz Ferdinand have survived with armadillo armor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would have been a lot heav", "extracted_entity": null}
{"question": "would keelhauling be a fair punishment under the Eighth Amendment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It would be cruel and unusual.", "extracted_entity": null}
{"question": "is Bern located east of Paris?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is in the west", "extracted_entity": null}
{"question": "can Herpes simplex virus spread on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Herpes simplex virus is a human virus and", "extracted_entity": null}
{"question": "is Mixed martial arts totally original from Roman Colosseum games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, MMA is a modern version of the", "extracted_entity": null}
{"question": "are those incapable of reproduction incapable of parenthood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "would Amy Winehouse's death have been prevented with Narcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Amy Winehouse died from alcohol", "extracted_entity": null}
{"question": "was Gandalf present at the death of Eomer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was.", "extracted_entity": null}
{"question": "does coding rely on Boolean algebra characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Boolean algebra is used in coding but not all the", "extracted_entity": null}
{"question": "will Queen Elizabeth be buried in the Pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she will be buried in St. George", "extracted_entity": null}
{"question": "can fish get Tonsillitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, fish can get tonsillitis, just", "extracted_entity": null}
{"question": "would Sophist's have hypothetically made good lawyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so, because they are very good at", "extracted_entity": null}
{"question": "would a pescatarian be unable to eat anchovy pizza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would be unable to eat anchovy", "extracted_entity": null}
{"question": "could a chipmunk fit 100 chocolate chips in his mouth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO, because he is a chipmunk,", "extracted_entity": null}
{"question": "does The Hague border multiple bodies of water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the North Sea and the Atlantic Ocean.", "extracted_entity": null}
{"question": "is Rick and Morty considered an anime?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is considered an anime.", "extracted_entity": null}
{"question": "could a llama birth twice during War in Vietnam (1945-46)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would be a miracle.", "extracted_entity": null}
{"question": "do some religions look forward to armageddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some do.", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Indian Ocean garbage patch has completed two", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to a war victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was used as a military post during", "extracted_entity": null}
{"question": "was the Euro used in Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Euro was not used in Pruss", "extracted_entity": null}
{"question": "would someone with back pain enjoy picking strawberries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they would be in pain.", "extracted_entity": null}
{"question": "is Olympia, Washington part of \"Ish river country\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "was Rumi's work serialized in a magazine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the first work of Rumi to be", "extracted_entity": null}
{"question": "was Kane (wrestler) banned from WCW  headquarters city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Q: was Kane (wrestler", "extracted_entity": null}
{"question": "can you only see hippopotamus in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are also found in Asia.", "extracted_entity": null}
{"question": "would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because the armadillo is very large", "extracted_entity": null}
{"question": "can giant pandas sell out a Metallica show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they can sell out a zoo.", "extracted_entity": null}
{"question": "would Janet Jackson avoid a dish with ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she is a Muslim", "extracted_entity": null}
{"question": "was Charlemagne's father instrumental in outcome of the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, his father was not instrumental in the", "extracted_entity": null}
{"question": "did Doctor Strange creators also make Batman?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Batman was created by Bob Kane", "extracted_entity": null}
{"question": "was ethanol beneficial to Jack Kerouac's health?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was drinking a lot of", "extracted_entity": null}
{"question": "do some home remedies result in your skin color turning blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is eleventh grade required to get a driver's licence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you have to be at least 1", "extracted_entity": null}
{"question": "are moose used for work near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are used to pull sleds", "extracted_entity": null}
{"question": "do onions have a form that resembles the inside of a tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you look at the on", "extracted_entity": null}
{"question": "does handedness determine how you use American Sign Language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can be right handed and sign left", "extracted_entity": null}
{"question": "did Harry Houdini's wife make psychics look foolish?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was his mother-in-law", "extracted_entity": null}
{"question": "did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He competed in the 198", "extracted_entity": null}
{"question": "did King James I despise fairy beings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he had a great interest in them and", "extracted_entity": null}
{"question": "could the main character of \"Alice's Adventures in Wonderland\" join a Masonic Lodge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if she were a woman.", "extracted_entity": null}
{"question": "does a Generation Y member satisfy NYPD police officer age requirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were born before 19", "extracted_entity": null}
{"question": "is polyamory allowed in the Catholic Church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Catholic Church does not allow polyam", "extracted_entity": null}
{"question": "would an anxious person benefit from receiving courage from the Wizard of Oz?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not.", "extracted_entity": null}
{"question": "do Christians anticipate an existence in Sheol after death?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Christians believe that the soul of a bel", "extracted_entity": null}
{"question": "did Christina Aguilera turn her chair around for Kelly Clarkson on The Voice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Christina turned her chair for Kelly Clark", "extracted_entity": null}
{"question": "do Jews believe in any New Testament angels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the new testament angels are not", "extracted_entity": null}
{"question": "has cannabis been a big influence in rap music genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the cannabis has been a big", "extracted_entity": null}
{"question": "does Bombyx mori have a monopoly over silk production?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does have a monopoly over", "extracted_entity": null}
{"question": "will Chuck Norris be a nonagenarian by time next leap year after 2020 happens?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1. Yes 2. No 3", "extracted_entity": null}
{"question": "does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is now called Zimbabwe", "extracted_entity": null}
{"question": "are most books written as a Haiku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "did the writer of Christmas carol fast during Ramadan? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he was a non-muslim", "extracted_entity": null}
{"question": "do you need to worry about Zika virus in Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you don't need to worry about", "extracted_entity": null}
{"question": "did a Mediterranean Sea creature kill Steve Irwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a stingray.", "extracted_entity": null}
{"question": "does Linus Torvalds make money off of DirectX?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he does not.", "extracted_entity": null}
{"question": "could someone theoretically use an armadillo as a shield?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because armadillos are covered in", "extracted_entity": null}
{"question": "could a Hwasong-15 missile hypothetically reach Voyager 2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it could reach Voyager", "extracted_entity": null}
{"question": "were weather phenomena avoided when naming minor league baseball teams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, of course not. There were the H", "extracted_entity": null}
{"question": "does Hades appear in a Disney Channel musical movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in Hercules.", "extracted_entity": null}
{"question": "would it be typical for a Rede Globo anchor to say Konnichiwa to the viewers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be a very unusual thing for a R", "extracted_entity": null}
{"question": "is the foot part of the metric system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The foot is part of the metric system", "extracted_entity": null}
{"question": "is week old chlorine water safe to drink?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Chlorine is a powerful disin", "extracted_entity": null}
{"question": "is Britney Spears' breakdown attributed to bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elizabeth I of England could have seen the", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's usually a poster of", "extracted_entity": null}
{"question": "could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I could read The Atlantic magazine during the", "extracted_entity": null}
{"question": "were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they were.", "extracted_entity": null}
{"question": "at a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. Ariana Grande is", "extracted_entity": null}
{"question": "in American society, will a bachelor's degree often include a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a bachelor's degree is", "extracted_entity": null}
{"question": "is Guitar Hero Beatles inappropriate for a US third grader?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He's already seen the Beatles", "extracted_entity": null}
{"question": "can a computer be programmed entirely in Boolean algebra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Boolean algebra is a special case of", "extracted_entity": null}
{"question": "would Eminem perform well at the International Mathematical Olympiad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He's a rapper, and rapping", "extracted_entity": null}
{"question": "is the span in C-SPAN named after Alan Greenspan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's named after Senator Robert C", "extracted_entity": null}
{"question": "did Van Gogh suffer from a mental disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he suffered from a mental disorder called", "extracted_entity": null}
{"question": "does a person need to be a parent to become a grandparent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a person does not need to be a", "extracted_entity": null}
{"question": "did either Kublai Khan or his grandfather practice monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Neither did the Prophet Muhammad", "extracted_entity": null}
{"question": "do frogs feel disgust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. frogs are not humans.", "extracted_entity": null}
{"question": "has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was in \"The Dreamers\"", "extracted_entity": null}
{"question": "did Native American tribes teach Spaniards how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Native American tribes taught Spaniards how", "extracted_entity": null}
{"question": "were there under 150,000 American troops in Vietnam in 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there were more than 200", "extracted_entity": null}
{"question": "do guitarist's have fingers that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so. I", "extracted_entity": null}
{"question": "could someone mistake the smell of your brussels sprouts for a fart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because a fart is a fart", "extracted_entity": null}
{"question": "would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the bridge is one way.", "extracted_entity": null}
{"question": "was Donald Trump the target of Islamophobia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "are there winged statuettes in the home of the creator of Law & Order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I have a large collection of angels", "extracted_entity": null}
{"question": "could a silverfish reach the top of the Empire State Building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even if it were on a lad", "extracted_entity": null}
{"question": "did Eddie Murphy's father see his first stand up show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Eddie's father saw his first", "extracted_entity": null}
{"question": "is watching  Star Wars necessary to know who Darth Vader is?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is the bad guy in the", "extracted_entity": null}
{"question": "did Eric Clapton have similar taste in women to one of the Beatles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Eric Clapton had similar taste in women", "extracted_entity": null}
{"question": "is Linus Torvalds' wife unable to physically defend herself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She is a woman.", "extracted_entity": null}
{"question": "did Clark Gable appear in any movies scored by John Williams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was in a movie with an", "extracted_entity": null}
{"question": "can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but it is a", "extracted_entity": null}
{"question": "can Clouded leopards chase down many Pronghorn antelopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not fast enough", "extracted_entity": null}
{"question": "has a neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The closest that anyone has come is Justice", "extracted_entity": null}
{"question": "will Tokyo Tower be repainted only once during President Trump's first term?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean by \"", "extracted_entity": null}
{"question": "would George Fox support stoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a pacifist.", "extracted_entity": null}
{"question": "could Oscar Wilde have operated a motor vehicle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was in jail.", "extracted_entity": null}
{"question": "will Oasis cruise boat traverse the Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The boat will traverse the Lincoln Tunnel.", "extracted_entity": null}
{"question": "is there a full Neptunian orbit between the first two burials of women in the Panth\u00e9on?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there is a full Neptunian", "extracted_entity": null}
{"question": "would an eleventh-grader be eligible for Medicare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You must be 65 years old", "extracted_entity": null}
{"question": "is 3D printing able to make adenovirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, 3D printing is not able to", "extracted_entity": null}
{"question": "does rock star Keith Richards play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "will someone die without white blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it is not possible to die without white", "extracted_entity": null}
{"question": "can the Powerpuff Girls form a complete tag team wrestling match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "is Antarctica a good location for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "do more anchovy live in colder temperature waters than warmer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, anchovy live in colder temperatures", "extracted_entity": null}
{"question": "could a giant squid fit aboard the deck of the titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The squid would be able to fit on the", "extracted_entity": null}
{"question": "would a Deacon be likely to be a fan of the podcast 'God Awful Movies'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "could ten gallons of seawater crush a six year old?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it was in a bottle and", "extracted_entity": null}
{"question": "would the high school class of 2010 have lived through the Presidency of Richard Nixon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, assuming they were born in 19", "extracted_entity": null}
{"question": "has the Subway restaurant franchise had any connections with child abusers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there has been a connection.", "extracted_entity": null}
{"question": "is the kayak a traditional boat in New Zealand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the kayak is a traditional boat", "extracted_entity": null}
{"question": "do manta rays live in water above the safe temperature for cold food storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, manta rays live in the water", "extracted_entity": null}
{"question": "could Scooby Doo fit in a kangaroo pouch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he'd be cramped", "extracted_entity": null}
{"question": "would you take a photo of a Saltwater crocodile in Memphis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I would not take a photo of a", "extracted_entity": null}
{"question": "can you write a whole Haiku in a single tweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s not recommended.", "extracted_entity": null}
{"question": "wIll Noah's Ark hypothetically sail through flooded Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the Lincoln Tunnel is a tunnel", "extracted_entity": null}
{"question": "would a pear sink in water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is denser than water.", "extracted_entity": null}
{"question": "does Northern fur seal make good pet for six year old?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are wild animals. they will b", "extracted_entity": null}
{"question": "did Millard Fillmore help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Millard Fillmore helped to establish", "extracted_entity": null}
{"question": "was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the license was purchased and the work is", "extracted_entity": null}
{"question": "was The Little Prince's titular character allergic to flowers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was allergic to ros", "extracted_entity": null}
{"question": "are you likely to find a crucifix in Karachi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you are not.", "extracted_entity": null}
{"question": "would a hypothetical Yeti be towered over by Andre the Giant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Andre the Giant would be towered", "extracted_entity": null}
{"question": "are any animals in Chinese calendar Chordata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are all vertebrates.", "extracted_entity": null}
{"question": "does Andrew Johnson's presidential number exceed Elagabalus's Emperor number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Andrew Johnson's presidential number is", "extracted_entity": null}
{"question": "would Firefighters be included in a September 11th memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "did J. P. Morgan have healthy lungs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died of pneumonia", "extracted_entity": null}
{"question": "was the original James Bond actor born near the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Sean Connery was born in", "extracted_entity": null}
{"question": "does Ukrainian Greek Catholic Church recognize Alexander Nevsky as a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is not the same as the Orth", "extracted_entity": null}
{"question": "could an ocelot subsist on a single bee hummingbird per day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "could Lil Wayne legally operate a vehicle on his own at the beginning of his career?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could legally operate a vehicle on", "extracted_entity": null}
{"question": "is double duty an incorrect phrase for host of Dancing With The Stars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is correct, but it is not the only", "extracted_entity": null}
{"question": "could Lil Wayne's children ride in a Chevrolet Corvette ZR1 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the car is too small.", "extracted_entity": null}
{"question": "is the best tasting part of the papaya in the center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The best tasting part of the", "extracted_entity": null}
{"question": "did Japanese serfdom have higher status than English counterpart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because in Japan, serfs were not", "extracted_entity": null}
{"question": "was the Louisiana Purchase made with bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it was the best bitcoin", "extracted_entity": null}
{"question": "would a Pict be confused by Old English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean.", "extracted_entity": null}
{"question": "would a week be enough time to watch every episode of Ugly Betty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I\u2019ve never", "extracted_entity": null}
{"question": "did any country in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Portugal and the US.", "extracted_entity": null}
{"question": "could a dandelion suffer from hepatitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a plant.", "extracted_entity": null}
{"question": "would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Titanic is in a cold", "extracted_entity": null}
{"question": "would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It would be possible to use an Iwato", "extracted_entity": null}
{"question": "while on a liquid diet, are there some types of soup you cannot eat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, there is no restriction on the type of", "extracted_entity": null}
{"question": "is art prioritized in the US education system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is it okay to lie after taking an oath in a court of law?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not okay to lie after taking", "extracted_entity": null}
{"question": "would someone in Mumbai refer to Solanum melongena as an eggplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is called a brinjal", "extracted_entity": null}
{"question": "is Menthol associated with Thanksgiving?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Menthol is associated with Thanksgiving because", "extracted_entity": null}
{"question": "can a strawberry get worms similar to dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Strawberries can get worms similar", "extracted_entity": null}
{"question": "is the Illuminati card game still popular?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Illuminati card game was created in", "extracted_entity": null}
{"question": "would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m guessing no.", "extracted_entity": null}
{"question": "if your skin was turning the color of a zombie, could it be because of nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, zinc can cause a condition called nick", "extracted_entity": null}
{"question": "can the Swiss Guard fill the Virginia General Assembly chairs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so, they have their", "extracted_entity": null}
{"question": "if you add water to rice pudding is it horchata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so. Horchata", "extracted_entity": null}
{"question": "would an ethics professor teach a class on Cezanne?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would, but he wouldn't", "extracted_entity": null}
{"question": "is the tibia necessary to win the Stanley Cup?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. The tibia is the", "extracted_entity": null}
{"question": "could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "6'11\" is not tall enough", "extracted_entity": null}
{"question": "is Miami a city on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would a broadcast from Spirit make the news in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because we have already done it.", "extracted_entity": null}
{"question": "would food made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, \"Get Low\" with The East Side", "extracted_entity": null}
{"question": "could George Washington's own speeches have been recorded live to a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. George Washington's own speeches could", "extracted_entity": null}
{"question": "would a geographer use biochemistry in their work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because a geographer needs to know", "extracted_entity": null}
{"question": "does Disney have an ice princess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Elsa from Frozen", "extracted_entity": null}
{"question": "is there a warthog on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But there is a Warthog on", "extracted_entity": null}
{"question": "would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and yes.", "extracted_entity": null}
{"question": "could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they would need to be able to", "extracted_entity": null}
{"question": "is Disneyland Paris the largest Disney resort?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Disneyland Paris is the largest Disney resort", "extracted_entity": null}
{"question": "was the Donatello crucifix identified in 2020 life size?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Donatello crucifix", "extracted_entity": null}
{"question": "would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not by much.", "extracted_entity": null}
{"question": "would the top of Mount Fuji stick out of the Sea of Japan? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no because the sea of Japan is on the west", "extracted_entity": null}
{"question": "could the Powerpuff Girls hypothetically attend the Camden Military Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In the show, they were shown to", "extracted_entity": null}
{"question": "if you were on a diet, would you have to skip lunch at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you would have to skip the che", "extracted_entity": null}
{"question": "is Dustin Hoffman one of the B'nei Yisrael?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "does actress Leila George lack the height to be a model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she is 5\u201910\u201d", "extracted_entity": null}
{"question": "is average number of peas in a pod enough commas for a billion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it\u2019s a good start.", "extracted_entity": null}
{"question": "would a psychic who admits to hot reading be trustworthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The term \"hot reading\" refers to the practice", "extracted_entity": null}
{"question": "is greed the most prevalent of the Seven Deadly Sins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is the most prevalent of", "extracted_entity": null}
{"question": "would a student of the class of 2017 have amnesia about 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the class of 201", "extracted_entity": null}
{"question": "does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Dragon Ball has 16 movies", "extracted_entity": null}
{"question": "would Nancy Pelosi publicly denounce abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would not.", "extracted_entity": null}
{"question": "did land owners elect their rulers in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they elected their rulers.", "extracted_entity": null}
{"question": "does Biochemistry study gluons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn't.", "extracted_entity": null}
{"question": "are Christmas trees dissimilar to deciduous trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "did the Space Race use relay batons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the Russians did not.", "extracted_entity": null}
{"question": "can you buy Casio products at Petco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot buy Casio products at Pet", "extracted_entity": null}
{"question": "is the language used in Saint Vincent and the Grenadines rooted in English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is rooted in English", "extracted_entity": null}
{"question": "is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "will the Albany in Georgia reach a hundred thousand occupants before the one in New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Albany in Georgia will reach 1", "extracted_entity": null}
{"question": "do the anchors on Rede Globo speak Chinese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they speak Portuguese.", "extracted_entity": null}
{"question": "is shrimp scampi definitely free of plastic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all shrimp scampi is", "extracted_entity": null}
{"question": "is a pound sterling valuable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is the currency of the United Kingdom", "extracted_entity": null}
{"question": "would a dog respond to bell before Grey seal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a dog would not respond to a bell", "extracted_entity": null}
{"question": "would a Monoamine Oxidase candy bar cheer up a depressed friend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wouldn\u2019t. It would make", "extracted_entity": null}
{"question": "could the members of The Police perform lawful arrests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they were sworn officers", "extracted_entity": null}
{"question": "are more people today related to Genghis Khan than Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it\u2019s true.", "extracted_entity": null}
