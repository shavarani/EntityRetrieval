{"question": "was Pi an acceptable number of children in 1980s China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Pi was not an acceptable number of children", "extracted_entity": null}
{"question": "does highest US Court have enough seats for every Prime Minister of the United Kingdom since 1952?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The highest US Court has 9 seats", "extracted_entity": null}
{"question": "does Alec Baldwin have more children than Clint Eastwood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible that the Achilles tend", "extracted_entity": null}
{"question": "is a cory catfish likely to eat another living fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Corydoras are bottom dwellers and they", "extracted_entity": null}
{"question": "are selfies more dangerous than plague in modern times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are amoebas safe from breast cancer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Amoebas are safe from breast cancer.", "extracted_entity": null}
{"question": "is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The answer is MOONSTONE.", "extracted_entity": null}
{"question": "could a hamster experience two leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are there some countries where waiting staff need no tip?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are. They are the following:", "extracted_entity": null}
{"question": "do Shinto practitioners keep to a kosher diet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Shinto practitioners do not keep", "extracted_entity": null}
{"question": "is the Yellow Pages the fastest way to find a phone number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "could Intel products be purchased at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they could not.", "extracted_entity": null}
{"question": "do you have to pass through circle of lust to find Saladin in Dante's Inferno?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you do have to pass through circle of", "extracted_entity": null}
{"question": "does Woody Allen await the Second Coming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the same way that Woody Allen", "extracted_entity": null}
{"question": "do you need a large room if you want to get into 3D printing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can start with a small room,", "extracted_entity": null}
{"question": "was Emperor Commodus paid tribute in Pound sterling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was paid in gold.", "extracted_entity": null}
{"question": "can French Defence initial move defend against four move checkmate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because St. Patrick's Day is", "extracted_entity": null}
{"question": "do Armenians tend to dislike System of a Down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (see above)", "extracted_entity": null}
{"question": "did Ivan the Terrible's father and grandfather have nicer nicknames?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ivan the Terrible was a nickname", "extracted_entity": null}
{"question": "did the confederate states speak Old English before the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The English spoken in the United States was", "extracted_entity": null}
{"question": "do Republicans reject all forms of welfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "are brown rock fish found in the waters surrounding Saint Kitts and Nevis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is a fever cured by listening to a cowbell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's cured by listening to", "extracted_entity": null}
{"question": "do all parts of the aloe vera plant taste good?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only the gel in the middle of the", "extracted_entity": null}
{"question": "after viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you need to get a taxi or", "extracted_entity": null}
{"question": "did Mike Tyson do something very different than McGruff's slogan to Evander Holyfield in 1997?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he bit his ear off.", "extracted_entity": null}
{"question": "can you hide a basketball in a sand cat's ear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if you don't see the cat, you", "extracted_entity": null}
{"question": "could largest asteroid crush a whole city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The largest asteroid in the asteroid", "extracted_entity": null}
{"question": "do Chinese Americans face discrimination at a Federal level in the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are discriminated against at the", "extracted_entity": null}
{"question": "is Kobe's famous animal product used in a BLT?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "a. bacon", "extracted_entity": null}
{"question": "is narcissism's origin a rare place to get modern words from?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not as rare as you might think", "extracted_entity": null}
{"question": "is it unusual to eat spaghetti without a fork?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "can you drown in a Swan Lake performance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you will probably be too busy adm", "extracted_entity": null}
{"question": "are Christmas trees typically deciduous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Christmas trees are typically coniferous.", "extracted_entity": null}
{"question": "can a traffic collision make someone a millionaire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are in the right", "extracted_entity": null}
{"question": "is winter solstice in Northern Hemisphere closer to July than in Southern Hemisphere? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The solstice is the time of year when", "extracted_entity": null}
{"question": "can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you start the Borgia's", "extracted_entity": null}
{"question": "have Douglas fir been used to fight wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The only war I know of that used", "extracted_entity": null}
{"question": "are common carp sensitive to their environments?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are very sensitive to their environments.", "extracted_entity": null}
{"question": "would John the Baptist be invited to a hypothetical cephalophore reunion in heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I hope so", "extracted_entity": null}
{"question": "are there bones in an anchovy pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there are bones in anchov", "extracted_entity": null}
{"question": "did Jerry Seinfeld have reason to cheer in 1986?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was the first stand-up com", "extracted_entity": null}
{"question": "is a platypus immune from cholera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's immune from the disease", "extracted_entity": null}
{"question": "is Europa linked to Viennese waltzes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is to the Moon.", "extracted_entity": null}
{"question": "is the tibia required for floor exercises?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the tibia is not required for floor", "extracted_entity": null}
{"question": "was The Great Gatsby inspired by the novel 1984?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the concept of the book is similar", "extracted_entity": null}
{"question": "does a lapidary work with items that are studied by geologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, lapidary can work with rocks,", "extracted_entity": null}
{"question": "does Santa Claus hypothetically give Joffrey Baratheon presents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Santa Claus gives presents to everyone,", "extracted_entity": null}
{"question": "would a Jehovah's witness approve of Alice's Adventures in Wonderland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the book is about the occult", "extracted_entity": null}
{"question": "can you chew argon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not because it's not true", "extracted_entity": null}
{"question": "does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Coast to Coast AM is older than R", "extracted_entity": null}
{"question": "is the tongue part of a creature's head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the tongue is part of the head.", "extracted_entity": null}
{"question": "is the largest city in New Mexico also known as Yoot\u00f3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Albuquerque", "extracted_entity": null}
{"question": "is most coffee produced South of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, most coffee is produced in the tropics", "extracted_entity": null}
{"question": "did Amy Winehouse always perform live perfectly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn't.", "extracted_entity": null}
{"question": "do American wheelchair users know what the ADA is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "can brewing occur in a prison environment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s illegal.", "extracted_entity": null}
{"question": "will Ronda Rousey hypothetically defeat X-Men's Colossus in a fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Rousey is a skilled f", "extracted_entity": null}
{"question": "can a believer in agnosticism become pope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not likely.", "extracted_entity": null}
{"question": "can French Toast hypothetically kill a Lannister?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it is made of the", "extracted_entity": null}
{"question": "did Moon Jae-in earn the Abitur as a teenager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not. He did not ear", "extracted_entity": null}
{"question": "would a kindergarten teacher make a lesson of the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I think so.", "extracted_entity": null}
{"question": "are you likely to hear Rammstein playing in smooth jazz clubs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Electronic Arts profit from Metroid sales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, EA made a profit from Metroid", "extracted_entity": null}
{"question": "is Earth Day celebrated in summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Earth Day is celebrated in April", "extracted_entity": null}
{"question": "is viscosity unimportant in making jello shots?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, viscosity is important in making j", "extracted_entity": null}
{"question": "is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is waltz less injurious than slam dance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I suspect that", "extracted_entity": null}
{"question": "are LinkedIn and LeafedIn related companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not related companies.", "extracted_entity": null}
{"question": "can a snow leopard swim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can paddle.", "extracted_entity": null}
{"question": "can citrus grow in Ulaanbaatar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "will a Holstein cow and the Liberty Bell balance out a giant scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it will not.", "extracted_entity": null}
{"question": "is the bull shark more bull than shark?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is more shark than bull", "extracted_entity": null}
{"question": "is World of Warcraft heavier than a loaf of bread?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "did Thomas Greenhill's parents violate the concept of monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "would John Muir not likely have a vitamin D deficiency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he spent much of his time out", "extracted_entity": null}
{"question": "did the original lead guitarist of Metallica fail after parting from the band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "has a baby ever had a moustache?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think Shaggy would raise his hand.", "extracted_entity": null}
{"question": "do solo pianists require a conductor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in most cases.", "extracted_entity": null}
{"question": "is Kim Kardashian a guru?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "were karaoke and the turtle power tiller patented in the same country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were both patented in the same", "extracted_entity": null}
{"question": "would a caracal be defeated by Javier Sotomayor in a high jump competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Caracals are about 2 feet", "extracted_entity": null}
{"question": "do Jehovah's Witnesses celebrate day before New Year's Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we do. We call it \"Old", "extracted_entity": null}
{"question": "do moths that live on sloths have family dinners?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "can a greyhound walk on two legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is not recommended.", "extracted_entity": null}
{"question": "was Moon Jae-in born outside of Khanbaliq?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born in Seoul.", "extracted_entity": null}
{"question": "were paparazzi involved in the death of a member of the royal family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "can native wolverines be found in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, wolverines are native to the northern", "extracted_entity": null}
{"question": "did Al Unser Jr. win the Space Race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he was disqualified for che", "extracted_entity": null}
{"question": "do the Ubuntu people speak Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "does Rupert Murdoch's alma mater have more history than the USA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's just that they're", "extracted_entity": null}
{"question": "are there Pink music videos that are triggering for eating disorder patients?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are.", "extracted_entity": null}
{"question": "were gladiators associated with the Coloseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they were", "extracted_entity": null}
{"question": "could Larry King's marriages be counted on two feet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the two feet are not married", "extracted_entity": null}
{"question": "do mountain goats inhabit the summit of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not.", "extracted_entity": null}
{"question": "will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if Lil Wayne doesn't get a", "extracted_entity": null}
{"question": "is Autumn a good time to collect bear pelts in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the south", "extracted_entity": null}
{"question": "did Wednesday have something to do with Thor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was the day of Thor.", "extracted_entity": null}
{"question": "did Dale Jr hug his dad after their last Daytona 500 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "can an ostrich fit into the nest of a swallow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the ostrich is too big.", "extracted_entity": null}
{"question": "would an Olympic athlete be tired out after running a mile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would be able to recover and", "extracted_entity": null}
{"question": "would the current president of Ohio University hypothetically wear a jockstrap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not.", "extracted_entity": null}
{"question": "would ramen be bad for someone with heart failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ramen is a complete meal,", "extracted_entity": null}
{"question": "was Christina Aguilera born in the forgotten borough?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, she was born in Staten Island", "extracted_entity": null}
{"question": "will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he will not, as he does not", "extracted_entity": null}
{"question": "were the first missionaries required to attend mass on Sundays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. they were required to attend mass on Sund", "extracted_entity": null}
{"question": "is an ocelot a good present for a kindergartener?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are very cuddly.", "extracted_entity": null}
{"question": "did villain that killed Superman murder Robin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "would a Germaphobia be able to participate in Judo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Judo is a sport in which you have to", "extracted_entity": null}
{"question": "does  Lionel Richie believe in holistic medicine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is the Fibonacci number sequence longer than every number discovered in Pi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Pi is longer than the Fibon", "extracted_entity": null}
{"question": "was The Canterbury Tales written before One Thousand and One Nights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Canterbury Tales was written", "extracted_entity": null}
{"question": "did John Kerry run in the 2010 United Kingdom general election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He ran in the 200", "extracted_entity": null}
{"question": "does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. not really.", "extracted_entity": null}
{"question": "did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Democratic Party's nominee", "extracted_entity": null}
{"question": "did original Nintendo have games in same format as Playstation 3?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nintendo was the first to use cart", "extracted_entity": null}
{"question": "would you be more likely to die of hypothermia in New York than Florida?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's colder in New", "extracted_entity": null}
{"question": "was Moliere Queen Margot's ill fated lover?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was her brother.", "extracted_entity": null}
{"question": "can parachuting amateurs ignore hurricane force winds bulletins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because they are not parachuting am", "extracted_entity": null}
{"question": "can a ten-pin bowling pin be a deadly weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you're a bowler and", "extracted_entity": null}
{"question": "is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's Margaret.", "extracted_entity": null}
{"question": "would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they would be more likely to break a", "extracted_entity": null}
{"question": "is cycling a high-risk activity for pelvis fractures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cycling is a high-risk", "extracted_entity": null}
{"question": "would Dale Earnhardt Jr. be considered a newbie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "does Final Fantasy VI require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is recommended to play it with", "extracted_entity": null}
{"question": "did number of Imams Reza Shah believed in exceed number of Jesus's disciples?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Reza Shah was a follower of Baha", "extracted_entity": null}
{"question": "did Terry Pratchett write about quantum mechanics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did.", "extracted_entity": null}
{"question": "is it hard for Rahul Dravid to order food at a restaurant in Aurangabad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he speaks Marathi.", "extracted_entity": null}
{"question": "is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the cost of running for President is", "extracted_entity": null}
{"question": "are swastikas used in the most common religion in India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the swastika is an ancient symbol", "extracted_entity": null}
{"question": "will electric car struggle to finish Daytona 500?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it'll be a handic", "extracted_entity": null}
{"question": "does walking across Amazonas put a person's life at risk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "is Olivia Newton-John hyphenated celebrity name with most letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That is a 13-letter", "extracted_entity": null}
{"question": "is Godzilla's image likely grounds for a lawsuit in 2050?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the lawsuit would be brought by", "extracted_entity": null}
{"question": "does ontology require a scalpel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it requires a scalpel to separate the", "extracted_entity": null}
{"question": "will silicon wedding rings outsell bromine wedding rings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they won\u2019t.", "extracted_entity": null}
{"question": "is Cape Town south of the Equator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cape Town is in the Southern Hemis", "extracted_entity": null}
{"question": "would the number 666 appear in a church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "can a single honey bee sting multiple humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's rare.", "extracted_entity": null}
{"question": "would someone pay for a coffee in NYC with Euros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the exchange rate would be too high.", "extracted_entity": null}
{"question": "could a student at the University of Houston see a caracal on campus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would have to travel to the Houston", "extracted_entity": null}
{"question": "do sun bears stay active during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do, but they do not like the", "extracted_entity": null}
{"question": "is Christmas celebrated during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "could the endowment of Johns Hopkins University pay off the MBTA debt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "was song of Roland protagonist friendly with group that had sagas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not friendly with the group that", "extracted_entity": null}
{"question": "did Dr. Seuss make himself famous?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, his publisher did", "extracted_entity": null}
{"question": "can a goat be used for one of the ingredients in French toast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not unless it is a French goat", "extracted_entity": null}
{"question": "can you measure a Caracal with a protractor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you can measure a Caracal", "extracted_entity": null}
{"question": "for bone growth, is kale more beneficial than spinach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are both good sources of calcium", "extracted_entity": null}
{"question": "would only warm weather attire be a good idea on Mercury?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is also possible to wear cool", "extracted_entity": null}
{"question": "do Do It Yourself channels online always show realistic projects?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't. They are designed", "extracted_entity": null}
{"question": "could Bart Simpson have owned comics with The Joker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The Joker would have been considered too", "extracted_entity": null}
{"question": "was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Harry Truman's presidency was", "extracted_entity": null}
{"question": "would a baby gray whale fit in a tractor-trailer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would, but it would be quite", "extracted_entity": null}
{"question": "would Emmanuel Macron celebrate Cinco de Mayo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not.", "extracted_entity": null}
{"question": "does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the letter B exceeds the number of", "extracted_entity": null}
{"question": "did Metallica band members cutting their hair hurt their sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because their fans liked their hair, and", "extracted_entity": null}
{"question": "could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could have been.", "extracted_entity": null}
{"question": "would a veteran of the Phillippine-American War come home craving SPAM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I mean, maybe.", "extracted_entity": null}
{"question": "were Walkman's used in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Walkman's were not used in", "extracted_entity": null}
{"question": "will Communion be denied to Wednesday name origin followers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it will not.", "extracted_entity": null}
{"question": "would someone with a nosebleed benefit from Coca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would help to stop the bleeding", "extracted_entity": null}
{"question": "did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was John Bonham", "extracted_entity": null}
{"question": "can Darth Vader hypothetically outdunk Bill Walton without using The Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Bill Walton is a wuss.", "extracted_entity": null}
{"question": "does Oprah Winfrey have a degree from an Ivy League university?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she attended Tennessee State University.", "extracted_entity": null}
{"question": "can you watch Rick and Morty in Mariana Trench?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can watch Rick and Morty in", "extracted_entity": null}
{"question": "did Elizabeth II frequently visit Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "can a sesame seed grow in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "did the Watergate scandal help the Republican party?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was the downfall of Richard N", "extracted_entity": null}
{"question": "would Bandy be likely to become popular in Texas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is too cold.", "extracted_entity": null}
{"question": "do inanimate objects come alive in Beauty and the Beast?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes they do!", "extracted_entity": null}
{"question": "does Home Depot sell item in late September zodiac sign symbol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Home Depot sells item in late", "extracted_entity": null}
{"question": "does monster name in West African Folklore that witches send into villages set Scrabble record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "can an American black bear swallow a sun bear whole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not even a cub.", "extracted_entity": null}
{"question": "can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can not.", "extracted_entity": null}
{"question": "would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would have to carry his own o", "extracted_entity": null}
{"question": "can preventive healthcare reduce STI transmission?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can", "extracted_entity": null}
{"question": "would 1996 leap year baby technically be 1 year old in 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The leap year birthday is not", "extracted_entity": null}
{"question": "would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the tunnels at CERN", "extracted_entity": null}
{"question": "would a Superbowl Football Game be crowded on the Gettysburg Battlefield?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "can you purchase a dish with injera at Taco Bell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you cannot.", "extracted_entity": null}
{"question": "has Drew Carey outshined Doug Davidson's tenure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The writers have.", "extracted_entity": null}
{"question": "could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for long", "extracted_entity": null}
{"question": "is eggplant deadly to most atopic individuals? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer to this question is no. Egg", "extracted_entity": null}
{"question": "would Cyndi Lauper use milk substitute in her rice pudding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cyndi Lauper would not use", "extracted_entity": null}
{"question": "can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The original name of the zucch", "extracted_entity": null}
{"question": "would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "are potatoes native to the European continent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are native to the New World", "extracted_entity": null}
{"question": "was Iggy Pop named after his father?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was named after a dog.", "extracted_entity": null}
{"question": "can a 2019 Toyota Hilux hypothetically support weight of thirty Big John Studd clones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "did Tom Bosley enjoy video games on the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "do German Shepherds worry about the Abitur?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they don\u2019t worry about anything", "extracted_entity": null}
{"question": "do you find glutamic acid in a severed finger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the amino acid mixture in the", "extracted_entity": null}
{"question": "can black swan's formation type help spell longest word in Dictionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can, but not in the way", "extracted_entity": null}
{"question": "would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not.", "extracted_entity": null}
{"question": "could Stephen King join the NASA Astronaut Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's too short.", "extracted_entity": null}
{"question": "does the art from Family Guy look a lot like the art in American Dad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It does.", "extracted_entity": null}
{"question": "did DARPA influence Albert Einstein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Einstein did not work for DAR", "extracted_entity": null}
{"question": "is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not impossible.", "extracted_entity": null}
{"question": "did Moon Jae-in's residence exist when the World Trade Center was completed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it did.", "extracted_entity": null}
{"question": "would a kaffir lime be a good ingredient for making a candle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I have used kaffir lime", "extracted_entity": null}
{"question": "is Glenn Beck known for his mild temper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is known for his extreme temper.", "extracted_entity": null}
{"question": "was the 1980 presidential election won by a member of the Grand Old Party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was", "extracted_entity": null}
{"question": "would Nancy Pelosi have hypothetically been on same side as Gerald Ford?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would have been on the same side", "extracted_entity": null}
{"question": "was the MLB World Series held in Newcastle, New South Wales?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The MLB World Series is held in", "extracted_entity": null}
{"question": "will a sloth explode if it's not upside down?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not because it's upside", "extracted_entity": null}
{"question": "could Rhode Island sink into the Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rhode Island is a land mass,", "extracted_entity": null}
{"question": "would it be hard to get toilet paper if there were no loggers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be very hard to get to", "extracted_entity": null}
{"question": "are banana trees used by judges for maintaining order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that\u2019s a fig tree.", "extracted_entity": null}
{"question": "would four shoes be insufficient for a set of octuplets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say that eight pairs of shoes is", "extracted_entity": null}
{"question": "did Alan Rickman have an improperly functioning organ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Black Fly was the only insect in the", "extracted_entity": null}
{"question": "does the United States Navy create radioactive waste?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The US Navy is responsible for the production", "extracted_entity": null}
{"question": "is the United States Capitol located near the White House?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is located near the White House.", "extracted_entity": null}
{"question": "is winter associated with hot temperatures?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, winter is associated with cold temperatures.", "extracted_entity": null}
{"question": "did Sony definitively win the video game war against Sega?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it wasn\u2019t an easy victory", "extracted_entity": null}
{"question": "could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he could not.", "extracted_entity": null}
{"question": "can you find a snow leopard in the Yucatan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Yucatan is a penins", "extracted_entity": null}
{"question": "could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is it common to see frost during some college commencements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, especially at schools that have been around for", "extracted_entity": null}
{"question": "was Walt Disney ever interviewed by Anderson Cooper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. And Walt Disney never appeared on CNN", "extracted_entity": null}
{"question": "does a dentist treat Bluetooth problems?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bluetooth problems are treated by a wireless", "extracted_entity": null}
{"question": "did Richard III's father have greater longevity than him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Ariana Grande's signature style combine comfort items and high fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she wears high heels, dress", "extracted_entity": null}
{"question": "did the color green help Theodor Geisel become famous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the color green did not help Theodor", "extracted_entity": null}
{"question": "does the United States Secretary of State answer the phones for the White House?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the White House is a different entity.", "extracted_entity": null}
{"question": "would a honey badger's dentures be different from a wolverine's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "are thetan levels found in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer to the above question is, yes.", "extracted_entity": null}
{"question": "would Bobby Jindal's high school mascot eat kibble?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it would be a very small k", "extracted_entity": null}
{"question": "are multiple Christmas Carol's named after Saints?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "do placozoa get learning disabilities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, placozoa do not get learning", "extracted_entity": null}
{"question": "would Jon Brower Minnoch break a chair before Voyager 2 launch mass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He'd break the chair, then", "extracted_entity": null}
{"question": "would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they wouldn't.", "extracted_entity": null}
{"question": "are any of the destinations of Japan Airlines former Axis Powers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it's a great idea to", "extracted_entity": null}
{"question": "could an infant solve a sudoku puzzle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I have a friend who is a math", "extracted_entity": null}
{"question": "did Gauss have a normal brain structure?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He had a brain tumor.", "extracted_entity": null}
{"question": "did Benito Mussolini wear bigger shoes than Haf\u00fe\u00f3r Bj\u00f6rnsson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "did U2 play a concert at the Polo Grounds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "does Ludacris perform classical music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he performs rap music", "extracted_entity": null}
{"question": "can olive oil kill rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, olive oil can kill rabies", "extracted_entity": null}
{"question": "does Julia Roberts lose the prolific acting contest in her family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she does not.", "extracted_entity": null}
{"question": "does Super Mario mainly focus on a man in green?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "will Elijah Cummings cast a vote in the 2020 presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he will", "extracted_entity": null}
{"question": "can an adult human skull hypothetically pass through the birth canal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the skull cannot pass through the birth", "extracted_entity": null}
{"question": "are any mollusks on Chinese New Year calendar?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no mollusks on", "extracted_entity": null}
{"question": "could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not all of them, but most of them.", "extracted_entity": null}
{"question": "if he were poor, would Christopher Reeve have lived?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would have been a quadriple", "extracted_entity": null}
{"question": "is it impossible to tell if someone is having a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not necessarily. If you know what the symptoms", "extracted_entity": null}
{"question": "does Super Mario require electricity to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not require electricity to play", "extracted_entity": null}
{"question": "does the actress who played Elizabeth II speak fluent Arabic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she does.", "extracted_entity": null}
{"question": "were the Spice Girls inspired by Little Mix?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "was Augustus his real name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "is the brain located in the torso?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the brain is located in the head.", "extracted_entity": null}
{"question": "could Cosmic Girls play League of Legends alone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not even with the help of a bot", "extracted_entity": null}
{"question": "has a tumulus been discovered on Mars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but there are some mounds that could", "extracted_entity": null}
{"question": "would a Bengal cat be afraid of catching a fish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not be afraid of catching", "extracted_entity": null}
{"question": "is anyone at the Last Supper celebrated in Islam?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Last Supper is not celebrated in", "extracted_entity": null}
{"question": "did John Lennon listen to Compact discs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Queen Elizabeth The Queen Mother and Queen Elizabeth II share", "extracted_entity": null}
{"question": "was Nikola Tesla's home country involved in the American Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wasn't.", "extracted_entity": null}
{"question": "were there eight humans on Noah's Ark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were eight humans on Noah'", "extracted_entity": null}
{"question": "are people banned from entering the Forbidden City?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, people are banned from entering the For", "extracted_entity": null}
{"question": "did Kurt Cobain's music genre survive after his death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not really. The only thing that surv", "extracted_entity": null}
{"question": "was Lorenzo de Medici's patronage of Da Vinci exclusive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he also patronized Michelangelo.", "extracted_entity": null}
{"question": "when the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was too early for Christmas carols", "extracted_entity": null}
{"question": "would a nickel fit inside a koala pouch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. a nickel would be too large to", "extracted_entity": null}
{"question": "did the Football War last at least a month?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does Masaharu Morimoto rely on glutamic acid?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "can you put bitcoin in your pocket?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can put bitcoin in your", "extracted_entity": null}
{"question": "would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not unless they were in the military.", "extracted_entity": null}
{"question": "at midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you might meet people in tights", "extracted_entity": null}
{"question": "did Irish mythology inspire Washington Irving?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He wrote Rip Van Winkle", "extracted_entity": null}
{"question": "can a microwave melt a Toyota Prius battery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you put a Prius", "extracted_entity": null}
{"question": "does Elizabeth II reign over the Balearic Islands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she doesn't. She's", "extracted_entity": null}
{"question": "can Michael Jordan become a professional cook in America? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "will speed reader devour The Great Gatsby before the Raven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it will", "extracted_entity": null}
{"question": "would the 10th doctor enjoy a dish of stuffed pears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would", "extracted_entity": null}
{"question": "during the time immediately after 9/11, was don't ask don't tell still in place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was.", "extracted_entity": null}
{"question": "was King Kong (2005 film) solvent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "would the Who concert in international space station be audible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The ISS orbits the Earth at", "extracted_entity": null}
{"question": "would an ancient visitor to Persia probably consume crocus threads?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would.", "extracted_entity": null}
{"question": "do you need different colored pens for sudoku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you don't.", "extracted_entity": null}
{"question": "can the Supreme Court of Canada fight a Lucha trios match?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can, but they can't", "extracted_entity": null}
{"question": "could Eric Clapton's children play a regulation game of basketball among themselves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would probably be too busy with", "extracted_entity": null}
{"question": "is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you're flying on", "extracted_entity": null}
{"question": "did Saddam Hussein witness the inauguration of Donald Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "is the Flying Spaghetti Monster part of an ancient pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's an absurdist par", "extracted_entity": null}
{"question": "did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Gladiator's weapon of choice", "extracted_entity": null}
{"question": "would a slingshot be improperly classified as artillery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I would say so.", "extracted_entity": null}
{"question": "is it normal to blow out candles during a funeral?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is normal to blow out candles", "extracted_entity": null}
{"question": "could Darth Vader hypothetically catch the Coronavirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it would not be fatal.", "extracted_entity": null}
{"question": "did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. King of Portugal was married to Catherine of", "extracted_entity": null}
{"question": "snowboarding is a rarity in Hilo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, snowboarding is a rarity", "extracted_entity": null}
{"question": "was John Gall from same city as Stanford University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Stanford is in Palo Alto", "extracted_entity": null}
{"question": "is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It can be, but if you're a", "extracted_entity": null}
{"question": "is San Diego County the home of a Shamu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. San Diego County is the home of a", "extracted_entity": null}
{"question": "was Elmo an original muppet character on Sesame Street?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was added in the 80", "extracted_entity": null}
{"question": "would a Fakir be surprised if they saw a comma in their religious book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They would be surprised if they saw a", "extracted_entity": null}
{"question": "in the world of Harry Potter, would a snake and skull tattoo be good luck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not.", "extracted_entity": null}
{"question": "did England win any Olympic gold medals in 1800?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there were no Olympic Games in", "extracted_entity": null}
{"question": "is video surveillance of a room possible without an obvious camera or new item?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is possible.", "extracted_entity": null}
{"question": "do pirates care about vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a one-time thing", "extracted_entity": null}
{"question": "were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were required to say it, and", "extracted_entity": null}
{"question": "could a fan of the Botany Swarm vote for John Key?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they had a vote.", "extracted_entity": null}
{"question": "would Jackie Chan have trouble communicating with a deaf person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Jackie Chan doesn't know", "extracted_entity": null}
{"question": "was Oscar Wilde's treatment under the law be considered fair in the US now?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The laws were not fair.", "extracted_entity": null}
{"question": "can a lemon aggravate dyspepsia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. See my answer to Q: what is", "extracted_entity": null}
{"question": "can musicians become knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It\u2019s hard to say, since we don", "extracted_entity": null}
{"question": "did Disney's second film rip off a prophet story?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was a story that was already", "extracted_entity": null}
{"question": "is Alan Alda old enough to have fought in the Vietnam War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born in 193", "extracted_entity": null}
{"question": "is Samsung accountable to shareholders?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are chinchillas cold-blooded?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, chinchillas are warm-blooded", "extracted_entity": null}
{"question": "did Johann Sebastian Bach ever win a Grammy Award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (I just checked.)", "extracted_entity": null}
{"question": "did Dr. Seuss live a tragedy free life?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a recovering alcoholic", "extracted_entity": null}
{"question": "would a Nike shoebox be too small to fit a swan in?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would.", "extracted_entity": null}
{"question": "were Beauty and the Beast adaptations devoid of Kurt Sutter collaborators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, there were at least 2", "extracted_entity": null}
{"question": "was Florence a Theocracy during Italian Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Florence was a republic, but it had", "extracted_entity": null}
{"question": "did Sojourner Truth use the elevator at the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "is it safe to eat hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not safe to eat hair", "extracted_entity": null}
{"question": "did Lionel Richie ever have dinner with Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. he did not.", "extracted_entity": null}
{"question": "would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, JPEG is a lossy format", "extracted_entity": null}
{"question": "are fresh garlic cloves as easy to eat as roasted garlic cloves?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. fresh garlic cloves are much harder", "extracted_entity": null}
{"question": "did Osama bin Laden likely abstain from alcohol?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He did not drink alcohol.", "extracted_entity": null}
{"question": "do Youtube viewers get unsolicited audiobook advice often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes they do", "extracted_entity": null}
{"question": "is 2018 Ashland, Oregon population inadequate to be a hypothetical military division?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "2018 Ashland, Oregon population", "extracted_entity": null}
{"question": "does Carmen Electra own a junk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she does.", "extracted_entity": null}
{"question": "could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have been a very small", "extracted_entity": null}
{"question": "would Glen Beck and Stephen Colbert be likely to tour together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are very different.", "extracted_entity": null}
{"question": "could a young Wizard of Oz Scarecrow have gotten Cerebral palsy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would have been born with Cere", "extracted_entity": null}
{"question": "can the Toyota Hilux tip the scales against Mr. Ed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can't.", "extracted_entity": null}
{"question": "is it normally unnecessary to wear a coat in Hollywood in July?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could the Atlantic readers fill 500 battalions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they were all in the same place", "extracted_entity": null}
{"question": "can oysters be preserved without refrigeration? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you dry them, or pickle", "extracted_entity": null}
{"question": "would a hippie hypothetically be bummed out by Augustus's Pax Romana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only because it would be an un", "extracted_entity": null}
{"question": "can professional boxers expect to have low dental bills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Boxing is a contact sport. There", "extracted_entity": null}
{"question": "did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "would  bald eagle deliver an urgent message before B-52?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, of course.", "extracted_entity": null}
{"question": "is it difficult to conduct astrophotography in the summer in Sweden?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not difficult, but it", "extracted_entity": null}
{"question": "was a woman Prime Minister directly before or after Stanley Baldwin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Margaret Thatcher", "extracted_entity": null}
{"question": "do people in middle school usually get breast exams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don\u2019t.", "extracted_entity": null}
{"question": "is Black Lives Matter connected with capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "does Capricorn astrology symbol have all of the parts of a chimera?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "would Lord Voldemort hypothetically be an effective fighter after Final Fantasy silence is cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Voldemort would be incapable", "extracted_entity": null}
{"question": "does Sam Harris worship Shiva?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He worships himself.", "extracted_entity": null}
{"question": "do the James Bond and Doctor Who series have a similarity in format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "they both have a secret agent and a time machine", "extracted_entity": null}
{"question": "can jackfruit be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are a tough guy", "extracted_entity": null}
{"question": "does Disney own a major comic book publisher?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they do own a major motion picture", "extracted_entity": null}
{"question": "did William Shaespeare read the Daily Mirror?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did", "extracted_entity": null}
{"question": "did the 40th president of the United States forward lolcats to his friends?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he did.", "extracted_entity": null}
{"question": "did Clark Gable marry more women once than Richard Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he married three times.", "extracted_entity": null}
{"question": "would an expensive tailor use adhesive to create a shorter hem on slacks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the seamstress was in", "extracted_entity": null}
{"question": "would the yearly precipitation on Snowdon submerge an upright bowling pin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the pin would be submerged.", "extracted_entity": null}
{"question": "can a honey bee sting a human more than once?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a bee can sting more than", "extracted_entity": null}
{"question": "did U.S. soldiers listen to Justin Bieber's Believe album during the Battle of Baghdad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they were scared.", "extracted_entity": null}
{"question": "can sunlight travel to the deepest part of the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "is Christopher Walken close to achieving EGOT status?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but he\u2019s not going to get", "extracted_entity": null}
{"question": "are the brooms from curling good for using on house floors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The brooms are designed for the ice", "extracted_entity": null}
{"question": "would a dog easily notice ammonia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the dog would notice ammonia.", "extracted_entity": null}
{"question": "are vinegar pickled cucumbers rich in lactobacillus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not.", "extracted_entity": null}
{"question": "is a construction worker required to build a portfolio?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Construction workers do not have to build", "extracted_entity": null}
{"question": "is it hard to get a BLT in Casablanca?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "would half muggle wizards fear Lord Voldemort?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are half-bloods and", "extracted_entity": null}
{"question": "would ISIS agree with Al-Farabi's religious sect?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they all believe in the same thing", "extracted_entity": null}
{"question": "is Casio's founding year a composite number?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the sum of its digits is not", "extracted_entity": null}
{"question": "would it be unusual to use paypal for drug deals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's still not illegal.", "extracted_entity": null}
{"question": "is growing seedless cucumber good for a gardener with entomophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cucumbers are the number one", "extracted_entity": null}
{"question": "can Spartina Patens thrive in the Sahara Desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it needs salt water.", "extracted_entity": null}
{"question": "does taking ukemi halt kinetic energy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t.", "extracted_entity": null}
{"question": "are there five different single-digit Fibonacci numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are.", "extracted_entity": null}
{"question": "can a snow leopard eat twice its own body weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can't.", "extracted_entity": null}
{"question": "are ground bell peppers the main ingredient of black pepper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the main ingredient of black pe", "extracted_entity": null}
{"question": "would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they have a plan B.", "extracted_entity": null}
{"question": "would a monkey outlive a human being on average?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Monkeys live on average 30", "extracted_entity": null}
{"question": "can Harry Potter book a flight on Asiana Airlines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he can't.", "extracted_entity": null}
{"question": "is latitude required to determine the coordinates of an area?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, latitude is required to determine the coordinates of", "extracted_entity": null}
{"question": "did polio medicine save the life of polio vaccine creator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Jonas Salk, creator of", "extracted_entity": null}
{"question": "are Donkeys part of Christmas celebrations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are the Christmas donkey!", "extracted_entity": null}
{"question": "if you have a serious injury in Bangladesh, would you probably dial a Fibonacci number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you are injured in Bangladesh", "extracted_entity": null}
{"question": "hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "12, 56, 6", "extracted_entity": null}
{"question": "is Great Pyramid of Giza the last wonder of its kind?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are still the other 7 w", "extracted_entity": null}
{"question": "do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do", "extracted_entity": null}
{"question": "will Elijah Cummings vote for Joe Biden in the next presidential elections?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he will not.", "extracted_entity": null}
{"question": "did Joan Crawford guest star on  JAG (TV series)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did, in the episode \"The", "extracted_entity": null}
{"question": "can an African Elephant get pregnant twice in a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it\u2019s unlikely that a female", "extracted_entity": null}
{"question": "would a sofer be a bad job for a vegan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it depends on how much you can compromise.", "extracted_entity": null}
{"question": "is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The question is not clear. It is not illegal", "extracted_entity": null}
{"question": "was Jackson Pollock straight edge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was a recovering alcohol", "extracted_entity": null}
{"question": "do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not. They only work during", "extracted_entity": null}
{"question": "would a fungal life-form be threatened by a pigment from copper?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a fungal life-form would not", "extracted_entity": null}
{"question": "do urban legends always have to occur in cities?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They can occur anywhere.", "extracted_entity": null}
{"question": "has Freemasonry been represented on the Moon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it was the US space agency", "extracted_entity": null}
{"question": "is Hamlet more common on IMDB than Comedy of Errors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by a factor of 200", "extracted_entity": null}
{"question": "coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because there is only one Audi T", "extracted_entity": null}
{"question": "was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was known for his humour.", "extracted_entity": null}
{"question": "can people die from brake failure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The only way someone can die from bra", "extracted_entity": null}
{"question": "if it socially acceptable to wear an icon depicting crucifixion? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "could a monarch butterfly rule a kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if it were a female.", "extracted_entity": null}
{"question": "can vitamin C rich fruits be bad for health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can, if taken in excess.", "extracted_entity": null}
{"question": "did breakdancing grow in popularity during WW2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it did during the 80", "extracted_entity": null}
{"question": "would it be unusual to see frost in September in Texas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "are Aldi's foods discounted due to being out of date?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Aldi's foods are disc", "extracted_entity": null}
{"question": "would a recruit for the United States Marine Corps be turned away for self harm?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The recruit would be evaluated by a psychiat", "extracted_entity": null}
{"question": "can you order an Alfa Romeo at Starbucks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can order a latte at Star", "extracted_entity": null}
{"question": "will Chick Fil A be open on Halloween 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chick-fil-A is closed on Thanks", "extracted_entity": null}
{"question": "do drag kings take testosterone to look masculine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, drag kings are not transgendered", "extracted_entity": null}
{"question": "are pennies commonly used in Canada?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not", "extracted_entity": null}
{"question": "could the Pope be on an episode of Pimp My Ride?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he asked for it.", "extracted_entity": null}
{"question": "can E6000 cure before a hoverboard finishes the Daytona 500? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. It\u2019s a", "extracted_entity": null}
{"question": "does Nigella Lawson care about solubility?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not in the slightest.", "extracted_entity": null}
{"question": "would Cuba Libre consumption help with insomnia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it might help with a hangover", "extracted_entity": null}
{"question": "is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is unlikely to compete with Bear", "extracted_entity": null}
{"question": "would someone on Venus be unlikely to experience hypothermia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "was Jean Valjean imprisoned due to hunger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Jean Valjean was imprisoned due", "extracted_entity": null}
{"question": "is Europa (moon) name origin related to Amunet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Europa is the name of a moon of Jupiter", "extracted_entity": null}
{"question": "does Adobe Suite have video game engine coding?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a graphics and publishing program", "extracted_entity": null}
{"question": "could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would have to go through the", "extracted_entity": null}
{"question": "would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is strep throat harmless to singer Rita Ora after her 2020 tonsilitis surgery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it\u2019s not. She should have", "extracted_entity": null}
{"question": "would an Alfa Romeo vehicle fit inside a barn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it would be a very nice bar", "extracted_entity": null}
{"question": "gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he does not.", "extracted_entity": null}
{"question": "could two newborn American Black Bear cubs fit on a king size bed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and they would fit well.", "extracted_entity": null}
{"question": "are coopers required in the beverage industry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the beverage industry requires coopers", "extracted_entity": null}
{"question": "was Superhero fiction invented in the digital format?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it was invented by a digital", "extracted_entity": null}
{"question": "does Antarctica have a lot of problems relating to homelessness?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's pretty much isolated and the", "extracted_entity": null}
{"question": "does an individual oceanographer study many sciences?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, an individual oceanographer studies many sciences", "extracted_entity": null}
{"question": "is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not sure what you mean by \u201csafe\u201d. There", "extracted_entity": null}
{"question": "did Julio Gonzalez like acetylene?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he was not a fan of acetylene", "extracted_entity": null}
{"question": "is the letter D influenced by the shape of ancient doors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The door is the entrance, the beginning", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a parody of the Bible?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a parody of the French", "extracted_entity": null}
{"question": "was the death of Heath Ledger caused by his work on The Dark Knight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did Donatello use a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did Rahul Dravid ever kick a field goal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a cricketer, not", "extracted_entity": null}
{"question": "do you often hear Marco Polo's name shouted near water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "because it's a game", "extracted_entity": null}
{"question": "are people more likely than normal to get sunburn at Burning Man?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (I think.)", "extracted_entity": null}
{"question": "is an espresso likely to assuage fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she will have a 2-year", "extracted_entity": null}
{"question": "would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he'd see the movement as", "extracted_entity": null}
{"question": "did a gladiator kill his opponent with a shotgun?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A gladiator was a warrior", "extracted_entity": null}
{"question": "did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they didn't.", "extracted_entity": null}
{"question": "could B be mistaken for an Arabic numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could be mistaken for an Arabic", "extracted_entity": null}
{"question": "can COVID-19 spread to maritime pilots?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can spread to maritime pilots", "extracted_entity": null}
{"question": "is 500GB USB device enough to save 10 hours of Netflix shows a day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You need a 1TB drive.", "extracted_entity": null}
{"question": "do hornets provide meaningful data for oceanographers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would Michael Phelps be good at pearl hunting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Michael Phelps is a human.", "extracted_entity": null}
{"question": "at Christmastime, do some films remind us that groundhog day is approaching?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact, there is a film that", "extracted_entity": null}
{"question": "was Great Recession the period of severest unemployment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not the worst period of un", "extracted_entity": null}
{"question": "could Katharine Hepburn have ridden the AirTrain JFK?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she would have had to take a", "extracted_entity": null}
{"question": "was Walt Disney able to email his illustrations to people living far away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the internet was not invented until", "extracted_entity": null}
{"question": "can a person who knows only English read Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it will take a lot of time", "extracted_entity": null}
{"question": "would three commas be sufficient for displaying US 2018 GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not.", "extracted_entity": null}
{"question": "would Brian Warner be a good singer for a soul music band?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "could someone listen to the entire U2 debut studio album during an episode of Peppa Pig?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because it\u2019s too long.", "extracted_entity": null}
{"question": "did Hanuman ever experience an orgasm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he did experience a release.", "extracted_entity": null}
{"question": "would you find a tibia beside parsley on a holiday plate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not a chance.", "extracted_entity": null}
{"question": "does someone from Japan need a passport to go to a Nordic country?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A Japanese passport is valid for all", "extracted_entity": null}
{"question": "would a black widow woman have use for peaches?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they were ripe.", "extracted_entity": null}
{"question": "does a person using tonsure have hair at the top of their scalp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a person using tonsure has hair at", "extracted_entity": null}
{"question": "would Terence Tao outperform Eminem in a math competition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he would be the best rapper", "extracted_entity": null}
{"question": "do any video games about the end of civilization have slot machines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you consider the world of Fallout", "extracted_entity": null}
{"question": "is number of different US President's in 1800s a lucky number in Hong Kong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. The number is not", "extracted_entity": null}
{"question": "can lettuce result in spontaneous abortion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not aware of any studies that have addressed", "extracted_entity": null}
{"question": "has mummification in the Andes been prevented by rainfall?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all. It is rainy", "extracted_entity": null}
{"question": "are some Do It Yourself projects potentially lethal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "was King Kong (2005 film) the lead actress's biggest box office role?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was", "extracted_entity": null}
{"question": "would a bodybuilder choose maize over chicken breast for dinner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "karachi was a part of Alexander the Great's success?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was part of Alexander's con", "extracted_entity": null}
{"question": "do guitarists need both hands to play?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope.", "extracted_entity": null}
{"question": "does Kenny G hold the qualifications to be a tax collector?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he has a lot of experience", "extracted_entity": null}
{"question": "in baseball, is a \"Homer\" named after the poet Homer who wrote the Odyssey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's named after a baseball player", "extracted_entity": null}
{"question": "is a felony jury enough people for a Bunco game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it depends on the size of the bunco", "extracted_entity": null}
{"question": "is letter C crucial to spelling the two most common words in English language?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "do most middle class families have butlers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Butlers are typically found in households of", "extracted_entity": null}
{"question": "is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is buried at Westminster Ab", "extracted_entity": null}
{"question": "did Mickey Mouse appear in a cartoon with Bugs Bunny in 1930?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did Jackson 5 members exceed number in The Osmonds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, The Jackson 5 had 6 members", "extracted_entity": null}
{"question": "can you get a ride on Amtrak to the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to take the CTA", "extracted_entity": null}
{"question": "are all limbs required for jujutsu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it is very helpful to have all", "extracted_entity": null}
{"question": "would The Dark Knight be appropriate for a preschool class?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not really.", "extracted_entity": null}
{"question": "did Jesus know anyone who spoke Estonian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he knew the Apostle Peter, who", "extracted_entity": null}
{"question": "does 2015 have more unlucky Friday's than usual?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "2015 is an odd year so", "extracted_entity": null}
{"question": "is Tony Bennett's middle name shared by a former UFC champion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "do you need both hot and cold water to peel a tomato?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you need only cold water.", "extracted_entity": null}
{"question": "is Newt Gingrich's nickname a type of Reptile?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes it is. His nickname is the \"", "extracted_entity": null}
{"question": "do spider wasps have eight legs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they have six legs.", "extracted_entity": null}
{"question": "does the New York Public Library sell Alpo products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it does sell 100", "extracted_entity": null}
{"question": "can petroleum jelly be used as fuel in a car?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it's a terrible idea.", "extracted_entity": null}
{"question": "can Aerosmith legally drive in the carpool lane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They're all too old.", "extracted_entity": null}
{"question": "are Durian fruits an olfactory delight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "could a wandering albatross fly from Bucharest to New York City without a rest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it had enough wind to carry it", "extracted_entity": null}
{"question": "do some people soak in olive oil and water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some people do.", "extracted_entity": null}
{"question": "do depressed people travel to the Golden Gate Bridge often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they don't want to go", "extracted_entity": null}
{"question": "can members of the Green Party of England and Wales vote in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only citizens of the USA.", "extracted_entity": null}
{"question": "is Poseidon similar to the god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Poseidon is the Greek god", "extracted_entity": null}
{"question": "is Glycol something United Airlines would buy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a petrochemical used", "extracted_entity": null}
{"question": "does a person suffering from Thalassophobia enjoy oceanography?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "can a Bengal cat survive eating only pancakes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's not a very health", "extracted_entity": null}
{"question": "does it seem like the Gorillaz is composed of more members than they have?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the way they are shown on", "extracted_entity": null}
{"question": "could Rich and Morty be triggered for children of alcoholics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Alcoholism is a family disease", "extracted_entity": null}
{"question": "was the Japanese street aesthetic once illuminated by noble gasses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was.", "extracted_entity": null}
{"question": "in isopropyl alcohol, is the solubility of salt low?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the solubility of salt is high", "extracted_entity": null}
{"question": "could you watch all of JAG in six months?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I couldn\u2019t watch all of J", "extracted_entity": null}
{"question": "do silicone suits make judo difficult?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "will The Exorcist stimulate limbic system?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the subject is a child.", "extracted_entity": null}
{"question": "was P. G. Wodehouse's favorite book The Hunger Games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was the Bible.", "extracted_entity": null}
{"question": "would J.K Rowling's top sellers be on a fantasy shelf?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they'd be on a children'", "extracted_entity": null}
{"question": "are some types of pancakes named after coins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "can Paprika be made without a dehydrator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it takes a lot longer.", "extracted_entity": null}
{"question": "in Doctor Who, did the war doctor get more screen time than his successor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "was Mesopotamia part of what is now China?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Mesopotamia was in the", "extracted_entity": null}
{"question": "could Arnold Schwarzenegger hypothetically defeat Haf\u00fe\u00f3r Bj\u00f6rnsson in a powerlifting competition if both are at their peak strength?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a very close call", "extracted_entity": null}
{"question": "was the British car, the Mini, the first car manufactured?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The first car was made in 1", "extracted_entity": null}
{"question": "will you see peach blossoms and Andromeda at the same time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they bloom at different times.", "extracted_entity": null}
{"question": "can The Hobbit be read in its entirety in four minutes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you read it out loud", "extracted_entity": null}
{"question": "is number of stars in Milky Way at least ten times earth's population?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are at least 100", "extracted_entity": null}
{"question": "could Christopher Walken enlist in the United States Marine Corps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "is white light the absence of color?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "can Jabberwocky be considered a sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The sonnet is a form of poetry", "extracted_entity": null}
{"question": "would Eye surgery on a fly be in vain?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the fly would be able to see again", "extracted_entity": null}
{"question": "should you ask a neighbor for candy on New Year's Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it's a holiday.", "extracted_entity": null}
{"question": "are all types of pottery safe to cook in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all types of pottery are safe", "extracted_entity": null}
{"question": "can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "does the United States of America touch the Indian Ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. The state of Florida is", "extracted_entity": null}
{"question": "would Donald Duck be allowed into most grocery stores?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he's a duck.", "extracted_entity": null}
{"question": "is it best to avoid kola nuts with colitis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kola nuts are not harmful in any", "extracted_entity": null}
{"question": "would 7 zucchini's satisfy potassium USDA daily recommendation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not even close. The daily potass", "extracted_entity": null}
{"question": "was the Treaty of Versailles settled over blueberry scones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Treaty of Versailles was", "extracted_entity": null}
{"question": "will the producer of Easy Rider become an octogenarian in 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He's 80.", "extracted_entity": null}
{"question": "does Osama bin Laden put a wafer on his tongue every Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's a catholic.", "extracted_entity": null}
{"question": "can you taste Law & Order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I don't think so.", "extracted_entity": null}
{"question": "are queen bees unnecessary for growing apples?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, queen bees are necessary to ensure the", "extracted_entity": null}
{"question": "does a bumblebee have to worry about spider veins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are so small.", "extracted_entity": null}
{"question": "can the Department of Defense perform a solo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they will need to learn how to", "extracted_entity": null}
{"question": "can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can", "extracted_entity": null}
{"question": "was Land of Israel in possession of an Islamic empire in 16th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, only the former Portuguese colony of Moz", "extracted_entity": null}
{"question": "could the Toyota Stadium sit a tenth of the population of Gotheburg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Toyota Stadium is a 3", "extracted_entity": null}
{"question": "does the word swastika have meaning in sanskrit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does. It is a symbol of", "extracted_entity": null}
{"question": "were there greater landslides than 1980 United States presidential election?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "can you hunt Iberian wolves in the Southern United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can only hunt Iberian", "extracted_entity": null}
{"question": "is hanging a viable execution method on a ship at sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is pig meat considered inedible within the cuisine of Hawaii?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "are the events of Star Trek: The Next Generation in the history of the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "if you're pregnant, might you be recommended ginger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if you're pregnant, you might", "extracted_entity": null}
{"question": "does The Jungle Book contain racist subtext?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and the racism is integral to the", "extracted_entity": null}
{"question": "does the Prime Minister of the United Kingdom have poor job security?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "would Gomer Pyle salute a lieutenant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I guess it depends on how he was feeling that", "extracted_entity": null}
{"question": "when en route from China to France, must pilots know their altitude in the imperial foot?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they should know their altitude in", "extracted_entity": null}
{"question": "could you brew beer from start to finish in the month of September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. i can.", "extracted_entity": null}
{"question": "is Freya a combination of Athena and Aphrodite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she is.", "extracted_entity": null}
{"question": "in most Mennonite homes, would children know of The Powerpuff Girls?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not unless they are the children of a", "extracted_entity": null}
{"question": "would a sophist use an \u00e9p\u00e9e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he has to, but he would", "extracted_entity": null}
{"question": "did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Paul the Apostle was beheaded", "extracted_entity": null}
{"question": "do sand cats avoid eating all of the prey of eels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "sand cats eat all of the prey of", "extracted_entity": null}
{"question": "was Muhammed a member of the Uniting Church in Australia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a member of the Church of", "extracted_entity": null}
{"question": "would the historic Hattori Hanz\u014d admire Naruto?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I\u2019m", "extracted_entity": null}
{"question": "should you wrap a gift for a mother of a stillborn in stork wrapping paper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "does crucifixion violate US eighth amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not.", "extracted_entity": null}
{"question": "would a TMNT coloring book have pizza in it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "could Sugar Ray Robinson box if he stole in Iran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not be allowed to box in", "extracted_entity": null}
{"question": "would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, E.T. would love Friendly", "extracted_entity": null}
{"question": "can a banana get a virus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a rare banana", "extracted_entity": null}
{"question": "is the use of the word Gypsy by non-Romani people considered okay?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s not.", "extracted_entity": null}
{"question": "are implants from an ORIF surgery affected by the magnetic field of the Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are made of titanium,", "extracted_entity": null}
{"question": "was Subway involved in a pedophilia scandal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was a pizza chain.", "extracted_entity": null}
{"question": "did Jack Dempsey have most title fight wins in either of his weight classes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He had 27 title fights", "extracted_entity": null}
{"question": "did Bill Nye vote for Franklin Delano Roosevelt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was too young.", "extracted_entity": null}
{"question": "is grief always obvious when it is being experienced?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Grief is often expressed in small ways", "extracted_entity": null}
{"question": "did either side score a touchdown during the Football War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "is there a Yeti associated with Disney theme parks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is a Yeti associated with the", "extracted_entity": null}
{"question": "did Zorro carve his name into items regularly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he carved his name into a lot", "extracted_entity": null}
{"question": "did Subway have a sex offender as a spokesperson?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Jared Fogle", "extracted_entity": null}
{"question": "could Christopher Nolan's movies finance Cyprus's entire GDP?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes they could.", "extracted_entity": null}
{"question": "is Steve Carell's character on The Office portrayed as one with tremendous leadership skills?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's portrayed as a b", "extracted_entity": null}
{"question": "could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Dominicans are not a mend", "extracted_entity": null}
{"question": "can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "does the texture of leaves remain the same independent of their coloring changing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the texture of leaves changes as their color", "extracted_entity": null}
{"question": "can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. That would be a violation of the", "extracted_entity": null}
{"question": "would Doctor Strange like the Pittsburgh Steelers logo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think he would. I think he would like", "extracted_entity": null}
{"question": "would Iris (mythology) and Hermes hypothetically struggle at a UPS job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iris would be more likely to be fired for", "extracted_entity": null}
{"question": "is pi in excess of square root of 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, pi > sqrt(5)", "extracted_entity": null}
{"question": "would it be uncommon for a high schooler to use the yellow pages?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are many people who do not use", "extracted_entity": null}
{"question": "could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and it would still have room for a", "extracted_entity": null}
{"question": "does the human stomach destroy a bee if ingested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "is blonde hair green eyed Sara Paxton considered a Latino?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is an American actress and singer.", "extracted_entity": null}
{"question": "are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but in very low concentration.", "extracted_entity": null}
{"question": "can you see live harbor seals in Washington DC?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The closest you can get is probably at", "extracted_entity": null}
{"question": "was story of Jesus inspired by Egyptian myth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the story of Jesus is inspired by Egypt", "extracted_entity": null}
{"question": "has CNES planted a French flag on the lunar surface?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1970", "extracted_entity": null}
{"question": "do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they don't believe in the", "extracted_entity": null}
{"question": "can Iowa be hidden in the English Channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is only hidden by the English", "extracted_entity": null}
{"question": "can you house a giant squid at Soldier Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They're too big.", "extracted_entity": null}
{"question": "was hippie culture encouraged by the government in the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Hippie culture was not encouraged", "extracted_entity": null}
{"question": "would Mickey Mouse blend in with the American flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But I bet he would blend in", "extracted_entity": null}
{"question": "does penicillin cure a learning disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It will not cure a learning dis", "extracted_entity": null}
{"question": "is it dark is Basel during the day in Los Angeles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not dark in Basel during", "extracted_entity": null}
{"question": "is honey associated with queens?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, honey is associated with queens.", "extracted_entity": null}
{"question": "was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jeanette Rankin", "extracted_entity": null}
{"question": "does a giant green lady stand in New York Harbor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she doesn\u2019t stand.", "extracted_entity": null}
{"question": "could chives be mistaken for grass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could.", "extracted_entity": null}
{"question": "could a two-year old win a Scrabble tournament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a two-year old could play", "extracted_entity": null}
{"question": "is Y2K relevant to the plot of The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In fact, the only reference to Y", "extracted_entity": null}
{"question": "is Sirius part of a constellation of an animal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sirius is a star, not", "extracted_entity": null}
{"question": "would alligator best saltwater crocodile in hypothetical Lake Urmia battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a close fight.", "extracted_entity": null}
{"question": "would a Common warthog starve in a greenhouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "is purchasing food for a Lolcat unnecessary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "do some psychotherapy patients have no mental illness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, some do.", "extracted_entity": null}
{"question": "did Harry Houdini appear on Chris Angel Mindfreak?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "could  jockey win Triple Crown between Eid al-Fitr endpoints?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would a vegetarian be able to eat something at Chick-fil-A?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the vegetarian wanted to.", "extracted_entity": null}
{"question": "does the Pixar film Brave feature Scottish people?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does.", "extracted_entity": null}
{"question": "do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they compete for resources.", "extracted_entity": null}
{"question": "do most college students own a fax machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I don'", "extracted_entity": null}
{"question": "would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he would be too big to fit", "extracted_entity": null}
{"question": "could a delicious recipe be made with The Onion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could be.", "extracted_entity": null}
{"question": "could a markhor give birth three times in a single year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could, if it was a mark", "extracted_entity": null}
{"question": "would a model be likely to frequently enjoy the menu at Cookout?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because models are often hungry and Cook", "extracted_entity": null}
{"question": "does the history of Europe include the age of dinosaurs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but if you\u2019re a paleont", "extracted_entity": null}
{"question": "was John George Bice's birthplace near Cornwall?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born in Shropshire", "extracted_entity": null}
{"question": "do white blood cells outnumber red blood cells in the human body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, red blood cells outnumber white blood cells", "extracted_entity": null}
{"question": "was Edward II crucial to England's victory at Battle of Falkirk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Edward II was a key figure in the Battle of", "extracted_entity": null}
{"question": "did Holy Land belong to Adamu's tribe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Holy Land was not part of Adamu", "extracted_entity": null}
{"question": "does Ahura Mazda have a rivalry with Zeus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "is an Eastern chipmunk likely to die before seeing two leap years?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The probability of a chipmunk seeing", "extracted_entity": null}
{"question": "would the owners of the company Peter Griffin works for need barley?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the owners of the company Peter Griff", "extracted_entity": null}
{"question": "is the CIA part of the Department of Defense?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the CIA is part of the Department", "extracted_entity": null}
{"question": "was Harry Potter and the Philosopher's Stone popular during the great depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure. I'm not", "extracted_entity": null}
{"question": "is a slime mold safe from cerebral palsy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. slime molds are not safe from", "extracted_entity": null}
{"question": "is British Airways the air force of the United Kingdom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the air force of the United Kingdom is", "extracted_entity": null}
{"question": "has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they have changed.", "extracted_entity": null}
{"question": "are aggressive bumblebees suicidal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "is Edward Snowden in hiding from the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is in Russia, where he has", "extracted_entity": null}
{"question": "can the Great Depression be treated with Prozac?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I'm", "extracted_entity": null}
{"question": "can photography be considered abstract art?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it depends on the photographer\u2019", "extracted_entity": null}
{"question": "could a bee hummingbird balance a scale with a single pea on it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The hummingbird weighs", "extracted_entity": null}
{"question": "is Disney associated with Los Angeles County?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Disney is in Orange County.", "extracted_entity": null}
{"question": "did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They both had a great deal in common", "extracted_entity": null}
{"question": "is a doctorate required to teach at a SUNY School?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a doctorate is not required to teach", "extracted_entity": null}
{"question": "does the swastika have positive uses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It was used for thousands of years by", "extracted_entity": null}
{"question": "does Nintendo's link ever see an astronomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "has Elon Musk's hairline changed?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's receding.", "extracted_entity": null}
{"question": "do American teams in National Hockey League outnumber Canadian teams?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She lasted 144 episodes", "extracted_entity": null}
{"question": "were all the materials to make a cannon known during the bronze age?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The bronze age ended in 12", "extracted_entity": null}
{"question": "is Cookie Monster's diet Paleo friendly?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is! Cookie Monster is", "extracted_entity": null}
{"question": "would it be impossible to seat every Chief Justice of the United States on a Boeing 737?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be impossible to seat every Chief", "extracted_entity": null}
{"question": "could Toyota stadium house people suffering homelessness in Michigan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as many as could be hous", "extracted_entity": null}
{"question": "can a martyr saint have been excommunicated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many examples.", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but that's not the point", "extracted_entity": null}
{"question": "does a Disney princess on Broadway have red hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (Ariel, Belle, Jasm", "extracted_entity": null}
{"question": "could Steven Spielberg send emails as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does Felix Potvin have a position on a dodgeball team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is the goalie", "extracted_entity": null}
{"question": "would someone with leukophobia enjoy looking at the Flag of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, unless they were also afraid of red,", "extracted_entity": null}
{"question": "would 1943-S penny be good for making silverware?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If it is in good condition, it should be", "extracted_entity": null}
{"question": "did mongoose come from later period than rhinos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "would it be possible to fit a football field in Alcatraz Island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be possible to fit a football", "extracted_entity": null}
{"question": "is Issac Newton often associated with a red fruit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he is associated with apples.", "extracted_entity": null}
{"question": "would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would be profitable.", "extracted_entity": null}
{"question": "can you buy spinal cord at Home Depot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you can buy the hardware but not the", "extracted_entity": null}
{"question": "are some chiropractic manipulations dangerous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Some chiropractic manipulations are", "extracted_entity": null}
{"question": "were some people afraid of New Years Day coming in 1999?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some people were afraid of the year", "extracted_entity": null}
{"question": "did travelers sing sea shanties on the Oregon Trail?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they didn't.", "extracted_entity": null}
{"question": "do members of the Supreme Court of the United States have longer terms than most senators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they don\u2019t.", "extracted_entity": null}
{"question": "paleography hypothetically helps to understand Cthulhu?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it helps to understand the language of", "extracted_entity": null}
{"question": "is surfing popular in Des Moines, Iowa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "is Statue of Unity hypothetically more level with Statue of Liberty than Lighthouse of Alexandria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Statue of Liberty is hypothet", "extracted_entity": null}
{"question": "is CEO of Nissan an internationally wanted fugitive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he is in Japan.", "extracted_entity": null}
{"question": "has type of political association Pompey had with Caesar influenced reality TV?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it is an example of a reality", "extracted_entity": null}
{"question": "would Rime of the Ancient Mariner make a good sonnet?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is too long and it is", "extracted_entity": null}
{"question": "do Snow White dwarves best The Hobbit dwarves in battle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Hobbit dwarves are better", "extracted_entity": null}
{"question": "can the history of art be learned by an amoeba?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it will be a very long and", "extracted_entity": null}
{"question": "can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you are driving, not walking.", "extracted_entity": null}
{"question": "walt Disney dominated his amusement park peers at Academy Awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He won 22 Academy Awards, which is", "extracted_entity": null}
{"question": "would human race go extinct without chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we would die without chlorophyll", "extracted_entity": null}
{"question": "is the average bulk carrier ideal for transporting bromine at room temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The bulk carrier is ideal for transport", "extracted_entity": null}
{"question": "can depression be mistaken for laziness?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all the time.", "extracted_entity": null}
{"question": "did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Easy Rider producer Peter Fonda", "extracted_entity": null}
{"question": "did pirates who had scurvy need more Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "could the leader of Heaven's Gate save images in JPEG format?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the leader of Heaven's Gate could", "extracted_entity": null}
{"question": "does the FDA require sell by dates using Roman Numerals?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do require sell by dates to", "extracted_entity": null}
{"question": "would Kylee Jenner ask for no cream in her coffee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Kylee Jenner would ask for", "extracted_entity": null}
{"question": "does Nicole Kidman know any Scientologists?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is Lionel Richie related to Sheila E?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they both dated the same man.", "extracted_entity": null}
{"question": "was a nuclear bomb used in the Napoleonic Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Napoleonic Wars were fought in the", "extracted_entity": null}
{"question": "is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you will fall and break your neck", "extracted_entity": null}
{"question": "can you buy a fair trade laptop?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Fair Trade is about products, not people", "extracted_entity": null}
{"question": "was Noah concerned with buoyancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was concerned with buoyancy", "extracted_entity": null}
{"question": "is a watchmaker likely to be able to fix an Apple Watch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "are sables related to wolverines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not related to wolverines", "extracted_entity": null}
{"question": "are some Brazilian Navy ships built in Britian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Brazilian Navy has a number of", "extracted_entity": null}
{"question": "could Carl Friedrich Gauss speak to someone 100 miles away?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he couldn't.", "extracted_entity": null}
{"question": "in Hey Arnold, did any characters stay on a porch all the time?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Pigeon Sisters.", "extracted_entity": null}
{"question": "has Johns Hopkins University always treated subjects ethically?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. In 1997, the", "extracted_entity": null}
{"question": "is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "do black-tailed jackrabbits fear the European wildcat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "has Kelly Clarkson outsold season 4 American Idol winner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes she has", "extracted_entity": null}
{"question": "is Hermes equivalent to the Roman god Vulcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are different gods.", "extracted_entity": null}
{"question": "can an elite runner circle the Pyrenees in one day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s not easy.", "extracted_entity": null}
{"question": "is the Golden eagle considered a scavenger bird?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Golden eagles are scavengers", "extracted_entity": null}
{"question": "can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the majority of vowels can be", "extracted_entity": null}
{"question": "are grapes essential to winemaking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is essential to know how to", "extracted_entity": null}
{"question": "would the Cookie Monster decline an offer of free Keebler products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he'd eat them all.", "extracted_entity": null}
{"question": "did Jeremy Irons master sweep picking as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was born with it.", "extracted_entity": null}
{"question": "for Hostas to look their best, do they need lots of chlorophyll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't need lots of ch", "extracted_entity": null}
{"question": "has spinach been a source of power in a comic movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in \u201cSpider-Man 2", "extracted_entity": null}
{"question": "can you see the Statue of Freedom from the Statue of Liberty?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can see the Statue of", "extracted_entity": null}
{"question": "does Ludacris have Greek heritage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Ludacris's mother is a", "extracted_entity": null}
{"question": "will bumblebees derail the United States presidential election?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they will not be the cause.", "extracted_entity": null}
{"question": "does Rahul Dravid belong to the family Gryllidae?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is not.", "extracted_entity": null}
{"question": "is Metallica protective over their music?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "does Dean Cain have less days to birthday than Will Ferrell every 4th of July?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Dean Cain has more days to birth", "extracted_entity": null}
{"question": "is Lord Voldemort associated with a staff member of Durmstrang?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all", "extracted_entity": null}
{"question": "are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is starving Hamas agent eating pig bad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it's bad to eat p", "extracted_entity": null}
{"question": "would Ringo Starr avoid the pot roast at a restaurant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would.", "extracted_entity": null}
{"question": "was ship that recovered Apollo 13 named after a World War II battle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the ship was named after the Battle of", "extracted_entity": null}
{"question": "are both founders of Ben & Jerry's still involved in the company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Ben Cohen is still involved in the company.", "extracted_entity": null}
{"question": "is a krabby patty similar to a cheeseburger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A cheeseburger is a h", "extracted_entity": null}
{"question": "did Beethoven enjoy listening to EDM?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "is it true that gay male couples cannot naturally reproduce?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they cannot.", "extracted_entity": null}
{"question": "could an American confuse breakfast in British cuisine for dinner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the meal is served at a", "extracted_entity": null}
{"question": "can Kit & Kaboodle hypothetically help someone past the Underworld gates?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is possible.", "extracted_entity": null}
{"question": "are two cans of Campbell's Soup a day good for hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not good for hypert", "extracted_entity": null}
{"question": "are there any official American knights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope.", "extracted_entity": null}
{"question": "is the Foreign and Commonwealth Office a European political agency?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes it is", "extracted_entity": null}
{"question": "do gorillas fight with panda bears?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do fight with humans.", "extracted_entity": null}
{"question": "would lumberjacks get full after eating three dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because dosa is like pizza.", "extracted_entity": null}
{"question": "is being 5 year Capital One Venture member more cost effective than being in Church of Satan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the Church of Satan is a", "extracted_entity": null}
{"question": "tata Hexa can accomodate every Spice Girl?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "could common warthog be useful for scrimshaw?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could be.", "extracted_entity": null}
{"question": "is breast cancer associated with a ribbon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, breast cancer is not associated with a rib", "extracted_entity": null}
{"question": "would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Nuno Gomes is a Portuguese", "extracted_entity": null}
{"question": "is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If the answer to this question is \"yes\"", "extracted_entity": null}
{"question": "was a Tiny House ceiling out of Osama bin Laden's reach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Tiny Houses are built on a", "extracted_entity": null}
{"question": "can you conduct surveillance from a teddy bear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. I don\u2019t", "extracted_entity": null}
{"question": "does table tennis use prime numbers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does", "extracted_entity": null}
{"question": "was the Carnation Revolution the deadliest revolution in Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Carnation Revolution was the deadliest revolution", "extracted_entity": null}
{"question": "is the Muslim world hostile to Israel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "is the saltwater crocodile less endangered than the European otter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the saltwater crocodile is less", "extracted_entity": null}
{"question": "can someone from New England profit by growing coffee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's a long term invest", "extracted_entity": null}
{"question": "could a dichromat probably easily distinguish chlorine gas from neon gas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because dichromats have only two types", "extracted_entity": null}
{"question": "nATO doesn't recognize double triangle flag countries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "That's right.", "extracted_entity": null}
{"question": "did Julius Caesar read books on Pharmacology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "does water have viscosity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Water is a liquid and has viscos", "extracted_entity": null}
{"question": "was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was", "extracted_entity": null}
{"question": "would you spend less on your food at Aldi than at Whole Foods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I would spend less", "extracted_entity": null}
{"question": "are Scottish people Albidosi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in fact they are the most Albidos", "extracted_entity": null}
{"question": "are System of a Down opposed to globalization?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are opposed to the way the world", "extracted_entity": null}
{"question": "was Alexander the Great baptized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Alexander was not baptized.", "extracted_entity": null}
{"question": "did Demi Lovato's ancestors help turn maize into popcorn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they did help turn maize into", "extracted_entity": null}
{"question": "would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be impossible, but the", "extracted_entity": null}
{"question": "is Pearl Harbor the mythical home of a shark goddess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would Elon Musk be more likely to know about astrology than physics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he\u2019s a rich and famous celebr", "extracted_entity": null}
{"question": "can paresthesia be caused by a white pigment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, paresthesia can be caused by", "extracted_entity": null}
{"question": "could Charlie Bucket be a hotel manager?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could.", "extracted_entity": null}
{"question": "can children be soldiers in the US Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the US Army does not allow children to", "extracted_entity": null}
{"question": "is Jack Black's height enough to satisfy Coronavirus distancing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not enough.", "extracted_entity": null}
{"question": "are peaches best eaten when firm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are best eaten when ripe", "extracted_entity": null}
{"question": "was Charles Manson's body unwanted?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was buried in an unmarked", "extracted_entity": null}
{"question": "was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, indirectly", "extracted_entity": null}
{"question": "can a human eat an entire 12-lb roast turkey in an hour? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if it's a very small person", "extracted_entity": null}
{"question": "would a house full of aloe vera hypothetically be ideal for Unsinkable Sam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I would like to try it.", "extracted_entity": null}
{"question": "would dual-energy X-ray absorptiometry be useful if performed on a crab?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it would be a good way to determine the amount", "extracted_entity": null}
{"question": "would Harvey Milk have approved of Obama?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would have.", "extracted_entity": null}
{"question": "would a jumping spider need over half a dozen contact lenses?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it would need more than one.", "extracted_entity": null}
{"question": "are seasons of Survivor surpassed by number of Ancient Greek letters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, seasons of Survivor are not sur", "extracted_entity": null}
{"question": "do citizens of Cheshire sing La Marseillaise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they sing The Floral Dance", "extracted_entity": null}
{"question": "are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not necessarily. I've seen many with red", "extracted_entity": null}
{"question": "in teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, SSRI medications are safe for te", "extracted_entity": null}
{"question": "could eating Chinook salmon help Ryan Reynolds?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "maybe, but the more important question is, could", "extracted_entity": null}
{"question": "would kaffir lime be good in a White Russian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Kaffir lime is a traditional ingred", "extracted_entity": null}
{"question": "is entire Common Era minuscule to lifespan of some trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but some trees can live longer than entire", "extracted_entity": null}
{"question": "did Charlemagne have a bar mitzvah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was baptized as a child.", "extracted_entity": null}
{"question": "did Monty Python write the Who's on First sketch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they didn't. it was written", "extracted_entity": null}
{"question": "in geometry terms, is the Royal Observatory in Greenwich similar to a yield sign?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's similar to a yield sign", "extracted_entity": null}
{"question": "did Sugar Ray Robinson win a fight against Canelo Alvarez?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Sugar Ray Robinson was a boxer", "extracted_entity": null}
{"question": "can an asteroid be linked with virginity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can", "extracted_entity": null}
{"question": "would a Yeti be likely to have prehensile limbs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the Yeti. If the Yet", "extracted_entity": null}
{"question": "is groundhog day used as a global season indicator? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it is used in the northern hemisphere", "extracted_entity": null}
{"question": "did mercenaries fight for England in the Glorious Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They fought for William of Orange.", "extracted_entity": null}
{"question": "was the sable depicted in Marvel comics anthropomorphic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was anthropomorphic.", "extracted_entity": null}
{"question": "are twinkies considered artisan made products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. not even close.", "extracted_entity": null}
{"question": "were mollusks an ingredient in the color purple?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they were dyed with", "extracted_entity": null}
{"question": "is Christianity better for global warming than Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christianity is better for global warming than Sat", "extracted_entity": null}
{"question": "was Lord Voldemort taught by Professor Dumbledore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was taught by Professor Quirrell", "extracted_entity": null}
{"question": "would costumes with robes and pointy hats be helpful for Macbeth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "costumes with robes and pointy hats", "extracted_entity": null}
{"question": "are the knights in the Medieval Times show not authentic knights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are real knights.", "extracted_entity": null}
{"question": "could William Franklyn-Miller win a 2020 Nascar Cup Series race?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Al-Farabi ever meet Mohammed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al-Farabi never met Mohammed,", "extracted_entity": null}
{"question": "did the Pearl Harbor attack occur during autumn?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it occurred during the winter.", "extracted_entity": null}
{"question": "have the Israelis played the Hammerstein Ballroom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they did play the Roseland Ball", "extracted_entity": null}
{"question": "if you're reducing salt intake, are olives a healthy snack?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, olives are high in sodium.", "extracted_entity": null}
{"question": "do pediatricians perform abortions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "could casualties from deadliest war rival France's population?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The deadliest war in history was World", "extracted_entity": null}
{"question": "does Lorem ipsum backwards fail to demonstrate alliteration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It does not fail to demonstrate alliter", "extracted_entity": null}
{"question": "can you see Stonehenge from a window in Dusseldorf?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can see the Rhine River", "extracted_entity": null}
{"question": "can Reiki be stored in a bottle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it cannot be stored in a bottle", "extracted_entity": null}
{"question": "would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'll say yes.", "extracted_entity": null}
{"question": "would Tom Cruise ever insult L. Ron Hubbard?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not in a million years.", "extracted_entity": null}
{"question": "does Lemon enhance the flavor of milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Harry Potter was a better investment", "extracted_entity": null}
{"question": "is the United States the largest exporter of Fair Trade products?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The United States is the largest importer", "extracted_entity": null}
{"question": "is Snickers helpful for weight loss?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Snickers has 220", "extracted_entity": null}
{"question": "could the Jackson 5 play a full game of rugby with each other?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they couldn't.", "extracted_entity": null}
{"question": "did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Isaac's father was almost killed by", "extracted_entity": null}
{"question": "did the founders of the biggest city in Orange County, California speak Italian?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "is an ammonia fighting cleaner good for pet owners?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it is a strong cleaner and may", "extracted_entity": null}
{"question": "could you buy Hershey's Kisses in red foil with farthings after 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the red foil Hershey", "extracted_entity": null}
{"question": "could morphine cure HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but morphine can help with HIV", "extracted_entity": null}
{"question": "would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. she's not a great choice.", "extracted_entity": null}
{"question": "would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would the chef at La Grenouille find salsa to be a strange request?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the chef would find it strange.", "extracted_entity": null}
{"question": "was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "would toast for a vegan have margarine instead of butter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Vegans do not use margarine.", "extracted_entity": null}
{"question": "did any cultures associate celery with death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Celery was associated with death in ancient Greek culture", "extracted_entity": null}
{"question": "are emus related to elks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "is the Forbidden City host to a wooden rollercoaster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "are slime lilies in a different scientific family than asparagus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are in a different family. The", "extracted_entity": null}
{"question": "in order to work in district management, does one need a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you do. You must have your own", "extracted_entity": null}
{"question": "can you buy Reddit at Walmart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Reddit is not available at Walmart", "extracted_entity": null}
{"question": "if you have black hair and want red hair, do you need bleach?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You just need to dye it.", "extracted_entity": null}
{"question": "is a curling iron necessary in curling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a curling iron is not necessary.", "extracted_entity": null}
{"question": "was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Alice was likely in need of succimer", "extracted_entity": null}
{"question": "was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Christopher Columbus", "extracted_entity": null}
{"question": "did Lamarck and Darwin agree about the origin of species diversity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Lamarck believed that species diversity is the", "extracted_entity": null}
{"question": "if you were at an Apple store, would most of the computers be running Ubuntu?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a good percentage would.", "extracted_entity": null}
{"question": "can you make an MP3 from the Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it will take 400", "extracted_entity": null}
{"question": "are there any chives hypothetically good for battling vampires?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, chives are not good for battling", "extracted_entity": null}
{"question": "could a Diwali celebration feature a crustacean?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think it could.", "extracted_entity": null}
{"question": "do astronomers write horoscopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they write horoscopes for stars", "extracted_entity": null}
{"question": "is the name of a mythical creature also the name of a Small Solar System body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. In Greek mythology, Charon was", "extracted_entity": null}
{"question": "did Medieval English lords engage in fair trade with peasants?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Peasants were forced to give up", "extracted_entity": null}
{"question": "are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "do anatomical and symbolic hearts look remarkably different?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "are you more likely to find bipolar disorder in a crowd than diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, diabetes is more common than b", "extracted_entity": null}
{"question": "does having lip piercings lead to more expensive dental bills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because you can't feel it when", "extracted_entity": null}
{"question": "are most mall Santa Claus actors white?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not always", "extracted_entity": null}
{"question": "does the Taco Bell kitchen contain cinnamon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Taco Bell kitchen does not contain", "extracted_entity": null}
{"question": "could white rice go rancid before sesame seeds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have not tried this but I would think that", "extracted_entity": null}
{"question": "can you listen to the entire Itunes song catalog in one year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If you listen to a song 2", "extracted_entity": null}
{"question": "do mollymawks live where albatrosses cannot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she would.", "extracted_entity": null}
{"question": "are Mayors safe from harm from the federal government?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If the Mayor is not a citizen", "extracted_entity": null}
{"question": "is it normal to see a red panda in Shanghai outside of a zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "would a human following a hyena diet be unwelcome at a vegan festival?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not unless they were a vegan human", "extracted_entity": null}
{"question": "can DRL Racer X drone get across Brooklyn Bridge in 18 seconds?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! DRL Racer X drone", "extracted_entity": null}
{"question": "would Temujin hypothetically be jealous of Charlemagne's conquests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Temujin and Charlemagne were not", "extracted_entity": null}
{"question": "would a bodybuilder enjoy wearing a cast for several weeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not", "extracted_entity": null}
{"question": "can you carry a Chrysler in a laptop bag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. But you can\u2019t", "extracted_entity": null}
{"question": "is Home Depot a one stop shop for crucifixion supplies?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is the Holy Land important to Eastern religions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Holy Land is important to Eastern relig", "extracted_entity": null}
{"question": "is immersion in virtual reality testable on cnidarians before humans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is testable on all animals,", "extracted_entity": null}
{"question": "did Jack Dempsey ever witness Conor McGregor's fights?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were not in the same weight class", "extracted_entity": null}
{"question": "are black and white prison uniforms made to resemble a zebra?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Prisoners are stripped of all", "extracted_entity": null}
{"question": "would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "did Malcolm X avoid eating ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would a snakebite hypothetically be a threat to T-1000?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's not made of organic", "extracted_entity": null}
{"question": "is there a Harry Potter character named after Florence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there is a character named after Florence", "extracted_entity": null}
{"question": "could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Pope was Leo X (15", "extracted_entity": null}
{"question": "can Poland Spring make money in the Sahara?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they'll have to sell a", "extracted_entity": null}
{"question": "do people with mood disorders need permanent institutionalization?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "will Dustin Hoffman likely vote for Trump in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only because he\u2019s a Dem", "extracted_entity": null}
{"question": "can your psychologist say hello to you while you are out at the supermarket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the supermarket is not a psych", "extracted_entity": null}
{"question": "did Johann Sebastian Bach influence heavy metal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "are flag of Gabon colors found in rainbow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the flag of Gabon colors are not", "extracted_entity": null}
{"question": "does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It refers to the name of a cable", "extracted_entity": null}
{"question": "can a grey seal swim in the same water as the subject of Moby Dick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they are not likely to be seen", "extracted_entity": null}
{"question": "did Modern Family win a Slammy award?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it won a Slammy award for", "extracted_entity": null}
{"question": "would Mount Wycheproof be a breeze for Edmund Hillary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. I think he would have no problem.", "extracted_entity": null}
{"question": "does Canada have a relationship with a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "would a compact disc melt in magma?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a compact disc would not melt in", "extracted_entity": null}
{"question": "is Sea World hazardous to leopard seal's health?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is to ours.", "extracted_entity": null}
{"question": "was being a mail carrier considered one of the most dangerous jobs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was considered one of the most b", "extracted_entity": null}
{"question": "were Depeche Mode heavily influenced by blues music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they were influenced by Kraftwerk,", "extracted_entity": null}
{"question": "do people with swallowing disorders need high viscosity drinks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. People with swallowing disorders can", "extracted_entity": null}
{"question": "can whole genome sequencing be used for COVID-19?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, whole genome sequencing is used for", "extracted_entity": null}
{"question": "is Anakin Skywalker from Star Wars associated with the color black?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is.", "extracted_entity": null}
{"question": "do you need to schedule separate preventive healthcare and sickness visits? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You can schedule your preventive healthcare and sick", "extracted_entity": null}
{"question": "was the original Metroid groundbreaking for its polygons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it was groundbreaking for its", "extracted_entity": null}
{"question": "would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because a leap year has 3", "extracted_entity": null}
{"question": "are any of the words that CAPTCHA stands for palindromes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is the Jurassic era a tourist destination?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a dead zone.", "extracted_entity": null}
{"question": "can a dolphin keep a diary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a dolphin can keep a", "extracted_entity": null}
{"question": "is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is hypothetically unimportant between Saladin", "extracted_entity": null}
{"question": "does Carl Linnaeus share the same final resting place as Michael Jackson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's in Uppsala", "extracted_entity": null}
{"question": "could Eddie Murphy dial 911 in a car as a young child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was born in 196", "extracted_entity": null}
{"question": "do flying fish have good eyesight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are blind.", "extracted_entity": null}
{"question": "does Justin Bieber vote in October?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he doesn't.", "extracted_entity": null}
{"question": "did Ice make people rich?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is J.D. Salinger's most successful work influential to killers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It is a classic.", "extracted_entity": null}
{"question": "is it impossible for pigs to use pig latin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pigs can use pig latin.", "extracted_entity": null}
{"question": "can the theory of cultural hegemony explain global warming?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Cultural hegemony is the idea", "extracted_entity": null}
{"question": "is March named after Jupiter's son in Roman mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, March is named after Mars, the Roman", "extracted_entity": null}
{"question": "can furniture be made of hair?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be.", "extracted_entity": null}
{"question": "is SnapCap an example of a retail store?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, SnapCap is not an example of", "extracted_entity": null}
{"question": "did Rand Paul frequently swim in Lake Michigan during his undergraduate years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he did not.", "extracted_entity": null}
{"question": "would Hannah Nixon be proud of Richard Nixon following the Watergate scandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know enough about Hannah N", "extracted_entity": null}
{"question": "would Tony Stark be considered a polymath?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he has a wide range of knowledge and", "extracted_entity": null}
{"question": "is Supreme Court of the United States analogous to High Courts of Justice of Spain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Supreme Court of the United States is", "extracted_entity": null}
{"question": "does Amtrak run from NYC directly to the Moai location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Amtrak does not run to Mo", "extracted_entity": null}
{"question": "does Ronda Rousey avoid BBQ restaraunts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the bbq sauce", "extracted_entity": null}
{"question": "can someone in Uberlandia work for Mitsubishi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because you are not allowed to work for", "extracted_entity": null}
{"question": "do oak trees have leaves during winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, oak trees do have leaves during winter", "extracted_entity": null}
{"question": "is Argon near Neon on the periodic table of elements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Argon is in the same group as", "extracted_entity": null}
{"question": "does the Constitution of the Philippines copy text from the British constitution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Philippine constitution has a lot of", "extracted_entity": null}
{"question": "did Ada Lovelace die tragically young for her era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did", "extracted_entity": null}
{"question": "is dyslexia the most common intellectual disability in US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is estimated that dyslexia", "extracted_entity": null}
{"question": "is the referee at a soccer match highly visible against the field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the referee is a black and white", "extracted_entity": null}
{"question": "does Family Guy take place on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Family Guy takes place in the fictional", "extracted_entity": null}
{"question": "could someone in the Canary Islands fish for largemouth bass?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could.", "extracted_entity": null}
{"question": "did the Presidency of Bill Clinton conclude with his impeachment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it didn't.", "extracted_entity": null}
{"question": "did Marco Polo travel with Christopher Columbus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Marco Polo was a 13", "extracted_entity": null}
{"question": "could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have to be a very", "extracted_entity": null}
{"question": "will a 2 Euro coin float across the Red Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the Red Sea is full of water", "extracted_entity": null}
{"question": "have any murderers outlasted Kane's Royal Rumble record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there have been a few who have", "extracted_entity": null}
{"question": "can the US branch of government that has power over the military also have the power to veto?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Executive Branch can't veto", "extracted_entity": null}
{"question": "are Brussels sprout particularly good for adrenal fatigue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "can oysters be used in guitar manufacturing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you use the oyst", "extracted_entity": null}
{"question": "are parodies of the President of the United States illegal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, as long as they are not defam", "extracted_entity": null}
{"question": "can lobster breathe in the desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, lobster can breathe in", "extracted_entity": null}
{"question": "is Phobos part of the Andromeda galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Phobos is a moon of Mars", "extracted_entity": null}
{"question": "were all of Heracles's children present for his funeral pyre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, all of his children were present for his", "extracted_entity": null}
{"question": "can Africanized bees be considered multicultural?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They are from the same family (A", "extracted_entity": null}
{"question": "is nickel dominant material in US 2020 nickels?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the United States Mint has been using", "extracted_entity": null}
{"question": "did Leonardo da Vinci lack contemporary peers in his home city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was the only artist", "extracted_entity": null}
{"question": "was there fear leading up to the year 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "can you buy furniture and meatballs in the same store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but you can buy a meatball and", "extracted_entity": null}
{"question": "during the Cuban revolution, did the US experience a population boom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the US experienced a population boom after", "extracted_entity": null}
{"question": "could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it had a good start", "extracted_entity": null}
{"question": "can a copy of The Daily Mirror sustain a campfire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it must be the Sunday edition.", "extracted_entity": null}
{"question": "would Bugs Bunny harm an olive tree in the real world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bugs Bunny would not harm an", "extracted_entity": null}
{"question": "did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "can actress Danica McKellar skip astronaut education requirements?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she cannot skip any education requirements, no", "extracted_entity": null}
{"question": "can you transport a coin along a sea of mercury?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as long as the coin is not too", "extracted_entity": null}
{"question": "are the founders of Skype from Asia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "was the tenth Amendment to the Constitution written using Pitman shorthand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it was written in shorthand", "extracted_entity": null}
{"question": "can you save every HD episode of Game of Thrones on Samsung Galaxy A10e?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can't.", "extracted_entity": null}
{"question": "does Super Mario protagonist hypothetically not need continuing education classes in Illinois?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, hypothetically.", "extracted_entity": null}
{"question": "would a body builder prefer an elk burger over a beef burger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "will NY Stock Exchange closing bell be heard in Universal Music Group's headquarters?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's been heard there before.", "extracted_entity": null}
{"question": "could Saint Peter watch television?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was dead.", "extracted_entity": null}
{"question": "are all United States Aldi locations owned by the same company?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Aldi is a German company with over", "extracted_entity": null}
{"question": "was the first Vice President of the United States an Ottoman descendant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "would it be safe to have a jackfruit thrown at your head?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it would be safe to have a jackfruit", "extracted_entity": null}
{"question": "can a woman on average have a baby before wheat seed blooms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a woman can have a baby before whe", "extracted_entity": null}
{"question": "did Donald Trump come up with the idea for the New York Harbor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Statue of Liberty was a", "extracted_entity": null}
{"question": "did Saint Augustine use the Julian calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used the Roman calendar, which was", "extracted_entity": null}
{"question": "do Koalas prefer Eucalyptus over meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Koalas are herbivores", "extracted_entity": null}
{"question": "are goats found on abyssal plains?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Goats are found on continental sh", "extracted_entity": null}
{"question": "are gorillas closely related to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, humans are apes", "extracted_entity": null}
{"question": "do Star Wars fans say \"beam me up\" often?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "does USA fail separation of church and state in multiple ways?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, see the answers", "extracted_entity": null}
{"question": "could Bob Marley's children hypothetically win tug of war against Kublai Khan's children?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The reason is that Bob Marley'", "extracted_entity": null}
{"question": "does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Mad Hatter's tea party, which", "extracted_entity": null}
{"question": "is dessert eaten before breakfast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is eaten after breakfast.", "extracted_entity": null}
{"question": "was Krishna skilled at using the bow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Krishna was a great archer.", "extracted_entity": null}
{"question": "was Aristotle a member of the House of Lords?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not.", "extracted_entity": null}
{"question": "would Ibn Saud tolerate salsa music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he would have loved the idea of", "extracted_entity": null}
{"question": "would a responsible bartender make a drink for Millie Bobby Brown?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She\u2019s not old enough.", "extracted_entity": null}
{"question": "is Bactrian Camel most impressive animal when it comes to number of humps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Dromedary Camel has", "extracted_entity": null}
{"question": "do giraffes require special facilities at zoos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they need a tall building with a long", "extracted_entity": null}
{"question": "can a wheelbarrow full of starch kill hyperglycemics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a wheelbarrow full of sugar", "extracted_entity": null}
{"question": "can a snake wear a snowshoe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if the snowshoe is", "extracted_entity": null}
{"question": "can a false pope become a saint?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he is not a Catholic.", "extracted_entity": null}
{"question": "does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it has a lot of smoggy", "extracted_entity": null}
{"question": "can dementia be cured with a cast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it cannot.", "extracted_entity": null}
{"question": "can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know.", "extracted_entity": null}
{"question": "would Dave Chappelle pray over a Quran?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he wouldn\u2019t, because he\u2019", "extracted_entity": null}
{"question": "is a jellyfish safe from atherosclerosis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is the Riksdag a political entity in Scandinavia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is the legislative branch of the", "extracted_entity": null}
{"question": "did Linnaeus edit Darwin's draft of Origin of Species?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "is Hermione Granger eligible for the Order of the British Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is not a British citizen.", "extracted_entity": null}
{"question": "was Saudi Aramco started due to an assassination?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Aramco was founded by Americans in", "extracted_entity": null}
{"question": "should a Celiac sufferer avoid spaghetti?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If you are a celiac, you", "extracted_entity": null}
{"question": "does Olympia Washington share name with Hephaestus's workshop location?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Olympia, Washington, is named after", "extracted_entity": null}
{"question": "could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the price is $2.5", "extracted_entity": null}
{"question": "does Jason have anything in common with Dr. Disrespect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they both love playing video games", "extracted_entity": null}
{"question": "is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the lifestyle of the", "extracted_entity": null}
{"question": "are ropes required to operate a frigate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can use a rope or not", "extracted_entity": null}
{"question": "did Brad Peyton need to know about seismology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he had to know enough to make", "extracted_entity": null}
{"question": "is Ludacris in same music genre as 2000's Binaural?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not.", "extracted_entity": null}
{"question": "would LeBron James hypothetically glance upwards at Yuri Gagarin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if LeBron James had any interest", "extracted_entity": null}
{"question": "do you need a farmer to make a circuit board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a farmer is not required to make", "extracted_entity": null}
{"question": "did Gandhi watch the television show Bonanza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is Shakespeare famous because of the infinitive form?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Shakespeare is famous because he was a great", "extracted_entity": null}
{"question": "would the crew of Apollo 15 have difficulty riding a unicycle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "do Squidward Tentacles and Alan Greenspan have different musical passions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "can a minor replicate the double-slit experiment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The double-slit experiment is the", "extracted_entity": null}
{"question": "can an anchovy born in 2020 survive 25th US census?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it will be dead by then.", "extracted_entity": null}
{"question": "do you have to put on glasses to read a QR code?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can just use your phone to scan", "extracted_entity": null}
{"question": "would Seroquel be the first treatment recommended by a doctor to someone with depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, maybe.", "extracted_entity": null}
{"question": "do women often need new shoes during their pregnancy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not necessary to change your sho", "extracted_entity": null}
{"question": "would Persephone be a good consultant to a landscape architect?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would be a terrible consultant.", "extracted_entity": null}
{"question": "is University of Pittsburgh easier to enter than FBI?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "University of Pittsburgh is more selective, slower", "extracted_entity": null}
{"question": "will the small intenstine break down a cotton ball?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it may not be the most effective", "extracted_entity": null}
{"question": "do the Eskimos sunbathe frequently?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They sit around all day eating p", "extracted_entity": null}
{"question": "was the French Revolution televised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the Russian Revolution was.", "extracted_entity": null}
{"question": "is the most expensive color in the world Blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is No, the most expensive color in", "extracted_entity": null}
{"question": "did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It was not until 195", "extracted_entity": null}
{"question": "can you substitute the pins in a bowling alley lane with Dustin Hoffman's Oscars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because Oscar is a statue.", "extracted_entity": null}
{"question": "did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was born in 139", "extracted_entity": null}
{"question": "is the Sea of Japan landlocked within Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Sea of Japan is not landlocked", "extracted_entity": null}
{"question": "is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "will Lhamo Thondup be considered by Catholic Church to be a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Not likely.", "extracted_entity": null}
{"question": "would someone in Boston not receive the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, someone in Boston would not receive the Toronto", "extracted_entity": null}
{"question": "is Christmas always celebrated on a Sunday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Christmas can be celebrated on a Monday.", "extracted_entity": null}
{"question": "are pancakes typically prepared in a pot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are typically prepared in a skillet", "extracted_entity": null}
{"question": "has Ringo Starr been in a relatively large number of bands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "did Richard III know his grandson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was a boy when he died,", "extracted_entity": null}
{"question": "can Kane challenge Joe Biden in this year's primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Not unless he wants to be a third", "extracted_entity": null}
{"question": "did Stone Cold Steve Austin wrestle in three different centuries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he wrestled in the 19", "extracted_entity": null}
{"question": "do embalmed bodies feel different at funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "are leaves from coca good for gaining weight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, leaves from coca are good for gain", "extracted_entity": null}
{"question": "did the Beatles write any music in the Disco genre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they did write \"I Want", "extracted_entity": null}
{"question": "is Nine Inch Nails a good guest for students in earliest grade to take Iowa tests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they should be watching the news.", "extracted_entity": null}
{"question": "did Harvey Milk ever run for governor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he ran for supervisor", "extracted_entity": null}
{"question": "is a bengal fox likely to see the Superbowl?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "could all Tahiti hotels hypothetically accommodate US D-Day troops?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The US military was too large to fit", "extracted_entity": null}
{"question": "was Amazon involved in the lunar landing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were the main sponsor of the", "extracted_entity": null}
{"question": "could the Spice Girls compete against \u017dRK Kumanovo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Spice Girls have the same number", "extracted_entity": null}
{"question": "are saltwater crocodiles related to alligators?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are both crocodiles, but", "extracted_entity": null}
{"question": "would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would not be concerned with hypothermia.", "extracted_entity": null}
{"question": "has every astronaut survived their space journey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some have died.", "extracted_entity": null}
{"question": "can any person with a driver's license work in transport of aviation fuel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. A driver must have a commercial driver'", "extracted_entity": null}
{"question": "could a snowy owl survive in the Sonoran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it could not.", "extracted_entity": null}
{"question": "can you buy chlorine at a dollar store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can buy chlorine at a", "extracted_entity": null}
{"question": "is the voice of the Genie from Disney's Aladdin still alive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is.", "extracted_entity": null}
{"question": "was the amount of spinach Popeye ate unhealthy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was a great source of vitamin", "extracted_entity": null}
{"question": "would an explosion at a gunpowder storage facility result in a supersonic shock wave?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the explosion would probably not be", "extracted_entity": null}
{"question": "could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is too short.", "extracted_entity": null}
{"question": "can a person be diagnosed with a Great Depression?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a person cannot be diagnosed with a", "extracted_entity": null}
{"question": "could Godzilla have been killed by the Tohoku earthquake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not, but he could have been killed by", "extracted_entity": null}
{"question": "is retail a job anybody can be suited for?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have the right personality.", "extracted_entity": null}
{"question": "does cell biology teach about the life cycle of Al Qaeda?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not.", "extracted_entity": null}
{"question": "did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "has Burger King  contributed to a decrease in need for snowshoes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that is a good thing.", "extracted_entity": null}
{"question": "would a snake have reasons to fear a honey badger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a honey badger would not have", "extracted_entity": null}
{"question": "do most people only memorize slightly over half of their ZIP code?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (see below)", "extracted_entity": null}
{"question": "does Happy Gilmore Productions CEO own a Torah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Happy Gilmore Productions CEO does", "extracted_entity": null}
{"question": "does Mercury make for good Slip N Slide material?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is very slippery.", "extracted_entity": null}
{"question": "is RoboCop director from same country as Gaite Jansen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "was the Parc des Princes fully operational during June of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was closed.", "extracted_entity": null}
{"question": "would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "do worshipers of Shiva make a pilgrimage to the Holy Land?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Shiva is not a Jew.", "extracted_entity": null}
{"question": "was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was sandwiched between William Howard", "extracted_entity": null}
{"question": "was it typical to see Johnny Cash on stage in a rainbow-colored outfit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not at all.", "extracted_entity": null}
{"question": "is slitting your wrists an unreliable suicide method?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you don't slit your", "extracted_entity": null}
{"question": "would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure.", "extracted_entity": null}
{"question": "is Christopher Nolan indebted to Bob Kane?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nolan is a great filmmaker.", "extracted_entity": null}
{"question": "did Kim Il-sung network on LinkedIn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t.", "extracted_entity": null}
{"question": "could all People's Volunteer Army hypothetically be transported on Symphony of the Seas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, all of them.", "extracted_entity": null}
{"question": "is Bern a poor choice for a xenophobic Swiss citizen to live?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There is no xenophobia", "extracted_entity": null}
{"question": "would a vegan prefer a natural bongo drum over a synthetic one?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the synthetic one will sound better", "extracted_entity": null}
{"question": "is an inappropriate lullaby Love Song from November 11, 2000?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the lullaby Love Song from November", "extracted_entity": null}
{"question": "does Orange County, California require airplanes to be quiet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Orange County does not have any noise reg", "extracted_entity": null}
{"question": "are kayaks used at the summit of Mount Everest?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not yet.", "extracted_entity": null}
{"question": "would someone on a keto diet be able to eat Dosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Dosa is a South Indian dish made from", "extracted_entity": null}
{"question": "was Raphael's paintings influenced by the country of Guam?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. Raphael was inspired by the art", "extracted_entity": null}
{"question": "is it normal to find parsley in multiple sections of the grocery store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it's not uncommon to find", "extracted_entity": null}
{"question": "was the subject of Parsifal taken from British folklore?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was taken from the story of Sir", "extracted_entity": null}
{"question": "do drummers need spare strings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Drummers need spare strings.", "extracted_entity": null}
{"question": "would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. stomata are open in the day", "extracted_entity": null}
{"question": "is there a jukebox musical about a sweet transvestite from Transexual, Transylvania?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it\u2019s called \u201cDead Man", "extracted_entity": null}
{"question": "did Malcolm X use Unicode?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he used ASCII", "extracted_entity": null}
{"question": "would the fastest tortoise win a race against a Chicago \"L\"?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the \"L\" would win", "extracted_entity": null}
{"question": "does Hammurabi's Code violate Christians Golden Rule?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does", "extracted_entity": null}
{"question": "does the Roman god Vulcan have a Greek equivalent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hephaestus", "extracted_entity": null}
{"question": "can the Dalai Lama fit in a car?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he can, but he won't", "extracted_entity": null}
{"question": "is watermelon safe for people with a tricarboxylic acid allergy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, watermelon is safe for people", "extracted_entity": null}
{"question": "is a Chinchilla breed of felis catus a type of rodent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chinchillas are rodents.", "extracted_entity": null}
{"question": "can printing books in kanji instead of the Roman alphabet save trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can save a lot of time", "extracted_entity": null}
{"question": "is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it depends on the car and the sound barrier", "extracted_entity": null}
{"question": "would a Rockette look odd with a moustache? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what a Rockette is", "extracted_entity": null}
{"question": "would Hodor hypothetically be a good math mathematician?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no, because of his name.", "extracted_entity": null}
{"question": "would Hades and Osiris hypothetically compete for real estate in the Underworld?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hades and Osiris were not in competition", "extracted_entity": null}
{"question": "would a German Shepherd be welcome in an airport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. They\u2019re too big.", "extracted_entity": null}
{"question": "does a person need a college degree to become a bartender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it will help", "extracted_entity": null}
{"question": "is capsaicin associated with cooking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, capsaicin is found in pe", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger be unable to run for President of the US?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he could", "extracted_entity": null}
{"question": "when Hugh Jackman was a teacher, would he have taught The Great Gatsby?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "is Fiat Chrysler associated with Japanese cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Fiat Chrysler is associated with", "extracted_entity": null}
{"question": "can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "would many meals heavy in brussels sprouts benefit someone on Coumadin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Brussels sprouts are rich in vitamin", "extracted_entity": null}
{"question": "is it safe to share silverware with an HIV positive person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is no. HIV is spread through", "extracted_entity": null}
{"question": "do movies always show nerds as the losers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, sometimes they are the heroes.", "extracted_entity": null}
{"question": "are eagles and young bears both used as labels for skills-training youth groups?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and no.", "extracted_entity": null}
{"question": "could ancient Tony Bennett have a baby in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but his great-great-gre", "extracted_entity": null}
{"question": "can a lifeboat rescue people in the Hooke Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Hooke Sea is too big", "extracted_entity": null}
{"question": "would a stool be useful for a Lusotitan to reach the top of an almond tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a stool would not be useful for", "extracted_entity": null}
{"question": "would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would have.", "extracted_entity": null}
{"question": "would somebody leave reiki with bruises?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If they were not ready for it.", "extracted_entity": null}
{"question": "is there a popular Broadway character who is a missionary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there is a popular Broadway character who is", "extracted_entity": null}
{"question": "would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would", "extracted_entity": null}
{"question": "are months based on the solar cycle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are based on the moon.", "extracted_entity": null}
{"question": "do many fans of J.K Rowling know who Alan Rickman is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they've seen Harry Potter", "extracted_entity": null}
{"question": "could someone in a coma experience fear?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not possible.", "extracted_entity": null}
{"question": "could Johnny Carson's children fill out a water polo team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But it would be a weak team.", "extracted_entity": null}
{"question": "would downloading Mario 64 on an emulator be legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It is illegal to download copyrighted", "extracted_entity": null}
{"question": "are Tom and Jerry featured in a ride at Disneyland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "can a blind person tell time by Big Ben?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if they can't tell time by Big Ben", "extracted_entity": null}
{"question": "are sea turtles enjoying life during quarantine?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they are living in the sea.", "extracted_entity": null}
{"question": "has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "does Paulo Coelho's wife make a living through speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. He is an excellent listener and", "extracted_entity": null}
{"question": "is Final Fantasy VI closer to beginning than end of its franchise?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. it is closer to end than beginning.", "extracted_entity": null}
{"question": "were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the deaths from Apollo 1", "extracted_entity": null}
{"question": "is chaff produced by hydropower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Hydropower is a renewable", "extracted_entity": null}
{"question": "is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a U.S. passport is", "extracted_entity": null}
{"question": "is Garfield known for hating italian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he\u2019s known for hating Mond", "extracted_entity": null}
{"question": "is basil safe from Hypervitaminosis D?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is safe.", "extracted_entity": null}
{"question": "does a kangaroo incubate its offspring?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it does not", "extracted_entity": null}
{"question": "did J. Edgar Hoover take his calls in Langley, Virginia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The FBI headquarters is in Washington,", "extracted_entity": null}
{"question": "is Elijah part of a Jewish holiday?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Elijah is part of the Passover S", "extracted_entity": null}
{"question": "can a cheetah generate enough force to topple Big Show?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It can't.", "extracted_entity": null}
{"question": "could Buzz Aldrin have owned a computer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could have.", "extracted_entity": null}
{"question": "was Mercedes-Benz associated with the Nazis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were.", "extracted_entity": null}
{"question": "did Johnny Carson win enough Emmy's to fill a carton if Emmy's were eggs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did win enough to fill a", "extracted_entity": null}
{"question": "can bottlenose dolphins hypothetically outbreed human women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "if a baby was born on Halloween would they be a Scorpio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would be a Sagittarius", "extracted_entity": null}
{"question": "could the Great Wall of China connect the Dodgers to the White Sox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's too long.", "extracted_entity": null}
{"question": "would a honey badger fit inside an oven?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not if you try to put it", "extracted_entity": null}
{"question": "did Heracles famous labors exceed a baker's dozen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he had 12 labors.", "extracted_entity": null}
{"question": "does Guam have a state capital?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is a territory of the US,", "extracted_entity": null}
{"question": "are Disney's seven dwarves the original ones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There were seven dwarves in the", "extracted_entity": null}
{"question": "does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he goes to the Hague only when", "extracted_entity": null}
{"question": "is Jesse W. Moore a potential recipient of a Snoopy-themed award from NASA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "has Gorillaz creator been in more bands than Bernard Sumner?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only just.", "extracted_entity": null}
{"question": "is it safe to eat kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is safe to eat kidney.", "extracted_entity": null}
{"question": "is zoology unconcerned with strigoi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, zoology is unconcerned", "extracted_entity": null}
{"question": "did the phone Alexander Graham Bell use have call waiting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the phone was invented in 1", "extracted_entity": null}
{"question": "were paparazzi directly responsible for the death of Amy Winehouse?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Paparazzi were responsible for the", "extracted_entity": null}
{"question": "on August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "are more watermelons grown in Brazil than Antarctica?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it\u2019s not too far off", "extracted_entity": null}
{"question": "does store bought milk have cream at the top?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it has fat at the top.", "extracted_entity": null}
{"question": "could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if the port was a square 2", "extracted_entity": null}
{"question": "did eggs need to be kept cold in the middle ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, eggs were kept at room temperature.", "extracted_entity": null}
{"question": "would Topa Inca Yupanqui have encountered the western honey bee?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was in Peru, not Europe", "extracted_entity": null}
{"question": "is \"A Tale of Two Cities\" a popular science novel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is a novel", "extracted_entity": null}
{"question": "was Amy Winehouse a fan of Star Wars: Rogue One?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she was a fan of The Empire St", "extracted_entity": null}
{"question": "can shooting bald eagle get a person more prison time than Michael Vick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES, because it is illegal to kill Bald E", "extracted_entity": null}
{"question": "could you drive a Rowe 550 to the 2008 Summer Olympics?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you'd need to be in the", "extracted_entity": null}
{"question": "can you go water skiing on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Venus is the hottest planet", "extracted_entity": null}
{"question": "would a Wolverine and a Lynx be hard to tell apart?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If you don't know what a Wolver", "extracted_entity": null}
{"question": "could Plato have agreed with the beliefs of Jainism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he could have.", "extracted_entity": null}
{"question": "do bald eagles nest on Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they nest on the riverbank.", "extracted_entity": null}
{"question": "could James Brown's ex-wives hold a doubles game of tennis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they were playing with a tennis ball", "extracted_entity": null}
{"question": "should Peter Griffin be an expert at the craft of brewing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s a drunken id", "extracted_entity": null}
{"question": "did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Tex Watson, Patricia Kren", "extracted_entity": null}
{"question": "is there any absolute way to prevent abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Abortion is the taking of a", "extracted_entity": null}
{"question": "is Saturn named after king of gods in Greek mythology?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Saturn is named after the Greek god", "extracted_entity": null}
{"question": "were any of despised Pope Alexander VI's descendants canonized?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The only one I know of is a daughter of", "extracted_entity": null}
{"question": "would it be difficult to snowboard on Venus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be easy to snowboard on", "extracted_entity": null}
{"question": "can chemicals in onion help create a thermonuclear bomb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know. But I\u2019ll", "extracted_entity": null}
{"question": "are anchovies associated with Italian food?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "did Operation Barbarossa or Barbarossa's last expedition succeed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it succeeded", "extracted_entity": null}
{"question": "has numerology helped shape hotel layouts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Absolutely! It\u2019s an integral part", "extracted_entity": null}
{"question": "did The Who have to cancel tours due to World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they didn't.", "extracted_entity": null}
{"question": "can an art dealer buy Boeing 737-800 with a Da Vinci painting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "would a crocodile survive longer in Great Salt Lake than alligator?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because crocodiles can tolerate salt", "extracted_entity": null}
{"question": "would a clouded leopard encounter an awake pangolin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "does acupuncture cause pain in many people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, acupuncture does cause pain in", "extracted_entity": null}
{"question": "are twins always born during the same year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I was born 10 months apart", "extracted_entity": null}
{"question": "could a nymph tick pass through a standard hole punch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are too large.", "extracted_entity": null}
{"question": "would WWF be angrier if you killed koala instead of black swan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, koala is not a native animal", "extracted_entity": null}
{"question": "does New Year's Day always occur on a Wednesday?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it can occur on any day of the", "extracted_entity": null}
{"question": "is a pottery kiln inappropriate for use with glass blowing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not.", "extracted_entity": null}
{"question": "do people of the Iyer caste eat meat?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't.", "extracted_entity": null}
{"question": "do carpenters understand geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, geometry is the study of shapes and space", "extracted_entity": null}
{"question": "did the crew of Apollo 15 take pictures of Mount Sharp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the crew of Apollo 15", "extracted_entity": null}
{"question": "are human footprints absent from Mount Sharp?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "could Tom Cruise explain mental auditing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he couldn\u2019t.", "extracted_entity": null}
{"question": "is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s a car", "extracted_entity": null}
{"question": "would drinking a glass of lemonade provide Vitamin C?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lemonade is mostly water and sugar", "extracted_entity": null}
{"question": "does a game engine have a fuel injector?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "did J. D. Salinger ever ask his father for a quincea\u00f1era?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were not.", "extracted_entity": null}
{"question": "is the Joker in a healthy romantic relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would an adherent of Zoroastrianism consult the Quran for religious guidance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Quran is a false book", "extracted_entity": null}
{"question": "would a diet of ice eventually kill a person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would eventually kill a person.", "extracted_entity": null}
{"question": "can binary numbers and standard alphabet satisfy criteria for a strong password?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Phobos (moon) name origin similar to Roman god Pavor?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the name is derived from the Greek word", "extracted_entity": null}
{"question": "would members of Blue Lives Matter support every element of Grand Theft Auto III?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because some of the characters are gang members", "extracted_entity": null}
{"question": "will twenty pea pods contents cover entire chess board?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, 20 pea pods contents", "extracted_entity": null}
{"question": "was the Joker an enemy of the Avengers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was an enemy of Batman.", "extracted_entity": null}
{"question": "could Christopher Nolan borrow pants from Danny Devito?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by \u201c", "extracted_entity": null}
{"question": "could you watch Naruto and Puzzle Place on the same channel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's called Nicktoons.", "extracted_entity": null}
{"question": "is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is located in a city with a", "extracted_entity": null}
{"question": "was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the peak of the Andes was not", "extracted_entity": null}
{"question": "does Fraktur have a sordid history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it does.", "extracted_entity": null}
{"question": "would a hedgehog avoid animals without a spinal cord?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A hedgehog has no interest", "extracted_entity": null}
{"question": "can you worship Ahura Mazda at a mosque?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can't.", "extracted_entity": null}
{"question": "would a moose hypothetically be too much for a minotaur to devour whole?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a minotaur can devour", "extracted_entity": null}
{"question": "is Gandalf hypothetically a formidable foe for Charmed's Barbas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Barbas is a demon of the first", "extracted_entity": null}
{"question": "do suburbs encourage the use of cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. because there is no other way to get", "extracted_entity": null}
{"question": "could Al Capone have read works from the Harlem Renaissance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Harlem Renaissance was a literary movement", "extracted_entity": null}
{"question": "can pancreas removal cause bankruptcy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you have pancreas removal you", "extracted_entity": null}
{"question": "is shrimp prevalent in Ethiopian cuisine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "does Marco Rubio have a close relationship with Allah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He is a Muslim.", "extracted_entity": null}
{"question": "does The Doctor keep his ship in his childhood home?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the TARDIS is located in the", "extracted_entity": null}
{"question": "did Archduke Franz Ferdinand of Austria participate in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he died in 1914", "extracted_entity": null}
{"question": "is a beard is moss that grows on a human?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a beard is a moss", "extracted_entity": null}
{"question": "is there radiation where Nikola Tesla once worked?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There is a radiation zone.", "extracted_entity": null}
{"question": "would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Artemis was a Greek goddess", "extracted_entity": null}
{"question": "could one Amazon share ever buy twenty year Netflix subscription?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if the Amazon share price stays", "extracted_entity": null}
{"question": "would you have to wear a coat when on Phobos?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the atmosphere is too thin for a coat", "extracted_entity": null}
{"question": "is Cambodia too expensive for 2020 richest billionaire to own?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not expensive for richest billion", "extracted_entity": null}
{"question": "did Emma Stone pursue a higher education?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she did not.", "extracted_entity": null}
{"question": "was San Antonio the site of a major battle in the 19th century?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "the battle of the Alamo", "extracted_entity": null}
{"question": "can the United Nations Framework Convention on Climate Change be held at the Javits Center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the UNFCCC is held", "extracted_entity": null}
{"question": "can music be used as a weapon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can.", "extracted_entity": null}
{"question": "could an ocelot outrun a kindergartner? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are both 1500", "extracted_entity": null}
{"question": "was Elizabeth II the Queen during the Persian Gulf War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was Queen when the Iraq war", "extracted_entity": null}
{"question": "would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if it is in Pis", "extracted_entity": null}
{"question": "did Woodrow Wilson consider Blacks to be equal members of society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Wilson did not consider Blacks to be", "extracted_entity": null}
{"question": "if your electric stove has a glass top, should you use cast iron skillets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you should use non-stick p", "extracted_entity": null}
{"question": "can soup be eaten with the hands?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, soup can be eaten with the hands", "extracted_entity": null}
{"question": "did Cleopatra have ethnicity closer to Egyptians than Greeks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she was of Macedonian descent,", "extracted_entity": null}
{"question": "does the judo rank system reach the triple digits?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does.", "extracted_entity": null}
{"question": "was the son of Tsar Nicholas a daredevil?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a daredevil.", "extracted_entity": null}
{"question": "did Bill Gates achieve Latin honors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Gates was a member of the L", "extracted_entity": null}
{"question": "has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he has.", "extracted_entity": null}
{"question": "if Goofy were a pet, would he need heartworm prevention?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, heartworm prevention is recommended for", "extracted_entity": null}
{"question": "did Richard Wagner support the Nazis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was historical Dracula from a town in Bucharest?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was from a town in Transylvan", "extracted_entity": null}
{"question": "is electricity necessary to balance an account in Microsoft Excel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is necessary to balance an account in", "extracted_entity": null}
{"question": "did Pink Floyd have a song about the French Riviera?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "\u201cRiviera\u201d", "extracted_entity": null}
{"question": "has the creator of Futurama lived in multiple centuries?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did live in multiple cities.", "extracted_entity": null}
{"question": "is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The sternum is not connected to any of the", "extracted_entity": null}
{"question": "did Elizabeth I of England own any viscose fiber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she did.", "extracted_entity": null}
{"question": "could a cat ride Panzer VIII Maus tank missile from Barcelona to Madrid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it could not", "extracted_entity": null}
{"question": "did Jack Dempsey fight the current WBC heavyweight champion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "would a cauliflower farmer prosper at a latitude of 75\u00b0 N?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the cauliflower would not prosper", "extracted_entity": null}
{"question": "would a student in eleventh grade be unable to run for president of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because he or she is not old enough", "extracted_entity": null}
{"question": "was Amy Winehouse familiar with Brexit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She was familiar with Blairite Labour", "extracted_entity": null}
{"question": "can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she can.", "extracted_entity": null}
{"question": "are Scottish people descended from Mary, Queen of Scots part French?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "are quadrupeds represented on Chinese calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "is the Louvre in billionaire George Soros's price range?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
{"question": "would a duke hypothetically be subservient to a Tsar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know if there would be a", "extracted_entity": null}
{"question": "can a jet plane be made without society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because society is made of people, and", "extracted_entity": null}
{"question": "would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Jimmy Vee is eligible to compete", "extracted_entity": null}
{"question": "could Ryan Crouser throw a bengal fox with ease?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he could.", "extracted_entity": null}
{"question": "would a physician be unlikely to recommend Reiki?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a physician would be unlikely to recommend", "extracted_entity": null}
{"question": "can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he can fund the debt of", "extracted_entity": null}
{"question": "can Burundi's communicate with citizens of New Brunswick?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Burundi's can communicate with", "extracted_entity": null}
{"question": "is Morocco an ideal location for water skiing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is, it is in the middle", "extracted_entity": null}
{"question": "did Al Capone carry a smartphone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did have a radio.", "extracted_entity": null}
{"question": "was Mark Twain a struggling inventor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a famous author", "extracted_entity": null}
{"question": "is B's place in alphabet same as Prince Harry's birth order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the place in alphabet is the same.", "extracted_entity": null}
{"question": "would a vegan eat a traditional Paella dish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a vegan would eat a traditional Pa", "extracted_entity": null}
{"question": "is cow methane safer for environment than cars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because methane is a more effective", "extracted_entity": null}
{"question": "was Darth Vader monogamous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "were there footprints on the moon in 1960?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there were footprints on the moon", "extracted_entity": null}
{"question": "does Pantheon in Paris have a unique name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is the most famous building in Paris", "extracted_entity": null}
{"question": "can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can understand some of the Albanian", "extracted_entity": null}
{"question": "would a Beaver's teeth rival that of a Smilodon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Beavers do not have the same", "extracted_entity": null}
{"question": "can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you'd have to make it", "extracted_entity": null}
{"question": "do calico cat patterns cover every drain fly color variety?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. drain flies are not all cal", "extracted_entity": null}
{"question": "are all cucumbers the same texture?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not", "extracted_entity": null}
{"question": "would Library of Alexandria need less shelf space than Library of Congress?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I'm not sure why you'", "extracted_entity": null}
{"question": "would a spider wasp be more effective than a bullet ant to stop a criminal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be more effective.", "extracted_entity": null}
{"question": "would a triples tandem bike support Apollo 15 crew?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It is not wide enough.", "extracted_entity": null}
{"question": "if one of your feet is in a leg cast, should the other be in a sandal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if one of your feet is in a leg cast", "extracted_entity": null}
{"question": "did Klingons appear in the movie The Last Jedi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only in a blink-and", "extracted_entity": null}
{"question": "can you find Depala's race in World of Warcraft?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they don't exist.", "extracted_entity": null}
{"question": "would a Dodo hypothetically tower over Ma Petite?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a Dodo would not tower over Ma", "extracted_entity": null}
{"question": "can the Very Large Telescope observe the largest mountain on Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Very Large Telescope is", "extracted_entity": null}
{"question": "was Morris County named after a chief justice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, William Paterson, who was the first", "extracted_entity": null}
{"question": "is the Gujarati script the same category of script as Kanji?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Kanji is an ideographic script.", "extracted_entity": null}
{"question": "did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, earth was still orbiting the sun at", "extracted_entity": null}
{"question": "would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not unusual.", "extracted_entity": null}
{"question": "if Martin Luther did one theses a day would he run out in half a year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so.", "extracted_entity": null}
{"question": "are honey badgers and hyenas anatomically dissimilar? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "did Al Pacino act in a movie during World War II?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was born in 194", "extracted_entity": null}
{"question": "could you watch a new Seinfeld episode every day for a year?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I'd watch it again", "extracted_entity": null}
{"question": "did Barack Obama participate in the Reformation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was too busy making a documentary", "extracted_entity": null}
{"question": "did President William Howard Taft read DC Comics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The finisher is a move that requires", "extracted_entity": null}
{"question": "are there Americans still enlisted in the Confederate States Army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are.", "extracted_entity": null}
{"question": "are right wing Amreicans opposed to marxism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "can a sea turtle play tennis using a tennis racket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Turtles can play tennis with a", "extracted_entity": null}
{"question": "in a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "is shoe soup innocuous?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they watched a lot of the Japanese", "extracted_entity": null}
{"question": "did Holy Saturday 2019 have special significance to pot smokers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was the first time since 1", "extracted_entity": null}
{"question": "do you need lactobacillus to make pickles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You can pickle without Lactob", "extracted_entity": null}
{"question": "would a Gray Whale fit easily in an above ground pool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "will a celibate cleric likely suffer a stoning in Somalia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a celibate cleric will", "extracted_entity": null}
{"question": "could Little Women have been a book read by veterans of the civil war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it was written in 18", "extracted_entity": null}
{"question": "is MIX a word and a roman numeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, both", "extracted_entity": null}
{"question": "would Downton Abbey finale viewership defeat every Kazakhstan citizen in tug of war?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a very short t", "extracted_entity": null}
{"question": "was dynamite used during Middle Ages warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was invented in 18", "extracted_entity": null}
{"question": "is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "can Kate Gosselin's household fill out a Bandy team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they'd have to use the", "extracted_entity": null}
{"question": "is the tree species that the name Leipzig refers to an evergeen tree?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "will Gremlins sequels tie number of Matrix sequels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are all released within 5", "extracted_entity": null}
{"question": "is Snow White an example of good consent?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. She was a child, and had no", "extracted_entity": null}
{"question": "is MF Doom a Fantastic Four villain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s a Marvel villain", "extracted_entity": null}
{"question": "would most grand masters know what the French Defense is?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, most grandmasters know the French Def", "extracted_entity": null}
{"question": "are there options for students who struggle to understand the writing style of Othello?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There are many options for students who struggle to understand", "extracted_entity": null}
{"question": "would an environmentalist advocate for preventing domestic canine reproduction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. environmentalists advocate for preventing human", "extracted_entity": null}
{"question": "could a monolingual American read Lenovo's native name?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Lenovo's name is not a", "extracted_entity": null}
{"question": "can 1980 United States presidential election result be considered a photo finish?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because of the 271 elect", "extracted_entity": null}
{"question": "did Cynthia Powell celebrate a silver anniversary with John Lennon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope, she married him on 20", "extracted_entity": null}
{"question": "is it possible for biologist Rachel Carson to have flown to the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is a biologist, not an", "extracted_entity": null}
{"question": "did Confederate States Army influence West Point fashion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not really. West Point fashion was influenced", "extracted_entity": null}
{"question": "would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he\u2019s already won one.", "extracted_entity": null}
{"question": "is the Mona Lisa in the same museum as the Venus de Milo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are in different museums in different", "extracted_entity": null}
{"question": "does the land in close proximity to beaver dams suffer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but that is not what this study is", "extracted_entity": null}
{"question": "was Jackson Pollock trained by Leonardo da Vinci?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was influenced by the Renaissance", "extracted_entity": null}
{"question": "did Francois Mitterrand ever meet Barak Obama while they both held the position of President?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, they met twice.", "extracted_entity": null}
{"question": "would Carrie Poppy be likely to trust a psychic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She's a very open-mind", "extracted_entity": null}
{"question": "would stargazers prefer binoculars over a telescope?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, stargazers would prefer a teles", "extracted_entity": null}
{"question": "is it possible to binge entire EastEnders series without water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, I don't think so.", "extracted_entity": null}
{"question": "was Mother Theresa a follower of atheism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she was a Catholic nun.", "extracted_entity": null}
{"question": "is the BBC World Service hosted in Europe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not in London.", "extracted_entity": null}
{"question": "did Melania Trump have same profession as Olga Kurylenko?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Both were models.", "extracted_entity": null}
{"question": "was Robert Downey Jr. a good role model as a young man?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "should a finished website have lorem ipsum paragraphs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Lorem ipsum paragraphs are the foundation", "extracted_entity": null}
{"question": "would Jacques Du\u00e8ze have been friends with Richard Dawkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course", "extracted_entity": null}
{"question": "would Gordon Ramsey use uranium as a seasoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he would use it as a decor", "extracted_entity": null}
{"question": "does Robert De Niro use a microscope at work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he does not.", "extracted_entity": null}
{"question": "is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is a catch-phrase used", "extracted_entity": null}
{"question": "are pirate lieutenants like navy lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pirate lieutenants are not like", "extracted_entity": null}
{"question": "does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the surgery may have finished before", "extracted_entity": null}
{"question": "does Homer Simpson need two hands worth of fingers to count to 5?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he can use one hand.", "extracted_entity": null}
{"question": "can you see the moon in Wembley Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it's in the sky", "extracted_entity": null}
{"question": "is the Asian black bear multicolored?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not multicolored.", "extracted_entity": null}
{"question": "can United States Secretary of State do crimes in U.K. without being arrested?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but if he is arrested, the U", "extracted_entity": null}
{"question": "did Alice's Adventures in Wonderland inspire Macbeth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it did", "extracted_entity": null}
{"question": "could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it can be used as a source", "extracted_entity": null}
{"question": "was England directly involved in the Arab-Israeli conflict?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was, and still is.", "extracted_entity": null}
{"question": "can an adult male stand on top Donatello's bronze David and touch the Sistine Chapel ceiling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but you have to be a 6", "extracted_entity": null}
{"question": "was Dioskourides a lapidary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was a lapidary, but", "extracted_entity": null}
{"question": "would a CEO typically clean the toilets in a company's building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that's what janitors are for", "extracted_entity": null}
{"question": "can you purchase General Motors products at a movie theater?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But you can buy popcorn and", "extracted_entity": null}
{"question": "did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Xena herself, Lucy Lawless,", "extracted_entity": null}
{"question": "could all of the people who pass through 30th Street Station every day fit in Dorton Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it\u2019s not far off", "extracted_entity": null}
{"question": "can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it is a pretty tight sque", "extracted_entity": null}
{"question": "could all the unemployed people due to 1933 Great Depression fit in Tiger Stadium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they could fit in the old Com", "extracted_entity": null}
{"question": "would a duck ever need a Caesarean section?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A duck would never need a Ca", "extracted_entity": null}
{"question": "is nickel a better payout than mercury if given a dollar per atomic number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because mercury is more expensive.", "extracted_entity": null}
{"question": "does Pikachu like Charles Darwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I know he", "extracted_entity": null}
{"question": "can Simon Cowell vote for the next Supreme Court judge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s British", "extracted_entity": null}
{"question": "is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because it's not a matter of", "extracted_entity": null}
{"question": "is US route 1 dominated by historically red states?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. And that's why it's", "extracted_entity": null}
{"question": "can a cell fit inside of a shoebox?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the box is big enough", "extracted_entity": null}
{"question": "is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's portrayed as a negative", "extracted_entity": null}
{"question": "can Tame Impala's studio band play a proper game of Hot Potato?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they can play a proper game of", "extracted_entity": null}
{"question": "is the title of Shirley Bassey's 1971 diamond song a true statement?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a lie", "extracted_entity": null}
{"question": "would P. G. Wodehouse be taught in second grade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he were a second grader.", "extracted_entity": null}
{"question": "can Curiosity take samples of rocks from Lacus Temporis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope.", "extracted_entity": null}
{"question": "would Phineas and Ferb enjoy winter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think Phineas and Ferb would enjoy", "extracted_entity": null}
{"question": "could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "can children become lieutenants?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. But the game is meant to be played", "extracted_entity": null}
{"question": "could you drive from New England to a Sainsbury's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. We have to drive to a Sains", "extracted_entity": null}
{"question": "do seven McDonald's hamburgers exceed USDA recommended fat allowance?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. According to the USDA, the recommended", "extracted_entity": null}
{"question": "was Mozart accused of stealing from Richard Wagner?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he was.", "extracted_entity": null}
{"question": "will Justin Bieber take over Mike Pence's position in 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES! He will be our next president.", "extracted_entity": null}
{"question": "did any citizen of San Antonio vote for Boris Johnson?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No", "extracted_entity": null}
{"question": "does being good at guitar hero make you a good guitarist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it makes you a good guitar hero.", "extracted_entity": null}
{"question": "can Roman numerals fill the normal number of Sudoku box options?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "is a Cassowary safer pet than a crane?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, a crane is much safer than", "extracted_entity": null}
{"question": "was proofreading Edgar Allan Poe works lucrative?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I would say", "extracted_entity": null}
{"question": "was Hillary Clinton's deputy chief of staff in 2009 baptised?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and he was also a former head of", "extracted_entity": null}
{"question": "can dessert be made with vegetables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I made a delicious vegetable dess", "extracted_entity": null}
{"question": "would Othello be Shakespeare's play to buy Scheherazade most time with king?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Othello would not be the play", "extracted_entity": null}
{"question": "did Jon Brower Minnoch suffer from anorexia nervosa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Jon Brower Minnoch suffered from", "extracted_entity": null}
{"question": "does ancient Olympics crown fail to hide tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it's a bad wig", "extracted_entity": null}
{"question": "can someone with dermatitis be a hand model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it is very rare.", "extracted_entity": null}
{"question": "is it bad to have lactic acid in your body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. It is the by", "extracted_entity": null}
{"question": "are cucumbers often found in desert?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cucumbers are not found in", "extracted_entity": null}
{"question": "are blue lips normal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Blue lips are not normal. They are usually a", "extracted_entity": null}
{"question": "can someone with celiac disease have potato vodka?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not if it contains wheat or bar", "extracted_entity": null}
{"question": "are deaf people left out of enjoying music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "NO. They can enjoy music too. They just", "extracted_entity": null}
{"question": "phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Because he was only pope for 33", "extracted_entity": null}
{"question": "would Cardi B. benefit from soy milk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it's too late. She", "extracted_entity": null}
{"question": "are the names of The Powerpuff Girls alliterative? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The names of the Powerpuff Girls are not", "extracted_entity": null}
{"question": "did the leader of Heaven's Gate consider himself a prophet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "would King Leonidas have succeeded with an army the size of Mozart's compositions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "do Elementary School students typically need graphing calculators?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the student.", "extracted_entity": null}
{"question": "can Family of Barack Obama ride comfortably in 2020 Jaguar F Type?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they cannot.", "extracted_entity": null}
{"question": "could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they could.", "extracted_entity": null}
{"question": "does the Eighth Amendment to the United States Constitution protect freedom of speech?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Eighth Amendment protects", "extracted_entity": null}
{"question": "do most fans follow Katy Perry for gospel music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, most fans follow her for her music and", "extracted_entity": null}
{"question": "does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he does.", "extracted_entity": null}
{"question": "are rainbows devoid of color made by mixing yin and yang colors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "did Christopher Columbus sail representing a different country than his original home?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He sailed representing the Kingdom of Spain", "extracted_entity": null}
{"question": "can an Asian black bear use chopsticks?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a pain in the", "extracted_entity": null}
{"question": "did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they did with the Coen brothers", "extracted_entity": null}
{"question": "would a Durian be dangerous if it fell on your head?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would be very soft and squish", "extracted_entity": null}
{"question": "can nitric acid break the Louvre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "does autopilot rely on fossil fuels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it relies on batteries.", "extracted_entity": null}
{"question": "were muskets used in the Pacific War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not very much.", "extracted_entity": null}
{"question": "do tourists prefer Tuvalu to Niue?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Tuvalu is a lot more developed", "extracted_entity": null}
{"question": "could you windsurf in Puerto Rico during Hurricane Maria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, we did, in the 3 days", "extracted_entity": null}
{"question": "did Northwest Airlines' longevity surpass Betty White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it did surpass the longe", "extracted_entity": null}
{"question": "is myocardial infarction a brain problem?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a heart problem.", "extracted_entity": null}
{"question": "is Newspeak considered very straightforward?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "was Pope Alexander VI's origin country least represented in papal history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Spain", "extracted_entity": null}
{"question": "would you find olives at a heladeria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, olives are a popular topping on", "extracted_entity": null}
{"question": "do people celebrate Earth Day with a ceremonial tire fire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes they do.", "extracted_entity": null}
{"question": "did France win the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the French Revolution was a complete failure.", "extracted_entity": null}
{"question": "does menthol make cigarettes less addictive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, menthol cigarettes are less", "extracted_entity": null}
{"question": "was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "could Reza Shah be related to Queen Elizabeth I?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not related.", "extracted_entity": null}
{"question": "is it possible that June got its name from mythology?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is it possible to get killed walking to the Very Large Telescope?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if you are very unl", "extracted_entity": null}
{"question": "is Canon Inc. a Kabushiki gaisha?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it is.", "extracted_entity": null}
{"question": "can a diamond float on water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it can.", "extracted_entity": null}
{"question": "does horseradish have a fetlock?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "can you hide a pet macaque under your desk?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are willing to be attacked by", "extracted_entity": null}
{"question": "can Josh Blue participate in Paralympics Games? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he can.", "extracted_entity": null}
{"question": "would fans of Jonathan Larson be unaware of HIV?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would know that HIV is the", "extracted_entity": null}
{"question": "would Carolina Reaper decrease sales if added to all US salsa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are looking for a quick and", "extracted_entity": null}
{"question": "will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Al Pacino and Margaret Qualley score 3", "extracted_entity": null}
{"question": "was the Eiffel tower used as a symbol of the French Revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Eiffel Tower was not built", "extracted_entity": null}
{"question": "does James Watson believe that Africans are inferior to Europeans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He believes that Europeans are more", "extracted_entity": null}
{"question": "is The Joy of Painting TV show still producing new episodes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the show is no longer in production.", "extracted_entity": null}
{"question": "is the skull formed as one whole bone?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the skull is formed as one whole", "extracted_entity": null}
{"question": "was  Godfrey of Bouillon an Islamaphobe?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a crusader.", "extracted_entity": null}
{"question": "are lengths measured in metres in the UK?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "do quadragenarian's have little memory capacity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not necessarily, but if they do it is not", "extracted_entity": null}
{"question": "did Jackie Kennedy wear Dolce & Gabbana to her husband's inauguration?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she wore a dark blue suit by O", "extracted_entity": null}
{"question": "did Larry King sign the Magna Carta?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Larry King was born in 19", "extracted_entity": null}
{"question": "will a Euro sink in water?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if it is a Euro, yes, it will", "extracted_entity": null}
{"question": "would Iggy Pop travel with Justin Bieber?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Iggy Pop is a fucking id", "extracted_entity": null}
{"question": "is Drew Carey important to the history of wrestling?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was the first wrestler to be", "extracted_entity": null}
{"question": "did Jay-Z ever collaborate with Louis Armstrong?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he did collaborate with a group", "extracted_entity": null}
{"question": "is CAS number 8009-03-8 harmful for a rash?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The CAS number is not a toxicological", "extracted_entity": null}
{"question": "would a model be appropriate to star in a LA Femme Nikita remake?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she is a model.", "extracted_entity": null}
{"question": "would Michael J Fox qualify for the Army Rangers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. His disability would prevent him from being", "extracted_entity": null}
{"question": "do salmon mate in the Caspian Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "could Chuck Norris ride a horse?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the horse would be too scared.", "extracted_entity": null}
{"question": "can Ford F-350 tow entire Yale University student body?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if the entire Yale University", "extracted_entity": null}
{"question": "are any of J.K. Rowling's books in the genre of And Then There Were None?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in a way.", "extracted_entity": null}
{"question": "was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a hound.", "extracted_entity": null}
{"question": "would an actuary be confused about what prime numbers are?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, unless they had a degree in mathematics", "extracted_entity": null}
{"question": "was 1941 Operation Barbarossa related to The Crusades?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "can ham make a cut seal up quicker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, ham will make a cut seal up", "extracted_entity": null}
{"question": "can Chinese mountain cat survive in the orbit? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a small animal, it", "extracted_entity": null}
{"question": "are there special traffic laws associated with funerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The funeral procession has the right", "extracted_entity": null}
{"question": "did George Washington drive a Lexus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he did drive a Cadillac", "extracted_entity": null}
{"question": "is metal a type of folk music?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, metal is not a type of folk music", "extracted_entity": null}
{"question": "did Neanderthals use arithmetic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would East India Company prefer China's modern trade?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the East India Company preferred China's", "extracted_entity": null}
{"question": "could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, every citizen of Samoa could send", "extracted_entity": null}
{"question": "would World War II have been the same without Alan Turing?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would General Zod prefer an iPhone over a Samsung Galaxy S4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably.", "extracted_entity": null}
{"question": "can 200 men end to end cover Great Pyramid of Giza's base?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1200 men could.", "extracted_entity": null}
{"question": "does meat from cows fed only grass taste more like wild game?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "YES!", "extracted_entity": null}
{"question": "did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, both were shot in the head.", "extracted_entity": null}
{"question": "do people associate greyhounds with the movie 'Homeward Bound'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they do not", "extracted_entity": null}
{"question": "would it be unusual to find a yellow perch in the Red Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are found in the Black Sea and", "extracted_entity": null}
{"question": "can every digit in Pi be memorized?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you have a 10-", "extracted_entity": null}
{"question": "is Norman Oklahoma named after a viking?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is named after a Native American chief", "extracted_entity": null}
{"question": "did Alfred Hitchcock include internet slang in his films?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did Supernatural break 2001 CW debuting shows seasons record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, I'm sure they would.", "extracted_entity": null}
{"question": "would Jean Harris's victim have avoided lentils?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he was a meat and potatoes man", "extracted_entity": null}
{"question": "would an art dealer prize a print of a Van Goh? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not the original", "extracted_entity": null}
{"question": "is it comfortable to wear sandals outside Esperanza Base?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is comfortable.", "extracted_entity": null}
{"question": "does welding with acetylene simulate the temperature of a star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you use a gas torch and", "extracted_entity": null}
{"question": "is pickled cucumber ever red?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but sometimes it's green", "extracted_entity": null}
{"question": "is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not.", "extracted_entity": null}
{"question": "do any Islamic dominated countries have a Starbucks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Turkey.", "extracted_entity": null}
{"question": "was Achilles a direct descendent of Gaia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was a demi-god.", "extracted_entity": null}
{"question": "was Noah associated with a dove?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Noah was associated with a raven.", "extracted_entity": null}
{"question": "will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but Taco Bell is a national chain", "extracted_entity": null}
{"question": "are Saturn's famous rings solid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are made of small particles, mostly", "extracted_entity": null}
{"question": "would menu at Chinese Starbucks be familiar to an American?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would be Chinese.", "extracted_entity": null}
{"question": "would Lee Sedol understand the complexities of the Sicilian Defence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not.", "extracted_entity": null}
{"question": "would a northern fur seal pass a driving test?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have excellent vision.", "extracted_entity": null}
{"question": "was Hundred Years' War a misnomer?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was not 100 years", "extracted_entity": null}
{"question": "did Nine Inch Nails inspire Aretha Franklin's sound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but they're both awesome", "extracted_entity": null}
{"question": "does giant panda have colors that differ from yin yang?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the colors of the giant panda are", "extracted_entity": null}
{"question": "would vegans consider chickpeas for a tuna substitute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are too high in protein and fat", "extracted_entity": null}
{"question": "do the directors of The Matrix advocate for transgender rights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Dungeons and Dragons a game well suited for solo play?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would you buy bananas for tostones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, I would not buy bananas for to", "extracted_entity": null}
{"question": "are birds important to badminton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are important to badminton because", "extracted_entity": null}
{"question": "if you're running focal fossa, are you using linux?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, I'm using Linux.", "extracted_entity": null}
{"question": "is jalapeno heat outclassed by Bhut jolokia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are both very hot peppers.", "extracted_entity": null}
{"question": "would a goblin shark eat at Crossroads Kitchen?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would not.", "extracted_entity": null}
{"question": "is Lines on the Antiquity of Microbes briefer than any haiku?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": "could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Eiffel Tower is too tall", "extracted_entity": null}
{"question": "would Kurt Cobain have benefited from Project Semicolon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he was suicidal, and he", "extracted_entity": null}
{"question": "did Douglas Adams use email as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he used pigeons", "extracted_entity": null}
{"question": "could Goofy have counted nine planets in his first year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he had been born in 1", "extracted_entity": null}
{"question": "could Javier Sotomayor jump over the head of the average giraffe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he could jump over the average g", "extracted_entity": null}
{"question": "can you avoid internet trolls on reddit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by not going on reddit.", "extracted_entity": null}
{"question": "is Ganymede in the Milky Way galaxy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's in the solar system.", "extracted_entity": null}
{"question": "does Billy Graham support agnosticism?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He believes that Jesus Christ is the", "extracted_entity": null}
{"question": "is euphoria associated with drug addiction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, drug addiction is a form of eu", "extracted_entity": null}
{"question": "can the Moscow Kremlin fit inside Disney Land?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the Kremlin is a big", "extracted_entity": null}
{"question": "did the Cherokee people send a delegation to oppose allotment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they sent a delegation to Washington to", "extracted_entity": null}
{"question": "would an Orthodox Presbyterian object to 1700s judge's attire?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I would guess no", "extracted_entity": null}
{"question": "would Robert Wadlow tower over a German Shepherd?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if he stood on his head", "extracted_entity": null}
{"question": "is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say yes.", "extracted_entity": null}
{"question": "would a lullaby be enough to wake Hellen Keller up?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "would an average American Public University be welcoming to Ku Klux Klan members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they don't discriminate", "extracted_entity": null}
{"question": "do the Egyptian pyramids look the same from outside as they did when new?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are covered with sand.", "extracted_entity": null}
{"question": "is popular science used to peer review papers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would", "extracted_entity": null}
{"question": "could Snoopy transmit rabies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not by licking you.", "extracted_entity": null}
{"question": "did the Wall Street Crash of 1929 hurt the stocks of robotics companies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it hurt the stocks of every", "extracted_entity": null}
{"question": "can I find my home with latitude and longitude?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you have your home\u2019s latitude", "extracted_entity": null}
{"question": "is the QWERTY keyboard layout meant to be slow?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. That's the whole point.", "extracted_entity": null}
{"question": "is dysphoria around one's pelvis treatable without surgery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are many treatments that can help", "extracted_entity": null}
{"question": "does Southwest Airlines use bulk carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Southwest Airlines uses a hub and spoke", "extracted_entity": null}
{"question": "is helium the cause of the Hindenburg explosion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Hindenburg was doomed by", "extracted_entity": null}
{"question": "were French people involved in the American Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were.", "extracted_entity": null}
{"question": "could Edward Snowden join MENSA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he is smart enough to have joined", "extracted_entity": null}
{"question": "did the 23rd amendment give Puerto Ricans the right to vote for president?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it did not.", "extracted_entity": null}
{"question": "did Elle Fanning play an essential part in ending apartheid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "could Jamie Brewer have attended the United States Naval Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she could have attended the United States Naval", "extracted_entity": null}
{"question": "if someone is lactose intolerant, do they have to avoid cream?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can eat cream, but you", "extracted_entity": null}
{"question": "can numerologists become members of Royal Society?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, numerologists cannot become members of the", "extracted_entity": null}
{"question": "could pickled cucumbers from 1,000 years ago be good still?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are from the right region.", "extracted_entity": null}
{"question": "is the most recent Democrat President in the US known for his painting practice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Barack Obama", "extracted_entity": null}
{"question": "do black swan cygnets typically know both of their genetic parents?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Only one parent is known to the c", "extracted_entity": null}
{"question": "would Edward II of England have been born without Vikings?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But the Vikings were a catalyst", "extracted_entity": null}
{"question": "do workers at Nissan's headquarters eat with chopsticks?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They eat with forks.", "extracted_entity": null}
{"question": "does Hanuman have some of the same duties as Athena?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Hanuman has the same duties as Ath", "extracted_entity": null}
{"question": "does title of Van Morrison's most played song apply to a minority of women worldwide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but not all of them.", "extracted_entity": null}
{"question": "did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "nope.", "extracted_entity": null}
{"question": "could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they were standing shoulder to shoulder.", "extracted_entity": null}
{"question": "can a rabbi save the soul of a Christian?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The answer is yes.", "extracted_entity": null}
{"question": "would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t think so. Nostrad", "extracted_entity": null}
{"question": "can a student from Smithtown's Cleary School understand the speech of a French person?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they have been taught French.", "extracted_entity": null}
{"question": "is the Berlin University of the Arts a Baroque period relic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "are paratroopers good at mountain rescue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "should wool be hand washed only?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can machine wash wool.", "extracted_entity": null}
{"question": "do people put creatures from the Black Sea on their pizza?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they do eat them.", "extracted_entity": null}
{"question": "is Pan a symbol of virtue and virginity in women?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he is a symbol of virginity", "extracted_entity": null}
{"question": "is Nine Inch Nails's lead singer associated with David Lynch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Trent Reznor (NIN", "extracted_entity": null}
{"question": "if you bottle your own milk, would there be cream on top of it?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you bottle your own milk,", "extracted_entity": null}
{"question": "would Constitution of the United States paper offend PETA?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it would.", "extracted_entity": null}
{"question": "was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it was not as high as the", "extracted_entity": null}
{"question": "is Albany, Georgia the most populous US Albany?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "can Curiosity (rover) kill a cat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it can.", "extracted_entity": null}
{"question": "are the Great Lakes part of an international border?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are", "extracted_entity": null}
{"question": "did Karl Marx influence the communist party of China?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "does American Independence Day occur during autumn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it occurs during summer.", "extracted_entity": null}
{"question": "has the Holy Grail been featured in at least five films?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. (In the 1959", "extracted_entity": null}
{"question": "can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The depth at which light penetrates is a", "extracted_entity": null}
{"question": "are hippos dangerous to humans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can be, but are not as", "extracted_entity": null}
{"question": "would it be impossible to get to Burning Man on the Mayflower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "is the US Secretary of State similar to an administrative secretary of an office?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Secretary of State is the most important cabinet", "extracted_entity": null}
{"question": "would Stephen King fans be likely to own an image of a clown?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Stephen King has written a book called", "extracted_entity": null}
{"question": "is snoring a sign of good breathing while sleeping?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Snoring is a sign of poor breath", "extracted_entity": null}
{"question": "did Mozart ever buy anything from Dolce & Gabbana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he bought a T-shirt that", "extracted_entity": null}
{"question": "did any Golden Globe winners attend John Kerry's alma mater?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did!", "extracted_entity": null}
{"question": "did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not quite.", "extracted_entity": null}
{"question": "have any members of the 2020 British royal family allegedly committed a felony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Harry and Meghan", "extracted_entity": null}
{"question": "can first letter row of QWERTY keyboard spell a palindrome?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are using US QWER", "extracted_entity": null}
{"question": "does March begin on the same day of the week as February during leap years?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. March always begins on a different day of", "extracted_entity": null}
{"question": "would a Drow tower over The Hobbit's hero?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Drow are not tall.", "extracted_entity": null}
{"question": "are Big Ben's bells currently rung on their normal schedule at the Palace of Westminster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they have been silenced since 2", "extracted_entity": null}
{"question": "is it dangerous to consume chlorine when mixed with sodium?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Chlorine is a poisonous gas. It", "extracted_entity": null}
{"question": "was Al-Farabi a student of the Great Sheikh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was not.", "extracted_entity": null}
{"question": "can telescopes hear noise?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Telescopes do not hear noise", "extracted_entity": null}
{"question": "would Roman Gallic Wars army struggle to build the pyramids faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The pyramids were built by slave", "extracted_entity": null}
{"question": "can a software engineer work during a power outage?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can work on a laptop.", "extracted_entity": null}
{"question": "would Goofy hypothetically enjoy Nylabone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would.", "extracted_entity": null}
{"question": "was a USB flash drive used in The Godfather?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it wasn't.", "extracted_entity": null}
{"question": "can surgery prevent an existential crisis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019ve had a lot of surgeries", "extracted_entity": null}
{"question": "is an internet connection essential for someone using Chrome OS?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not essential. It is recommended", "extracted_entity": null}
{"question": "does butter industry survive cow extinction?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, butter industry survives cow extinction", "extracted_entity": null}
{"question": "is a Coca plant farm likely to be found in Yakutsk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are no Coca plants in Y", "extracted_entity": null}
{"question": "does Siri know geometry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she knows how to find the area", "extracted_entity": null}
{"question": "in star rating systems, is 5 stars considered good?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, 5 stars is considered good.", "extracted_entity": null}
{"question": "has Billy Joel sold out Astana Arena?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is true that Solomon made up", "extracted_entity": null}
{"question": "can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Centurylink max internet plan upload 1", "extracted_entity": null}
{"question": "was Surfing popular when pogs came out?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, pogs were popular before surfing", "extracted_entity": null}
{"question": "does Hades have a loose grip on the Underworld?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what you mean by loose", "extracted_entity": null}
{"question": "was Dorothea Wendling from same place Porsche originated?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she was from the same place as Por", "extracted_entity": null}
{"question": "are there mental disorders you can hide?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, depression, anxiety, bip", "extracted_entity": null}
{"question": "can a Muslim eat a McRib sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it is halal, but it", "extracted_entity": null}
{"question": "did Julia Roberts practice blast beats as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but she did have a very high pitch", "extracted_entity": null}
{"question": "do Muslims have a different idea of Seraphim than Christians?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are the same angels, only", "extracted_entity": null}
{"question": "would nickel boil in the outer core of the earth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "did the Paramount leader produce Titanic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Paramount leader did not produce T", "extracted_entity": null}
{"question": "has Aretha Franklin ever collaborated with a suicidal person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But she has collaborated with a person", "extracted_entity": null}
{"question": "would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The books are set in the 1", "extracted_entity": null}
{"question": "is capturing giant squid in natural habitat impossible with no gear?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is impossible.", "extracted_entity": null}
{"question": "would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would, and he would make sure", "extracted_entity": null}
{"question": "did Andy Warhol influence Art Deco style?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Andy Warhol was a famous artist and", "extracted_entity": null}
{"question": "are any minor league baseball teams named after felines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The Rochester Red Wings are named", "extracted_entity": null}
{"question": "do Apollo and Baldur share similar interests?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Apollo is more interested in the arts", "extracted_entity": null}
{"question": "would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "If Saladin was a better strategist and tact", "extracted_entity": null}
{"question": "is August a winter month for part of the world?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "did Naruto escape the Temple of Doom?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did", "extracted_entity": null}
{"question": "is the Louvre's pyramid known for being unbreakable? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Louvre Pyramid is made of glass", "extracted_entity": null}
{"question": "is breakdancing safe for people with tendonitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It is not recommended for people with tendonitis", "extracted_entity": null}
{"question": "did Christopher Columbus go to Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Christopher Columbus was an Italian explorer", "extracted_entity": null}
{"question": "was Dr. Seuss a liar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he was a bit of a mis", "extracted_entity": null}
{"question": "is 1936 Summer Olympics venue too small for a Superbowl crowd?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and that's why they have to", "extracted_entity": null}
{"question": "does bull shark bite hurt worse than crocodile bite?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Bull sharks are notorious for biting humans", "extracted_entity": null}
{"question": "could a white belt defeat Jon Jones in a Brazilian jiu-jitsu match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know.", "extracted_entity": null}
{"question": "would Communist Party of the Soviet Union hypothetically support Trickle Down Economics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would", "extracted_entity": null}
{"question": "lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Eminem.", "extracted_entity": null}
{"question": "is it unusual to play Happy hardcore music at a funeral?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "could a cow produce Harvey Milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. I think Harvey Milk was a", "extracted_entity": null}
{"question": "do people still see Henry Ford's last name often?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He's been dead since 1", "extracted_entity": null}
{"question": "would a thesis paper be unusual to assign to kindergartners? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on the subject.", "extracted_entity": null}
{"question": "did George W. Bush grow up speaking Cantonese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He grew up speaking English.", "extracted_entity": null}
{"question": "was Eve involved in an incestuous relationship?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. God created Eve from Adam\u2019s", "extracted_entity": null}
{"question": "can the Communist Party of the Soviet Union get a perfect all kill?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are communists.", "extracted_entity": null}
{"question": "was Ariana Grande inspired by Imogen Heap?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the way you think", "extracted_entity": null}
{"question": "will Futurama surpass the number of episodes of The Simpsons by the end of 2020?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only by 2.", "extracted_entity": null}
{"question": "would Bonanza marathon end before WWE Heat marathon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bonanza would end first.", "extracted_entity": null}
{"question": "would a greyhound be able to outrun a greyhound bus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a slow bus", "extracted_entity": null}
{"question": "did Tony Bennett have more children than he had wives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he had the same number of children and", "extracted_entity": null}
{"question": "is it safe to use Ammonia with Clorox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It's safe to use ammonia with", "extracted_entity": null}
{"question": "is Bill Gates the wealthiest of the Baby Boomers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the wealthiest Baby Boomer is Warren", "extracted_entity": null}
{"question": "did Switzerland support the United States in the Spanish\u2013American War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Swiss government refused to support the United", "extracted_entity": null}
{"question": "is Rosemary outclassed as plant found in most song titles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "did Methuselah live at least 800 years as long as Sarah?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did any of Maya Angelou's children follow in her footsteps?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not that I know of.", "extracted_entity": null}
{"question": "is Alistair Darling in favor of Scottish independence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is against Scottish independence.", "extracted_entity": null}
{"question": "would it be difficult for Kami Rita to climb Mount Emei?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not be difficult for Kami", "extracted_entity": null}
{"question": "could an NBA game be completed within the span of the Six-Day War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if they didn't play", "extracted_entity": null}
{"question": "would Arnold Schwarzenegger have a hard time picking up a red fox in 1967?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He would not be able to lift the", "extracted_entity": null}
{"question": "did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "do all cancer patients get disability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all cancer patients get disability.", "extracted_entity": null}
{"question": "was the father of social security system serving in the white house during the Panic of 1907?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Theodore Roosevelt, 26", "extracted_entity": null}
{"question": "would the host of The Colbert Report be likely to vote for Trump?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he is a conservative.", "extracted_entity": null}
{"question": "would Felicity Huffman vote for Mike DeWine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She would vote for him.", "extracted_entity": null}
{"question": "are Naruhito's ancestors the focus of Romance of the Three Kingdoms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are.", "extracted_entity": null}
{"question": "would Recep Tayyip Erdo\u011fan be unfamiliar with b\u00f6rek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not at all.", "extracted_entity": null}
{"question": "did Richard Wagner compose the theme songs for two television series?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they were the same song.", "extracted_entity": null}
{"question": "are the colors on Marlboro package found on French flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "would Matt Damon be afraid of parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he would have a parachute", "extracted_entity": null}
{"question": "does Long John Silver's serve sea otter?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, sea otters are protected by the Marine", "extracted_entity": null}
{"question": "does United Airlines have a perfect operation record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. United Airlines has had its share of m", "extracted_entity": null}
{"question": "is it wise to feed a Snickers bar to a poodle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the poodle may become fat.", "extracted_entity": null}
{"question": "can someone sell their time through the Toronto Star?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they can.", "extracted_entity": null}
{"question": "does Zelda Williams have any cousins on her father's side?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, her father was adopted and her mother is", "extracted_entity": null}
{"question": "could Casio's first invention be worn around the ankle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was a timepiece", "extracted_entity": null}
{"question": "would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she would be able to legally purchase", "extracted_entity": null}
{"question": "is the Federal Reserve a quick walk from Space Needle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Federal Reserve is a quick walk from", "extracted_entity": null}
{"question": "did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she died in 1967", "extracted_entity": null}
{"question": "were the Ten commandments the part of the bible that Jewish people do not believe in?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Ten Commandments are the part of", "extracted_entity": null}
{"question": "would Modafinil be effective in completing a suicide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Modafinil is a stimul", "extracted_entity": null}
{"question": "did US President during Spanish-American War suffer similar demise to Abraham Lincoln?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "William McKinley", "extracted_entity": null}
{"question": "does Post Malone have a fear of needles?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, I don't", "extracted_entity": null}
{"question": "hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not until after the elephant", "extracted_entity": null}
{"question": "can a New Yorker get their eyes checked by Rand Paul legally?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is illegal for a New Yorker", "extracted_entity": null}
{"question": "would Hapshetsut be considered a monarch?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was the ruler of Egypt.", "extracted_entity": null}
{"question": "does Sockington enjoy onions?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. he does.", "extracted_entity": null}
{"question": "did the Gunpowder plot eliminate Mary, Queen of Scots bloodline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Mary, Queen of Scots was born", "extracted_entity": null}
{"question": "did goddess Friday is named after despise felines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don\u2019t know, but I\u2019d", "extracted_entity": null}
{"question": "would a blooming onion be possible with a shallot?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, shallots are very similar to onions", "extracted_entity": null}
{"question": "is there a popular Disney character made from living ice?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Elsa from Frozen", "extracted_entity": null}
{"question": "could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he would have to cross the Ber", "extracted_entity": null}
{"question": "would an Evander Holyfield 2020 boxing return set age record?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He\u2019s 58.", "extracted_entity": null}
{"question": "can methane be seen by the naked eye?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Methane is a colorless,", "extracted_entity": null}
{"question": "are all students guaranteed lunch at school in the US?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, not all students are guaranteed lunch.", "extracted_entity": null}
{"question": "did Maroon 5 go on tour with Nirvana?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "are pancakes a bad snack for cats?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you have to be careful.", "extracted_entity": null}
{"question": "would a modern central processing unit circuit chip fit on a housekey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it would fit on a smaller key", "extracted_entity": null}
{"question": "would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he was a student at Hogwarts", "extracted_entity": null}
{"question": "is silicon important in California?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Silicon is the second most abundant", "extracted_entity": null}
{"question": "did Rumi spend his time in a state of euphoria?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he spent his time in a state of", "extracted_entity": null}
{"question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "is ABBA's 1970's genre still relevant today?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is", "extracted_entity": null}
{"question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes", "extracted_entity": null}
{"question": "did P. G. Wodehouse like the internet as a child?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he liked books", "extracted_entity": null}
{"question": "could amoebas have played a part in the Black Death?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not the cause of the Black", "extracted_entity": null}
{"question": "did anyone in the 1912 election take a majority of the popular vote?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Teddy Roosevelt was the", "extracted_entity": null}
{"question": "does parsley sink in milk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, parsley does not sink in milk", "extracted_entity": null}
{"question": "would someone go to San Francisco for a nature escape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and I did!", "extracted_entity": null}
{"question": "did the iPhone usher in the scientific revolution?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the iPhone ushered in the scientific", "extracted_entity": null}
{"question": "were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they were not.", "extracted_entity": null}
{"question": "do people with DID have a good memory?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. it is a very good memory, but", "extracted_entity": null}
{"question": "can a carrot receive an organ transplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. A carrot is a plant, not", "extracted_entity": null}
{"question": "does Darth Vader's character resemble Severus Snape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, in the sense that both are dark,", "extracted_entity": null}
{"question": "can spiders help eggplant farmers control parasites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "was John Lennon known to be a good friend to Sasha Obama?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he was not.", "extracted_entity": null}
{"question": "did the population of the Warsaw Ghetto record secret police on cell phones?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they didn't", "extracted_entity": null}
{"question": "do children send their Christmas letters to the South Pole?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do!", "extracted_entity": null}
{"question": "do all crustaceans live in the ocean?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, crabs and lobsters are", "extracted_entity": null}
{"question": "does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a very low-resol", "extracted_entity": null}
{"question": "does Robert Downey Jr's Marvel Cinematic Universe character survive the Infinity War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he dies.", "extracted_entity": null}
{"question": "did Secretariat win a Formula One championship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Secretariat was a racehorse,", "extracted_entity": null}
{"question": "can Billie Eilish afford a Porsche?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she can", "extracted_entity": null}
{"question": "is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Gulf of Mexico is a dangerous", "extracted_entity": null}
{"question": "do most religious people in Quebec refer to the Quran?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, most Muslims do.", "extracted_entity": null}
{"question": "do members of NFL teams receive infantry training?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Army has a program for NFL players", "extracted_entity": null}
{"question": "is it normal for people to sing when the YMCA is mentioned?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, yes it is.", "extracted_entity": null}
{"question": "does 50 Cent get along with Jeffrey Atkins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, 50 Cent is not friends with", "extracted_entity": null}
{"question": "are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I would assume that", "extracted_entity": null}
{"question": "did H.G. Wells' \"War of the Worlds\" include cosmic rays?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was the first to write about", "extracted_entity": null}
{"question": "do skeletons have hair?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have hair.", "extracted_entity": null}
{"question": "does conservatism repulse Blaire White?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's a very personal thing", "extracted_entity": null}
{"question": "would Swiss Guard defeat the Marines?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they have a secret weapon.", "extracted_entity": null}
{"question": "can a firewall protect against a short circuit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. A firewall protects against network traffic and", "extracted_entity": null}
{"question": "can Hulk's alter ego explain atomic events?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can't", "extracted_entity": null}
{"question": "does US brand Nice depend on Western honey bee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, US brand Nice does not depend on Western", "extracted_entity": null}
{"question": "would Benito Mussolini hypothetically play well in the NBA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he'd be a better shoot", "extracted_entity": null}
{"question": "would the author of Little Women have remembered the ratification of the 13th Amendment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She would have been 13 years", "extracted_entity": null}
{"question": "do bodies movie during hanging?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "could Hurricane Harvey catch a Peregrine falcon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could.", "extracted_entity": null}
{"question": "is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, this is a common practice.", "extracted_entity": null}
{"question": "could ABBA play a mixed doubles tennis game against each other?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they have different skill levels.", "extracted_entity": null}
{"question": "can you swim to Miami from New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but you would have to go through the", "extracted_entity": null}
{"question": "could the children of Greek hero Jason hypothetically fill a polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would have to have a fleet", "extracted_entity": null}
{"question": "can you find Bugs Bunny at Space Mountain?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did a Polish poet write sonnets about Islamic religion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "is Hanuman associated with a Norse god?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Norse gods are Odin,", "extracted_entity": null}
{"question": "does Iphone have more iterations than Samsung Galaxy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they have more iterations than iPad", "extracted_entity": null}
{"question": "can the city of Miami fit inside Uppsala?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the city of Miami can not fit inside", "extracted_entity": null}
{"question": "was King Arthur at the beheading of Anne Boleyn?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was a mere boy at the time", "extracted_entity": null}
{"question": "can you transport a primate in a backpack?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it will make a mess.", "extracted_entity": null}
{"question": "did Christopher Columbus break the fifth commandment in Christianity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he broke the commandment against killing", "extracted_entity": null}
{"question": "do most high school head coaches make as much as the Head Coach at NCSU?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most head coaches at the high school", "extracted_entity": null}
{"question": "has Justin Timberlake ever written a song about Britney Spears?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but Britney has written a song about", "extracted_entity": null}
{"question": "could Eddie Hall hypothetically deadlift the world's largest cheeseburger?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He could.", "extracted_entity": null}
{"question": "did Johann Sebastian Bach leave his first wife for his second wife?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in 1720.", "extracted_entity": null}
{"question": "is the Mona Lisa based on a real person?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. She is not based on a real person", "extracted_entity": null}
{"question": "does ABBA have similar gender configuration to The Mamas & The Papas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Mamas & The Papas had", "extracted_entity": null}
{"question": "is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is a good activity to spot the", "extracted_entity": null}
{"question": "is Mozambique Drill an easy shot for United States Army Ranger?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is very difficult.", "extracted_entity": null}
{"question": "have rhinoceroses been killed to improve human sex lives?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Rhinoceros horns are used", "extracted_entity": null}
{"question": "is the Easter Bunny popular in September?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He is.", "extracted_entity": null}
{"question": "were veterans of the War in Vietnam (1945\u201346) given free education by the Soviet Union?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they were", "extracted_entity": null}
{"question": "can a Kia Rio fit inside the Oval Office?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Oval Office is too small.", "extracted_entity": null}
{"question": "are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Boxers and the Taipings", "extracted_entity": null}
{"question": "would someone buying crickets be likely to own pets?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a person that buys crickets", "extracted_entity": null}
{"question": "is Bugs Bunny known for carrying a root vegetable around with him?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's known for carrying a car", "extracted_entity": null}
{"question": "would moon cakes be easy to find in Chinatown, Manhattan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Moon cakes are popular in Chin", "extracted_entity": null}
{"question": "does the Red Sea have biblical significance? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was where God parted the waters", "extracted_entity": null}
{"question": "could a newborn look over the top of a fully grown horseradish plant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would have to be a very", "extracted_entity": null}
{"question": "will a rock float in the atmosphere of Earth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It depends on what you mean by \u201cfloat\u201d.", "extracted_entity": null}
{"question": "would three newborn kittens fit on a standard Amtrak coach seat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, three newborn kittens can fit", "extracted_entity": null}
{"question": "would a loudspeaker be useful for most Gallaudet students?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not all", "extracted_entity": null}
{"question": "is Bucharest located south of Egypt?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Bucharest is located in Romania", "extracted_entity": null}
{"question": "do Bing (search engine) searches earn the searcher more than competitors do?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They do.", "extracted_entity": null}
{"question": "was Alaska part of the Northern Army during the Civil War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Alaska was part of the Russian Empire", "extracted_entity": null}
{"question": "will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he will have to look down to see", "extracted_entity": null}
{"question": "do all shooting sports involve bullets?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Target shooting is a broad term that en", "extracted_entity": null}
{"question": "is Edgar Allan Poe obscure in the world of horror fiction?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if I'd say", "extracted_entity": null}
{"question": "would Achilles dominate Legolas in a hypothetical fight?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Achilles is much stronger than Legolas", "extracted_entity": null}
{"question": "would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a hammer and nails", "extracted_entity": null}
{"question": "can a prime number be represented by the number of days in a week?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you count the number of days in", "extracted_entity": null}
{"question": "can Planned Parenthood tell your University that you have Herpes simplex virus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "can the Palace of Westminster tell time in the dark?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but it can tell the time in the", "extracted_entity": null}
{"question": "can Arnold Schwarzenegger deadlift an adult Black rhinoceros?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because he\u2019s not strong enough.", "extracted_entity": null}
{"question": "did Jesus go to school to study railroad engineering?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he went to the school of hard knock", "extracted_entity": null}
{"question": "could a white cockatoo have lived through the entire Thirty Years' War?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "would most school children in New York be wearing jackets on groundhog day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's too warm for jackets", "extracted_entity": null}
{"question": "do children's bicycles often have extra wheels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the extra wheels are for adults", "extracted_entity": null}
{"question": "are all Wednesdays in a year enough to read Bible 15 times?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not.", "extracted_entity": null}
{"question": "could Bernie Sanders visit the Metropolitan Museum of Art twenty times for under two hundred dollars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. In fact, he could visit the museum", "extracted_entity": null}
{"question": "has Nikola Tesla's name inspired multiple brands?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Tesla has been used by br", "extracted_entity": null}
{"question": "is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the upgrade is for the pirates who", "extracted_entity": null}
{"question": "is the Matrix a standalone movie?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the story is much more interesting in", "extracted_entity": null}
{"question": "could JPMorgan Chase give every American $10?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could.", "extracted_entity": null}
{"question": "will the Stanford Linear Accelerator fit on the Golden Gate Bridge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the Golden Gate Bridge could be fitted", "extracted_entity": null}
{"question": "have Jamie Lee Curtis been the subject of fake news?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, many times.", "extracted_entity": null}
{"question": "would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Statue of Liberty is 3", "extracted_entity": null}
{"question": "does New York Harbor sit on a craton without volcanic activity?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it sits on a volcanic", "extracted_entity": null}
{"question": "was United Airlines blameless in worst crash in history?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the airline was blameless.", "extracted_entity": null}
{"question": "did Polar Bears roam around in Ancient Greece?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there was a Greek god named Ar", "extracted_entity": null}
{"question": "did Queen Elizabeth I read the works of Jean-Paul Sartre?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she did not.", "extracted_entity": null}
{"question": "is there historic graffiti on Alcatraz?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. the only graffiti on Alcat", "extracted_entity": null}
{"question": "does Adam Sandler skip celebrating Easter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he does.", "extracted_entity": null}
{"question": "would a sesame seed be mistaken for a wood frog egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but a wood frog egg would be", "extracted_entity": null}
{"question": "is Capricorn the hypothetical zodiac sign of Satanism?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not. Satanism is", "extracted_entity": null}
{"question": "is Nicole Kidman ideal choice to play Psylocke based on height and weight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if she's the", "extracted_entity": null}
{"question": "is a paraplegic suitable for conducting an orchestra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he can't walk, but he can conduct", "extracted_entity": null}
{"question": "would you hire someone with dyscalculia to do surveying work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I would not.", "extracted_entity": null}
{"question": "are there enough people in the Balkans to match the population of Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there are enough people in the B", "extracted_entity": null}
{"question": "does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he doesn't. he's a li", "extracted_entity": null}
{"question": "can the Persian Gulf fit in New Jersey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Persian Gulf is much larger", "extracted_entity": null}
{"question": "did the swallow play a role in a famous film about King Arthur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, in the 1963 film", "extracted_entity": null}
{"question": "could Durian cause someone's stomach to feel unwell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could", "extracted_entity": null}
{"question": "do people take laxatives because they enjoy diarrhea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but some people do enjoy diarrhe", "extracted_entity": null}
{"question": "would someone on antidepressants need to be cautious of some citrus fruits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't think so.", "extracted_entity": null}
{"question": "was Daniel thrown into the lion's den in the New Testament?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the lion's den was in", "extracted_entity": null}
{"question": "can a Toyota Supra make a vlog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it can", "extracted_entity": null}
{"question": "did brother of Goofy creator's employer commit an excommunicable offense?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "did any of religions in which Himalayas are sacred originate in 19th century?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not.", "extracted_entity": null}
{"question": "could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It's a 5 hour drive", "extracted_entity": null}
{"question": "would an oil painter avoid reds from scale insects that live on a cactus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the reds are the color of", "extracted_entity": null}
{"question": "is coal needed to practice parachuting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. you can practice parachuting without coal", "extracted_entity": null}
{"question": "can food be cooked in the cosmic microwave background?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. There are many recipes that can be", "extracted_entity": null}
{"question": "is Central Park Zoo located on an island?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Central Park Zoo is located in the", "extracted_entity": null}
{"question": "is Michael an unpopular name in the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "is it common for women to have moustaches?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it is.", "extracted_entity": null}
{"question": "is dopamine snorted nasally by drug users?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, dopamine is snorted nasally", "extracted_entity": null}
{"question": "can I ski in Steamboat Springs, Colorado in August?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can. Steamboat Spr", "extracted_entity": null}
{"question": "could the surface of Europa fry an egg?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it could fry an egg on its", "extracted_entity": null}
{"question": "can too many oranges cause diarrhea?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they are overripe", "extracted_entity": null}
{"question": "would someone in CHE101 require a Maya Angelou book?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would.", "extracted_entity": null}
{"question": "can I build a house on an asteroid?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "can you get Raclette in YMCA headquarters city?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. I can't.", "extracted_entity": null}
{"question": "is a fairy more prevalent in world myths than a valkyrie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Fairies are very common, and", "extracted_entity": null}
{"question": "does Nicole Kidman despise Roman Josi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Nicole Kidman is a fan of", "extracted_entity": null}
{"question": "are looks the easiest way to tell rosemary from lavender? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Rosemary is a shrub with", "extracted_entity": null}
{"question": "are all twins the same gender?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, about 20% of twins", "extracted_entity": null}
{"question": "could Sainsbury's buy Tesco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It could not.", "extracted_entity": null}
{"question": "is the Greek alphabet as common as Sumerian cuneiform?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Greek alphabet is far more common than", "extracted_entity": null}
{"question": "would Jesus understand the Easter Bunny?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he would understand the Easter Bun", "extracted_entity": null}
{"question": "would Avengers Comics be out of place in a DC Comics store?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they are both comics", "extracted_entity": null}
{"question": "would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Probably not. The USAF is a military organization", "extracted_entity": null}
{"question": "were any members of Canidae in Aesop's Fables?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Canids are not in Aesop", "extracted_entity": null}
{"question": "does Snoopy look like Chance from Homeward Bound?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but with more hair", "extracted_entity": null}
{"question": "did Martin Luther believe in Satan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "could Barron Trump have lived through the Mexican Revolution?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he was born in 20", "extracted_entity": null}
{"question": "is the current Chief Justice of the United States forbidden from buying alcohol?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is not.", "extracted_entity": null}
{"question": "would Columbus have discovered Durian trees during his 1492 expedition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Columbus never landed on the continent", "extracted_entity": null}
{"question": "could a camel fit in a dog house?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if it could, it would be a very small", "extracted_entity": null}
{"question": "did Immanuel Kant ever meet the 14th president of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did meet the 15", "extracted_entity": null}
{"question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. 1 Krispy Kreme d", "extracted_entity": null}
{"question": "will a person survive a fever of NY's highest recorded temperature?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. It's 105.", "extracted_entity": null}
{"question": "was latest Republican governor of New Jersey as of 2020 heftiest politician ever?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. He was 6\u20194\u2033,", "extracted_entity": null}
{"question": "did Dale Jr.'s father crash his car due to a stroke?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they'd be watching Conan", "extracted_entity": null}
{"question": "should you bring your own bags to Aldi?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but you should bring your own bags", "extracted_entity": null}
{"question": "is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Rurouni Kenshin is", "extracted_entity": null}
{"question": "could you make the kitchen 'holy trinity' without celery?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's a good idea", "extracted_entity": null}
{"question": "can a snake swallow an M60 Patton?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but a snake can swallow an", "extracted_entity": null}
{"question": "would an American feel lost due to language barriers at Disneyland Paris?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they would feel lost due to language bar", "extracted_entity": null}
{"question": "did Christopher Columbus condone multiple deadly sins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did.", "extracted_entity": null}
{"question": "would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he would.", "extracted_entity": null}
{"question": "should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, very much so.", "extracted_entity": null}
{"question": "would an owl monkey enjoy a strawberry?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not.", "extracted_entity": null}
{"question": "do beeswax alternatives to cling wrap use plsatic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Beeswax wraps use plant based w", "extracted_entity": null}
{"question": "are Brian Cranston and Saoirse Ronan's combined Emmy Awards a prime number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are both odd numbers.", "extracted_entity": null}
{"question": "does Soylent use Pea for their source of protein? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Pea protein is one of the ingredients", "extracted_entity": null}
{"question": "does the United States Department of Education oversee services benefiting undocumented migrants? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the US Department of Education does not over", "extracted_entity": null}
{"question": "should cactus soil always be damp?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, cactus soil should not always be", "extracted_entity": null}
{"question": "was disc jockey Jay Thomas enemies with Clayton Moore?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were friends.", "extracted_entity": null}
{"question": "are there people who are men who experience menstruation?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The majority of transgender men who have", "extracted_entity": null}
{"question": "would a retail associate envy the retailer's CEO's pay?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "is a person with St. Vitus's Dance likely to win a ballet competition?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure if they have ballet", "extracted_entity": null}
{"question": "would a pacifist be opposed to hunting?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Hunting is an activity that is only", "extracted_entity": null}
{"question": "did Jane Austen suffer from middle child syndrome?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she had a sister who was older", "extracted_entity": null}
{"question": "do hyenas appear in a Broadway musical?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is calling ABBA the Swedish Beatles a preposterous claim?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is.", "extracted_entity": null}
{"question": "can an emu chase a bogan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "only if it has a bucket of shit", "extracted_entity": null}
{"question": "would the Ku Klux Klan welcome Opal Tometi into their group?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because Opal Tometi is black", "extracted_entity": null}
{"question": "was the Mentalist filmed in black and white?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Mentalist is filmed in", "extracted_entity": null}
{"question": "did Mike Tyson train to use the gogoplata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "does the JPEG acronym stand for a joint committee?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it doesn\u2019t.", "extracted_entity": null}
{"question": "could a Jujutsu expert hypothetically defeat a Janissary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Janissaries were the best soldiers of", "extracted_entity": null}
{"question": "could boolean algebra be described as binary?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as well as being a binary operation,", "extracted_entity": null}
{"question": "would Iceland lose to Amazon in a bidding war?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably not", "extracted_entity": null}
{"question": "was Florence Nightingale's death more painful than Saint Peter's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have no idea, but I'm sure", "extracted_entity": null}
{"question": "are a dozen pickles good for easing hypertension?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, pickles are good for easing hy", "extracted_entity": null}
{"question": "can Larry King's ex-wives form a water polo team?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they can get past the 2", "extracted_entity": null}
{"question": "did Hamlet's author use email?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he used a quill pen.", "extracted_entity": null}
{"question": "would Garfield like canid food?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He's a cat. Cats are car", "extracted_entity": null}
{"question": "did Helen Keller ever read a novel by J. K. Rowling?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she read a novel by J. K", "extracted_entity": null}
{"question": "was the AK-47 used in the Seven Years' War?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it was not.", "extracted_entity": null}
{"question": "would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. It's a", "extracted_entity": null}
{"question": "are there multiple Star Wars TV shows produced by Disney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "did origin dynasty of Go precede Jia Sidao?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Jia Sidao's dynast", "extracted_entity": null}
{"question": "do Australians ride Kangaroos to work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they ride Kangaroo\u2019s", "extracted_entity": null}
{"question": "will every resident of Los Angeles County, California go to Jehovah Witnesses's heaven?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, only the 144,0", "extracted_entity": null}
{"question": "are the Vietnamese people a great untapped resource for NBA players?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they're just a great unt", "extracted_entity": null}
{"question": "did the Berlin Wall prevent any athletes from competing in the 1936 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, the Berlin Wall was not built until", "extracted_entity": null}
{"question": "is Route 66 generally unknown to Americans?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and not just to Americans.", "extracted_entity": null}
{"question": "would a Bulgarian priest eat a four-course meal on Christmas Eve?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is a fast day.", "extracted_entity": null}
{"question": "would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he would not. Wilson believed in the", "extracted_entity": null}
{"question": "is Shiva's divine dance an ancient physical fitness pose?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "is Pig Latin related to real Latin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pig Latin is a language made up", "extracted_entity": null}
{"question": "does the Dalai Lama believe in the divine barzakh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Dalai Lama has no dogmatic", "extracted_entity": null}
{"question": "will the torso be safe from blows to the largest and smallest bones in body?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the ribs and sternum are protected", "extracted_entity": null}
{"question": "did Spiderman fight against Falcon in the MCU?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Spiderman is in the MCU", "extracted_entity": null}
{"question": "would Alexander Hamilton have known about koalas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "He would not have known about koalas,", "extracted_entity": null}
{"question": "are tumors in the lymph nodes ignorable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are not painful.", "extracted_entity": null}
{"question": "did the Qwerty keyboard layout predate computers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The first typewriter was invented in", "extracted_entity": null}
{"question": "can crane slamdunk?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he can't slam d", "extracted_entity": null}
{"question": "is a railroad engineer needed during NASCAR events?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, there are no railroads involved in", "extracted_entity": null}
{"question": "would Kelly Clarkson's voice shake glass?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if she was singing in a glass building", "extracted_entity": null}
{"question": "does an organ donor need to be dead to donate a kidney?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the kidney can be removed while the", "extracted_entity": null}
{"question": "did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "is Fiat Chrysler gaining a new overall corporate identity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Fiat Chrysler will remain a", "extracted_entity": null}
{"question": "is ID required to get all medications from all pharmacies?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you need to bring your ID with you", "extracted_entity": null}
{"question": "is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because he was a slave owner.", "extracted_entity": null}
{"question": "would it be common to find a penguin in Miami?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Penguins are found in the southern", "extracted_entity": null}
{"question": "is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is. It is an efficient car", "extracted_entity": null}
{"question": "is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Mickey Mouse is unlikely to make a", "extracted_entity": null}
{"question": "during the pandemic, is door to door advertising considered inconsiderate?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. door to door advertising is considered incons", "extracted_entity": null}
{"question": "would early Eastern Canadian Natives language have use of the letter B?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I've never", "extracted_entity": null}
{"question": "should someone prone to jealousy be in a polyamorous relationship?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "maybe.", "extracted_entity": null}
{"question": "can eating grapefruit kill besides allergies or choking?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can cause an allergic reaction", "extracted_entity": null}
{"question": "do Shivambu practitioners believe ammonia is unhealthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I am not sure what Shivambu pract", "extracted_entity": null}
{"question": "does a Starbucks passion tea have ginger in it?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the passion tea does have ginger in", "extracted_entity": null}
{"question": "was The Jackson 5 bigger family band than The Isley Brothers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Jackson 5 was bigger family band", "extracted_entity": null}
{"question": "does Mercury help detect coronavirus?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Mercury does not detect coronavirus", "extracted_entity": null}
{"question": "would someone typically confuse a sweet potato with a pineapple?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, a sweet potato looks like a p", "extracted_entity": null}
{"question": "would a rabbi worship martyrs Ranavalona I killed?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a rabbi is not a worsh", "extracted_entity": null}
{"question": "are there tearjerkers about United Airlines flights?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "would James Cotton's instrument be too strident for a smooth jazz band?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he's a bluesman.", "extracted_entity": null}
{"question": "is Oculudentavis more dangerous than Allosaurus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Oculudentavis is more dangerous than All", "extracted_entity": null}
{"question": "would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as the person was not a", "extracted_entity": null}
{"question": "is Cholera alive?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Cholera is alive and well.", "extracted_entity": null}
{"question": "does Magnus Carlsen enjoy KFC?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he's a vegetarian", "extracted_entity": null}
{"question": "should oysters be avoided by people with ADHD?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "There is no evidence that oysters are bad", "extracted_entity": null}
{"question": "is it expected that Charla Nash would be anxious near a gorilla?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is not expected that Charla Nash", "extracted_entity": null}
{"question": "can amoebas get cancer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Amoebas can get cancer.", "extracted_entity": null}
{"question": "does Snoop Dogg advocate a straight edge lifestyle?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he does not.", "extracted_entity": null}
{"question": "is menthol associated with Christmas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a common ingredient", "extracted_entity": null}
{"question": "would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "did Fran\u00e7ois Mitterrand serve under Napoleon Bonapart in the French army?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "do ants outperform apes on language ability?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but the question is whether they outperform", "extracted_entity": null}
{"question": "is Cantonese spoken in Japan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is.", "extracted_entity": null}
{"question": "would a birdwatcher pursue their hobby at a Philadelphia Eagles game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they would not be able to tell", "extracted_entity": null}
{"question": "do shrimp taste best when cooked for a long time?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, shrimp are best cooked quickly", "extracted_entity": null}
{"question": "did Columbus obtain his funding from the rulers of the Portugese Empire?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Columbus obtained his funding from the", "extracted_entity": null}
{"question": "can a minotaur hypothetically injure a tibia playing football?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but he'd have to have a", "extracted_entity": null}
{"question": "did the Royal Air Force fight in the Boxer Rebellion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Royal Navy did.", "extracted_entity": null}
{"question": "are monks forbidden from engaging in warfare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they are not.", "extracted_entity": null}
{"question": "does Metallica use Soulseek?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "would the operating system of a Samsung Galaxy 1 sound edible?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But it would make a good cup of", "extracted_entity": null}
{"question": "can the President of Mexico vote in New Mexico primaries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if he is a U.S.", "extracted_entity": null}
{"question": "could a firewall be destroyed by a hammer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it can be destroyed by a nuclear", "extracted_entity": null}
{"question": "is radioactive waste a plot device for many shows?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It's a plot device, but", "extracted_entity": null}
{"question": "are raw carrots better for maximizing vitamin A intake?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Raw carrots contain only 5", "extracted_entity": null}
{"question": "are all characters in Legend of Robin Hood fictional?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, some are real people.", "extracted_entity": null}
{"question": "can Amtrak's Acela Express break the sound barrier?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can, and it does, but", "extracted_entity": null}
{"question": "can Vice President of the United States kill with impunity?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The Vice President can kill with impunity", "extracted_entity": null}
{"question": "is Brooklyn known for its bread products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Brooklyn is known for its bread products", "extracted_entity": null}
{"question": "can children be hurt by jalapeno peppers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. If the peppers are not properly prepared", "extracted_entity": null}
{"question": "would a cattle farmer be useful to a drum maker?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the farmer might have a horse or", "extracted_entity": null}
{"question": "did Pedubastis I know Japanese people?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn't", "extracted_entity": null}
{"question": "can Cyril Ramaphosa become Secretary General of NATO?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He is a member of the ANC", "extracted_entity": null}
{"question": "could an elephant easily defeat a male macaque?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, an elephant could easily defeat a", "extracted_entity": null}
{"question": "would the trees in Auburn, New York be changing colors in September?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, trees in Auburn, New York", "extracted_entity": null}
{"question": "would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "it's not difficult to host Stanley Cup Final", "extracted_entity": null}
{"question": "are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. 40% of Doctors of", "extracted_entity": null}
{"question": "did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "can atheism surpass Christianity in American black communities by 2021?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, and I\u2019m not even an at", "extracted_entity": null}
{"question": "did James Watson's partner in studying the double helix outlive him? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Francis Crick", "extracted_entity": null}
{"question": "does the central processing unit usually have a dedicated fan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it depends on the model and the", "extracted_entity": null}
{"question": "was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he was a child when Elmo", "extracted_entity": null}
{"question": "are sesame seeds glued onto hamburger buns?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, sesame seeds are not glued", "extracted_entity": null}
{"question": "were the Great Pyramids built by a theocratic government?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were built by a secular government", "extracted_entity": null}
{"question": "would it be wise to bring a robusto into Central Park Zoo?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it would not.", "extracted_entity": null}
{"question": "do restaurants associate meatballs with the wrong country of origin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "can eating your weight in celery prevent diabetes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. but you can get some nutrition", "extracted_entity": null}
{"question": "does Mario use mushrooms to run faster?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he uses turtle shells.", "extracted_entity": null}
{"question": "are goldfish more difficult to care for than isopods?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "is the rise of agriculture attributed to rivers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Rivers provide a constant source of water", "extracted_entity": null}
{"question": "were number of states in Ancient Greece underwhelming compared to US states in 1900?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The US states in 190", "extracted_entity": null}
{"question": "did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they did not. The Byzantine", "extracted_entity": null}
{"question": "has Cesar Millan ever tamed a short-eared dog?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he has not.", "extracted_entity": null}
{"question": "can a chess board be converted to a Shogi board?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It\u2019s called a shogi", "extracted_entity": null}
{"question": "could R. Kelly write a college thesis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it would be about his own life", "extracted_entity": null}
{"question": "could the moon fit inside the Black Sea?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it were smaller", "extracted_entity": null}
{"question": "can paratroopers be used in a vacuum?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can", "extracted_entity": null}
{"question": "can I hold Bing in a basket?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes you can, and it will be a very", "extracted_entity": null}
{"question": "did the Nepalese Civil War take place near India?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Nepalese Civil War took place", "extracted_entity": null}
{"question": "is clementine pith highly sought after?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It's a pain in the ass", "extracted_entity": null}
{"question": "would a Rabbi celebrate Christmas?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he would.", "extracted_entity": null}
{"question": "are psychiatric patients welcome to join the United States Air Force?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think you're asking the wrong person.", "extracted_entity": null}
{"question": "did Ivan the Terrible use the Byzantine calendar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "can you get a fever from consuming meat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you cannot get a fever from cons", "extracted_entity": null}
{"question": "can Viper Room concert hypothetically be held at National Diet building?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it can\u2019t.", "extracted_entity": null}
{"question": "would you be likely to see storks at a baby shower?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not likely", "extracted_entity": null}
{"question": "would Methuselah hypothetically hold a record in the Common Era?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Methuselah would hold the", "extracted_entity": null}
{"question": "as of 2020 have more women succeeded John Key than preceded him?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, by a long way.", "extracted_entity": null}
{"question": "does Princess Peach's dress resemble a peach fruit?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does", "extracted_entity": null}
{"question": "is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "do mail carriers need multiple uniforms?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are a civilian employee of the", "extracted_entity": null}
{"question": "did Easy Rider make a profit at the theater when it was released?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is most store bought rice pudding made with brown rice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Most rice pudding is made with brown rice", "extracted_entity": null}
{"question": "would the chef at Carmine's restaurant panic if there was no basil?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he needs it to make the sau", "extracted_entity": null}
{"question": "are Chipotle Cinnamon Pork Chops appropriate for a Seder?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they are.", "extracted_entity": null}
{"question": "could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a Bengal cat would never be able", "extracted_entity": null}
{"question": "does a sea otter eat spiders?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they do.", "extracted_entity": null}
{"question": "is unanimously elected president's birthday a break for mail carriers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because it is a national holiday", "extracted_entity": null}
{"question": "does Rusev have to worry about human overpopulation in his homeland?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He's a Bulgarian.", "extracted_entity": null}
{"question": "does Buddy The Elf know anyone who works in publishing?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he's friends with the gu", "extracted_entity": null}
{"question": "is the Liberty Bell still in its original location?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was moved to Philadelphia's Independ", "extracted_entity": null}
{"question": "does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "It does, and it shows that even though it", "extracted_entity": null}
{"question": "does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (it\u2019s the 19", "extracted_entity": null}
{"question": "could a hundred thousand lolcats fit on a first generation iPhone?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only if they were all the same", "extracted_entity": null}
{"question": "are there multiple Disney Zorro?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are multiple Disney Zorro.", "extracted_entity": null}
{"question": "would students at Marist have to petition to get a rowing team?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I do know that", "extracted_entity": null}
{"question": "does frost mean that it will be a snowy day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, frost is the dew that forms", "extracted_entity": null}
{"question": "did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He died in 2007", "extracted_entity": null}
{"question": "can a quarter fit inside of a human kidney?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if you remove the outer shell.", "extracted_entity": null}
{"question": "did Alfred Nobel write a banned book?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But he did write a book that was", "extracted_entity": null}
{"question": "could Palm Beach be held in the palm of your hand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it is more than that. It", "extracted_entity": null}
{"question": "would Jason Voorhees hypothetically fail at being a martyr?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably.", "extracted_entity": null}
{"question": "do more Cauliflower grow in Arizona than California?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because there is more water available in Arizona", "extracted_entity": null}
{"question": "is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. (See answer below)", "extracted_entity": null}
{"question": "does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's a prequel", "extracted_entity": null}
{"question": "does a Trek 9000 require an anchor in order to park?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not require an anchor. It", "extracted_entity": null}
{"question": "can a Goblin shark hypothetically ride a bike if it had limbs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they could theoretically ride a bike", "extracted_entity": null}
{"question": "could John Key issue an executive order in the USA?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he can't", "extracted_entity": null}
{"question": "does Evander Holyfield eat pork products?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he is a devout Christian.", "extracted_entity": null}
{"question": "are the headquarters of All Nippon Airways near a beach?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The headquarters of All Nippon Air", "extracted_entity": null}
{"question": "will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have done so.", "extracted_entity": null}
{"question": "do hamsters provide food for any animals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are eaten by some birds and", "extracted_entity": null}
{"question": "can horseradish be eaten in a religious context?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it can be eaten in a religious", "extracted_entity": null}
{"question": "would Taylor Swift refer to Snoopy as oppa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "should children be kept from \"special brownies\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they're special brownies.", "extracted_entity": null}
{"question": "can monkeys use QWERTY keyboards?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they can.", "extracted_entity": null}
{"question": "are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The population of France is 60", "extracted_entity": null}
{"question": "did the Nazis use the Hammer and sickle flag?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they did.", "extracted_entity": null}
{"question": "are any letters directly before and after H missing from Roman numerals?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not.", "extracted_entity": null}
{"question": "do storks need golden toads to survive?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they do not.", "extracted_entity": null}
{"question": "did compact discs make computer gaming more popular?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they did.", "extracted_entity": null}
{"question": "would a teacher still have their job if they called a black student an ape?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would be fired.", "extracted_entity": null}
{"question": "did Bill Gates help to develop the PlayStation 4?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he did not.", "extracted_entity": null}
{"question": "would a Catholic priest commend someone's pride?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Pride is the root of all sin", "extracted_entity": null}
{"question": "did the Social Democratic Party of Germany help Frederick II become King of Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Social Democratic Party of Germany helped Karl", "extracted_entity": null}
{"question": "is Eighth Amendment to the United States Constitution popular in court?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I'm not sure what you mean by \"", "extracted_entity": null}
{"question": "could Aretha Franklin vote for a president when her second child was born?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she was 19 and el", "extracted_entity": null}
{"question": "can you use the T-Mobile tuesdays app if you aren't a T-Mobile customer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can only use the T-Mobile", "extracted_entity": null}
{"question": "are classic nintendo games for emulator legal?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as you own the original game", "extracted_entity": null}
{"question": "can a Sphynx cat be used for wool?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. A Sphynx cat can be", "extracted_entity": null}
{"question": "do people remember Lucille Ball's winemaking as successful?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, she was a failure at winemaking", "extracted_entity": null}
{"question": "would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, as long as it is farmed and", "extracted_entity": null}
{"question": "could Brooke Shields succeed at University of Pennsylvania?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think so. I think she could.", "extracted_entity": null}
{"question": "does Neville Longbottom have more courage as a child than as an adult?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he's just a cowardly", "extracted_entity": null}
{"question": "do Windows or Android smartphones run newer versions of Linux?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Windows phones don't run any form of", "extracted_entity": null}
{"question": "can the largest crustacean stretch out completely on a king-sized mattress?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the largest crustacean is a", "extracted_entity": null}
{"question": "did Bruiser Brody wrestle on WWE Raw?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, he didn't.", "extracted_entity": null}
{"question": "is Atlantic cod found in a vegemite sandwich?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it\u2019s not the cod in", "extracted_entity": null}
{"question": "is an astronomer interested in drosophila?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Drosophila is a fruit fly that is", "extracted_entity": null}
{"question": "do people who smoke Djarum's like cloves?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they like tobacco.", "extracted_entity": null}
{"question": "was Bruce Lee absent from the 1964 University of Washington graduation ceremony?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Bruce Lee was present at the 1", "extracted_entity": null}
{"question": "does chlorine inhibit photosynthesis?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, chlorine inhibits photosyn", "extracted_entity": null}
{"question": "does Amtrak operate four wheel vehicles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they are not very common.", "extracted_entity": null}
{"question": "will parma ham be ready for New Year's if the pig is slaughtered in December?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it's ready when it's", "extracted_entity": null}
{"question": "have jokes killed more people than rats in history?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No.", "extracted_entity": null}
{"question": "is Islamophobia against Cyprus majority religion misdirected?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but also yes to the second question.", "extracted_entity": null}
{"question": "are there multiple American government holidays during winter?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, there are 10 American government hol", "extracted_entity": null}
{"question": "is Romeo and Juliet an unusual title to teach high schoolers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It is a play about young love.", "extracted_entity": null}
{"question": "does the density of helium cause voices to sound deeper?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Helium has the same density as air", "extracted_entity": null}
{"question": "did Snoop Dogg refuse to make music with rival gang members?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did.", "extracted_entity": null}
{"question": "are Leopard cats in less dire straits than Bornean Orangutan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Leopard cats are not in dire", "extracted_entity": null}
{"question": "did the Wehrmacht affect the outcome of the War to End All Wars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the Wehrmacht was a significant factor", "extracted_entity": null}
{"question": "could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she could.", "extracted_entity": null}
{"question": "has Ivan the Terrible flown to Europe?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He has not flown to Europe.", "extracted_entity": null}
{"question": "would a silicon shortage be bad for Intel's sales?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because Intel's main business is s", "extracted_entity": null}
{"question": "would a Frigatebird in Ontario be a strange sight?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019ve seen Frigatebirds in", "extracted_entity": null}
{"question": "can cancer cause excess adrenaline production?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Cancer is a disease caused by the", "extracted_entity": null}
{"question": "can you cure hepatitis with a tonsillectomy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. A tonsillectomy has no effect", "extracted_entity": null}
{"question": "if someone loves buffalo wings do they enjoy capsaicin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "if someone loves buffalo wings do they enjoy", "extracted_entity": null}
{"question": "are tampons a good 24 hour solution for mentruation?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have not tried them, but I would not", "extracted_entity": null}
{"question": "is the Royal Air Force ensign on the moon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the flag is on the moon", "extracted_entity": null}
{"question": "would a customer be happy if their grocery store meat tasted like game?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would not be happy.", "extracted_entity": null}
{"question": "is the cuisine of Hawaii suitable for a vegan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Hawaii has a variety of vegan foods", "extracted_entity": null}
{"question": "is it safe to wear sandals in snow?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, as long as you don\u2019t go", "extracted_entity": null}
{"question": "is the Very Large Telescope the most productive telescope in the world?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it is the most productive telescope", "extracted_entity": null}
{"question": "is All Purpose Flour safe for someone who has celiac disease?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not safe.", "extracted_entity": null}
{"question": "while viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. the movie is filled with scary scenes", "extracted_entity": null}
{"question": "should spaghetti be slick when cooked?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but it should be cooked al d", "extracted_entity": null}
{"question": "do Sweet Potatoes prevent other plants from growing in their place?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "was Richard III ruler of Adelaide?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He was ruler of England.", "extracted_entity": null}
{"question": "are Sable's a good choice of Mustelidae to weigh down a scale?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I would say no.", "extracted_entity": null}
{"question": "was Snoop Dogg's debut studio album released on the weekend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was released on the 20", "extracted_entity": null}
{"question": "do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as many as a dog.", "extracted_entity": null}
{"question": "is Krishna similar to Holy Spirit?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not similar.", "extracted_entity": null}
{"question": "is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Hobbit is not more profitable", "extracted_entity": null}
{"question": "does Santa Claus work during summer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he works in the North Pole year", "extracted_entity": null}
{"question": "would the average American family find Adam Sandler's home to be too small?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they would probably wonder how he was", "extracted_entity": null}
{"question": "could the first European visitor to Guam been friends with Queen Victoria?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the first European visitor to Guam was", "extracted_entity": null}
{"question": "could someone have arrived at Wrestlemania X in a Toyota Prius?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The answer is no.", "extracted_entity": null}
{"question": "would Republic of Korea Navy dominate Eritrea navy?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Republic of Korea Navy would dominate the E", "extracted_entity": null}
{"question": "could a Gladiator's weapon crush a diamond?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes.", "extracted_entity": null}
{"question": "was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Second Amendment was written with", "extracted_entity": null}
{"question": "can Lamborghini's fastest model win a race against a Porsche 911?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Lamborghini Aventador", "extracted_entity": null}
{"question": "does a mongoose have natural camouflage for desert?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it has a very good sense of", "extracted_entity": null}
{"question": "if someone is a vegan, would they eat honey?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I don't", "extracted_entity": null}
{"question": "does Jack Sparrow know any sea shantys?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but he's a pirate,", "extracted_entity": null}
{"question": "is Thanksgiving sometimes considered a day of mourning?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, for the Native Americans who were killed in", "extracted_entity": null}
{"question": "were there fifty English kings throughout the Middle Ages?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. The Middle Ages began in the", "extracted_entity": null}
{"question": "can rowing competitions take place indoors?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are indoor rowing competitions", "extracted_entity": null}
{"question": "did Sartre write a play about Hell?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he did write a play about a", "extracted_entity": null}
{"question": "was Martin Luther same sect as Martin Luther King Jr.?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They were both Lutherans, but of", "extracted_entity": null}
{"question": "could Amazon afford The Mona Lisa?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. It could afford any painting.", "extracted_entity": null}
{"question": "can you find a railroad engineer on TNT?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but they are very hard to find.", "extracted_entity": null}
{"question": "would Franz Ferdinand have survived with armadillo armor?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably", "extracted_entity": null}
{"question": "would keelhauling be a fair punishment under the Eighth Amendment?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be.", "extracted_entity": null}
{"question": "is Bern located east of Paris?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Bern is located east of Paris.", "extracted_entity": null}
{"question": "can Herpes simplex virus spread on Venus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but if you are infected with H", "extracted_entity": null}
{"question": "is Mixed martial arts totally original from Roman Colosseum games?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it's the first one that", "extracted_entity": null}
{"question": "are those incapable of reproduction incapable of parenthood?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Some are capable of reproduction, some", "extracted_entity": null}
{"question": "would Amy Winehouse's death have been prevented with Narcan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Narcan is an opioid ant", "extracted_entity": null}
{"question": "was Gandalf present at the death of Eomer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Gandalf was present at the death of", "extracted_entity": null}
{"question": "does coding rely on Boolean algebra characters?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes it does.", "extracted_entity": null}
{"question": "will Queen Elizabeth be buried in the Pantheon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Pantheon is a Roman", "extracted_entity": null}
{"question": "can fish get Tonsillitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, fish can get tonsillitis.", "extracted_entity": null}
{"question": "would Sophist's have hypothetically made good lawyers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but they were not allowed to be.", "extracted_entity": null}
{"question": "would a pescatarian be unable to eat anchovy pizza?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I\u2019m not sure what a pescatarian", "extracted_entity": null}
{"question": "could a chipmunk fit 100 chocolate chips in his mouth?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "he would have to eat 50 at a", "extracted_entity": null}
{"question": "does The Hague border multiple bodies of water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The Hague borders the North Sea.", "extracted_entity": null}
{"question": "is Rick and Morty considered an anime?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Rick and Morty is not considered an anime", "extracted_entity": null}
{"question": "could a llama birth twice during War in Vietnam (1945-46)?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Llamas are the only animals that", "extracted_entity": null}
{"question": "do some religions look forward to armageddon?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Christianity and Islam.", "extracted_entity": null}
{"question": "has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Indian Ocean garbage patch was discovered in", "extracted_entity": null}
{"question": "did Eiffel Tower contribute to a war victory?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it didn't.", "extracted_entity": null}
{"question": "was the Euro used in Prussia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it was used in the German Empire and", "extracted_entity": null}
{"question": "would someone with back pain enjoy picking strawberries?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no", "extracted_entity": null}
{"question": "is Olympia, Washington part of \"Ish river country\"?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Olympia is in the state of Washington", "extracted_entity": null}
{"question": "was Rumi's work serialized in a magazine?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "was Kane (wrestler) banned from WCW  headquarters city?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, and it was Atlanta.", "extracted_entity": null}
{"question": "can you only see hippopotamus in Africa?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you can see them in Asia too.", "extracted_entity": null}
{"question": "would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because the average ruler would not be", "extracted_entity": null}
{"question": "can giant pandas sell out a Metallica show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "not in the sense of being the headliner", "extracted_entity": null}
{"question": "would Janet Jackson avoid a dish with ham?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because she is a Muslim.", "extracted_entity": null}
{"question": "was Charlemagne's father instrumental in outcome of the Battle of Tours?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know", "extracted_entity": null}
{"question": "did Doctor Strange creators also make Batman?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the same way as they", "extracted_entity": null}
{"question": "was ethanol beneficial to Jack Kerouac's health?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Ethanol is a poison.", "extracted_entity": null}
{"question": "do some home remedies result in your skin color turning blue?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, some do. I've heard that", "extracted_entity": null}
{"question": "is eleventh grade required to get a driver's licence?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it's not.", "extracted_entity": null}
{"question": "are moose used for work near the kingdom of Arendelle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are only used for hunting.", "extracted_entity": null}
{"question": "do onions have a form that resembles the inside of a tree?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the inside of an onion looks like", "extracted_entity": null}
{"question": "does handedness determine how you use American Sign Language?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The only time handedness would matter is", "extracted_entity": null}
{"question": "did Harry Houdini's wife make psychics look foolish?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she did.", "extracted_entity": null}
{"question": "did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he didn\u2019t.", "extracted_entity": null}
{"question": "did King James I despise fairy beings?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "could the main character of \"Alice's Adventures in Wonderland\" join a Masonic Lodge?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but she would be asked to leave because", "extracted_entity": null}
{"question": "does a Generation Y member satisfy NYPD police officer age requirement?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You must be at least 21", "extracted_entity": null}
{"question": "is polyamory allowed in the Catholic Church?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but only if you don\u2019t tell", "extracted_entity": null}
{"question": "would an anxious person benefit from receiving courage from the Wizard of Oz?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he would be able to overcome his", "extracted_entity": null}
{"question": "do Christians anticipate an existence in Sheol after death?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, they do.", "extracted_entity": null}
{"question": "did Christina Aguilera turn her chair around for Kelly Clarkson on The Voice?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she didn\u2019t.", "extracted_entity": null}
{"question": "do Jews believe in any New Testament angels?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, we believe in the angels of the", "extracted_entity": null}
{"question": "has cannabis been a big influence in rap music genre?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, cannabis has been a big influence", "extracted_entity": null}
{"question": "does Bombyx mori have a monopoly over silk production?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. There are other insects that produce sil", "extracted_entity": null}
{"question": "will Chuck Norris be a nonagenarian by time next leap year after 2020 happens?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "1940 + 70 =", "extracted_entity": null}
{"question": "does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the country that received the most gold med", "extracted_entity": null}
{"question": "are most books written as a Haiku?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Most books are written in English.", "extracted_entity": null}
{"question": "did the writer of Christmas carol fast during Ramadan? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes he did.", "extracted_entity": null}
{"question": "do you need to worry about Zika virus in Antarctica? ?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, you do not need to worry about Z", "extracted_entity": null}
{"question": "did a Mediterranean Sea creature kill Steve Irwin?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, a stingray did", "extracted_entity": null}
{"question": "does Linus Torvalds make money off of DirectX?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. Microsoft has not paid Linus a d", "extracted_entity": null}
{"question": "could someone theoretically use an armadillo as a shield?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in a way that would be", "extracted_entity": null}
{"question": "could a Hwasong-15 missile hypothetically reach Voyager 2?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, Voyager 2 is 1", "extracted_entity": null}
{"question": "were weather phenomena avoided when naming minor league baseball teams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "does Hades appear in a Disney Channel musical movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as a character.", "extracted_entity": null}
{"question": "would it be typical for a Rede Globo anchor to say Konnichiwa to the viewers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, that would be a little strange.", "extracted_entity": null}
{"question": "is the foot part of the metric system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the foot is a part of the metric", "extracted_entity": null}
{"question": "is week old chlorine water safe to drink?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Chlorine is a powerful disin", "extracted_entity": null}
{"question": "is Britney Spears' breakdown attributed to bipolar disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it's not. She's", "extracted_entity": null}
{"question": "could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, she could have", "extracted_entity": null}
{"question": "can you find Bob Marley's face in most smoke shops?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, you can", "extracted_entity": null}
{"question": "could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, because the magazine wasn't published during", "extracted_entity": null}
{"question": "were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "The King of Rock'n Roll's sn", "extracted_entity": null}
{"question": "at a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but Ariana Grande is a topic of", "extracted_entity": null}
{"question": "in American society, will a bachelor's degree often include a leap year?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if you are a bachelorette", "extracted_entity": null}
{"question": "is Guitar Hero Beatles inappropriate for a US third grader?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s not inappropriate", "extracted_entity": null}
{"question": "can a computer be programmed entirely in Boolean algebra?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "would Eminem perform well at the International Mathematical Olympiad?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he would not.", "extracted_entity": null}
{"question": "is the span in C-SPAN named after Alan Greenspan?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the name C-SPAN is an", "extracted_entity": null}
{"question": "did Van Gogh suffer from a mental disorder?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he suffered from epilepsy,", "extracted_entity": null}
{"question": "does a person need to be a parent to become a grandparent?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but if a person is a parent,", "extracted_entity": null}
{"question": "did either Kublai Khan or his grandfather practice monogamy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, both were polygamous.", "extracted_entity": null}
{"question": "do frogs feel disgust?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, frogs feel disgust", "extracted_entity": null}
{"question": "has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. She played Dorian Gray's (", "extracted_entity": null}
{"question": "did Native American tribes teach Spaniards how to cultivate maize?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They also taught them how to make ch", "extracted_entity": null}
{"question": "were there under 150,000 American troops in Vietnam in 1965?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there were.", "extracted_entity": null}
{"question": "do guitarist's have fingers that can handle pain better than average?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they are used to it.", "extracted_entity": null}
{"question": "could someone mistake the smell of your brussels sprouts for a fart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if they are not careful.", "extracted_entity": null}
{"question": "would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they would just be a few feet apart", "extracted_entity": null}
{"question": "was Donald Trump the target of Islamophobia?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. He was.", "extracted_entity": null}
{"question": "are there winged statuettes in the home of the creator of Law & Order?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are.", "extracted_entity": null}
{"question": "could a silverfish reach the top of the Empire State Building?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if it was on fire", "extracted_entity": null}
{"question": "did Eddie Murphy's father see his first stand up show?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, at a club called The Improv.", "extracted_entity": null}
{"question": "is watching  Star Wars necessary to know who Darth Vader is?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, you can find out who Darth V", "extracted_entity": null}
{"question": "did Eric Clapton have similar taste in women to one of the Beatles?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "is Linus Torvalds' wife unable to physically defend herself?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, she is.", "extracted_entity": null}
{"question": "did Clark Gable appear in any movies scored by John Williams?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You must get one stripe for each", "extracted_entity": null}
{"question": "does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, of course it does.", "extracted_entity": null}
{"question": "can Clouded leopards chase down many Pronghorn antelopes?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they can't.", "extracted_entity": null}
{"question": "has a neanderthal ever served on the Supreme Court of the United States?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. (But we all know a few who", "extracted_entity": null}
{"question": "will Tokyo Tower be repainted only once during President Trump's first term?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. It will be repainted once during", "extracted_entity": null}
{"question": "would George Fox support stoning?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he did.", "extracted_entity": null}
{"question": "could Oscar Wilde have operated a motor vehicle?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think not.", "extracted_entity": null}
{"question": "will Oasis cruise boat traverse the Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It is a large boat. It will", "extracted_entity": null}
{"question": "is there a full Neptunian orbit between the first two burials of women in the Panth\u00e9on?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but the second burial is the first", "extracted_entity": null}
{"question": "would an eleventh-grader be eligible for Medicare?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the answer is no.", "extracted_entity": null}
{"question": "is 3D printing able to make adenovirus?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. but it is not easy.", "extracted_entity": null}
{"question": "does rock star Keith Richards play a captain of a boat in a movie?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, he plays the captain of a boat in", "extracted_entity": null}
{"question": "will someone die without white blood cells?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, if they don\u2019t get them from", "extracted_entity": null}
{"question": "can the Powerpuff Girls form a complete tag team wrestling match?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but they can make a complete tag team", "extracted_entity": null}
{"question": "is Antarctica a good location for Groundhog Day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not.", "extracted_entity": null}
{"question": "do more anchovy live in colder temperature waters than warmer?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, more anchovies live in colder", "extracted_entity": null}
{"question": "could a giant squid fit aboard the deck of the titanic?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it could.", "extracted_entity": null}
{"question": "would a Deacon be likely to be a fan of the podcast 'God Awful Movies'?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. He would probably have better things to do", "extracted_entity": null}
{"question": "could ten gallons of seawater crush a six year old?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, if the six year old was inside a", "extracted_entity": null}
{"question": "would the high school class of 2010 have lived through the Presidency of Richard Nixon?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The class of 2010", "extracted_entity": null}
{"question": "has the Subway restaurant franchise had any connections with child abusers?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, they have", "extracted_entity": null}
{"question": "is the kayak a traditional boat in New Zealand?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, the kayak is a traditional boat", "extracted_entity": null}
{"question": "do manta rays live in water above the safe temperature for cold food storage?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. But they do have a very large mouth", "extracted_entity": null}
{"question": "could Scooby Doo fit in a kangaroo pouch?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he\u2019s too fat", "extracted_entity": null}
{"question": "would you take a photo of a Saltwater crocodile in Memphis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I would take a photo of any cro", "extracted_entity": null}
{"question": "can you write a whole Haiku in a single tweet?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think you can.", "extracted_entity": null}
{"question": "wIll Noah's Ark hypothetically sail through flooded Lincoln Tunnel?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. If the flood waters rise to the", "extracted_entity": null}
{"question": "would a pear sink in water?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would float", "extracted_entity": null}
{"question": "does Northern fur seal make good pet for six year old?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I have not personally owned one, but I have", "extracted_entity": null}
{"question": "did Millard Fillmore help to establish the University of Pittsburgh?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he did.", "extracted_entity": null}
{"question": "was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not for the reason you think.", "extracted_entity": null}
{"question": "was The Little Prince's titular character allergic to flowers?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. The Prince's allergy was to", "extracted_entity": null}
{"question": "are you likely to find a crucifix in Karachi?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. There are no Christians there.", "extracted_entity": null}
{"question": "would a hypothetical Yeti be towered over by Andre the Giant?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "probably.", "extracted_entity": null}
{"question": "are any animals in Chinese calendar Chordata?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, there are many animals in Chinese calendar Ch", "extracted_entity": null}
{"question": "does Andrew Johnson's presidential number exceed Elagabalus's Emperor number?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, Andrew Johnson's presidential number is", "extracted_entity": null}
{"question": "would Firefighters be included in a September 11th memorial?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. They are not first responders.", "extracted_entity": null}
{"question": "did J. P. Morgan have healthy lungs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, he had tuberculosis", "extracted_entity": null}
{"question": "was the original James Bond actor born near the Washington Monument?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but he was born in Scotland and was", "extracted_entity": null}
{"question": "does Ukrainian Greek Catholic Church recognize Alexander Nevsky as a saint?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, it does.", "extracted_entity": null}
{"question": "could an ocelot subsist on a single bee hummingbird per day?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it would starve.", "extracted_entity": null}
{"question": "could Lil Wayne legally operate a vehicle on his own at the beginning of his career?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. He was a minor.", "extracted_entity": null}
{"question": "is double duty an incorrect phrase for host of Dancing With The Stars?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it is not incorrect.", "extracted_entity": null}
{"question": "could Lil Wayne's children ride in a Chevrolet Corvette ZR1 together?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not if you want to keep them alive", "extracted_entity": null}
{"question": "is the best tasting part of the papaya in the center?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes. The middle of the papaya is the", "extracted_entity": null}
{"question": "did Japanese serfdom have higher status than English counterpart?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Japanese serfdom was lower than English counterpart", "extracted_entity": null}
{"question": "was the Louisiana Purchase made with bitcoin?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it was not.", "extracted_entity": null}
{"question": "would a Pict be confused by Old English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not as confused as they would be", "extracted_entity": null}
{"question": "would a week be enough time to watch every episode of Ugly Betty?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but it would be a little tedious", "extracted_entity": null}
{"question": "did any country in Portuguese Colonial War share Switzerlands role in WWII?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Portugal.", "extracted_entity": null}
{"question": "could a dandelion suffer from hepatitis?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know, but I bet it", "extracted_entity": null}
{"question": "would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Titanic would be well preserved", "extracted_entity": null}
{"question": "would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it would be impossible.", "extracted_entity": null}
{"question": "while on a liquid diet, are there some types of soup you cannot eat?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "You can eat anything you want while on a liquid", "extracted_entity": null}
{"question": "is art prioritized in the US education system?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s not.", "extracted_entity": null}
{"question": "is it okay to lie after taking an oath in a court of law?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it is not okay to lie after taking", "extracted_entity": null}
{"question": "would someone in Mumbai refer to Solanum melongena as an eggplant?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they would not.", "extracted_entity": null}
{"question": "is Menthol associated with Thanksgiving?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, because it's in the name.", "extracted_entity": null}
{"question": "can a strawberry get worms similar to dogs?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, worms can be transferred to st", "extracted_entity": null}
{"question": "is the Illuminati card game still popular?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, the Illuminati card game is still", "extracted_entity": null}
{"question": "would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, unless the cousins were gay,", "extracted_entity": null}
{"question": "if your skin was turning the color of a zombie, could it be because of nickel?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I don't know. I'm not", "extracted_entity": null}
{"question": "can the Swiss Guard fill the Virginia General Assembly chairs?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, but only for the House of Deleg", "extracted_entity": null}
{"question": "if you add water to rice pudding is it horchata?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it\u2019s still rice pudding", "extracted_entity": null}
{"question": "would an ethics professor teach a class on Cezanne?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, unless he was a Cezanne expert", "extracted_entity": null}
{"question": "is the tibia necessary to win the Stanley Cup?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, but it helps.", "extracted_entity": null}
{"question": "could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes.", "extracted_entity": null}
{"question": "is Miami a city on the American West Coast?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. It\u2019s on the East Coast.", "extracted_entity": null}
{"question": "would a broadcast from Spirit make the news in 2020?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. not even close.", "extracted_entity": null}
{"question": "would food made with black salt smell of sulfur?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, it wouldn't.", "extracted_entity": null}
{"question": "was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was a collaboration with a member of", "extracted_entity": null}
{"question": "could George Washington's own speeches have been recorded live to a compact disc?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Not only would it be impossible to record", "extracted_entity": null}
{"question": "would a geographer use biochemistry in their work?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes a geographer can use biochemistry", "extracted_entity": null}
{"question": "does Disney have an ice princess?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, Elsa", "extracted_entity": null}
{"question": "is there a warthog on Broadway?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but there are a lot of pigs", "extracted_entity": null}
{"question": "would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because he is a good person and he", "extracted_entity": null}
{"question": "could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Many people have fine motor control issues,", "extracted_entity": null}
{"question": "is Disneyland Paris the largest Disney resort?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, Disneyland Paris is the largest Disney resort", "extracted_entity": null}
{"question": "was the Donatello crucifix identified in 2020 life size?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, it was 1.25 meters", "extracted_entity": null}
{"question": "would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, because they don't have the money", "extracted_entity": null}
{"question": "would the top of Mount Fuji stick out of the Sea of Japan? ?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. The highest point of Mount Fuji is", "extracted_entity": null}
{"question": "could the Powerpuff Girls hypothetically attend the Camden Military Academy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, the Powerpuff Girls are not human", "extracted_entity": null}
{"question": "if you were on a diet, would you have to skip lunch at McDonald's?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, but I'd probably skip the f", "extracted_entity": null}
{"question": "is Dustin Hoffman one of the B'nei Yisrael?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, he is.", "extracted_entity": null}
{"question": "does actress Leila George lack the height to be a model?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, she has the height and is a model", "extracted_entity": null}
{"question": "is average number of peas in a pod enough commas for a billion?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. You need 9 commas to get", "extracted_entity": null}
{"question": "would a psychic who admits to hot reading be trustworthy?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, not at all.", "extracted_entity": null}
{"question": "is greed the most prevalent of the Seven Deadly Sins?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. I think it is.", "extracted_entity": null}
{"question": "would a student of the class of 2017 have amnesia about 9/11?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because they were born in 19", "extracted_entity": null}
{"question": "does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, it does not.", "extracted_entity": null}
{"question": "would Nancy Pelosi publicly denounce abortion?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no.", "extracted_entity": null}
{"question": "did land owners elect their rulers in the Kingdom of Hungary?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, they were chosen by the king", "extracted_entity": null}
{"question": "does Biochemistry study gluons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, gluons are not part of the", "extracted_entity": null}
{"question": "are Christmas trees dissimilar to deciduous trees?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No, they are not. They are both wo", "extracted_entity": null}
{"question": "did the Space Race use relay batons?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, but not in the way you think.", "extracted_entity": null}
{"question": "can you buy Casio products at Petco?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes, you can buy Casio products at Pet", "extracted_entity": null}
{"question": "is the language used in Saint Vincent and the Grenadines rooted in English?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, English is the official language of Saint Vincent", "extracted_entity": null}
{"question": "is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no, because the movie is not a Boeing", "extracted_entity": null}
{"question": "will the Albany in Georgia reach a hundred thousand occupants before the one in New York?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. Albany, Georgia will reach a hundred", "extracted_entity": null}
{"question": "do the anchors on Rede Globo speak Chinese?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "no. they speak Portuguese.", "extracted_entity": null}
{"question": "is shrimp scampi definitely free of plastic?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Shrimp scampi is a classic Italian", "extracted_entity": null}
{"question": "is a pound sterling valuable?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes, a pound sterling is valuable.", "extracted_entity": null}
{"question": "would a dog respond to bell before Grey seal?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "I think they would. I think they would respond", "extracted_entity": null}
{"question": "would a Monoamine Oxidase candy bar cheer up a depressed friend?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "No. Depression is caused by a chemical im", "extracted_entity": null}
{"question": "could the members of The Police perform lawful arrests?", "answer": "No", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["No", "no", "No.", "no.", "Incorrect", "incorrect", "Incorrect.", "incorrect.", "False", "false", "False.", "false.", "Negative", "negative", "Negative.", "negative."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "Yes. They were all sworn police officers.", "extracted_entity": null}
{"question": "are more people today related to Genghis Khan than Julius Caesar?", "answer": "Yes", "dataset": "strategy_qa", "split": "train", "answer_aliases": ["Yes", "yes", "Yes.", "yes.", "Correct", "correct", "Correct.", "correct.", "True", "true", "True.", "true.", "Positive", "positive", "Positive.", "positive."], "entity_annotations": null, "answer_entity_name": null, "predicted_answer": "yes", "extracted_entity": null}
